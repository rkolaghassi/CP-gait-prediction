{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: check if lines 8-11 are needed \n",
    "\n",
    "RANDOM_SEED = 6\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# torch.backends.cudnn.benchmark = False # uses deterministic convolution algorithm (may reduce performance)\n",
    "# torch.backends.cudnn.deterministic = True #\n",
    "\n",
    "#Sets the seed of RNG (GPU and CPU)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: ['Hips Flexion-Extension Left', 'Knees Flexion-Extension Left', 'Ankles Dorsiflexion-Plantarflexion Left', 'Hips Flexion-Extension Right', 'Knees Flexion-Extension Right', 'Ankles Dorsiflexion-Plantarflexion Right']\n",
      "input_window: 100\n",
      "output_window: 1\n",
      "stride: 1\n"
     ]
    }
   ],
   "source": [
    "# Set settings\n",
    "features, input_window, output_window, stride = set_settings()\n",
    "\n",
    "print(f'features: {features}')\n",
    "print(f'input_window: {input_window}')\n",
    "print(f'output_window: {output_window}')\n",
    "print(f'stride: {stride}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device used in this notebook is: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "print(f'The device used in this notebook is: {setDevice()}')\n",
    "\n",
    "DEVICE = setDevice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'D:\\\\Study 2 Data\\\\Healthy Gait\\\\Train - Copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19692/2463149729.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfile_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'D:\\Study 2 Data\\Healthy Gait\\Train - Copy'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Changes the working directory to get the data from their location\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'D:\\\\Study 2 Data\\\\Healthy Gait\\\\Train - Copy'"
     ]
    }
   ],
   "source": [
    "file_dir = r'D:\\Study 2 Data\\Healthy Gait\\Train - Copy'\n",
    "train_files = os.listdir(file_dir) \n",
    "\n",
    "# Changes the working directory to get the data from their location \n",
    "os.chdir(file_dir)\n",
    "cwd = os.getcwd()\n",
    "\n",
    "print(f'Current working directory is: {cwd}')\n",
    "print(f\"There are {len(train_files)} files in the specified path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create feature list to extract values needed from CSV files\n",
    "all_features = ['Trial', 'Time'] + features\n",
    "all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Data from CSV into Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = create_dataframe(train_files, all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.reset_index(drop=True, inplace=True) #reset the index of the table\n",
    "# path = r'D:\\Study 2 Data\\Healthy Gait' + '\\\\' + 'all_data_healthy_train.csv'\n",
    "# all_data.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = all_data['Trial'].max()\n",
    "print(f'maximum number of trials is: {n_trials}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Patient ID'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subjects = ['AB3154 BF T6-10', \n",
    "                     'AB6751 BF T1-5', \n",
    "                     'AB7779 BF T1-5',\n",
    "                     'AB9737 BF T1-5', \n",
    "                     'AB9737 BF T6-10',\n",
    "                     'AB7422 BF T1-5',\n",
    "                     ]\n",
    "\n",
    "val_subjects = ['AB9119 BF T1-5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = all_data.loc[all_data['Patient ID'].isin(train_subjects)]\n",
    "\n",
    "val_data = all_data.loc[all_data['Patient ID'].isin(val_subjects)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'all_data: {all_data.shape}')\n",
    "print(f'train_data: {train_data.shape}')\n",
    "print(f'val_data: {val_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.violinplot(x=\"Patient ID\", y=\"Hips Flexion-Extension Left\", \n",
    "                    data=all_data, palette=\"muted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = sns.violinplot(x=\"Trial\", y=\"Hips Flexion-Extension Right\", \n",
    "#                     data=all_data[all_data['Patient ID'] == \"AB3154 T6-10 BF\"], palette=\"muted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = sns.violinplot(x=\"Trial\", y=\"Hips Flexion-Extension Right\", \n",
    "#                     data=all_data[all_data['Patient ID'] == \"AB5498 BF T6-10\"], palette=\"muted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = count_nsamples(train_data)\n",
    "val_samples = count_nsamples(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of train samples: {train_samples}')\n",
    "print(f'Number of validation samples: {val_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data['Patient ID'] == \"AB5498 BF T6-10\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs('D:\\Study 2 Data\\Pre-process sample', exist_ok=True)  #check if directory exists\n",
    "# all_data.to_csv('D:\\Study 2 Data\\Pre-process sample/exported-data3.csv')  #export data to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Data into numpy array for forming windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = all_data['Patient ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pd_to_np_converter(data, n_samples, features):\n",
    "    #create a numpy array that stores the data for export\n",
    "    sample_ID = []\n",
    "    # patients = 2\n",
    "    # n_trials = 10\n",
    "    # # samples = patients * n_trials\n",
    "    data_store = np.zeros((n_samples, 2000, len(features)), dtype=np.float32)\n",
    "    i = 0\n",
    "\n",
    "    for p in data['Patient ID'].unique(): #loop over patients \n",
    "        for t in data['Trial'].unique(): #loop over trials starting with trials 1 to trial 9 (inclusive)\n",
    "            pd_array = data[(data['Patient ID'] == p) & (data['Trial'] == t)]\n",
    "            if pd_array.empty:\n",
    "                continue\n",
    "                # print('DataFrame is empty!')\n",
    "                # print(f'Trail {t} does not exist in {p}')\n",
    "            else:\n",
    "                np_array = pd_array.to_numpy()\n",
    "                data_store[i, :np_array.shape[0], :] = np_array[:,3:] \n",
    "                sample_ID.append(p+ ' Ts'+str(t)) \n",
    "                i +=1\n",
    "\n",
    "    return pd_array.columns, data_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns, train_data_np = pd_to_np_converter(train_data, train_samples, features)\n",
    "val_columns, val_data_np = pd_to_np_converter(val_data, val_samples, features)\n",
    "\n",
    "print(f'train_data_np.shape: {train_data_np.shape}')\n",
    "print(f'val_data_np.shape: {val_data_np.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns[3:].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features == train_columns[3:].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_keys = train_columns[3:].tolist() #copy the train columns removing the first column headers'Patient ID', 'Trial', 'Time'\n",
    "\n",
    "\n",
    "if features == labels_keys: # check that the features are the same as the label keys \n",
    "    print('YAY! Column headers of dataframe match features')\n",
    "else:\n",
    "    print('ERROR: Features and labels_keys do not match!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Dictionary for Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_keys = features \n",
    "\n",
    "labels_idx = np.arange(0, len(labels_keys), 1)\n",
    "\n",
    "labels = dict(zip(labels_keys, labels_idx))\n",
    "\n",
    "len(labels)\n",
    "labels\n",
    "# labels_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Data: Window Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training datasets\n",
    "# Selecting the features to be used when creating windows \n",
    "approx_seq_len = 2000 # appoximate the length of the longest sequence that can be encountered \n",
    "# samples_per_file = ((approx_seq_len - (input_window+output_window)) // stride) + 1 #number of window samples generated per file \n",
    "samples_per_file = 800\n",
    "\n",
    "\n",
    "# create a zero-filled 3D array with shape (number of samples * number of files, window size, number of features)\n",
    "X_train_windows = np.zeros((samples_per_file*train_samples, input_window, len(features)), dtype=np.float32) #size can be reduced by decreasing train_size \n",
    "Y_train_windows = np.zeros((samples_per_file*train_samples, output_window, len(features)), dtype=np.float32) \n",
    "\n",
    "\n",
    "start_idx = 0 #setting start index to equal zero \n",
    "train_sample_sum = 0\n",
    "train_excluded_samples = []\n",
    "# Create training windows \n",
    "\n",
    "#for i in tqdm(range(train_size)): #Use for including all data including outliers \n",
    "for i in range(train_samples): \n",
    "       \n",
    "    X_values, Y_values = window_generator(\n",
    "        train_data_np[i,:,:],\n",
    "        input_window=input_window, \n",
    "        output_window=output_window, \n",
    "        stride=stride, \n",
    "        features=features,\n",
    "        labels=labels\n",
    "        )\n",
    "\n",
    "    end_idx = start_idx + X_values.shape[0]\n",
    "\n",
    "    # print(f'file number [{f}] start index: {start_idx}, end index: {end_idx}, number of samples: {X_values.shape[0]}')\n",
    "\n",
    "    X_train_windows[start_idx:end_idx, :, :] = X_values\n",
    "    Y_train_windows[start_idx:end_idx, :, :] = Y_values\n",
    "\n",
    "    # print(f'number of samples copied: {X_train_data_store[start_idx:end_idx, :, :].shape[0]}')\n",
    "\n",
    "    start_idx = end_idx \n",
    "    train_sample_sum += X_values.shape[0]\n",
    "\n",
    "    # except Exception:\n",
    "    #     exception_msg(i)\n",
    "    #     train_excluded_samples.append(i)\n",
    "\n",
    "# print(f\"Completed storage of training windows samples, which contains {X_train_data_store.shape[0]} samples\")\n",
    "X_train_data = X_train_windows[:end_idx, :, :]\n",
    "Y_train_data = Y_train_windows[:end_idx, :, :]\n",
    "\n",
    "\n",
    "print(f'shape of X_train_windows: {X_train_windows.shape}')\n",
    "print(f'shape of Y_train_windows: {Y_train_windows.shape}')\n",
    "\n",
    "print(f'shape of X_train_data: {X_train_data.shape}')\n",
    "print(f'shape of Y_train_data: {Y_train_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating validation datasets\n",
    "# Selecting the features to be used when creating windows \n",
    "approx_seq_len = 2000 # appoximate the length of the longest sequence that can be encountered \n",
    "# samples_per_file = ((approx_seq_len - (input_window+output_window)) // stride) + 1 #number of window samples generated per file \n",
    "samples_per_file = 800\n",
    "\n",
    "\n",
    "# create a zero-filled 3D array with shape (number of samples * number of files, window size, number of features)\n",
    "X_val_windows = np.zeros((samples_per_file*val_samples, input_window, len(features)), dtype=np.float32) #size can be reduced by decreasing train_size \n",
    "Y_val_windows = np.zeros((samples_per_file*val_samples, output_window, len(features)), dtype=np.float32) \n",
    "\n",
    "\n",
    "start_idx = 0 #setting start index to equal zero \n",
    "val_sample_sum = 0\n",
    "val_excluded_samples = []\n",
    "# Create training windows \n",
    "\n",
    "#for i in tqdm(range(train_size)): #Use for including all data including outliers \n",
    "for i in range(val_samples): \n",
    "       \n",
    "    X_values, Y_values = window_generator(\n",
    "        val_data_np[i,:,:],\n",
    "        input_window=input_window, \n",
    "        output_window=output_window, \n",
    "        stride=stride, \n",
    "        features=features,\n",
    "        labels=labels\n",
    "        )\n",
    "\n",
    "    end_idx = start_idx + X_values.shape[0]\n",
    "\n",
    "    # print(f'file number [{f}] start index: {start_idx}, end index: {end_idx}, number of samples: {X_values.shape[0]}')\n",
    "\n",
    "    X_val_windows[start_idx:end_idx, :, :] = X_values\n",
    "    Y_val_windows[start_idx:end_idx, :, :] = Y_values\n",
    "\n",
    "    # print(f'number of samples copied: {X_train_data_store[start_idx:end_idx, :, :].shape[0]}')\n",
    "\n",
    "    start_idx = end_idx \n",
    "    val_sample_sum += X_values.shape[0]\n",
    "\n",
    "    # except Exception:\n",
    "    #     exception_msg(i)\n",
    "    #     train_excluded_samples.append(i)\n",
    "\n",
    "# print(f\"Completed storage of training windows samples, which contains {X_train_data_store.shape[0]} samples\")\n",
    "X_val_data = X_val_windows[:end_idx, :, :]\n",
    "Y_val_data = Y_val_windows[:end_idx, :, :]\n",
    "\n",
    "\n",
    "print(f'shape of X_val_windows: {X_val_windows.shape}')\n",
    "print(f'shape of Y_val_windows: {Y_val_windows.shape}')\n",
    "\n",
    "print(f'shape of X_val_data: {X_val_data.shape}')\n",
    "print(f'shape of Y_val_data: {Y_val_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of the data \n",
    "for f in range(len(features)):\n",
    "    plt.hist(Y_train_data[:,:,f].reshape(-1,1), label = features[f], bins=50, range=(Y_train_data.min(), Y_train_data.max()))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Data: Normalisation/Standarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm, scalars = normalise_fit(X_train_data)\n",
    "Y_train_norm = normalise_transform(Y_train_data, scalars)\n",
    "\n",
    "\n",
    "X_val_norm = normalise_transform(X_val_data, scalars)\n",
    "Y_val_norm = normalise_transform(Y_val_data, scalars)\n",
    "\n",
    "\n",
    "scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of the data \n",
    "for f in range(len(features)):\n",
    "    plt.hist(Y_train_norm[:,:,f].reshape(-1,1), bins=50, label = features[f])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Tensor \n",
    "# do not store on GPU (yet)\n",
    "X_train = torch.from_numpy(X_train_norm).float()\n",
    "Y_train = torch.from_numpy(Y_train_norm).float()\n",
    "\n",
    "X_val = torch.from_numpy(X_val_norm).float()\n",
    "Y_val = torch.from_numpy(Y_val_norm).float()\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'Y_train shape: {Y_train.shape}')\n",
    "\n",
    "print(f'X_val shape: {X_val.shape}')\n",
    "print(f'Y_val shape: {Y_val.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creates dataset object that gets individual samples for training/testing so that the Dataloader can generate batches\n",
    "# class gaitDataset(Dataset):\n",
    "#     def __init__(self, x, y):\n",
    "#         self.x = x \n",
    "#         self.y = y \n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.x.shape[0]\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         X_sample = self.x[index, :, :]\n",
    "#         # decoder_sample = self.y[index,:-1,:]\n",
    "#         Y_sample = self.y[index, 1:, :]\n",
    "#         return X_sample, Y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = gaitDataset(X_train, Y_train)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle = False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle = True)\n",
    "\n",
    "val_dataset = gaitDataset(X_val, Y_val) #ADJUSTED\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "print(f\"Train Dataset length: {len(train_dataset)}\")\n",
    "print(f\"Val Dataset length: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # useful resources: https://www.youtube.com/watch?v=8A6TEjG2DNw (LSTM Time Series Prediction Tutorial using PyTorch in Python | Coronavirus Daily Cases Forecasting)\n",
    "# LSTM model \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, in_seq_len, out_seq_len, output_size, device):\n",
    "        super(LSTM, self).__init__()\n",
    "        '''\n",
    "        nn.lstm: \n",
    "        input_size = number of features in input, if batch_first = TRUE (batchSize, seq_len, Hin) FEATURES\n",
    "        hidden_size = number of features in hidden state\n",
    "        num_layers\n",
    "        batch_first = if TRUE (batch, seq, Hin) Hin may mean input features\n",
    "        h_0 = (D * num_layers, batchSize, Hout)\n",
    "        c_0 = (D * num_layers, batchSize, Hcell)\n",
    "\n",
    "        output = (batchSize, seq_len, D * Hout) Hout may mean output features, if batch first = TRUE \n",
    "        h_0 = (D*num_layers, N, Hout) final state for each element in the batch \n",
    "        C_0 = (D*num_layers, N, Hcell) final cell state for each element \n",
    "\n",
    "        nn.linear:\n",
    "        input_size = (N,*, Hin) Hin = input_features (equal to hidden size therefore is arbitrarily set)\n",
    "        output_size = (N, *, Hout) Hout = output features (equal to output_size passed in to the model) \n",
    "\n",
    "        '''\n",
    "        # Pytorch documentation: \n",
    "        # >>> rnn = nn.LSTM(10, 20, 2) features, hidden_size, number of layers \n",
    "        # >>> input = torch.randn(5, 3, 10) in_seq_len, batch_size, hidden_size (batch size should not be input to the mode, inferred from the shape of the input and will be the first number if batch_first = TRUE)\n",
    "        # >>> h0 = torch.randn(2, 3, 20)\n",
    "        # >>> c0 = torch.randn(2, 3, 20)\n",
    "        # >>> output, (hn, cn) = rnn(input, (h0, c0))\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.output_size = output_size\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.device = device\n",
    "        \n",
    "        \n",
    "        # nn.LSTM(features, hidden_size, number of layers)\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first = True)\n",
    "\n",
    "        #nn.fc1\n",
    "        self.fc1 = nn.Linear(in_features=self.hidden_size, out_features=(self.output_size * self.out_seq_len))\n",
    "        # self.fc1 = nn.Linear(in_features=self.hidden_size, out_features=(self.output_size * self.out_seq_len))\n",
    "\n",
    "\n",
    "    def forward(self, input_data):\n",
    "\n",
    "        # initialise hidden and cell states after passing through each batch (this is skipped in stateless model)\n",
    "        h_0 = torch.zeros(self.num_layers, input_data.shape[0], self.hidden_size).to(self.device) # input_data.shape[0] is equal to batch size\n",
    "        c_0 = torch.zeros(self.num_layers, input_data.shape[0], self.hidden_size).to(self.device)\n",
    "\n",
    "        #propagate through LSTM\n",
    "        lstm_out, (h_out, c_out) = self.lstm(input_data, (h_0, c_0))\n",
    "        # lsmt_out.shape = (batch_size,seq_length, hidden_size)\n",
    "        # print('lstm_out[-1][-1]')\n",
    "        # print(lstm_out[-1][-1])\n",
    "\n",
    "        # print('h_out[-1][-1]')\n",
    "        # print(h_out[-1][-1])\n",
    "        # if lstm_out[-1][-1] == h_out[-1][-1]:\n",
    "        #     print(lstm_out[-1][-1])\n",
    "            \n",
    "        # print(f'lsmt_out: {lstm_out.shape}')\n",
    "        # print(f'h_out: {h_out.shape}')\n",
    "        # print(f'h_out: {h_out.shape}')\n",
    "\n",
    "        # propagate through linear layer \n",
    "        fc1_out = self.fc1(h_out[-1])\n",
    "        # fc1_out = self.fc1(lstm_out)\n",
    "        # print(f'preds.shape (before reshaping): {fc1_out.shape}')\n",
    "        \n",
    "        preds = fc1_out.reshape(input_data.shape[0], self.out_seq_len, self.output_size)\n",
    "        # print(f'preds.shape (after reshaping): {preds.shape}')\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### positional encoder: https://pytorch.org/tutorials/beginner/transformer_tutorial.html \n",
    "\n",
    "# the points is to generate a matrix, swize [max_len, 1, d_model]\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1) #shape would be [max_len, 1]\n",
    "\n",
    "        #an array with shape [d_model size /2] since we are only taking even numbers is multiplied by a constant. -math log(10000) is equal -9.2103. multiplying the array by a negative constant means all the numbers in the array are negative     \n",
    "        #get exp term for each element in the array #\n",
    "        #we divide d_model by two because we will multiply by cosine and sine so we will retain back the d_model shape in the end                                                          \n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)) \n",
    "\n",
    "        pe = torch.zeros(max_len, 1, d_model) #shape [max_len, 1 , d_model]\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe) #we do not need to store the gradients of the positional encoder since they will not change \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        #add positional encoding layer to input and apply dropout. Each timestep will be multiplied by a different positional encoder values\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        # print(f'x.shape {x.shape}')\n",
    "        # return self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: https://github.com/windofshadow/THAT/blob/1e35c0713ee7d4bc0494f46c3fa118bf406d9c58/TransCNN.py \n",
    "\n",
    "def normal_pdf(pos, mu, sigma):\n",
    "    a = pos - mu\n",
    "    log_p = -1*torch.mul(a, a)/(2*sigma) - torch.log(sigma)/2\n",
    "    return F.softmax(log_p, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: https://github.com/windofshadow/THAT/blob/1e35c0713ee7d4bc0494f46c3fa118bf406d9c58/TransCNN.py\n",
    "class Gaussian_Position(nn.Module):\n",
    "    def __init__(self, d_model, total_size, K=10):\n",
    "        super(Gaussian_Position, self).__init__()\n",
    "        #self.embedding = get_pe(d_model, K).to('cuda')\n",
    "        #self.register_buffer('pe', self.embedding)\n",
    "        self.embedding = nn.Parameter(torch.zeros([K, d_model], dtype=torch.float), requires_grad=True)\n",
    "        nn.init.xavier_uniform_(self.embedding, gain=1)\n",
    "        self.positions = torch.tensor([i for i in range(total_size)], requires_grad=False).unsqueeze(1).repeat(1, K).to('cuda')\n",
    "        s = 0.0\n",
    "        interval = total_size / K\n",
    "        mu = []\n",
    "        for _ in range(K):\n",
    "            mu.append(nn.Parameter(torch.tensor(s, dtype=torch.float), requires_grad=True))\n",
    "            s = s + interval\n",
    "        self.mu = nn.Parameter(torch.tensor(mu, dtype=torch.float).unsqueeze(0), requires_grad=True)\n",
    "        self.sigma = nn.Parameter(torch.tensor([torch.tensor([50.0], dtype=torch.float, requires_grad=True) for _ in range(K)]).unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        M = normal_pdf(self.positions, self.mu, self.sigma)\n",
    "        pos_enc = torch.matmul(M, self.embedding)\n",
    "        #print(M)\n",
    "        return x + pos_enc.unsqueeze(0).repeat(x.size(0), 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "# max_len = 5\n",
    "# d_model=4\n",
    "# position = torch.arange(max_len).unsqueeze(1) #shape would be [max_len, 1]\n",
    "\n",
    "# #an array with shape [d_model size /2] since we are only taking even numbers is multiplied by a constant. -math log(10000) is equal -9.2103. multiplying the array by a negative constant means all the numbers in the array are negative     \n",
    "# #get exp term for each element in the array #\n",
    "# #we divide d_model by two because we will multiply by cosine and sine so we will retain back the d_model shape in the end                                                          \n",
    "\n",
    "# div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)) \n",
    "\n",
    "# pe = torch.zeros(max_len, 1, d_model) #shape [max_len, 1 , d_model]\n",
    "# pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "# pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "# # self.register_buffer('pe', pe) #we do not need to store the gradients of the positional encoder since they will not change \n",
    "\n",
    "\n",
    "\n",
    "# # def forward(self, x):\n",
    "# #     \"\"\"\n",
    "# #     Args:\n",
    "# #         x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "# #     \"\"\"\n",
    "# #     x = x + self.pe[:x.size(0)]\n",
    "# #     print(f'x.shape {x.shape}')\n",
    "# #     return self.dropout(x)\n",
    "\n",
    "# pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arry = torch.zeros((5, 3, 4))\n",
    "# arry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pe[:arry.size(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arry = arry + pe[:arry.size(0)]\n",
    "# # \n",
    "# arry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e\n",
    "\n",
    "# https://github.com/Doheon/TimeSeriesForecast-Transformer/blob/main/TransformerTimeSeries.ipynb\n",
    "\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size: int,\n",
    "                    in_seq_len: int,  \n",
    "                    max_seq_len: int,\n",
    "                    out_seq_len: int,\n",
    "                    d_model: int=512, # aka d_model \n",
    "                    n_encoder_layers: int=4,\n",
    "                    n_decoder_layers: int=4,\n",
    "                    n_heads: int=8,\n",
    "                    dropout_encoder: float=0.2, \n",
    "                    dropout_decoder: float=0.2,\n",
    "                    dim_feedforward_encoder: int=2048, #2048 is default value\n",
    "                    linear_layer1_dim: int=200,\n",
    "                    dim_feedforward_decoder: int=200\n",
    "                    ): \n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # self.dec_seq_len = dec_seq_len\n",
    "\n",
    "\n",
    "        # self.in_seq_len = in_seq_len\n",
    "        # self.output_size = input_size\n",
    "        # self.out_seq_len = out_seq_len\n",
    "        # self.model_type = 'Transformer'\n",
    "\n",
    "        self.encoder_input_layer = nn.Linear( #incput to encoder is linear layer\n",
    "            in_features=input_size, \n",
    "            out_features=d_model\n",
    "            )\n",
    "     \n",
    "\n",
    "        # self.pos_encoder = PositionalEncoding(\n",
    "        #     d_model = d_model, \n",
    "        #     dropout = 0.5, \n",
    "        #     max_len = max_seq_len)\n",
    "\n",
    "        self.pos_encoder =  Gaussian_Position(\n",
    "            d_model = d_model, \n",
    "            total_size = max_seq_len)\n",
    "\n",
    "        #encoder layer is made of self attention and feedforward network (aka tiny transformer block)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=n_heads, \n",
    "            dim_feedforward=dim_feedforward_encoder, \n",
    "            dropout=dropout_encoder,\n",
    "            batch_first=True)\n",
    "\n",
    "\n",
    "        #pass the architecture of a single encoder layer designed and specify how many encoder layer put after each other\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layers,\n",
    "            num_layers=n_encoder_layers, \n",
    "            norm=None\n",
    "            )\n",
    "        \n",
    "        self.decoder_input_layer = nn.Linear(\n",
    "            in_features=input_size, \n",
    "            out_features=d_model\n",
    "            ) \n",
    "        \n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward_decoder,\n",
    "            dropout=dropout_decoder,\n",
    "            batch_first=True\n",
    "            )\n",
    "\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=decoder_layer,\n",
    "            num_layers=n_decoder_layers, \n",
    "            norm=None\n",
    "            )\n",
    "\n",
    "        self.linear_layer1 = nn.Linear(\n",
    "            in_features=3*d_model,   #\n",
    "            out_features=linear_layer1_dim\n",
    "            )\n",
    "\n",
    "        self.linear_layer2 = nn.Linear(\n",
    "            in_features=linear_layer1_dim,   #\n",
    "            out_features=out_seq_len*input_size\n",
    "            )\n",
    "\n",
    "        # self.init_weights()\n",
    "\n",
    "    # def init_weights(self) -> None:\n",
    "    #     initrange = 0.1\n",
    "    #     self.encoder_input_layer.weight.data.uniform_(-initrange, initrange)\n",
    "    #     self.linear_mapping.bias.data.zero_()\n",
    "    #     self.linear_mapping.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "\n",
    "        # self.init_weights()\n",
    "\n",
    "    # def init_weights(self) -> None:\n",
    "    #     initrange = 0.1\n",
    "    #     self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "    #     self.decoder.bias.data.zero_()\n",
    "    #     self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, \n",
    "                src: Tensor, #input sequence (to encoder)\n",
    "                tgt: Tensor, #decoder sequence (input to decoder)\n",
    "                src_mask: Tensor=None,  #input sequence mask (prevents data points from target sequence to be used)\n",
    "                tgt_mask: Tensor=None) -> Tensor:\n",
    "\n",
    "        # Pass throguh the input layer right before the encoder\n",
    "\n",
    "        # print(f'src: {src.shape}')\n",
    "        # print(f'tgt: {tgt.shape}')\n",
    "\n",
    "        \n",
    "        # print(f'encoder_input_layer output: {src.shape}')\n",
    "\n",
    "        # Pass through the positional encoding layer\n",
    "        \n",
    "        # print(f'position encoder output: {src.shape}')\n",
    "\n",
    "        src = self.encoder_input_layer(src)\n",
    "\n",
    "        \n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "        # Pass through all the stacked encoder layers in the encoder\n",
    "        src = self.encoder(src=src)\n",
    "\n",
    "        \n",
    "        # print(f'ecoder output: {src.shape}')\n",
    "\n",
    "        # # Pass decoder input through decoder input layer\n",
    "        decoder_output = self.decoder_input_layer(tgt)\n",
    "\n",
    "        # # print(f'decoder input layer output: {tgt.shape}')\n",
    "\n",
    "        # # Pass throguh decoder\n",
    "        decoder_output = self.decoder(\n",
    "            tgt=decoder_output,\n",
    "            memory=src,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=src_mask\n",
    "            )\n",
    "\n",
    "        # # print(f'decoder output: {decoder_output.shape}')\n",
    "\n",
    "        # # Pass through the linear mapping layer\n",
    "        # decoder_output= self.linear_mapping(decoder_output.flatten(start_dim=1))\n",
    "\n",
    "        linear_output = self.linear_layer1(decoder_output.flatten(start_dim=1))\n",
    "\n",
    "        linear_output=self.linear_layer2(linear_output)\n",
    "\n",
    "        preds = linear_output.reshape(src.shape[0], out_seq_len, input_size)\n",
    "\n",
    "        # print(f'decoder output after linear mapping {decoder_output.shape}')\n",
    "        # print(f'preds {preds.shape}')\n",
    "\n",
    "        return preds\n",
    "\n",
    "# def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "#     \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "#     return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(dim1: int, dim2: int, dim3: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Generates an upper-triangular matrix of -inf, with zeros on diag.\n",
    "    Modified from: \n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    Args:\n",
    "        dim1: int, batch_size * n_heads\n",
    "        dim2: int. For src and trg masking this must be target sequence length. \n",
    "        dim3: int. For src masking, this must be encoder sequence length.\n",
    "              For trg masking, this must be target sequence length \n",
    "    Return:\n",
    "        A Tensor of shape [dim1, dim2, dim3]\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones(dim1, dim2, dim3) * float('-inf'), diagonal=1)\n",
    "\n",
    "# Input length\n",
    "# enc_seq_len = 120\n",
    "\n",
    "# # Output length\n",
    "# output_sequence_length = 3\n",
    "\n",
    "# # Heads in attention layers\n",
    "# n_heads = 8\n",
    "\n",
    "# batch_size = 32\n",
    "\n",
    "# # Make src mask for decoder with size:\n",
    "# # [batch_size*n_heads, output_sequence_length, enc_seq_len]\n",
    "# src_mask = generate_square_subsequent_mask(\n",
    "#     dim1=batch_size*n_heads,\n",
    "#     dim2=output_sequence_length,\n",
    "#     dim3=enc_seq_len\n",
    "#     )\n",
    "\n",
    "# # Make tgt mask for decoder with size:\n",
    "# # [batch_size*n_heads, output_sequence_length, output_sequence_length]\n",
    "# tgt_mask = generate_square_subsequent_mask( \n",
    "#     dim1=batch_size*n_heads,\n",
    "#     dim2=output_sequence_length,\n",
    "#     dim3=output_sequence_length\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(model, train_dataloader, val_dataloader, num_epochs, learning_rate, device):\n",
    "    loss_function = nn.MSELoss(reduction='mean')\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimiser, 3.0, gamma=0.95)\n",
    "\n",
    "\n",
    "    # train_loss, val_loss = np.zeros(num_epochs), np.zeros(num_epochs)\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    # n_heads=4\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Loop over batch values \n",
    "        runningLoss_train = 0. \n",
    "        # print(f'Learning rate: {scheduler.get_last_lr()[0]}')\n",
    "\n",
    "        for idx, (batch_inputs, batch_targets) in enumerate(train_dataloader):\n",
    "            \n",
    "            # print(f'batch_inputs shape training set:{batch_inputs.shape[0]}')\n",
    "            \n",
    "            # Save batch on GPU\n",
    "            # print(f'batch_targets: {batch_targets.shape}')\n",
    "            batch_decoder_inputs = batch_inputs[:,-3:,:]\n",
    "            batch_inputs, batch_targets, batch_decoder_inputs = batch_inputs.to(device), batch_targets.to(device), batch_decoder_inputs.to(device)\n",
    "\n",
    "            # batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "            # print(f'batch inputs shape: {batch_inputs.shape}')\n",
    "\n",
    "            # src_mask = generate_square_subsequent_mask(\n",
    "            #     dim1=batch_inputs.shape[0]*n_heads,\n",
    "            #     dim2=batch_decoder_inputs.shape[1],\n",
    "            #     dim3=batch_inputs.shape[1] #encoder seq_length\n",
    "            #     ).to(device)\n",
    "            \n",
    "            # # print(f'src_mask.shape: {src_mask.shape}')\n",
    "\n",
    "            # tgt_mask = generate_square_subsequent_mask( \n",
    "            #     dim1=batch_inputs.shape[0]*n_heads,\n",
    "            #     dim2=batch_decoder_inputs.shape[1],\n",
    "            #     dim3=batch_decoder_inputs.shape[1]\n",
    "            #     ).to(device)\n",
    "            # print(f'tgt_mask.shape: {tgt_mask.shape}')\n",
    "\n",
    "            # Means we are training the model, so uses techniques such as dropout etc., otherwise model.train(model=False)\n",
    "            model.train() \n",
    "\n",
    "            #set gradients to zero\n",
    "            optimiser.zero_grad()\n",
    "            preds = model(batch_inputs, batch_decoder_inputs)\n",
    "            # print(f'shape of training predictions: {preds.shape}')\n",
    "            # print(f'shape of targets: {batch_targets.shape}')\n",
    "        \n",
    "            loss = loss_function(preds, batch_targets)\n",
    "            \n",
    "            \n",
    "\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            runningLoss_train += loss.item()\n",
    "\n",
    "        train_loss[epoch] = runningLoss_train / len(train_dataloader)\n",
    "\n",
    "        print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"Training loss: {runningLoss_train/len(train_dataloader)}\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "\n",
    "        model.eval() # means we are evaluating the model, stops process such as dropout etc. \n",
    "        runningLoss_val = 0.\n",
    "\n",
    "        with torch.no_grad(): # makes sure gradient is not stored \n",
    "            for idx, (batch_inputs, batch_targets) in enumerate(val_dataloader):\n",
    "\n",
    "            #     src_mask = generate_square_subsequent_mask(\n",
    "            #     dim1=batch_inputs.shape[0]*n_heads,\n",
    "            #     dim2=batch_decoder_inputs.shape[1],\n",
    "            #     dim3=batch_inputs.shape[1] #encoder seq_length\n",
    "            #     ).to(device)\n",
    "            \n",
    "            # # print(f'src_mask.shape: {src_mask.shape}')\n",
    "\n",
    "            #     tgt_mask = generate_square_subsequent_mask( \n",
    "            #         dim1=batch_inputs.shape[0]*n_heads,\n",
    "            #         dim2=batch_decoder_inputs.shape[1],\n",
    "            #         dim3=batch_decoder_inputs.shape[1]\n",
    "            #         ).to(device)\n",
    "\n",
    "\n",
    "            #     batch_decoder_inputs = batch_inputs[:,-50:,:]                \n",
    "                # batch_inputs, batch_targets= batch_inputs.to(device), batch_targets.to(device)\n",
    "                # batch_inputs, batch_targets, batch_decoder_inputs = batch_inputs.to(device), batch_targets.to(device), batch_decoder_inputs.to(device)\n",
    "\n",
    "                batch_decoder_inputs = batch_inputs[:,-3:,:]\n",
    "                batch_inputs, batch_targets, batch_decoder_inputs = batch_inputs.to(device), batch_targets.to(device), batch_decoder_inputs.to(device)\n",
    "\n",
    "                # src_mask = model.generate_square_subsequent_mask(batch_inputs.shape[1]).to(device)\n",
    "                # tgt_mask = model.generate_square_subsequent_mask(batch_decoder_inputs.shape[1]).to(device)\n",
    "                \n",
    "                optimiser.zero_grad() #WHY?\n",
    "                preds = model(batch_inputs, batch_decoder_inputs)\n",
    "                loss = loss_function(preds, batch_targets)\n",
    "                runningLoss_val += loss.item()\n",
    "\n",
    "        val_loss[epoch] = runningLoss_val/len(val_dataloader)\n",
    "\n",
    "        print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"Validation loss: {runningLoss_val/len(val_dataloader)}\")\n",
    "\n",
    "        # Additional information\n",
    "        EPOCH = epoch\n",
    "        PATH = r\"D:\\Study 2 Results and Models\\Study 2 Model Checkpoints\" + '\\\\' + 'model.pt'\n",
    "        LOSS_VAL = runningLoss_val/len(val_dataloader)\n",
    "        LOSS_TRAIN = runningLoss_train/len(train_dataloader)\n",
    "\n",
    "        torch.save({\n",
    "                    'epoch': EPOCH,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimiser.state_dict(),\n",
    "                    'loss_train': LOSS_TRAIN,\n",
    "                    'loss_val': LOSS_VAL,\n",
    "                    }, PATH)\n",
    "\n",
    "        # scheduler.step()\n",
    "\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_seq_len=output_window\n",
    "in_seq_len=input_window\n",
    "input_size=len(features)\n",
    "\n",
    "transformer_model = TransformerModel(\n",
    "    input_size=input_size,\n",
    "    in_seq_len = in_seq_len,  \n",
    "    max_seq_len= in_seq_len,\n",
    "    out_seq_len = out_seq_len,\n",
    "    d_model =60, # aka d_model \n",
    "    n_encoder_layers=2,\n",
    "    n_decoder_layers=2,\n",
    "    n_heads=4,\n",
    "    dropout_encoder=0.2, \n",
    "    dropout_decoder=0.2,\n",
    "    dim_feedforward_encoder=200, #2048 is default value\n",
    "    linear_layer1_dim=200,\n",
    "    dim_feedforward_decoder=200\n",
    "\n",
    "    ).to(DEVICE)\n",
    "\n",
    "num_epochs=50\n",
    "learning_rate=0.001\n",
    "\n",
    "train_history, val_history = train_transformer(transformer_model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, num_epochs=num_epochs, learning_rate=learning_rate, device=DEVICE) #ADJUSTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(transformer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output = transformer (\n",
    "    src=src, \n",
    "    tgt=trg,\n",
    "    src_mask=src_mask,\n",
    "    tgt_mask=tgt_mask\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_src_trg(\n",
    "#     sequence: torch.Tensor, \n",
    "#     enc_seq_len: int, \n",
    "#     dec_seq_len: int, \n",
    "#     target_seq_len: int\n",
    "#     ) -> Tuple[torch.tensor, torch.tensor, torch.tensor]:\n",
    "\n",
    "#     \"\"\"\n",
    "#     Generate the src (encoder input), trg (decoder input) and trg_y (the target)\n",
    "#     sequences from a sequence. \n",
    "#     Args:\n",
    "#         sequence: tensor, a 1D tensor of length n where \n",
    "#                 n = encoder input length + target sequence length  \n",
    "#         enc_seq_len: int, the desired length of the input to the transformer encoder\n",
    "#         target_seq_len: int, the desired length of the target sequence (the \n",
    "#                         one against which the model output is compared)\n",
    "#     Return: \n",
    "#         src: tensor, 1D, used as input to the transformer model\n",
    "#         trg: tensor, 1D, used as input to the transformer model\n",
    "#         trg_y: tensor, 1D, the target sequence against which the model output\n",
    "#             is compared when computing loss. \n",
    "#     \"\"\"\n",
    "#     assert len(sequence) == enc_seq_len + target_seq_len, \"Sequence length does not equal (input length + target length)\"\n",
    "\n",
    "#     # encoder input\n",
    "#     src = sequence[:enc_seq_len] \n",
    "\n",
    "#     # decoder input. As per the paper, it must have the same dimension as the \n",
    "#     # target sequence, and it must contain the last value of src, and all\n",
    "#     # values of trg_y except the last (i.e. it must be shifted right by 1)\n",
    "#     trg = sequence[enc_seq_len-1:len(sequence)-1]\n",
    "\n",
    "#     assert len(trg) == target_seq_len, \"Length of trg does not match target sequence length\"\n",
    "\n",
    "#     # The target sequence against which the model output will be compared to compute loss\n",
    "#     trg_y = sequence[-target_seq_len:]\n",
    "\n",
    "#     assert len(trg_y) == target_seq_len, \"Length of trg_y does not match target sequence length\"\n",
    "\n",
    "#     return src, trg, trg_y.squeeze(-1) # change size from [batch_size, target_seq_len, num_features] to [batch_size, target_seq_len] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import torch\n",
    "\n",
    "# # Create some dummy data - a univariate time series of random numbers\n",
    "# dummy_data = torch.rand(1000)\n",
    "\n",
    "# # Define some hyperparameters\n",
    "# enc_seq_len = 120 # length of input given to encoder\n",
    "# dec_seq_len = 12 # length of input given to decoder\n",
    "# target_seq_len = 3 # the desired length of the model forecast\n",
    "\n",
    "# # Make a sub-sequence from the dummy data\n",
    "# sub_sequence = dummy_data[0:enc_seq_len+target_seq_len]\n",
    "\n",
    "# # Get src, trg, trg_y\n",
    "# src, trg, trg_y = get_src_trg(\n",
    "#     sequence=sub_sequence,\n",
    "#     enc_seq_len=enc_seq_len,\n",
    "#     dec_seq_len=dec_seq_len,\n",
    "#     target_seq_len=target_seq_len\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_seq_len = 120 # length of input given to encoder\n",
    "# dec_seq_len = 12 # length of input given to decoder\n",
    "# target_seq_len = 3 # the desired length of the model forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ntokens = len(vocab)  # size of vocabulary\n",
    "# d_model = 512  # embedding dimension\n",
    "# d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "# nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "# nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "# dropout = 0.2  # dropout probability\n",
    "# in_seq_len=input_window\n",
    "# out_seq_len=output_window\n",
    "# output_size=len(features)\n",
    "\n",
    "\n",
    "# model = TransformerModel(d_model, nhead, d_hid, nlayers, dropout, in_seq_len, out_seq_len, output_size).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# class windowDataset(Dataset):\n",
    "#     def __init__(self, y, input_window=80, output_window=20, stride=5):\n",
    "#         #총 데이터의 개수\n",
    "#         L = y.shape[0]\n",
    "#         #stride씩 움직일 때 생기는 총 sample의 개수\n",
    "#         num_samples = (L - input_window - output_window) // stride + 1\n",
    "\n",
    "#         #input과 output\n",
    "#         X = np.zeros([input_window, num_samples])\n",
    "#         Y = np.zeros([output_window, num_samples])\n",
    "\n",
    "#         for i in np.arange(num_samples):\n",
    "#             start_x = stride*i\n",
    "#             end_x = start_x + input_window\n",
    "#             X[:,i] = y[start_x:end_x]\n",
    "\n",
    "#             start_y = stride*i + input_window\n",
    "#             end_y = start_y + output_window\n",
    "#             Y[:,i] = y[start_y:end_y]\n",
    "\n",
    "#         X = X.reshape(X.shape[0], X.shape[1], 1).transpose((1,0,2))\n",
    "#         Y = Y.reshape(Y.shape[0], Y.shape[1], 1).transpose((1,0,2))\n",
    "#         self.x = X\n",
    "#         self.y = Y\n",
    "        \n",
    "#         self.len = len(X)\n",
    "#     def __getitem__(self, i):\n",
    "#         return self.x[i], self.y[i, :-1], self.y[i,1:]\n",
    "#     def __len__(self):\n",
    "#         return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn import Transformer\n",
    "# from torch import nn\n",
    "# import torch\n",
    "# import math\n",
    "\n",
    "# class TFModel(nn.Module):\n",
    "#     def __init__(self,d_model, nhead, nhid, nlayers, dropout=0.5):\n",
    "#         super(TFModel, self).__init__()\n",
    "#         self.transformer = Transformer(d_model=d_model, nhead=nhead, dim_feedforward=nhid, num_encoder_layers=nlayers, num_decoder_layers=nlayers,dropout=dropout)\n",
    "#         self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "#         self.pos_encoder_d = PositionalEncoding(d_model, dropout)\n",
    "#         self.linear = nn.Linear(d_model, 1)\n",
    "#         self.encoder = nn.Linear(1, d_model)\n",
    "#         self.encoder_d = nn.Linear(1, d_model)\n",
    "\n",
    "#     def generate_square_subsequent_mask(self, sz):\n",
    "#         mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "#         mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "#         return mask\n",
    "\n",
    "#     def forward(self, src, tgt, srcmask, tgtmask):\n",
    "#         src = self.encoder(src)\n",
    "#         src = self.pos_encoder(src)\n",
    "\n",
    "#         tgt = self.encoder_d(tgt)\n",
    "#         tgt = self.pos_encoder_d(tgt)\n",
    "#         output = self.transformer(src.transpose(0,1), tgt.transpose(0,1), srcmask, tgtmask)\n",
    "#         output = self.linear(output)\n",
    "#         return output\n",
    "\n",
    "\n",
    "# def gen_attention_mask(x):\n",
    "#     mask = torch.eq(x, 0)\n",
    "#     return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLD VERSION\n",
    "def train_transformer(model, train_dataloader, val_dataloader, num_epochs, learning_rate, device):\n",
    "    loss_function = nn.MSELoss(reduction='mean')\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    # train_loss, val_loss = np.zeros(num_epochs), np.zeros(num_epochs)\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    n_heads=4\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Loop over batch values \n",
    "        runningLoss_train = 0. \n",
    "\n",
    "        for idx, (batch_inputs, batch_targets) in enumerate(train_dataloader):\n",
    "\n",
    "            # print(f'batch_inputs shape training set:{batch_inputs.shape[0]}')\n",
    "            \n",
    "            # Save batch on GPU\n",
    "            # print(f'batch_targets: {batch_targets.shape}')\n",
    "            batch_decoder_inputs = batch_inputs[:,-50:,:]\n",
    "            batch_inputs, batch_targets, batch_decoder_inputs = batch_inputs.to(device), batch_targets.to(device), batch_decoder_inputs.to(device)\n",
    "\n",
    "            # print(f'batch inputs shape: {batch_inputs.shape}')\n",
    "\n",
    "            src_mask = generate_square_subsequent_mask(\n",
    "                dim1=batch_inputs.shape[0]*n_heads,\n",
    "                dim2=batch_decoder_inputs.shape[1],\n",
    "                dim3=batch_inputs.shape[1] #encoder seq_length\n",
    "                ).to(device)\n",
    "            \n",
    "            # print(f'src_mask.shape: {src_mask.shape}')\n",
    "\n",
    "            tgt_mask = generate_square_subsequent_mask( \n",
    "                dim1=batch_inputs.shape[0]*n_heads,\n",
    "                dim2=batch_decoder_inputs.shape[1],\n",
    "                dim3=batch_decoder_inputs.shape[1]\n",
    "                ).to(device)\n",
    "            # print(f'tgt_mask.shape: {tgt_mask.shape}')\n",
    "\n",
    "            # Means we are training the model, so uses techniques such as dropout etc., otherwise model.train(model=False)\n",
    "            model.train() \n",
    "\n",
    "            #set gradients to zero\n",
    "            optimiser.zero_grad()\n",
    "            preds = model(batch_inputs, batch_decoder_inputs, src_mask = src_mask, \n",
    "                        tgt_mask=tgt_mask)\n",
    "            # print(f'shape of training predictions: {preds.shape}')\n",
    "            # print(f'shape of targets: {batch_targets.shape}')\n",
    "        \n",
    "            loss = loss_function(preds, batch_targets)\n",
    "            \n",
    "\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            runningLoss_train += loss.item()\n",
    "\n",
    "        train_loss[epoch] = runningLoss_train / len(train_dataloader)\n",
    "\n",
    "        print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"Training loss: {runningLoss_train/len(train_dataloader)}\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "\n",
    "        model.eval() # means we are evaluating the model, stops process such as dropout etc. \n",
    "        runningLoss_val = 0.\n",
    "\n",
    "        with torch.no_grad(): # makes sure gradient is not stored \n",
    "            for idx, (batch_inputs, batch_targets) in enumerate(val_dataloader):\n",
    "\n",
    "                src_mask = generate_square_subsequent_mask(\n",
    "                dim1=batch_inputs.shape[0]*n_heads,\n",
    "                dim2=batch_decoder_inputs.shape[1],\n",
    "                dim3=batch_inputs.shape[1] #encoder seq_length\n",
    "                ).to(device)\n",
    "            \n",
    "            # print(f'src_mask.shape: {src_mask.shape}')\n",
    "\n",
    "                tgt_mask = generate_square_subsequent_mask( \n",
    "                    dim1=batch_inputs.shape[0]*n_heads,\n",
    "                    dim2=batch_decoder_inputs.shape[1],\n",
    "                    dim3=batch_decoder_inputs.shape[1]\n",
    "                    ).to(device)\n",
    "\n",
    "\n",
    "                batch_decoder_inputs = batch_inputs[:,-50:,:]                \n",
    "                batch_inputs, batch_targets, batch_decoder_inputs = batch_inputs.to(device), batch_targets.to(device), batch_decoder_inputs.to(device)\n",
    "                # src_mask = model.generate_square_subsequent_mask(batch_inputs.shape[1]).to(device)\n",
    "                # tgt_mask = model.generate_square_subsequent_mask(batch_decoder_inputs.shape[1]).to(device)\n",
    "                \n",
    "                optimiser.zero_grad() #WHY?\n",
    "                preds = model(batch_inputs, batch_decoder_inputs, src_mask = src_mask, \n",
    "                        tgt_mask=tgt_mask)\n",
    "                loss = loss_function(preds, batch_targets)\n",
    "                runningLoss_val += loss.item()\n",
    "\n",
    "        val_loss[epoch] = runningLoss_val/len(val_dataloader)\n",
    "\n",
    "        print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"Validation loss: {runningLoss_val/len(val_dataloader)}\")\n",
    "\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transformer(model, dataloader, device):\n",
    "    loss_function = nn.MSELoss(reduction='mean')\n",
    "    model.eval()\n",
    "    actual_output, pred_output = [], []\n",
    "    running_loss = 0. \n",
    "    n_heads =4\n",
    "    with torch.no_grad():\n",
    "        for idx, (batch_inputs, batch_targets) in tqdm(enumerate(dataloader)):\n",
    "\n",
    "            # batch_decoder_inputs = batch_inputs[:,-50:,:]\n",
    "            batch_decoder_inputs = batch_inputs[:,-3:,:]\n",
    "            batch_inputs, batch_targets, batch_decoder_inputs = batch_inputs.to(device), batch_targets.to(device), batch_decoder_inputs.to(device)\n",
    "            # src_mask = generate_square_subsequent_mask(\n",
    "            #         dim1=batch_inputs.shape[0]*n_heads,\n",
    "            #         dim2=batch_decoder_inputs.shape[1],\n",
    "            #         dim3=batch_inputs.shape[1] #encoder seq_length\n",
    "            #         ).to(device)\n",
    "                \n",
    "            # # print(f'src_mask.shape: {src_mask.shape}')\n",
    "\n",
    "            # tgt_mask = generate_square_subsequent_mask( \n",
    "            #         dim1=batch_inputs.shape[0]*n_heads,\n",
    "            #         dim2=batch_decoder_inputs.shape[1],\n",
    "            #         dim3=batch_decoder_inputs.shape[1]\n",
    "            #         ).to(device)\n",
    "\n",
    "            # if idx==0:\n",
    "            #     batch_preds = model(batch_inputs)\n",
    "            #     # print(f'batch shape: {batch_preds.shape}')\n",
    "            #     loss = loss_function(batch_preds, batch_targets)\n",
    "            #     running_loss += loss.item()\n",
    "            #     current_preds = batch_preds\n",
    "            #     all_preds = batch_preds\n",
    "\n",
    "            # else:\n",
    "            #     batch_preds = model(batch_inputs)\n",
    "            #     print(f'batch shape: {batch_preds.shape}')\n",
    "            #     loss = loss_function(batch_preds, batch_targets)\n",
    "            #     running_loss += loss.item()\n",
    "            #     all_preds = torch.cat((current_preds, batch_preds), dim=0)\n",
    "            #     current_preds = batch_preds\n",
    "\n",
    "            batch_preds = model(batch_inputs, batch_decoder_inputs)\n",
    "                        \n",
    "            loss = loss_function(batch_preds, batch_targets)\n",
    "            running_loss += loss.item()\n",
    "            actual_output.append(batch_targets)\n",
    "            pred_output.append(batch_preds)\n",
    "\n",
    "\n",
    "            #             lst = []\n",
    "            # print(f'{x.size()}')\n",
    "            # for i in range(10):\n",
    "            #     x += i  # say we do something with x at iteration i\n",
    "            #     lst.append(x)\n",
    "            # # lstt = torch.stack([x for _ in range(10)])\n",
    "            # lstt = torch.stack(lst)\n",
    "            # print(lstt.size())\n",
    "\n",
    "        total_loss = running_loss / len(dataloader)\n",
    "\n",
    "        actual_output_tensor = torch.vstack(actual_output)\n",
    "        pred_output_tensor = torch.vstack(pred_output)\n",
    "    \n",
    "    return pred_output_tensor, actual_output_tensor, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch = 2000\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# model.train()\n",
    "# progress = tqdm(range(epoch))\n",
    "# for i in progress:\n",
    "#     batchloss = 0.0\n",
    "    \n",
    "#     for (inputs, dec_inputs, outputs) in DataLoader:\n",
    "#         optimizer.zero_grad()\n",
    "#         src_mask = model.generate_square_subsequent_mask(inputs.shape[1]).to(device)\n",
    "#         tgt_mask = model.generate_square_subsequent_mask(dec_inputs.shape[1]).to(device)\n",
    "\n",
    "#         result = model(inputs.float().to(device), dec_inputs.float().to(device), src_mask, tgt_mask)\n",
    "#         loss = criterion(result.permute(1,0,2), outputs.float().to(device))\n",
    "        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         batchloss += loss\n",
    "#     progress.set_description(\"{:0.5f}\".format(batchloss.cpu().item() / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # Train LSTM \n",
    "# input_size=len(features)\n",
    "# hidden_size=128\n",
    "# num_layers=4\n",
    "# in_seq_len=input_window\n",
    "# out_seq_len=output_window\n",
    "# output_size=len(features)\n",
    "# num_epochs=4\n",
    "# learning_rate= 0.001\n",
    "\n",
    "\n",
    "# model_LSTM = LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, in_seq_len=in_seq_len, out_seq_len=out_seq_len, output_size=output_size, device=DEVICE).to(DEVICE)\n",
    "\n",
    "# train_history, val_history = train_LSTM(model_LSTM, train_dataloader=train_dataloader, val_dataloader=val_dataloader, num_epochs=num_epochs, learning_rate=learning_rate, device=DEVICE) #ADJUSTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_history[2:], label = 'Training loss')\n",
    "plt.plot(val_history[2:], label = 'Val loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training and Validation MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_output, actual_val_output, val_loss = test_transformer(transformer_model, val_dataloader, DEVICE)\n",
    "\n",
    "predicted_values_val = pred_val_output.cpu().numpy()\n",
    "actual_values_val = actual_val_output.cpu().numpy()\n",
    "\n",
    "print(f'val loss: {val_loss}')\n",
    "print(f'Shape of predicted values test: {predicted_values_val.shape}')\n",
    "print(f'shape of actual values test: {actual_values_val.shape}')\n",
    "\n",
    "predicted_val_denorm = denormalise(predicted_values_val, scalars)\n",
    "actual_val_denorm = denormalise(actual_values_val, scalars)\n",
    "\n",
    "print(f'Shape of predicted values test post denormalisation: {predicted_val_denorm.shape}')\n",
    "print(f'shape of actual values test post denormalisation: {actual_val_denorm.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mse_loss, val_mse_std = mse_loss(predicted_val_denorm, actual_val_denorm, reduction='mean', format='np')\n",
    "val_mae_loss, val_mae_std = mae_loss(predicted_val_denorm, actual_val_denorm, reduction='mean', format='np')\n",
    "\n",
    "print(f'Val MSE Loss: {val_mse_loss}')\n",
    "print(f'Val MSE std: {val_mse_std}')\n",
    "print(f'Val MAE Loss: {val_mae_loss}')\n",
    "print(f'Val MAE std: {val_mae_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10000 ** (2*1/512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark against naive method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## benchmark against naive output (making prediction only last value)\n",
    "naive_1_train_output = np.zeros(Y_train_data.shape)\n",
    "\n",
    "for f in range(len(features)):\n",
    "    for s in range(Y_train_data.shape[0]):\n",
    "        naive_1_train_output[s,:,f] = np.ones((output_window)) * X_train_data[s,-1,f]\n",
    "\n",
    "\n",
    "\n",
    "# print(naive_1_test_output.shape)\n",
    "\n",
    "\n",
    "# naive_1_test_output[1,:,1]\n",
    "\n",
    "# X_test_data[1,:,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_1_mse_loss, naive_1_mse_std = mse_loss(naive_1_train_output, Y_train_data, reduction='mean', format='np')\n",
    "naive_1_mae_loss, naive_1_mae_std = mae_loss(naive_1_train_output, Y_train_data, reduction='mean', format='np')\n",
    "\n",
    "print(f'naive_1 MSE Loss: {naive_1_mse_loss}')\n",
    "print(f'naive_1 MSE std: {naive_1_mse_std} ')\n",
    "print(f'naive_1 MAE Loss: {naive_1_mae_loss}')\n",
    "print(f'naive_1 MAE std: {naive_1_mae_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## benchmark against mean output\n",
    "\n",
    "naive_average_train_output = np.zeros(Y_train_data.shape)\n",
    "\n",
    "for f in range(len(features)):\n",
    "    for s in range(Y_train_data.shape[0]):\n",
    "        naive_average_train_output[s,:,f] = np.ones((output_window)) * np.mean(X_train_data[s,:,f])\n",
    "\n",
    "\n",
    "\n",
    "print(naive_average_train_output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_average_mse_loss, naive_average_mse_std = mse_loss(naive_average_train_output, Y_train_data, reduction='mean', format='np')\n",
    "naive_average_mae_loss, naive_average_mae_std = mae_loss(naive_average_train_output, Y_train_data, reduction='mean', format='np')\n",
    "\n",
    "print(f'average MSE Loss: {naive_average_mse_loss}')\n",
    "print(f'average MSE std: {naive_average_mse_std} ')\n",
    "print(f'average MAE Loss: {naive_average_mae_loss}')\n",
    "print(f'average MAE std: {naive_average_mae_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Progress and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'Trasformer'\n",
    "exp_ID = '016'\n",
    "\n",
    "out_fname = r'D:\\Study 2 Results and Models\\Investigation Results' + '\\\\' + 'Study 2 Experimentation Results' + '.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(out_fname):\n",
    "#     print('ERROR: File with this name already exists, try alternative name.')\n",
    "    \n",
    "# else:\n",
    "with open(out_fname, 'a+') as text_file:\n",
    "    \n",
    "    print('Experiment Report', file=text_file)\n",
    "    print(f'{exp_ID}: {date.today()}',file=text_file)\n",
    "    # print('\\n', file=text_file)\n",
    "    print(f'model: {model}', file=text_file)\n",
    "    print(f'features: {features} \\n', file=text_file)\n",
    "\n",
    "    print('Information on Data', file=text_file)\n",
    "    print(f'train size: {train_sample_sum}', file=text_file)\n",
    "    print(f'validation size: {val_sample_sum}', file=text_file)\n",
    "    print(f'Train subjects: {train_subjects}', file=text_file)\n",
    "    print(f'Val subjects: {val_subjects}', file=text_file)\n",
    "    print(f'Scalars: {scalars} \\n', file=text_file)\n",
    "\n",
    "    print('Model Hyperparameters', file=text_file)\n",
    "    print(f'input window size: {input_window}', file=text_file)\n",
    "    print(f'output window size: {output_window}', file=text_file)\n",
    "    print(f'stride: {stride}', file=text_file)\n",
    "    print(f'epochs: {num_epochs}', file=text_file)\n",
    "    # print(f'hidden size: {hidden_size}', file=text_file)\n",
    "    # print(f'number of layers: {num_layers}', file=text_file)\n",
    "    print(f'learning rate: {learning_rate}', file=text_file)\n",
    "    print(f'optimiser: MSE', file=text_file)\n",
    "\n",
    "    print('\\n', file = text_file)\n",
    "    print(f'Model Architecture', file=text_file)\n",
    "    print(transformer_model, file=text_file)\n",
    "\n",
    "    print(f'Losses during training:', file=text_file)\n",
    "    print(f'training loss: {train_history}', file=text_file)\n",
    "    print(f'validation history: {val_history}', file=text_file)\n",
    "\n",
    "    print('\\n', file=text_file)\n",
    "    print(f'Denomralised MSE and MAE: ', file=text_file)\n",
    "    print(f'Validation MSE and std(degrees): {val_mse_loss:.3f}, {val_mse_std:.3f}', file=text_file)\n",
    "    print(f'Validation MAE and std (degrees): {val_mae_loss:.3f}, {val_mae_std:.3f}', file=text_file)\n",
    "    # print('\\n', file=text_file)\n",
    "    print('--------------------------------------------------------------------------------------------------------', file=text_file)\n",
    "\n",
    "\n",
    "print(\"Generation of text file complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_path = r'D:\\Study 2 Results and Models\\Investigations Plots' + '\\\\'  + 'Exp' + str(exp_ID) + '-' + str(date.today()) + '-' + model + '-In' + str(input_window) + '-Out' + str(output_window) + '(loss)' + '.png'\n",
    "fst_e = 2 #first epoch to start plotting for \n",
    "x = [i for i in range(fst_e,num_epochs)]\n",
    "# print(x)\n",
    "# print(train_history[20:145].shape)\n",
    "plt.plot(x,train_history[fst_e:], label = 'Training loss')\n",
    "plt.plot(x,val_history[fst_e:], label = 'Val loss')\n",
    "# plt.plot(train_history, label = 'Training loss')\n",
    "# plt.plot(val_history, label = 'Val loss')\n",
    "\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Train and Val MSE Loss')\n",
    "plt.legend()\n",
    "plt.savefig(plt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in transformer_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", transformer_model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model\n",
    "# export_path = r'C:\\Users\\Rania\\rbk9\\Study 1 Results and Models\\Study 1 Models' + '\\\\'  + 'Exp' + str(exp_ID) + '-' + str(date.today()) + '-' + model+ '-In' + str(input_window) + '-Out' + str(output_window) +'.pickle'\n",
    "export_path = r'D:\\Study 2 Results and Models\\Study 2 Model' + '\\\\'  + 'Exp' + str(exp_ID) + '-' + str(date.today()) + '-' + model+ '-In' + str(input_window) + '-Out' + str(output_window) +'.pickle'\n",
    "\n",
    "# export_path = r'C:\\Users\\Rania\\rbk9\\Study 1 Results and Models\\Study 1 Models\\LSTM_4' + '\\\\'  + 'Exp' + str(exp_ID) + '-' + str(date.today()) + '-' + model+ '-In' + str(input_window) + '-Out' + str(output_window) +'.pickle'\n",
    "\n",
    "\n",
    "export_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer_model.state_dict(), export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_path = r'D:\\Study 1 Results and Models\\Study 1 Models\\LSTM_4' + '\\\\' + 'Exp014-2022-02-10-LSTM-In120-Out3.pickle'\n",
    "# # # import_path = r'C:\\Users\\Rania\\rbk9\\Study 1 Results and Models\\Study 1 Models\\CNN_1' + '\\\\' + 'Exp001-2022-01-24-CNN-In96-Out1.pickle'\n",
    "# # # model_CNN = CNN(input_size, output_size=output_size, kernel_size=kernel_size, stride=1, in_seq_len=in_seq_len, out_seq_len=out_seq_len,  device = DEVICE).to(DEVICE)\n",
    "\n",
    "input_size=len(features)\n",
    "hidden_size=128\n",
    "num_layers=4\n",
    "in_seq_len=input_window\n",
    "out_seq_len=output_window\n",
    "output_size=len(features)\n",
    "# num_epochs=60\n",
    "\n",
    "# # learning_rate= 0.001\n",
    "\n",
    "model_LSTM = LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, in_seq_len=in_seq_len, out_seq_len=out_seq_len, output_size=output_size, device=DEVICE).to(DEVICE)\n",
    "model_LSTM.load_state_dict(torch.load(import_path))\n",
    "model_LSTM.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_output, actual_val_output, val_loss = test_LSTM(model_LSTM, val_dataloader, DEVICE)\n",
    "\n",
    "predicted_values_val = pred_val_output.cpu().numpy()\n",
    "actual_values_val = actual_val_output.cpu().numpy()\n",
    "\n",
    "print(f'val loss: {val_loss}')\n",
    "print(f'Shape of predicted values test: {predicted_values_val.shape}')\n",
    "print(f'shape of actual values test: {actual_values_val.shape}')\n",
    "\n",
    "predicted_val_denorm = denormalise(predicted_values_val, scalars)\n",
    "actual_val_denorm = denormalise(actual_values_val, scalars)\n",
    "\n",
    "print(f'Shape of predicted values test post denormalisation: {predicted_val_denorm.shape}')\n",
    "print(f'shape of actual values test post denormalisation: {actual_val_denorm.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mse_loss, val_mse_std = mse_loss(predicted_val_denorm, actual_val_denorm, reduction='mean', format='np')\n",
    "val_mae_loss, val_mae_std = mae_loss(predicted_val_denorm, actual_val_denorm, reduction='mean', format='np')\n",
    "\n",
    "print(f'Val MSE Loss: {val_mse_loss}')\n",
    "print(f'Val MSE std: {val_mse_std}')\n",
    "print(f'Val MAE Loss: {val_mae_loss}')\n",
    "print(f'Val MAE std: {val_mae_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in model_LSTM.parameters():\n",
    "#     print(p.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing function \n",
    "# def test_LSTM(model, dataloader):\n",
    "#     loss_function = nn.MSELoss(reduction='mean')\n",
    "#     model.eval()\n",
    "#     actual_output, pred_output = [], []\n",
    "#     running_loss = 0. \n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for idx, (batch_inputs, batch_targets) in tqdm(enumerate(dataloader)):\n",
    "\n",
    "#             batch_inputs, batch_targets = batch_inputs.to(DEVICE), batch_targets.to(DEVICE)\n",
    "\n",
    "#             # if idx==0:\n",
    "#             #     batch_preds = model(batch_inputs)\n",
    "#             #     # print(f'batch shape: {batch_preds.shape}')\n",
    "#             #     loss = loss_function(batch_preds, batch_targets)\n",
    "#             #     running_loss += loss.item()\n",
    "#             #     current_preds = batch_preds\n",
    "#             #     all_preds = batch_preds\n",
    "\n",
    "#             # else:\n",
    "#             #     batch_preds = model(batch_inputs)\n",
    "#             #     print(f'batch shape: {batch_preds.shape}')\n",
    "#             #     loss = loss_function(batch_preds, batch_targets)\n",
    "#             #     running_loss += loss.item()\n",
    "#             #     all_preds = torch.cat((current_preds, batch_preds), dim=0)\n",
    "#             #     current_preds = batch_preds\n",
    "\n",
    "#             batch_preds = model(batch_inputs)\n",
    "#             # print(f'batch preds: {batch_preds.type}')\n",
    "#             loss = loss_function(batch_preds, batch_targets)\n",
    "#             running_loss += loss.item()\n",
    "#             actual_output.append(batch_targets)\n",
    "#             pred_output.append(batch_preds)\n",
    "\n",
    "\n",
    "#             #             lst = []\n",
    "#             # print(f'{x.size()}')\n",
    "#             # for i in range(10):\n",
    "#             #     x += i  # say we do something with x at iteration i\n",
    "#             #     lst.append(x)\n",
    "#             # # lstt = torch.stack([x for _ in range(10)])\n",
    "#             # lstt = torch.stack(lst)\n",
    "#             # print(lstt.size())\n",
    "\n",
    "#         total_loss = running_loss / len(dataloader)\n",
    "\n",
    "#         actual_output_tensor = torch.vstack(actual_output)\n",
    "#         pred_output_tensor = torch.vstack(pred_output)\n",
    "    \n",
    "#     return pred_output_tensor, actual_output_tensor, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creates dataset object that gets individual samples for training/testing so that the Dataloader can generate batches\n",
    "# class gaitDataset(Dataset):\n",
    "#     def __init__(self, x, y):\n",
    "#         self.x = x \n",
    "#         self.y = y \n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.x.shape[0]\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         X_sample = self.x[index, :, :]\n",
    "#         Y_sample = self.y[index, :, :]\n",
    "#         return X_sample, Y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X_test_norm = normalise_transform(X_test_data, scalars)\n",
    "# # Y_test_norm = normalise_transform(Y_test_data, scalars)\n",
    "\n",
    "# # Convert to Tensor \n",
    "# # do not store on GPU (yet)\n",
    "# X_test = torch.from_numpy(X_train_norm).float()\n",
    "# Y_test = torch.from_numpy(Y_train_norm).float()\n",
    "\n",
    "# print(f'X_test shape: {X_test.shape}')\n",
    "# print(f'Y_test shape: {Y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = gaitDataset(X_test, Y_test)\n",
    "# # # test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False) #ADJUSTED\n",
    "\n",
    "# print(f\"Test Dataset length: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_test_output, actual_test_output, test_loss = test_LSTM(model_LSTM, test_dataloader)\n",
    "\n",
    "# predicted_values_test = pred_test_output.cpu().numpy() # premute again to make the order of the array (samples, windows, features)\n",
    "# actual_values_test =  actual_test_output.cpu().numpy()\n",
    "\n",
    "# print(f'test loss: {test_loss}')\n",
    "\n",
    "# print(f'Shape of predicted values test: {predicted_values_test.shape}')\n",
    "# print(f'shape of actual values test: {actual_values_test.shape}')\n",
    "\n",
    "# predicted_test_denorm = denormalise(predicted_values_test, scalars)\n",
    "# actual_test_denorm =  denormalise(actual_values_test, scalars)\n",
    "\n",
    "# print(f'Shape of predicted values test post denormalisation: {predicted_test_denorm.shape}')\n",
    "# print(f'shape of actual values test post denormalisation: {actual_test_denorm.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse_loss, test_mse_std = mse_loss(predicted_test_denorm, actual_test_denorm, reduction='mean', format='np')\n",
    "test_mae_loss, test_mae_std = mae_loss(predicted_test_denorm, actual_test_denorm, reduction='mean', format='np')\n",
    "\n",
    "print(f'Test MSE Loss: {test_mse_loss}')\n",
    "print(f'Test MSE std: {test_mse_std} ')\n",
    "\n",
    "# MDE= mae_loss(actual_test_denorm[:-1,:,:],actual_test_denorm[1:,:,:],reduction='mean', format='np')\n",
    "MDE= mae_loss(actual_test_denorm[:,:,:],actual_test_denorm[:,:,:],reduction='mean', format='np')\n",
    "\n",
    "print(f'Test MAE Loss: {test_mae_loss}')\n",
    "print(f'Test MAE std: {test_mae_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.abs(predicted_test_denorm[:,:,1].reshape(-1,1) - actual_test_denorm[:,:,1].reshape(-1,1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = np.array([23,36,28,33,44,22,12,10,1])\n",
    "predicted = np.array([23, 35.5, 28.1, 33, 45, 15, 13, 8, 0])\n",
    "\n",
    "actual-predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_values = np.abs(actual-predicted)\n",
    "\n",
    "plt.hist(abs_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.abs(actual-predicted).std()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27ff62b9f8f46ac7739d102fa0cb2cdb691dbf234c24f48db1edec61200604ab"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('study2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
