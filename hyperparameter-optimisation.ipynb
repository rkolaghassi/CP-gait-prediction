{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib import rcParams\n",
    "from utils.utils import *\n",
    "from dtw import dtw\n",
    "import optuna\n",
    "\n",
    "\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device used in this notebook is: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "print(f'The device used in this notebook is: {setDevice()}')\n",
    "\n",
    "DEVICE = setDevice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: ['Hips Flexion-Extension Left', 'Knees Flexion-Extension Left', 'Ankles Dorsiflexion-Plantarflexion Left', 'Hips Flexion-Extension Right', 'Knees Flexion-Extension Right', 'Ankles Dorsiflexion-Plantarflexion Right']\n",
      "input_window: 100\n",
      "output_window: 1\n",
      "stride: 1\n"
     ]
    }
   ],
   "source": [
    "# Set settings\n",
    "features, input_window, output_window, stride = set_settings()\n",
    "\n",
    "print(f'features: {features}')\n",
    "print(f'input_window: {input_window}')\n",
    "print(f'output_window: {output_window}')\n",
    "print(f'stride: {stride}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory is: D:\\Study 2 Data\\Healthy Gait\\NYU Train_val\n",
      "There are 3 files in the specified path.\n"
     ]
    }
   ],
   "source": [
    "file_dir = r'D:\\Study 2 Data\\Healthy Gait\\NYU Train_val' # Location of Healthy Gait Data\n",
    "\n",
    "train_files = os.listdir(file_dir) \n",
    "\n",
    "os.chdir(file_dir)\n",
    "cwd = os.getcwd()\n",
    "\n",
    "print(f'Current working directory is: {cwd}')\n",
    "print(f\"There are {len(train_files)} files in the specified path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AB2188 BF T1-5.csv', 'AB3154 BF T6-10.csv', 'AB9738 BF T1-5.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Trial',\n",
       " 'Time',\n",
       " 'Hips Flexion-Extension Left',\n",
       " 'Knees Flexion-Extension Left',\n",
       " 'Ankles Dorsiflexion-Plantarflexion Left',\n",
       " 'Hips Flexion-Extension Right',\n",
       " 'Knees Flexion-Extension Right',\n",
       " 'Ankles Dorsiflexion-Plantarflexion Right']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create feature list to extract values needed from CSV files\n",
    "all_features = ['Trial', 'Time'] + features\n",
    "all_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from: AB2188 BF T1-5.csv\n",
      "Extracting data from: AB3154 BF T6-10.csv\n",
      "Extracting data from: AB9738 BF T1-5.csv\n"
     ]
    }
   ],
   "source": [
    "all_data = create_dataframe(train_files, all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.reset_index(drop=True, inplace=True) #reset the index of the table\n",
    "# path = r'D:\\Study 2 Data\\Healthy Gait' + '\\\\' + 'all_data_healthy_train.csv'\n",
    "# all_data.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Trial</th>\n",
       "      <th>Time</th>\n",
       "      <th>Hips Flexion-Extension Left</th>\n",
       "      <th>Knees Flexion-Extension Left</th>\n",
       "      <th>Ankles Dorsiflexion-Plantarflexion Left</th>\n",
       "      <th>Hips Flexion-Extension Right</th>\n",
       "      <th>Knees Flexion-Extension Right</th>\n",
       "      <th>Ankles Dorsiflexion-Plantarflexion Right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AB2188 BF T1-5</td>\n",
       "      <td>1</td>\n",
       "      <td>223.83</td>\n",
       "      <td>24.69730</td>\n",
       "      <td>43.98116</td>\n",
       "      <td>13.16722</td>\n",
       "      <td>7.40105</td>\n",
       "      <td>19.83552</td>\n",
       "      <td>3.46198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AB2188 BF T1-5</td>\n",
       "      <td>1</td>\n",
       "      <td>223.84</td>\n",
       "      <td>24.44713</td>\n",
       "      <td>43.17039</td>\n",
       "      <td>13.07729</td>\n",
       "      <td>7.04358</td>\n",
       "      <td>19.57988</td>\n",
       "      <td>3.53245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AB2188 BF T1-5</td>\n",
       "      <td>1</td>\n",
       "      <td>223.85</td>\n",
       "      <td>24.17547</td>\n",
       "      <td>42.45852</td>\n",
       "      <td>13.01625</td>\n",
       "      <td>6.69202</td>\n",
       "      <td>19.29897</td>\n",
       "      <td>3.58736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AB2188 BF T1-5</td>\n",
       "      <td>1</td>\n",
       "      <td>223.86</td>\n",
       "      <td>23.88122</td>\n",
       "      <td>41.82635</td>\n",
       "      <td>12.98980</td>\n",
       "      <td>6.35465</td>\n",
       "      <td>19.00309</td>\n",
       "      <td>3.63057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AB2188 BF T1-5</td>\n",
       "      <td>1</td>\n",
       "      <td>223.87</td>\n",
       "      <td>23.56337</td>\n",
       "      <td>41.25273</td>\n",
       "      <td>13.00300</td>\n",
       "      <td>6.03926</td>\n",
       "      <td>18.70273</td>\n",
       "      <td>3.66602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22196</th>\n",
       "      <td>AB9738 BF T1-5</td>\n",
       "      <td>10</td>\n",
       "      <td>357.98</td>\n",
       "      <td>10.99086</td>\n",
       "      <td>7.10503</td>\n",
       "      <td>6.93556</td>\n",
       "      <td>24.15779</td>\n",
       "      <td>24.79361</td>\n",
       "      <td>-6.96295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22197</th>\n",
       "      <td>AB9738 BF T1-5</td>\n",
       "      <td>10</td>\n",
       "      <td>357.99</td>\n",
       "      <td>10.66388</td>\n",
       "      <td>6.95092</td>\n",
       "      <td>7.04563</td>\n",
       "      <td>23.36649</td>\n",
       "      <td>23.37365</td>\n",
       "      <td>-6.78526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22198</th>\n",
       "      <td>AB9738 BF T1-5</td>\n",
       "      <td>10</td>\n",
       "      <td>358.00</td>\n",
       "      <td>10.47453</td>\n",
       "      <td>6.99232</td>\n",
       "      <td>7.14855</td>\n",
       "      <td>22.59988</td>\n",
       "      <td>22.00132</td>\n",
       "      <td>-6.50384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22199</th>\n",
       "      <td>AB9738 BF T1-5</td>\n",
       "      <td>10</td>\n",
       "      <td>358.01</td>\n",
       "      <td>10.44759</td>\n",
       "      <td>7.27248</td>\n",
       "      <td>7.23479</td>\n",
       "      <td>21.89250</td>\n",
       "      <td>20.71157</td>\n",
       "      <td>-6.14128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22200</th>\n",
       "      <td>AB9738 BF T1-5</td>\n",
       "      <td>10</td>\n",
       "      <td>358.02</td>\n",
       "      <td>10.60070</td>\n",
       "      <td>7.82769</td>\n",
       "      <td>7.29123</td>\n",
       "      <td>21.27437</td>\n",
       "      <td>19.53785</td>\n",
       "      <td>-5.72671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22201 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Patient ID  Trial    Time  Hips Flexion-Extension Left  \\\n",
       "0      AB2188 BF T1-5      1  223.83                     24.69730   \n",
       "1      AB2188 BF T1-5      1  223.84                     24.44713   \n",
       "2      AB2188 BF T1-5      1  223.85                     24.17547   \n",
       "3      AB2188 BF T1-5      1  223.86                     23.88122   \n",
       "4      AB2188 BF T1-5      1  223.87                     23.56337   \n",
       "...               ...    ...     ...                          ...   \n",
       "22196  AB9738 BF T1-5     10  357.98                     10.99086   \n",
       "22197  AB9738 BF T1-5     10  357.99                     10.66388   \n",
       "22198  AB9738 BF T1-5     10  358.00                     10.47453   \n",
       "22199  AB9738 BF T1-5     10  358.01                     10.44759   \n",
       "22200  AB9738 BF T1-5     10  358.02                     10.60070   \n",
       "\n",
       "       Knees Flexion-Extension Left  Ankles Dorsiflexion-Plantarflexion Left  \\\n",
       "0                          43.98116                                 13.16722   \n",
       "1                          43.17039                                 13.07729   \n",
       "2                          42.45852                                 13.01625   \n",
       "3                          41.82635                                 12.98980   \n",
       "4                          41.25273                                 13.00300   \n",
       "...                             ...                                      ...   \n",
       "22196                       7.10503                                  6.93556   \n",
       "22197                       6.95092                                  7.04563   \n",
       "22198                       6.99232                                  7.14855   \n",
       "22199                       7.27248                                  7.23479   \n",
       "22200                       7.82769                                  7.29123   \n",
       "\n",
       "       Hips Flexion-Extension Right  Knees Flexion-Extension Right  \\\n",
       "0                           7.40105                       19.83552   \n",
       "1                           7.04358                       19.57988   \n",
       "2                           6.69202                       19.29897   \n",
       "3                           6.35465                       19.00309   \n",
       "4                           6.03926                       18.70273   \n",
       "...                             ...                            ...   \n",
       "22196                      24.15779                       24.79361   \n",
       "22197                      23.36649                       23.37365   \n",
       "22198                      22.59988                       22.00132   \n",
       "22199                      21.89250                       20.71157   \n",
       "22200                      21.27437                       19.53785   \n",
       "\n",
       "       Ankles Dorsiflexion-Plantarflexion Right  \n",
       "0                                       3.46198  \n",
       "1                                       3.53245  \n",
       "2                                       3.58736  \n",
       "3                                       3.63057  \n",
       "4                                       3.66602  \n",
       "...                                         ...  \n",
       "22196                                  -6.96295  \n",
       "22197                                  -6.78526  \n",
       "22198                                  -6.50384  \n",
       "22199                                  -6.14128  \n",
       "22200                                  -5.72671  \n",
       "\n",
       "[22201 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum number of trials is: 10\n"
     ]
    }
   ],
   "source": [
    "n_trials = all_data['Trial'].max()\n",
    "print(f'maximum number of trials is: {n_trials}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AB2188 BF T1-5', 'AB3154 BF T6-10', 'AB9738 BF T1-5'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['Patient ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide subjects into training and validation set\n",
    "train_subjects = ['AB2188 BF T1-5', \n",
    "                  'AB3154 BF T6-10',]\n",
    "\n",
    "val_subjects = ['AB9738 BF T1-5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AB9738 BF T1-5']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = all_data.loc[all_data['Patient ID'].isin(train_subjects)]\n",
    "\n",
    "val_data = all_data.loc[all_data['Patient ID'].isin(val_subjects)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_data: (22201, 9)\n",
      "train_data: (13769, 9)\n",
      "val_data: (8432, 9)\n"
     ]
    }
   ],
   "source": [
    "print(f'all_data: {all_data.shape}')\n",
    "print(f'train_data: {train_data.shape}')\n",
    "print(f'val_data: {val_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For patient: AB2188 BF T1-5, trial: 1, there are: 718 time-points\n",
      "For patient: AB2188 BF T1-5, trial: 2, there are: 801 time-points\n",
      "For patient: AB2188 BF T1-5, trial: 3, there are: 706 time-points\n",
      "For patient: AB2188 BF T1-5, trial: 4, there are: 651 time-points\n",
      "For patient: AB2188 BF T1-5, trial: 5, there are: 741 time-points\n",
      "For patient: AB2188 BF T1-5, trial: 6, there are: 691 time-points\n",
      "For patient: AB2188 BF T1-5, trial: 7, there are: 722 time-points\n",
      "For patient: AB2188 BF T1-5, trial: 8, there are: 726 time-points\n",
      "For patient: AB2188 BF T1-5, trial: 9, there are: 740 time-points\n",
      "For patient: AB2188 BF T1-5, trial: 10, there are: 766 time-points\n",
      "For patient: AB3154 BF T6-10, trial: 1, there are: 651 time-points\n",
      "For patient: AB3154 BF T6-10, trial: 2, there are: 541 time-points\n",
      "For patient: AB3154 BF T6-10, trial: 3, there are: 613 time-points\n",
      "For patient: AB3154 BF T6-10, trial: 4, there are: 601 time-points\n",
      "For patient: AB3154 BF T6-10, trial: 5, there are: 711 time-points\n",
      "For patient: AB3154 BF T6-10, trial: 6, there are: 651 time-points\n",
      "For patient: AB3154 BF T6-10, trial: 7, there are: 1031 time-points\n",
      "For patient: AB3154 BF T6-10, trial: 8, there are: 896 time-points\n",
      "For patient: AB3154 BF T6-10, trial: 9, there are: 812 time-points\n",
      "\n",
      "There are 19 samples\n",
      "For patient: AB9738 BF T1-5, trial: 1, there are: 896 time-points\n",
      "For patient: AB9738 BF T1-5, trial: 2, there are: 981 time-points\n",
      "For patient: AB9738 BF T1-5, trial: 3, there are: 1001 time-points\n",
      "For patient: AB9738 BF T1-5, trial: 4, there are: 906 time-points\n",
      "For patient: AB9738 BF T1-5, trial: 5, there are: 958 time-points\n",
      "For patient: AB9738 BF T1-5, trial: 6, there are: 951 time-points\n",
      "For patient: AB9738 BF T1-5, trial: 8, there are: 904 time-points\n",
      "For patient: AB9738 BF T1-5, trial: 9, there are: 926 time-points\n",
      "For patient: AB9738 BF T1-5, trial: 10, there are: 909 time-points\n",
      "\n",
      "There are 9 samples\n"
     ]
    }
   ],
   "source": [
    "train_samples = count_nsamples(train_data)\n",
    "val_samples = count_nsamples(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 19\n",
      "Number of validation samples: 9\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of train samples: {train_samples}')\n",
    "print(f'Number of validation samples: {val_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = all_data['Patient ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_np.shape: (19, 2000, 6)\n",
      "val_data_np.shape: (9, 2000, 6)\n"
     ]
    }
   ],
   "source": [
    "train_columns, train_data_np = pd_to_np_converter(train_data, train_samples, features)\n",
    "val_columns, val_data_np = pd_to_np_converter(val_data, val_samples, features)\n",
    "\n",
    "print(f'train_data_np.shape: {train_data_np.shape}')\n",
    "print(f'val_data_np.shape: {val_data_np.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hips Flexion-Extension Left',\n",
       " 'Knees Flexion-Extension Left',\n",
       " 'Ankles Dorsiflexion-Plantarflexion Left',\n",
       " 'Hips Flexion-Extension Right',\n",
       " 'Knees Flexion-Extension Right',\n",
       " 'Ankles Dorsiflexion-Plantarflexion Right']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_columns[3:].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features == train_columns[3:].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAY! Column headers of dataframe match features\n"
     ]
    }
   ],
   "source": [
    "labels_keys = train_columns[3:].tolist() #copy the train columns removing the first column headers 'Patient ID', 'Trial', 'Time'\n",
    "\n",
    "\n",
    "if features == labels_keys: # check that the features are the same as the label keys \n",
    "    print('Column headers of dataframe match features')\n",
    "else:\n",
    "    print('ERROR: Features and labels_keys do not match!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hips Flexion-Extension Left': 0,\n",
       " 'Knees Flexion-Extension Left': 1,\n",
       " 'Ankles Dorsiflexion-Plantarflexion Left': 2,\n",
       " 'Hips Flexion-Extension Right': 3,\n",
       " 'Knees Flexion-Extension Right': 4,\n",
       " 'Ankles Dorsiflexion-Plantarflexion Right': 5}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_keys = features \n",
    "\n",
    "labels_idx = np.arange(0, len(labels_keys), 1)\n",
    "\n",
    "labels = dict(zip(labels_keys, labels_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trimmed_seqLen: 718\n",
      "trimmed_seqLen_reduced: 418\n",
      "number of slides is: 318\n",
      "trimmed_seqLen: 801\n",
      "trimmed_seqLen_reduced: 501\n",
      "number of slides is: 401\n",
      "trimmed_seqLen: 706\n",
      "trimmed_seqLen_reduced: 406\n",
      "number of slides is: 306\n",
      "trimmed_seqLen: 651\n",
      "trimmed_seqLen_reduced: 351\n",
      "number of slides is: 251\n",
      "trimmed_seqLen: 741\n",
      "trimmed_seqLen_reduced: 441\n",
      "number of slides is: 341\n",
      "trimmed_seqLen: 691\n",
      "trimmed_seqLen_reduced: 391\n",
      "number of slides is: 291\n",
      "trimmed_seqLen: 722\n",
      "trimmed_seqLen_reduced: 422\n",
      "number of slides is: 322\n",
      "trimmed_seqLen: 726\n",
      "trimmed_seqLen_reduced: 426\n",
      "number of slides is: 326\n",
      "trimmed_seqLen: 740\n",
      "trimmed_seqLen_reduced: 440\n",
      "number of slides is: 340\n",
      "trimmed_seqLen: 766\n",
      "trimmed_seqLen_reduced: 466\n",
      "number of slides is: 366\n",
      "trimmed_seqLen: 651\n",
      "trimmed_seqLen_reduced: 351\n",
      "number of slides is: 251\n",
      "trimmed_seqLen: 541\n",
      "trimmed_seqLen_reduced: 241\n",
      "number of slides is: 141\n",
      "trimmed_seqLen: 613\n",
      "trimmed_seqLen_reduced: 313\n",
      "number of slides is: 213\n",
      "trimmed_seqLen: 601\n",
      "trimmed_seqLen_reduced: 301\n",
      "number of slides is: 201\n",
      "trimmed_seqLen: 711\n",
      "trimmed_seqLen_reduced: 411\n",
      "number of slides is: 311\n",
      "trimmed_seqLen: 651\n",
      "trimmed_seqLen_reduced: 351\n",
      "number of slides is: 251\n",
      "trimmed_seqLen: 1031\n",
      "trimmed_seqLen_reduced: 731\n",
      "number of slides is: 631\n",
      "trimmed_seqLen: 896\n",
      "trimmed_seqLen_reduced: 596\n",
      "number of slides is: 496\n",
      "trimmed_seqLen: 812\n",
      "trimmed_seqLen_reduced: 512\n",
      "number of slides is: 412\n",
      "shape of X_train_windows: (17100, 100, 6)\n",
      "shape of Y_train_windows: (17100, 1, 6)\n",
      "shape of X_train_data: (6169, 100, 6)\n",
      "shape of Y_train_data: (6169, 1, 6)\n"
     ]
    }
   ],
   "source": [
    "# Creating training datasets\n",
    "approx_seq_len = 2000 # appoximate the length of the longest sequence that can be encountered \n",
    "samples_per_file = 900\n",
    "\n",
    "\n",
    "# create a zero-filled 3D array with shape (number of samples * number of files, window size, number of features)\n",
    "X_train_windows = np.zeros((samples_per_file*train_samples, input_window, len(features)), dtype=np.float64) \n",
    "Y_train_windows = np.zeros((samples_per_file*train_samples, output_window, len(features)), dtype=np.float64) \n",
    "\n",
    "\n",
    "start_idx = 0 \n",
    "train_sample_sum = 0\n",
    "train_excluded_samples = []\n",
    "\n",
    "# Create training windows \n",
    "for i in range(train_samples): \n",
    "       \n",
    "    X_values, Y_values = window_generator_fltrd(\n",
    "        train_data_np[i,:,:],\n",
    "        input_window=input_window, \n",
    "        output_window=output_window, \n",
    "        stride=stride, \n",
    "        features=features,\n",
    "        labels=labels\n",
    "        )\n",
    "\n",
    "    end_idx = start_idx + X_values.shape[0]\n",
    "\n",
    "    # print(f'file number [{f}] start index: {start_idx}, end index: {end_idx}, number of samples: {X_values.shape[0]}')\n",
    "\n",
    "    X_train_windows[start_idx:end_idx, :, :] = X_values\n",
    "    Y_train_windows[start_idx:end_idx, :, :] = Y_values\n",
    "\n",
    "    # print(f'number of samples copied: {X_train_data_store[start_idx:end_idx, :, :].shape[0]}')\n",
    "\n",
    "    start_idx = end_idx \n",
    "    train_sample_sum += X_values.shape[0]\n",
    "\n",
    "    # except Exception:\n",
    "    #     exception_msg(i)\n",
    "    #     train_excluded_samples.append(i)\n",
    "\n",
    "# print(f\"Completed storage of training windows samples, which contains {X_train_data_store.shape[0]} samples\")\n",
    "X_train_data = X_train_windows[:end_idx, :, :].astype('float64')\n",
    "Y_train_data = Y_train_windows[:end_idx, :, :].astype('float64')\n",
    "\n",
    "\n",
    "print(f'shape of X_train_windows: {X_train_windows.shape}')\n",
    "print(f'shape of Y_train_windows: {Y_train_windows.shape}')\n",
    "\n",
    "print(f'shape of X_train_data: {X_train_data.shape}')\n",
    "print(f'shape of Y_train_data: {Y_train_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trimmed_seqLen: 896\n",
      "trimmed_seqLen_reduced: 596\n",
      "number of slides is: 496\n",
      "trimmed_seqLen: 981\n",
      "trimmed_seqLen_reduced: 681\n",
      "number of slides is: 581\n",
      "trimmed_seqLen: 1001\n",
      "trimmed_seqLen_reduced: 701\n",
      "number of slides is: 601\n",
      "trimmed_seqLen: 906\n",
      "trimmed_seqLen_reduced: 606\n",
      "number of slides is: 506\n",
      "trimmed_seqLen: 958\n",
      "trimmed_seqLen_reduced: 658\n",
      "number of slides is: 558\n",
      "trimmed_seqLen: 951\n",
      "trimmed_seqLen_reduced: 651\n",
      "number of slides is: 551\n",
      "trimmed_seqLen: 904\n",
      "trimmed_seqLen_reduced: 604\n",
      "number of slides is: 504\n",
      "trimmed_seqLen: 926\n",
      "trimmed_seqLen_reduced: 626\n",
      "number of slides is: 526\n",
      "trimmed_seqLen: 909\n",
      "trimmed_seqLen_reduced: 609\n",
      "number of slides is: 509\n",
      "shape of X_val_windows: (14400, 100, 6)\n",
      "shape of Y_val_windows: (14400, 1, 6)\n",
      "shape of X_val_data: (4832, 100, 6)\n",
      "shape of Y_val_data: (4832, 1, 6)\n"
     ]
    }
   ],
   "source": [
    "# Creating validation datasets\n",
    "approx_seq_len = 2000 # appoximate the length of the longest sequence that can be encountered \n",
    "samples_per_file = 1600\n",
    "\n",
    "\n",
    "# create a zero-filled 3D array with shape (number of samples * number of files, window size, number of features)\n",
    "X_val_windows = np.zeros((samples_per_file*val_samples, input_window, len(features)), dtype=np.float64) \n",
    "Y_val_windows = np.zeros((samples_per_file*val_samples, output_window, len(features)), dtype=np.float64) \n",
    "\n",
    "\n",
    "start_idx = 0\n",
    "val_sample_sum = 0\n",
    "val_excluded_samples = []\n",
    "\n",
    "\n",
    "for i in range(val_samples): \n",
    "       \n",
    "    X_values, Y_values = window_generator_fltrd(\n",
    "        val_data_np[i,:,:],\n",
    "        input_window=input_window, \n",
    "        output_window=output_window, \n",
    "        stride=stride, \n",
    "        features=features,\n",
    "        labels=labels\n",
    "        )\n",
    "\n",
    "    end_idx = start_idx + X_values.shape[0]\n",
    "\n",
    "    # print(f'file number [{f}] start index: {start_idx}, end index: {end_idx}, number of samples: {X_values.shape[0]}')\n",
    "\n",
    "    X_val_windows[start_idx:end_idx, :, :] = X_values\n",
    "    Y_val_windows[start_idx:end_idx, :, :] = Y_values\n",
    "\n",
    "    # print(f'number of samples copied: {X_train_data_store[start_idx:end_idx, :, :].shape[0]}')\n",
    "\n",
    "    start_idx = end_idx \n",
    "    val_sample_sum += X_values.shape[0]\n",
    "\n",
    "    # except Exception:\n",
    "    #     exception_msg(i)\n",
    "    #     train_excluded_samples.append(i)\n",
    "\n",
    "# print(f\"Completed storage of training windows samples, which contains {X_train_data_store.shape[0]} samples\")\n",
    "X_val_data = X_val_windows[:end_idx, :, :].astype('float64')\n",
    "Y_val_data = Y_val_windows[:end_idx, :, :].astype('float64')\n",
    "\n",
    "\n",
    "print(f'shape of X_val_windows: {X_val_windows.shape}')\n",
    "print(f'shape of Y_val_windows: {Y_val_windows.shape}')\n",
    "\n",
    "print(f'shape of X_val_data: {X_val_data.shape}')\n",
    "print(f'shape of Y_val_data: {Y_val_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trimmed_seqLen: 896\n",
      "trimmed_seqLen_reduced: 596\n",
      "number of slides is: 297\n",
      "trimmed_seqLen: 981\n",
      "trimmed_seqLen_reduced: 681\n",
      "number of slides is: 382\n",
      "trimmed_seqLen: 1001\n",
      "trimmed_seqLen_reduced: 701\n",
      "number of slides is: 402\n",
      "trimmed_seqLen: 906\n",
      "trimmed_seqLen_reduced: 606\n",
      "number of slides is: 307\n",
      "trimmed_seqLen: 958\n",
      "trimmed_seqLen_reduced: 658\n",
      "number of slides is: 359\n",
      "trimmed_seqLen: 951\n",
      "trimmed_seqLen_reduced: 651\n",
      "number of slides is: 352\n",
      "trimmed_seqLen: 904\n",
      "trimmed_seqLen_reduced: 604\n",
      "number of slides is: 305\n",
      "trimmed_seqLen: 926\n",
      "trimmed_seqLen_reduced: 626\n",
      "number of slides is: 327\n",
      "trimmed_seqLen: 909\n",
      "trimmed_seqLen_reduced: 609\n",
      "number of slides is: 310\n",
      "shape of X_val_lt_windows: (14400, 100, 6)\n",
      "shape of Y_val_lt_windows: (14400, 200, 6)\n",
      "shape of X_val_lt_data: (3041, 100, 6)\n",
      "shape of Y_val_lt_data: (3041, 200, 6)\n"
     ]
    }
   ],
   "source": [
    "# Creating validation datasets (long term predictions)\n",
    "approx_seq_len = 2000 # appoximate the length of the longest sequence that can be encountered \n",
    "samples_per_file = 1600\n",
    "\n",
    "future_window = 200\n",
    "\n",
    "# create a zero-filled 3D array with shape (number of samples * number of files, window size, number of features)\n",
    "X_val_lt_windows = np.zeros((samples_per_file*val_samples, input_window, len(features)), dtype=np.float64) \n",
    "Y_val_lt_windows = np.zeros((samples_per_file*val_samples, future_window, len(features)), dtype=np.float64) \n",
    "\n",
    "\n",
    "start_idx = 0 \n",
    "val_lt_sample_sum = 0\n",
    "val_lt_excluded_samples = []\n",
    "\n",
    "\n",
    "for i in range(val_samples): \n",
    "       \n",
    "    X_values, Y_values = window_generator_lt_fltrd(\n",
    "        val_data_np[i,:,:],\n",
    "        input_window=input_window, \n",
    "        future_window=future_window, \n",
    "        stride=stride, \n",
    "        features=features,\n",
    "        labels=labels\n",
    "        )\n",
    "\n",
    "    end_idx = start_idx + X_values.shape[0]\n",
    "\n",
    "    # print(f'file number [{f}] start index: {start_idx}, end index: {end_idx}, number of samples: {X_values.shape[0]}')\n",
    "\n",
    "    X_val_lt_windows[start_idx:end_idx, :, :] = X_values\n",
    "    Y_val_lt_windows[start_idx:end_idx, :, :] = Y_values\n",
    "\n",
    "    # print(f'number of samples copied: {X_train_data_store[start_idx:end_idx, :, :].shape[0]}')\n",
    "\n",
    "    start_idx = end_idx \n",
    "    val_lt_sample_sum += X_values.shape[0]\n",
    "\n",
    "    # except Exception:\n",
    "    #     exception_msg(i)\n",
    "    #     train_excluded_samples.append(i)\n",
    "\n",
    "# print(f\"Completed storage of training windows samples, which contains {X_train_data_store.shape[0]} samples\")\n",
    "X_val_lt_data = X_val_lt_windows[:end_idx, :, :].astype('float64')\n",
    "Y_val_lt_data = Y_val_lt_windows[:end_idx, :, :].astype('float64')\n",
    "\n",
    "\n",
    "print(f'shape of X_val_lt_windows: {X_val_lt_windows.shape}')\n",
    "print(f'shape of Y_val_lt_windows: {Y_val_lt_windows.shape}')\n",
    "\n",
    "print(f'shape of X_val_lt_data: {X_val_lt_data.shape}')\n",
    "print(f'shape of Y_val_lt_data: {Y_val_lt_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-30.96504974,  -9.91796017, -49.7908783 , -32.48342896,\n",
       "         -8.37281036, -42.67766953],\n",
       "       [ 54.61846924,  88.07071686,  24.52638054,  54.92926025,\n",
       "         90.81022644,  28.89576912]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_norm, scalars = normalise_fit(X_train_data)\n",
    "Y_train_norm = normalise_transform(Y_train_data, scalars)\n",
    "\n",
    "X_val_norm = normalise_transform(X_val_data, scalars)\n",
    "Y_val_norm = normalise_transform(Y_val_data, scalars)\n",
    "\n",
    "X_val_lt_norm = normalise_transform(X_val_lt_data, scalars)\n",
    "Y_val_lt_norm = normalise_transform(Y_val_lt_data, scalars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([6169, 100, 6])\n",
      "Y_train shape: torch.Size([6169, 1, 6])\n",
      "X_val shape: torch.Size([4832, 100, 6])\n",
      "Y_val shape: torch.Size([4832, 1, 6])\n",
      "X_val_lt shape: torch.Size([3041, 100, 6])\n",
      "Y_val_lt shape: torch.Size([3041, 200, 6])\n"
     ]
    }
   ],
   "source": [
    "# Convert to Tensor \n",
    "X_train = torch.from_numpy(X_train_norm).to(torch.float64)\n",
    "Y_train = torch.from_numpy(Y_train_norm).to(torch.float64)\n",
    "\n",
    "X_val = torch.from_numpy(X_val_norm).to(torch.float64)\n",
    "Y_val = torch.from_numpy(Y_val_norm).to(torch.float64)\n",
    "\n",
    "\n",
    "X_val_lt = torch.from_numpy(X_val_lt_norm).to(torch.float64)\n",
    "Y_val_lt = torch.from_numpy(Y_val_lt_norm).to(torch.float64)\n",
    "\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'Y_train shape: {Y_train.shape}')\n",
    "\n",
    "print(f'X_val shape: {X_val.shape}')\n",
    "print(f'Y_val shape: {Y_val.shape}')\n",
    "\n",
    "print(f'X_val_lt shape: {X_val_lt.shape}') #long term predictions\n",
    "print(f'Y_val_lt shape: {Y_val_lt.shape}') #long term predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape of X_train: {X_train.shape}, shape of Y_train: {Y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = params['hidden_size']\n",
    "        self.num_layers = params['num_layers']\n",
    "        self.in_seq_len = 100\n",
    "        self.output_size = len(features)\n",
    "        self.out_seq_len = output_window\n",
    "        self.device = DEVICE\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first = True)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=self.hidden_size, out_features=(self.output_size * self.out_seq_len))\n",
    "\n",
    "    def forward(self, input_data):\n",
    "\n",
    "        h_0 = torch.zeros(self.num_layers, input_data.shape[0], self.hidden_size).double().to(self.device) \n",
    "        c_0 = torch.zeros(self.num_layers, input_data.shape[0], self.hidden_size).double().to(self.device)\n",
    "\n",
    "        lstm_out, (h_out, c_out) = self.lstm(input_data, (h_0, c_0))\n",
    "        fc1_out = self.fc1(h_out[-1])\n",
    "       \n",
    "        preds = fc1_out.reshape(input_data.shape[0], self.out_seq_len, self.output_size)\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = len(features)\n",
    "        self.output_size = len(features)\n",
    "        self.in_seq_len = 100\n",
    "        self.out_seq_len = 1\n",
    "        self.device = DEVICE \n",
    "        self.layers = params['layers']\n",
    "        self.num_units = params['num_units']\n",
    "        \n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "\n",
    "        linear_layers_lst = [] #linear layer list\n",
    " \n",
    "        for i in range(self.layers):\n",
    "            linear_layers_lst.append(nn.Linear(self.num_units, self.num_units))\n",
    "            linear_layers_lst.append(nn.ReLU())\n",
    "\n",
    "       \n",
    "        linear_layers = tuple((linear_layers_lst))\n",
    "\n",
    "        self.MLP = nn.Sequential(nn.Linear(in_features=self.input_size*self.in_seq_len, out_features=self.num_units), \n",
    "                                nn.ReLU(), \n",
    "                                *linear_layers, \n",
    "                                nn.Linear(self.num_units, self.output_size*self.out_seq_len))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.flatten(x)\n",
    "        out = self.MLP(out)\n",
    "        out = self.sigmoid(out)\n",
    "        preds = out.reshape(x.shape[0],  self.out_seq_len,self.output_size)\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.input_size = len(features)\n",
    "        self.output_size = len(features)\n",
    "        # self.kernel_size = kernel_size\n",
    "        # self.stride = stride\n",
    "        self.device = DEVICE \n",
    "        self.in_seq_len = input_window\n",
    "        self.out_seq_len = output_window\n",
    "        self.conv1_channels = params['conv1_channels']\n",
    "        self.conv2_channels = params['conv2_channels']\n",
    "        self.conv3_channels = params['conv3_channels']\n",
    "        self.conv4_channels = params['conv4_channels']\n",
    "        self.kernel_12 = params['kernel_12']\n",
    "        self.kernel_34 = params['kernel_34']\n",
    "        self.padding = params['padding']\n",
    "        self.stride = params['stride']\n",
    "        self.dilation = params['dilation']\n",
    "\n",
    "        def calc_shape(input, kernel_size, stride, padding=0, dilation =1, operation = 'conv'):\n",
    "            if operation == 'conv':\n",
    "                size = ((input + 2 * padding - dilation * (kernel_size-1) -1)/stride )+ 1\n",
    "\n",
    "            if operation == 'pool':\n",
    "                size = ((input - kernel_size)/stride)  + 1\n",
    "\n",
    "            return size\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels = self.input_size, out_channels = self.conv1_channels, kernel_size = self.kernel_12, stride = self.stride, padding = self.padding, dilation=self.dilation)\n",
    "        post_conv1 = calc_shape(input = self.in_seq_len, kernel_size = self.kernel_12, stride = self.stride, padding = self.padding,  dilation=self.dilation, operation='conv')\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels = self.conv1_channels, out_channels = self.conv2_channels, kernel_size = self.kernel_12 , stride = self.stride, padding = self.padding,  dilation=self.dilation)\n",
    "        post_conv2 = calc_shape(input = post_conv1, kernel_size =self.kernel_12 , stride = self.stride, padding = self.padding,  dilation=self.dilation,  operation='conv')\n",
    "\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size = 2, stride = 2)\n",
    "        post_pool1 = calc_shape(input = post_conv2, kernel_size = 2, stride = 2,  operation='pool')\n",
    "\n",
    "        self.conv3 = nn.Conv1d(in_channels=self.conv2_channels, out_channels=self.conv3_channels, kernel_size = self.kernel_34, stride = self.stride, padding = self.padding,  dilation=self.dilation )\n",
    "        post_conv3 = calc_shape(input = post_pool1, kernel_size = self.kernel_34, stride = self.stride, padding = self.padding, dilation=self.dilation, operation='conv')\n",
    "\n",
    "        self.conv4 = nn.Conv1d(in_channels=self.conv3_channels, out_channels=self.conv4_channels, kernel_size = self.kernel_34, stride = self.stride, padding = self.padding,  dilation=self.dilation)\n",
    "        post_conv4 = calc_shape(input = post_conv3, kernel_size = self.kernel_34, stride = self.stride, padding = self.padding,  dilation=self.dilation, operation='conv')\n",
    "\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size = 2, stride = 2)\n",
    "        post_pool2 = calc_shape(input = post_conv4, kernel_size = 2, stride = 2,  operation='pool')\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features = (int(post_pool2) * self.conv4_channels), out_features=(self.output_size * self.out_seq_len))\n",
    "\n",
    "\n",
    "    def forward(self, input_data):\n",
    "\n",
    "        input = input_data.permute(0,2,1)\n",
    "        out = self.conv1(input)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool1(out)\n",
    "        out = self.conv3(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv4(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool2(out)\n",
    "        out = torch.flatten(out, start_dim = 1, end_dim=-1) #do not flatten batches \n",
    "        out = self.fc1(out)\n",
    "        preds = out.reshape(input_data.shape[0],  self.out_seq_len, self.output_size)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = 'Exp033_220916_LSTM_batch'\n",
    "# folder_path = r'D:\\Study 2 Results and Models\\Study 2 Experiments' + '\\\\' + fname \n",
    "# path = r'D:\\Study 2 Results and Models\\Study 2 Experiments'+ '\\\\' + fname + '\\\\'  + fname + '.pth'\n",
    "\n",
    "# if not os.path.exists(folder_path):\n",
    "#     os.makedirs(folder_path)\n",
    "# else:\n",
    "#     print('File already exists, choose a different name')\n",
    "#     raise \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global trial_number\n",
    "trial_number = 0\n",
    "trial_descrbn = 'CNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = gaitDataset(X_train, Y_train)\n",
    "val_dataset = gaitDataset(X_val, Y_val)\n",
    "val_lt_dataset = gaitDataset(X_val_lt, Y_val_lt) #long term predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, val_lt_dataset, params):\n",
    "    global trial_number\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle = True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    val_lt_dataloader = DataLoader(val_lt_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "\n",
    "    \n",
    "    MODEL_PATH= r'D:\\Study 2 Results and Models\\Study 2 Optimisation\\CNN\\2022_10_05 MLP Optimisation' + '\\\\'  + str(trial_descrbn) + '-' + 'trial' + str(trial_number) + '.pt'\n",
    "\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    lt_loss = []\n",
    "    dtw_loss = []\n",
    "\n",
    "    lowest_dtw = 1000.\n",
    "\n",
    "    for epoch in tqdm(range(40)):\n",
    "        train_loss_total = 0.\n",
    "        i = 0\n",
    "        model.train()\n",
    "        for idx, (batch_inputs, batch_targets) in enumerate(train_dataloader):\n",
    "            i += 1\n",
    "            model.zero_grad()\n",
    "\n",
    "            batch_inputs, batch_targets = batch_inputs.to(DEVICE), batch_targets.to(DEVICE)\n",
    "            \n",
    "            predictions = model(batch_inputs)\n",
    "\n",
    "            loss = loss_function(predictions, batch_targets)\n",
    "\n",
    "            train_loss_total += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss_total = train_loss_total/len(train_dataloader)\n",
    "        train_loss.append(train_loss_total)\n",
    "\n",
    "        # Calculate validation loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_total = 0.\n",
    "            for idx, (batch_inputs, batch_targets) in enumerate(val_dataloader):\n",
    "                \n",
    "\n",
    "                batch_inputs, batch_targets = batch_inputs.to(DEVICE), batch_targets.to(DEVICE)\n",
    "                predictions = model(batch_inputs)\n",
    "                loss = loss_function(predictions, batch_targets)\n",
    "                val_loss_total += loss.item()\n",
    "        \n",
    "        val_loss_total = val_loss_total/len(val_dataloader)\n",
    "        val_loss.append(val_loss_total)\n",
    "\n",
    "        # Calculating long term loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            lt_loss_total = 0.\n",
    "            dtw_total = 0.\n",
    "\n",
    "            for idx, (batch_inputs, batch_targets) in enumerate(val_lt_dataloader):\n",
    "\n",
    "                extrapolation = torch.zeros((batch_inputs.shape[0], 200, len(features))).to(DEVICE)\n",
    "\n",
    "                batch_inputs, batch_targets = batch_inputs.to(DEVICE), batch_targets.to(DEVICE)\n",
    "                current_inputs = batch_inputs\n",
    "                for i in range(200):\n",
    "                    predictions = model(current_inputs)\n",
    "\n",
    "                    extrapolation[:,i,:] = predictions.squeeze(1)\n",
    "\n",
    "                    current_inputs = torch.cat((current_inputs[:,1:,:], predictions), axis=1)\n",
    "\n",
    "                loss = loss_function(extrapolation, batch_targets)\n",
    "        \n",
    "                lt_loss_total += loss.item()\n",
    "\n",
    "                #dtw\n",
    "                running_dtw = 0.\n",
    "                for s in range(batch_targets.shape[0]):\n",
    "                    for f in range(len(features)):\n",
    "                        dis = dtw(extrapolation[s,:,f].squeeze().cpu(),batch_targets[s,:,f].squeeze().cpu(), distance_only=True).distance\n",
    "                        \n",
    "                        running_dtw += dis\n",
    "\n",
    "                dtw_total += running_dtw / (batch_targets.shape[0] * len(features))\n",
    "\n",
    "        lt_loss_total = lt_loss_total/len(val_lt_dataloader)\n",
    "        lt_loss.append(lt_loss_total)\n",
    "\n",
    "        dtw_total = dtw_total / (len(val_lt_dataloader))\n",
    "        dtw_loss.append(dtw_total)\n",
    "\n",
    "        if dtw_total < lowest_dtw:\n",
    "            lowest_dtw = dtw_total\n",
    "\n",
    "            checkpoint = {'model': model,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'epoch': epoch+1,\n",
    "                    'optimizer' : optimizer.state_dict(),\n",
    "                    'val_loss': val_loss_total, \n",
    "                    'dtw_distance': dtw_total,\n",
    "                    }\n",
    "\n",
    "            torch.save(checkpoint, MODEL_PATH)\n",
    "    \n",
    "    trial_number+=1\n",
    "    return lowest_dtw\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Hyperparameter Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "def objective(trial):\n",
    "\n",
    "    params = {\n",
    "            'learning_rate': trial.suggest_categorical('learning_rate', [0.1, 0.01, 0.001, 0.0001, 0.00001]),\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [32,64,128,256,512]),   \n",
    "            'conv1_channels': trial.suggest_categorical('conv1_channels', [16, 32, 48]),\n",
    "            'conv2_channels': trial.suggest_categorical('conv2_channels', [32, 48, 64]),\n",
    "            'conv3_channels': trial.suggest_categorical('conv3_channels', [64, 128, 256]),\n",
    "            'conv4_channels': trial.suggest_categorical('conv4_channels', [128, 256, 512]),\n",
    "            'kernel_12': trial.suggest_int('kernel_12', 1, 5),\n",
    "            'kernel_34': trial.suggest_int('kernel_34', 1, 5),\n",
    "            'padding':trial.suggest_int('padding', 0, 4),\n",
    "            'stride':trial.suggest_int('stride', 1,4),\n",
    "            'dilation': trial.suggest_categorical('dilation', [1,2,4])\n",
    "            }\n",
    "\n",
    "\n",
    "    for t in trial.study.trials:\n",
    "        # print(t)\n",
    "        if t.state != optuna.structs.TrialState.COMPLETE:\n",
    "            continue\n",
    "\n",
    "        if t.params == trial.params:\n",
    "            global trial_number\n",
    "            trial_number += 1\n",
    "            return t.value\n",
    "            \n",
    "    try:\n",
    "        model = CNN(params).double().to(DEVICE)\n",
    "        \n",
    "        dtw_distance = train_model(model, train_dataset, val_dataset, val_lt_dataset, params)\n",
    "    \n",
    "    except Exception:\n",
    "        dtw_distance=1000000\n",
    "\n",
    "    return dtw_distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM\n",
    "def objective(trial):\n",
    "\n",
    "    params = {\n",
    "              'learning_rate': trial.suggest_categorical('learning_rate', [0.1, 0.01, 0.001, 0.0001, 0.00001]),\n",
    "              'num_layers': trial.suggest_categorical('num_layers', [1,2,3,4]),\n",
    "              'hidden_size': trial.suggest_categorical('hidden_size', [4,8,16,32,64]), \n",
    "              'batch_size': trial.suggest_categorical('batch_size', [32,64,128,256,512])\n",
    "              }\n",
    "\n",
    "    for t in trial.study.trials:\n",
    "        if t.state != optuna.structs.TrialState.COMPLETE:\n",
    "            continue\n",
    "\n",
    "        if t.params == trial.params:\n",
    "            global trial_number\n",
    "            trial_number += 1\n",
    "            return t.value\n",
    "            \n",
    "    \n",
    "    model = LSTM(params).double().to(DEVICE)\n",
    "    \n",
    "    dtw_distance = train_model(model, train_dataset, val_dataset, val_lt_dataset, params)\n",
    "\n",
    "    return dtw_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FCN\n",
    "def objective(trial):\n",
    "\n",
    "    params = {           \n",
    "            'learning_rate': trial.suggest_categorical('learning_rate', [0.1, 0.01, 0.001, 0.0001, 0.00001]),\n",
    "            'layers': trial.suggest_categorical('layers', [1,2,3,4,6,8,10]),\n",
    "            'num_units': trial.suggest_categorical('num_units', [10,30,50,70,100,130,150,200]),\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [32,64,128,256,512])           \n",
    "            }\n",
    "\n",
    "    for t in trial.study.trials:\n",
    "        if t.state != optuna.structs.TrialState.COMPLETE:\n",
    "            continue\n",
    "\n",
    "        if t.params == trial.params:\n",
    "            global trial_number\n",
    "            trial_number += 1\n",
    "            return t.value\n",
    "            \n",
    "    \n",
    "    model = MLP(params).double().to(DEVICE)\n",
    "    \n",
    "    dtw_distance = train_model(model, train_dataset, val_dataset, val_lt_dataset, params)\n",
    "\n",
    "    return dtw_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_param_importances(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_parallel_coordinate(study,params=['learning_rate','learning_rate','batch_size','conv1_channels','conv2_channels','conv3_channels','conv4_channels',\n",
    " 'kernel_12', 'kernel_34','padding', 'stride','dilation'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.matplotlib.plot_slice(study, params=['learning_rate','learning_rate','batch_size','conv1_channels','conv2_channels','conv3_channels','conv4_channels',\n",
    " 'kernel_12', 'kernel_34','padding', 'stride','dilation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(8, 6), dpi=80)\n",
    "\n",
    "optuna.visualization.matplotlib.plot_contour(study, params=['learning_rate','learning_rate','batch_size','conv1_channels','conv2_channels','conv3_channels','conv4_channels',\n",
    " 'kernel_12', 'kernel_34','padding', 'stride','dilation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Optimial Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH= r'D:\\Study 2 Results and Models\\Study 2 Optimisation\\CNN\\2022_10_05 CNN Optimisation'\n",
    "\n",
    "model_file = os.listdir(MODEL_PATH) \n",
    "\n",
    "os.chdir(MODEL_PATH)\n",
    "cwd = os.getcwd()\n",
    "\n",
    "print(f'Current working directory is: {cwd}') \n",
    "print(f\"There are {len(model_file)} files in the specified path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in model_file:\n",
    "    if f.endswith((\".pt\")):\n",
    "        checkpoint = torch.load(f)\n",
    "        best_epoch = checkpoint['epoch']\n",
    "        best_val_loss = checkpoint['val_loss']\n",
    "        dtw_total = checkpoint['dtw_distance']\n",
    "\n",
    "        print(f'{f}: best_epoch: {best_epoch}, best_val_loss: {best_val_loss:.5f}, dtw_distance: {dtw_total:.5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('study2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27ff62b9f8f46ac7739d102fa0cb2cdb691dbf234c24f48db1edec61200604ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
