{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import *\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: check if lines 8-11 are needed \n",
    "\n",
    "RANDOM_SEED = 6\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# torch.backends.cudnn.benchmark = False # uses deterministic convolution algorithm (may reduce performance)\n",
    "# torch.backends.cudnn.deterministic = True #\n",
    "\n",
    "#Sets the seed of RNG (GPU and CPU)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: ['Hips Flexion-Extension Left']\n",
      "input_window: 100\n",
      "output_window: 1\n",
      "stride: 1\n"
     ]
    }
   ],
   "source": [
    "# Set settings\n",
    "features, input_window, output_window, stride = set_settings()\n",
    "\n",
    "print(f'features: {features}')\n",
    "print(f'input_window: {input_window}')\n",
    "print(f'output_window: {output_window}')\n",
    "print(f'stride: {stride}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device used in this notebook is: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "print(f'The device used in this notebook is: {setDevice()}')\n",
    "\n",
    "DEVICE = setDevice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory is: D:\\Study 2 Data\\Healthy Gait\\Train\n",
      "There are 7 files in the specified path.\n"
     ]
    }
   ],
   "source": [
    "file_dir = r'D:\\Study 2 Data\\Healthy Gait\\Train' \n",
    "# file_dir = r'D:\\Study 2 Data\\CP Gait\\Train'\n",
    "train_files = os.listdir(file_dir) \n",
    "\n",
    "# Changes the working directory to get the data from their location \n",
    "os.chdir(file_dir)\n",
    "cwd = os.getcwd()\n",
    "\n",
    "print(f'Current working directory is: {cwd}')\n",
    "print(f\"There are {len(train_files)} files in the specified path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AB3154 BF T6-10.csv',\n",
       " 'AB6751 BF T1-5.csv',\n",
       " 'AB7422 BF T1-5.csv',\n",
       " 'AB7779 BF T1-5.csv',\n",
       " 'AB9119 BF T1-5.csv',\n",
       " 'AB9737 BF T1-5.csv',\n",
       " 'AB9737 BF T6-10.csv']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Trial', 'Time', 'Hips Flexion-Extension Left']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create feature list to extract values needed from CSV files\n",
    "all_features = ['Trial', 'Time'] + features\n",
    "all_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Data from CSV into Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from: AB3154 BF T6-10.csv\n",
      "Extracting data from: AB6751 BF T1-5.csv\n",
      "Extracting data from: AB7422 BF T1-5.csv\n",
      "Extracting data from: AB7779 BF T1-5.csv\n",
      "Extracting data from: AB9119 BF T1-5.csv\n",
      "Extracting data from: AB9737 BF T1-5.csv\n",
      "Extracting data from: AB9737 BF T6-10.csv\n"
     ]
    }
   ],
   "source": [
    "all_data = create_dataframe(train_files, all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.reset_index(drop=True, inplace=True) #reset the index of the table\n",
    "# path = r'D:\\Study 2 Data\\Healthy Gait' + '\\\\' + 'all_data_healthy_train.csv'\n",
    "# all_data.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Trial</th>\n",
       "      <th>Time</th>\n",
       "      <th>Hips Flexion-Extension Left</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AB3154 BF T6-10</td>\n",
       "      <td>1</td>\n",
       "      <td>161.00</td>\n",
       "      <td>-1.76414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AB3154 BF T6-10</td>\n",
       "      <td>1</td>\n",
       "      <td>161.01</td>\n",
       "      <td>-1.71541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AB3154 BF T6-10</td>\n",
       "      <td>1</td>\n",
       "      <td>161.02</td>\n",
       "      <td>-1.66432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AB3154 BF T6-10</td>\n",
       "      <td>1</td>\n",
       "      <td>161.03</td>\n",
       "      <td>-1.60888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AB3154 BF T6-10</td>\n",
       "      <td>1</td>\n",
       "      <td>161.04</td>\n",
       "      <td>-1.54643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69453</th>\n",
       "      <td>AB9737 BF T6-10</td>\n",
       "      <td>14</td>\n",
       "      <td>474.70</td>\n",
       "      <td>5.59699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69454</th>\n",
       "      <td>AB9737 BF T6-10</td>\n",
       "      <td>14</td>\n",
       "      <td>474.71</td>\n",
       "      <td>5.45820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69455</th>\n",
       "      <td>AB9737 BF T6-10</td>\n",
       "      <td>14</td>\n",
       "      <td>474.72</td>\n",
       "      <td>5.31928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69456</th>\n",
       "      <td>AB9737 BF T6-10</td>\n",
       "      <td>14</td>\n",
       "      <td>474.73</td>\n",
       "      <td>5.17878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69457</th>\n",
       "      <td>AB9737 BF T6-10</td>\n",
       "      <td>14</td>\n",
       "      <td>474.74</td>\n",
       "      <td>5.03528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69458 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Patient ID  Trial    Time  Hips Flexion-Extension Left\n",
       "0      AB3154 BF T6-10      1  161.00                     -1.76414\n",
       "1      AB3154 BF T6-10      1  161.01                     -1.71541\n",
       "2      AB3154 BF T6-10      1  161.02                     -1.66432\n",
       "3      AB3154 BF T6-10      1  161.03                     -1.60888\n",
       "4      AB3154 BF T6-10      1  161.04                     -1.54643\n",
       "...                ...    ...     ...                          ...\n",
       "69453  AB9737 BF T6-10     14  474.70                      5.59699\n",
       "69454  AB9737 BF T6-10     14  474.71                      5.45820\n",
       "69455  AB9737 BF T6-10     14  474.72                      5.31928\n",
       "69456  AB9737 BF T6-10     14  474.73                      5.17878\n",
       "69457  AB9737 BF T6-10     14  474.74                      5.03528\n",
       "\n",
       "[69458 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum number of trials is: 16\n"
     ]
    }
   ],
   "source": [
    "n_trials = all_data['Trial'].max()\n",
    "print(f'maximum number of trials is: {n_trials}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AB3154 BF T6-10', 'AB6751 BF T1-5', 'AB7422 BF T1-5',\n",
       "       'AB7779 BF T1-5', 'AB9119 BF T1-5', 'AB9737 BF T1-5',\n",
       "       'AB9737 BF T6-10'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['Patient ID'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide into training and valation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Healthy Subjects # Original\n",
    "# train_subjects = ['AB3154 BF T6-10', \n",
    "#                      'AB6751 BF T1-5', \n",
    "#                      'AB7779 BF T1-5',\n",
    "#                      'AB9737 BF T1-5', \n",
    "#                      'AB9737 BF T6-10',\n",
    "#                      'AB7422 BF T1-5',\n",
    "#                      ]\n",
    "\n",
    "# val_subjects = ['AB9119 BF T1-5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Healthy Subjects \n",
    "train_subjects = [\n",
    " 'AB9737 BF T1-5', \n",
    " 'AB9737 BF T6-10', \n",
    "'AB3154 BF T6-10',\n",
    " 'AB6751 BF T1-5', \n",
    " 'AB7779 BF T1-5', \n",
    " 'AB7422 BF T1-5']\n",
    "\n",
    "val_subjects = ['AB9119 BF T1-5']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AB9119 BF T1-5']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CP Subjects\n",
    "# train_subjects = ['RP1677 V1 BF T1-5', \n",
    "#                     'RP1677 V2 BF T1-5', \n",
    "#                     'RP4774 V2 BF T1-2',\n",
    "#                     'RP4774 V2 BF T3-7', \n",
    "#                     'RP4774 V2 BF T8-12', \n",
    "#                     'RP4907 V2 BF T1-5',\n",
    "#                     'RP5498 V1 BF T1-2',\n",
    "#                     'RP5498 V1 BF T3-5', \n",
    "#                     'RP7422 V1 BF T6-10', \n",
    "#                     'RP7422 V2 BF T1-5',\n",
    "#                     'RP9534 V2 BF T1-5', \n",
    "#                     'RP9571 V1 BF T6-10', \n",
    "#                     'RP9571 V2 BF T4-8']\n",
    "\n",
    "# val_subjects = ['RP6751 V1 BF T1-5',\n",
    "#                     'RP6751 V2 BF T1-5',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = all_data.loc[all_data['Patient ID'].isin(train_subjects)]\n",
    "\n",
    "val_data = all_data.loc[all_data['Patient ID'].isin(val_subjects)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_data: (69458, 4)\n",
      "train_data: (61289, 4)\n",
      "val_data: (8169, 4)\n"
     ]
    }
   ],
   "source": [
    "print(f'all_data: {all_data.shape}')\n",
    "print(f'train_data: {train_data.shape}')\n",
    "print(f'val_data: {val_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Patient ID                     0\n",
       "Trial                          0\n",
       "Time                           0\n",
       "Hips Flexion-Extension Left    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1368x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rcParams['figure.figsize'] = 19,6\n",
    "sns.set(font_scale = 1.5)\n",
    "ax = sns.violinplot(x=\"Patient ID\", y=\"Hips Flexion-Extension Left\", \n",
    "                    data=all_data, palette=\"muted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = sns.violinplot(x=\"Trial\", y=\"Hips Flexion-Extension Right\", \n",
    "#                     data=all_data[all_data['Patient ID'] == \"AB3154 T6-10 BF\"], palette=\"muted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = sns.violinplot(x=\"Trial\", y=\"Hips Flexion-Extension Right\", \n",
    "#                     data=all_data[all_data['Patient ID'] == \"AB5498 BF T6-10\"], palette=\"muted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For patient: AB3154 BF T6-10, trial: 1, there are: 651 time-points\n",
      "For patient: AB3154 BF T6-10, trial: 2, there are: 541 time-points\n",
      "For patient: AB3154 BF T6-10, trial: 3, there are: 613 time-points\n",
      "For patient: AB3154 BF T6-10, trial: 4, there are: 601 time-points\n",
      "For patient: AB3154 BF T6-10, trial: 5, there are: 711 time-points\n",
      "For patient: AB3154 BF T6-10, trial: 6, there are: 651 time-points\n",
      "For patient: AB3154 BF T6-10, trial: 7, there are: 1031 time-points\n",
      "For patient: AB3154 BF T6-10, trial: 8, there are: 896 time-points\n",
      "For patient: AB3154 BF T6-10, trial: 9, there are: 812 time-points\n",
      "For patient: AB6751 BF T1-5, trial: 1, there are: 761 time-points\n",
      "For patient: AB6751 BF T1-5, trial: 2, there are: 733 time-points\n",
      "For patient: AB6751 BF T1-5, trial: 3, there are: 731 time-points\n",
      "For patient: AB6751 BF T1-5, trial: 4, there are: 682 time-points\n",
      "For patient: AB6751 BF T1-5, trial: 5, there are: 710 time-points\n",
      "For patient: AB6751 BF T1-5, trial: 6, there are: 708 time-points\n",
      "For patient: AB6751 BF T1-5, trial: 7, there are: 682 time-points\n",
      "For patient: AB6751 BF T1-5, trial: 8, there are: 675 time-points\n",
      "For patient: AB6751 BF T1-5, trial: 9, there are: 900 time-points\n",
      "For patient: AB6751 BF T1-5, trial: 10, there are: 761 time-points\n",
      "For patient: AB7422 BF T1-5, trial: 1, there are: 1004 time-points\n",
      "For patient: AB7422 BF T1-5, trial: 2, there are: 781 time-points\n",
      "For patient: AB7422 BF T1-5, trial: 3, there are: 689 time-points\n",
      "For patient: AB7422 BF T1-5, trial: 4, there are: 792 time-points\n",
      "For patient: AB7422 BF T1-5, trial: 5, there are: 735 time-points\n",
      "For patient: AB7422 BF T1-5, trial: 6, there are: 835 time-points\n",
      "For patient: AB7422 BF T1-5, trial: 7, there are: 761 time-points\n",
      "For patient: AB7422 BF T1-5, trial: 8, there are: 790 time-points\n",
      "For patient: AB7422 BF T1-5, trial: 9, there are: 687 time-points\n",
      "For patient: AB7422 BF T1-5, trial: 10, there are: 775 time-points\n",
      "For patient: AB7779 BF T1-5, trial: 1, there are: 1055 time-points\n",
      "For patient: AB7779 BF T1-5, trial: 2, there are: 914 time-points\n",
      "For patient: AB7779 BF T1-5, trial: 3, there are: 893 time-points\n",
      "For patient: AB7779 BF T1-5, trial: 4, there are: 962 time-points\n",
      "For patient: AB7779 BF T1-5, trial: 5, there are: 830 time-points\n",
      "For patient: AB7779 BF T1-5, trial: 6, there are: 721 time-points\n",
      "For patient: AB7779 BF T1-5, trial: 7, there are: 731 time-points\n",
      "For patient: AB7779 BF T1-5, trial: 8, there are: 771 time-points\n",
      "For patient: AB7779 BF T1-5, trial: 9, there are: 663 time-points\n",
      "For patient: AB7779 BF T1-5, trial: 10, there are: 754 time-points\n",
      "For patient: AB9737 BF T1-5, trial: 1, there are: 1027 time-points\n",
      "For patient: AB9737 BF T1-5, trial: 2, there are: 1066 time-points\n",
      "For patient: AB9737 BF T1-5, trial: 3, there are: 1061 time-points\n",
      "For patient: AB9737 BF T1-5, trial: 4, there are: 880 time-points\n",
      "For patient: AB9737 BF T1-5, trial: 5, there are: 995 time-points\n",
      "For patient: AB9737 BF T1-5, trial: 6, there are: 921 time-points\n",
      "For patient: AB9737 BF T1-5, trial: 7, there are: 897 time-points\n",
      "For patient: AB9737 BF T1-5, trial: 8, there are: 920 time-points\n",
      "For patient: AB9737 BF T1-5, trial: 9, there are: 836 time-points\n",
      "For patient: AB9737 BF T1-5, trial: 10, there are: 874 time-points\n",
      "For patient: AB9737 BF T1-5, trial: 11, there are: 801 time-points\n",
      "For patient: AB9737 BF T1-5, trial: 12, there are: 877 time-points\n",
      "For patient: AB9737 BF T1-5, trial: 13, there are: 833 time-points\n",
      "For patient: AB9737 BF T1-5, trial: 14, there are: 811 time-points\n",
      "For patient: AB9737 BF T1-5, trial: 15, there are: 909 time-points\n",
      "For patient: AB9737 BF T1-5, trial: 16, there are: 824 time-points\n",
      "For patient: AB9737 BF T6-10, trial: 1, there are: 1843 time-points\n",
      "For patient: AB9737 BF T6-10, trial: 2, there are: 1565 time-points\n",
      "For patient: AB9737 BF T6-10, trial: 3, there are: 1296 time-points\n",
      "For patient: AB9737 BF T6-10, trial: 4, there are: 1241 time-points\n",
      "For patient: AB9737 BF T6-10, trial: 5, there are: 1251 time-points\n",
      "For patient: AB9737 BF T6-10, trial: 6, there are: 951 time-points\n",
      "For patient: AB9737 BF T6-10, trial: 7, there are: 1170 time-points\n",
      "For patient: AB9737 BF T6-10, trial: 8, there are: 1201 time-points\n",
      "For patient: AB9737 BF T6-10, trial: 9, there are: 1051 time-points\n",
      "For patient: AB9737 BF T6-10, trial: 10, there are: 1023 time-points\n",
      "For patient: AB9737 BF T6-10, trial: 11, there are: 1026 time-points\n",
      "For patient: AB9737 BF T6-10, trial: 12, there are: 1063 time-points\n",
      "For patient: AB9737 BF T6-10, trial: 13, there are: 1020 time-points\n",
      "For patient: AB9737 BF T6-10, trial: 14, there are: 1063 time-points\n",
      "\n",
      "There are 69 samples\n",
      "For patient: AB9119 BF T1-5, trial: 1, there are: 890 time-points\n",
      "For patient: AB9119 BF T1-5, trial: 2, there are: 1001 time-points\n",
      "For patient: AB9119 BF T1-5, trial: 3, there are: 831 time-points\n",
      "For patient: AB9119 BF T1-5, trial: 4, there are: 1051 time-points\n",
      "For patient: AB9119 BF T1-5, trial: 5, there are: 736 time-points\n",
      "For patient: AB9119 BF T1-5, trial: 6, there are: 766 time-points\n",
      "For patient: AB9119 BF T1-5, trial: 7, there are: 664 time-points\n",
      "For patient: AB9119 BF T1-5, trial: 8, there are: 764 time-points\n",
      "For patient: AB9119 BF T1-5, trial: 9, there are: 711 time-points\n",
      "For patient: AB9119 BF T1-5, trial: 10, there are: 755 time-points\n",
      "\n",
      "There are 10 samples\n"
     ]
    }
   ],
   "source": [
    "train_samples = count_nsamples(train_data)\n",
    "val_samples = count_nsamples(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 69\n",
      "Number of validation samples: 10\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of train samples: {train_samples}')\n",
    "print(f'Number of validation samples: {val_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs('D:\\Study 2 Data\\Pre-process sample', exist_ok=True)  #check if directory exists\n",
    "# all_data.to_csv('D:\\Study 2 Data\\Pre-process sample/exported-data3.csv')  #export data to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Data into numpy array for forming windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = all_data['Patient ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pd_to_np_converter(data, n_samples, features):\n",
    "    #create a numpy array that stores the data for export\n",
    "    sample_ID = []\n",
    "    # patients = 2\n",
    "    # n_trials = 10\n",
    "    # # samples = patients * n_trials\n",
    "    data_store = np.zeros((n_samples, 2000, len(features)), dtype=np.float32)\n",
    "    i = 0\n",
    "\n",
    "    for p in data['Patient ID'].unique(): #loop over patients \n",
    "        for t in data['Trial'].unique(): #loop over trials starting with trials 1 to trial 9 (inclusive)\n",
    "            pd_array = data[(data['Patient ID'] == p) & (data['Trial'] == t)]\n",
    "            if pd_array.empty:\n",
    "                continue\n",
    "                # print('DataFrame is empty!')\n",
    "                # print(f'Trail {t} does not exist in {p}')\n",
    "            else:\n",
    "                np_array = pd_array.to_numpy()\n",
    "                data_store[i, :np_array.shape[0], :] = np_array[:,3:] \n",
    "                sample_ID.append(p+ ' Ts'+str(t)) \n",
    "                i +=1\n",
    "\n",
    "    return pd_array.columns, data_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_np.shape: (69, 2000, 1)\n",
      "val_data_np.shape: (10, 2000, 1)\n"
     ]
    }
   ],
   "source": [
    "train_columns, train_data_np = pd_to_np_converter(train_data, train_samples, features)\n",
    "val_columns, val_data_np = pd_to_np_converter(val_data, val_samples, features)\n",
    "\n",
    "print(f'train_data_np.shape: {train_data_np.shape}')\n",
    "print(f'val_data_np.shape: {val_data_np.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hips Flexion-Extension Left']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_columns[3:].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features == train_columns[3:].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAY! Column headers of dataframe match features\n"
     ]
    }
   ],
   "source": [
    "labels_keys = train_columns[3:].tolist() #copy the train columns removing the first column headers'Patient ID', 'Trial', 'Time'\n",
    "\n",
    "\n",
    "if features == labels_keys: # check that the features are the same as the label keys \n",
    "    print('YAY! Column headers of dataframe match features')\n",
    "else:\n",
    "    print('ERROR: Features and labels_keys do not match!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Dictionary for Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hips Flexion-Extension Left': 0}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_keys = features \n",
    "\n",
    "labels_idx = np.arange(0, len(labels_keys), 1)\n",
    "\n",
    "labels = dict(zip(labels_keys, labels_idx))\n",
    "\n",
    "len(labels)\n",
    "labels\n",
    "# labels_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Data: Window Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_generator_fltrd(sequence, input_window, output_window, stride, features, labels):\n",
    "    \"\"\"\n",
    "    Trims the input sequence from leading and trailing zeros, then generates an array with input windows and another array for the corresponding output windows\n",
    "    Args:\n",
    "        sequence: (np.array, float32) columns are features while rows are time points\n",
    "        features: (list, strin~g) column names\n",
    "        input_window: (int)\n",
    "        stride (int): the value the input window shifts along the sequence \n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    # shortest_seqLen = float('inf')\n",
    "\n",
    "    # f_zeros = [] #array that stores the number of leading zeros for each feature\n",
    "    b_zeros = [] #array that stores the number of trailing zeros for each feacture \n",
    "\n",
    "    for f in features:\n",
    "        # trim the leading and training zeros\n",
    "        # f_zeros.append(sequence[:,labels[f]].shape[0] - np.trim_zeros(sequence[:,labels[f]], 'f').shape[0]) #forward zeros\n",
    "        b_zeros.append(sequence[:,labels[f]].shape[0] - np.trim_zeros(sequence[:,labels[f]], 'b').shape[0]) #backward zeros\n",
    "\n",
    "    # max_f_zeros = max(f_zeros) #find the maximum number of leading zeros\n",
    "    max_b_zeros = max(b_zeros) #find the maximum number of trailing/backward zeros \n",
    "    # print(max_b_zeros)\n",
    "\n",
    "    #total sequence length minus max leading and trailing zeros \n",
    "    trimmed_seqLen = sequence[:,0].shape[0] - (max_b_zeros)\n",
    "    trimmed_seqLen_reduced = trimmed_seqLen - 200 #reducing sequence size to remove the first and last 200 timesteps which may contain errors   \n",
    "    print(f'trimmed_seqLen: {trimmed_seqLen}')\n",
    "    print(f'trimmed_seqLen_reduced: {trimmed_seqLen_reduced}')\n",
    "\n",
    "\n",
    "    # Slides are the number of times the input window can scan the sequence \n",
    "    # Using the equation that calculates the number of outputs as in convolution  (W â€“ F + 2P) / S + 1, W=input image width, F=filter width, P=padding, S=stride\n",
    "    # The width of the image is taken as the number of time steps in the sequence, corresponding to the length of any TRIMMED column in the data \n",
    "    slides = ((trimmed_seqLen_reduced - (input_window+output_window)) // stride) + 1\n",
    "    print(f\"number of slides is: {slides}\")\n",
    "\n",
    "    # Calculating the first index of each of the output sequences (first index always f_zeros as its always shifted to start with the first non-zero element)\n",
    "    seq_indicies = (np.arange(slides) * stride) + 100\n",
    "\n",
    "    if slides <= 0:\n",
    "        raise ValueError(\"input window and output window length are greater than sequence length, check their values\")\n",
    "\n",
    "    # Creates an zero numpy array to store the samples in \n",
    "    X_values = np.zeros((len(seq_indicies) , input_window, len(features)))\n",
    "    Y_values = np.zeros((len(seq_indicies), output_window, len(features)))\n",
    "\n",
    "    # Loop through the features, then loop through the list of sequence indicies needed for input and output windows \n",
    "    for j, feature in enumerate(features):\n",
    "        # print(j)\n",
    "        # print(feature)\n",
    "        for i, idx in enumerate(seq_indicies):\n",
    "            X_values[i, :, j] = sequence[idx:idx+input_window, labels[feature]]\n",
    "            Y_values[i, :, j] = sequence[idx+input_window:idx+input_window + output_window, labels[feature]]\n",
    "\n",
    "    return X_values, Y_values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_generator_lt_fltrd(sequence, input_window, future_window, stride, features, labels): #window gernerator long term fltrd (creats a validation window up to 200 timesteps in advance to measure error on long term future predictions)\n",
    "    \"\"\"\n",
    "    Trims the input sequence from leading and trailing zeros, then generates an array with input windows and another array for the corresponding output windows\n",
    "    Args:\n",
    "        sequence: (np.array, float32) columns are features while rows are time points\n",
    "        features: (list, strin~g) column names\n",
    "        input_window: (int)\n",
    "        stride (int): the value the input window shifts along the sequence \n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    b_zeros = [] #array that stores the number of trailing zeros for each feacture \n",
    "\n",
    "    for f in features:\n",
    "        # trim the leading and training zeros\n",
    "        b_zeros.append(sequence[:,labels[f]].shape[0] - np.trim_zeros(sequence[:,labels[f]], 'b').shape[0]) #backward zeros\n",
    "\n",
    "    max_b_zeros = max(b_zeros) #find the maximum number of trailing/backward zeros \n",
    "    # print(max_b_zeros)\n",
    "\n",
    "    fltrd_samples = 2 * 100 #remove 150 timesteps from the beggining and ending of the entire sequence\n",
    "    # lt_len = 200 # number of timesteps to predict in the future based on a single input window (to be used in measuring errors based on prediction input)\n",
    "    \n",
    "    #total sequence length minus max leading and trailing zeros \n",
    "    trimmed_seqLen = sequence[:,0].shape[0] - (max_b_zeros)\n",
    "    trimmed_seqLen_reduced = trimmed_seqLen - (fltrd_samples) # (- fltrd_samples is done to reduce sequence size to remove the first and last 150 timesteps which may contain errors since they corresponding to beggining and ending of the trials \n",
    "    print(f'trimmed_seqLen: {trimmed_seqLen}')\n",
    "    print(f'trimmed_seqLen_reduced: {trimmed_seqLen_reduced}')\n",
    "\n",
    "\n",
    "    # Slides are the number of times the input window can scan the sequence \n",
    "    # Using the equation that calculates the number of outputs as in convolution  (W â€“ F + 2P) / S + 1, W=input image width, F=filter width, P=padding, S=stride\n",
    "    # The width of the image is taken as the number of time steps in the sequence, corresponding to the length of any TRIMMED column in the data \n",
    "    slides = ((trimmed_seqLen_reduced - (input_window+future_window)) // stride) + 1\n",
    "    print(f\"number of slides is: {slides}\")\n",
    "\n",
    "    # Calculating the first index of each of the output sequences (first index always f_zeros as its always shifted to start with the first non-zero element)\n",
    "    seq_indicies = (np.arange(slides) * stride) + 100\n",
    "\n",
    "    if slides <= 0:\n",
    "        raise ValueError(\"input window and output window length are greater than sequence length, check their values\")\n",
    "\n",
    "    # Creates an zero numpy array to store the samples in \n",
    "    X_values = np.zeros((len(seq_indicies) , input_window, len(features)))\n",
    "    Y_values = np.zeros((len(seq_indicies), future_window, len(features)))\n",
    "\n",
    "    # Loop through the features, then loop through the list of sequence indicies needed for input and output windows \n",
    "    for j, feature in enumerate(features):\n",
    "        # print(j)\n",
    "        # print(feature)\n",
    "        for i, idx in enumerate(seq_indicies):\n",
    "            X_values[i, :, j] = sequence[idx:idx+input_window, labels[feature]]\n",
    "            Y_values[i, :, j] = sequence[idx+input_window:idx+input_window + future_window, labels[feature]]\n",
    "\n",
    "    return X_values, Y_values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trimmed_seqLen: 651\n",
      "trimmed_seqLen_reduced: 451\n",
      "number of slides is: 351\n",
      "trimmed_seqLen: 541\n",
      "trimmed_seqLen_reduced: 341\n",
      "number of slides is: 241\n",
      "trimmed_seqLen: 613\n",
      "trimmed_seqLen_reduced: 413\n",
      "number of slides is: 313\n",
      "trimmed_seqLen: 601\n",
      "trimmed_seqLen_reduced: 401\n",
      "number of slides is: 301\n",
      "trimmed_seqLen: 711\n",
      "trimmed_seqLen_reduced: 511\n",
      "number of slides is: 411\n",
      "trimmed_seqLen: 651\n",
      "trimmed_seqLen_reduced: 451\n",
      "number of slides is: 351\n",
      "trimmed_seqLen: 1031\n",
      "trimmed_seqLen_reduced: 831\n",
      "number of slides is: 731\n",
      "trimmed_seqLen: 896\n",
      "trimmed_seqLen_reduced: 696\n",
      "number of slides is: 596\n",
      "trimmed_seqLen: 812\n",
      "trimmed_seqLen_reduced: 612\n",
      "number of slides is: 512\n",
      "trimmed_seqLen: 761\n",
      "trimmed_seqLen_reduced: 561\n",
      "number of slides is: 461\n",
      "trimmed_seqLen: 733\n",
      "trimmed_seqLen_reduced: 533\n",
      "number of slides is: 433\n",
      "trimmed_seqLen: 731\n",
      "trimmed_seqLen_reduced: 531\n",
      "number of slides is: 431\n",
      "trimmed_seqLen: 682\n",
      "trimmed_seqLen_reduced: 482\n",
      "number of slides is: 382\n",
      "trimmed_seqLen: 710\n",
      "trimmed_seqLen_reduced: 510\n",
      "number of slides is: 410\n",
      "trimmed_seqLen: 708\n",
      "trimmed_seqLen_reduced: 508\n",
      "number of slides is: 408\n",
      "trimmed_seqLen: 682\n",
      "trimmed_seqLen_reduced: 482\n",
      "number of slides is: 382\n",
      "trimmed_seqLen: 675\n",
      "trimmed_seqLen_reduced: 475\n",
      "number of slides is: 375\n",
      "trimmed_seqLen: 900\n",
      "trimmed_seqLen_reduced: 700\n",
      "number of slides is: 600\n",
      "trimmed_seqLen: 761\n",
      "trimmed_seqLen_reduced: 561\n",
      "number of slides is: 461\n",
      "trimmed_seqLen: 1004\n",
      "trimmed_seqLen_reduced: 804\n",
      "number of slides is: 704\n",
      "trimmed_seqLen: 781\n",
      "trimmed_seqLen_reduced: 581\n",
      "number of slides is: 481\n",
      "trimmed_seqLen: 689\n",
      "trimmed_seqLen_reduced: 489\n",
      "number of slides is: 389\n",
      "trimmed_seqLen: 792\n",
      "trimmed_seqLen_reduced: 592\n",
      "number of slides is: 492\n",
      "trimmed_seqLen: 735\n",
      "trimmed_seqLen_reduced: 535\n",
      "number of slides is: 435\n",
      "trimmed_seqLen: 835\n",
      "trimmed_seqLen_reduced: 635\n",
      "number of slides is: 535\n",
      "trimmed_seqLen: 761\n",
      "trimmed_seqLen_reduced: 561\n",
      "number of slides is: 461\n",
      "trimmed_seqLen: 790\n",
      "trimmed_seqLen_reduced: 590\n",
      "number of slides is: 490\n",
      "trimmed_seqLen: 687\n",
      "trimmed_seqLen_reduced: 487\n",
      "number of slides is: 387\n",
      "trimmed_seqLen: 775\n",
      "trimmed_seqLen_reduced: 575\n",
      "number of slides is: 475\n",
      "trimmed_seqLen: 1055\n",
      "trimmed_seqLen_reduced: 855\n",
      "number of slides is: 755\n",
      "trimmed_seqLen: 914\n",
      "trimmed_seqLen_reduced: 714\n",
      "number of slides is: 614\n",
      "trimmed_seqLen: 893\n",
      "trimmed_seqLen_reduced: 693\n",
      "number of slides is: 593\n",
      "trimmed_seqLen: 962\n",
      "trimmed_seqLen_reduced: 762\n",
      "number of slides is: 662\n",
      "trimmed_seqLen: 830\n",
      "trimmed_seqLen_reduced: 630\n",
      "number of slides is: 530\n",
      "trimmed_seqLen: 721\n",
      "trimmed_seqLen_reduced: 521\n",
      "number of slides is: 421\n",
      "trimmed_seqLen: 731\n",
      "trimmed_seqLen_reduced: 531\n",
      "number of slides is: 431\n",
      "trimmed_seqLen: 771\n",
      "trimmed_seqLen_reduced: 571\n",
      "number of slides is: 471\n",
      "trimmed_seqLen: 663\n",
      "trimmed_seqLen_reduced: 463\n",
      "number of slides is: 363\n",
      "trimmed_seqLen: 754\n",
      "trimmed_seqLen_reduced: 554\n",
      "number of slides is: 454\n",
      "trimmed_seqLen: 1027\n",
      "trimmed_seqLen_reduced: 827\n",
      "number of slides is: 727\n",
      "trimmed_seqLen: 1066\n",
      "trimmed_seqLen_reduced: 866\n",
      "number of slides is: 766\n",
      "trimmed_seqLen: 1061\n",
      "trimmed_seqLen_reduced: 861\n",
      "number of slides is: 761\n",
      "trimmed_seqLen: 880\n",
      "trimmed_seqLen_reduced: 680\n",
      "number of slides is: 580\n",
      "trimmed_seqLen: 995\n",
      "trimmed_seqLen_reduced: 795\n",
      "number of slides is: 695\n",
      "trimmed_seqLen: 921\n",
      "trimmed_seqLen_reduced: 721\n",
      "number of slides is: 621\n",
      "trimmed_seqLen: 897\n",
      "trimmed_seqLen_reduced: 697\n",
      "number of slides is: 597\n",
      "trimmed_seqLen: 920\n",
      "trimmed_seqLen_reduced: 720\n",
      "number of slides is: 620\n",
      "trimmed_seqLen: 836\n",
      "trimmed_seqLen_reduced: 636\n",
      "number of slides is: 536\n",
      "trimmed_seqLen: 874\n",
      "trimmed_seqLen_reduced: 674\n",
      "number of slides is: 574\n",
      "trimmed_seqLen: 801\n",
      "trimmed_seqLen_reduced: 601\n",
      "number of slides is: 501\n",
      "trimmed_seqLen: 877\n",
      "trimmed_seqLen_reduced: 677\n",
      "number of slides is: 577\n",
      "trimmed_seqLen: 833\n",
      "trimmed_seqLen_reduced: 633\n",
      "number of slides is: 533\n",
      "trimmed_seqLen: 811\n",
      "trimmed_seqLen_reduced: 611\n",
      "number of slides is: 511\n",
      "trimmed_seqLen: 909\n",
      "trimmed_seqLen_reduced: 709\n",
      "number of slides is: 609\n",
      "trimmed_seqLen: 824\n",
      "trimmed_seqLen_reduced: 624\n",
      "number of slides is: 524\n",
      "trimmed_seqLen: 1843\n",
      "trimmed_seqLen_reduced: 1643\n",
      "number of slides is: 1543\n",
      "trimmed_seqLen: 1565\n",
      "trimmed_seqLen_reduced: 1365\n",
      "number of slides is: 1265\n",
      "trimmed_seqLen: 1296\n",
      "trimmed_seqLen_reduced: 1096\n",
      "number of slides is: 996\n",
      "trimmed_seqLen: 1241\n",
      "trimmed_seqLen_reduced: 1041\n",
      "number of slides is: 941\n",
      "trimmed_seqLen: 1251\n",
      "trimmed_seqLen_reduced: 1051\n",
      "number of slides is: 951\n",
      "trimmed_seqLen: 951\n",
      "trimmed_seqLen_reduced: 751\n",
      "number of slides is: 651\n",
      "trimmed_seqLen: 1170\n",
      "trimmed_seqLen_reduced: 970\n",
      "number of slides is: 870\n",
      "trimmed_seqLen: 1201\n",
      "trimmed_seqLen_reduced: 1001\n",
      "number of slides is: 901\n",
      "trimmed_seqLen: 1051\n",
      "trimmed_seqLen_reduced: 851\n",
      "number of slides is: 751\n",
      "trimmed_seqLen: 1023\n",
      "trimmed_seqLen_reduced: 823\n",
      "number of slides is: 723\n",
      "trimmed_seqLen: 1026\n",
      "trimmed_seqLen_reduced: 826\n",
      "number of slides is: 726\n",
      "trimmed_seqLen: 1063\n",
      "trimmed_seqLen_reduced: 863\n",
      "number of slides is: 763\n",
      "trimmed_seqLen: 1020\n",
      "trimmed_seqLen_reduced: 820\n",
      "number of slides is: 720\n",
      "trimmed_seqLen: 1063\n",
      "trimmed_seqLen_reduced: 863\n",
      "number of slides is: 763\n",
      "shape of X_train_windows: (62100, 100, 1)\n",
      "shape of Y_train_windows: (62100, 1, 1)\n",
      "shape of X_train_data: (40589, 100, 1)\n",
      "shape of Y_train_data: (40589, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Creating training datasets\n",
    "# Selecting the features to be used when creating windows \n",
    "approx_seq_len = 2000 # appoximate the length of the longest sequence that can be encountered \n",
    "# samples_per_file = ((approx_seq_len - (input_window+output_window)) // stride) + 1 #number of window samples generated per file \n",
    "samples_per_file = 900\n",
    "\n",
    "\n",
    "# create a zero-filled 3D array with shape (number of samples * number of files, window size, number of features)\n",
    "X_train_windows = np.zeros((samples_per_file*train_samples, input_window, len(features)), dtype=np.float32) #size can be reduced by decreasing train_size \n",
    "Y_train_windows = np.zeros((samples_per_file*train_samples, output_window, len(features)), dtype=np.float32) \n",
    "\n",
    "\n",
    "start_idx = 0 #setting start index to equal zero \n",
    "train_sample_sum = 0\n",
    "train_excluded_samples = []\n",
    "# Create training windows \n",
    "\n",
    "#for i in tqdm(range(train_size)): #Use for including all data including outliers \n",
    "for i in range(train_samples): \n",
    "       \n",
    "    X_values, Y_values = window_generator_fltrd(\n",
    "        train_data_np[i,:,:],\n",
    "        input_window=input_window, \n",
    "        output_window=output_window, \n",
    "        stride=stride, \n",
    "        features=features,\n",
    "        labels=labels\n",
    "        )\n",
    "\n",
    "    end_idx = start_idx + X_values.shape[0]\n",
    "\n",
    "    # print(f'file number [{f}] start index: {start_idx}, end index: {end_idx}, number of samples: {X_values.shape[0]}')\n",
    "\n",
    "    X_train_windows[start_idx:end_idx, :, :] = X_values\n",
    "    Y_train_windows[start_idx:end_idx, :, :] = Y_values\n",
    "\n",
    "    # print(f'number of samples copied: {X_train_data_store[start_idx:end_idx, :, :].shape[0]}')\n",
    "\n",
    "    start_idx = end_idx \n",
    "    train_sample_sum += X_values.shape[0]\n",
    "\n",
    "    # except Exception:\n",
    "    #     exception_msg(i)\n",
    "    #     train_excluded_samples.append(i)\n",
    "\n",
    "# print(f\"Completed storage of training windows samples, which contains {X_train_data_store.shape[0]} samples\")\n",
    "X_train_data = X_train_windows[:end_idx, :, :]\n",
    "Y_train_data = Y_train_windows[:end_idx, :, :]\n",
    "\n",
    "\n",
    "print(f'shape of X_train_windows: {X_train_windows.shape}')\n",
    "print(f'shape of Y_train_windows: {Y_train_windows.shape}')\n",
    "\n",
    "print(f'shape of X_train_data: {X_train_data.shape}')\n",
    "print(f'shape of Y_train_data: {Y_train_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trimmed_seqLen: 890\n",
      "trimmed_seqLen_reduced: 690\n",
      "number of slides is: 590\n",
      "trimmed_seqLen: 1001\n",
      "trimmed_seqLen_reduced: 801\n",
      "number of slides is: 701\n",
      "trimmed_seqLen: 831\n",
      "trimmed_seqLen_reduced: 631\n",
      "number of slides is: 531\n",
      "trimmed_seqLen: 1051\n",
      "trimmed_seqLen_reduced: 851\n",
      "number of slides is: 751\n",
      "trimmed_seqLen: 736\n",
      "trimmed_seqLen_reduced: 536\n",
      "number of slides is: 436\n",
      "trimmed_seqLen: 766\n",
      "trimmed_seqLen_reduced: 566\n",
      "number of slides is: 466\n",
      "trimmed_seqLen: 664\n",
      "trimmed_seqLen_reduced: 464\n",
      "number of slides is: 364\n",
      "trimmed_seqLen: 764\n",
      "trimmed_seqLen_reduced: 564\n",
      "number of slides is: 464\n",
      "trimmed_seqLen: 711\n",
      "trimmed_seqLen_reduced: 511\n",
      "number of slides is: 411\n",
      "trimmed_seqLen: 755\n",
      "trimmed_seqLen_reduced: 555\n",
      "number of slides is: 455\n",
      "shape of X_val_windows: (16000, 100, 1)\n",
      "shape of Y_val_windows: (16000, 1, 1)\n",
      "shape of X_val_data: (5169, 100, 1)\n",
      "shape of Y_val_data: (5169, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Creating validation datasets\n",
    "# Selecting the features to be used when creating windows \n",
    "approx_seq_len = 2000 # appoximate the length of the longest sequence that can be encountered \n",
    "# samples_per_file = ((approx_seq_len - (input_window+output_window)) // stride) + 1 #number of window samples generated per file \n",
    "samples_per_file = 1600\n",
    "\n",
    "\n",
    "# create a zero-filled 3D array with shape (number of samples * number of files, window size, number of features)\n",
    "X_val_windows = np.zeros((samples_per_file*val_samples, input_window, len(features)), dtype=np.float32) #size can be reduced by decreasing train_size \n",
    "Y_val_windows = np.zeros((samples_per_file*val_samples, output_window, len(features)), dtype=np.float32) \n",
    "\n",
    "\n",
    "start_idx = 0 #setting start index to equal zero \n",
    "val_sample_sum = 0\n",
    "val_excluded_samples = []\n",
    "# Create training windows \n",
    "\n",
    "#for i in tqdm(range(train_size)): #Use for including all data including outliers \n",
    "for i in range(val_samples): \n",
    "       \n",
    "    X_values, Y_values = window_generator_fltrd(\n",
    "        val_data_np[i,:,:],\n",
    "        input_window=input_window, \n",
    "        output_window=output_window, \n",
    "        stride=stride, \n",
    "        features=features,\n",
    "        labels=labels\n",
    "        )\n",
    "\n",
    "    end_idx = start_idx + X_values.shape[0]\n",
    "\n",
    "    # print(f'file number [{f}] start index: {start_idx}, end index: {end_idx}, number of samples: {X_values.shape[0]}')\n",
    "\n",
    "    X_val_windows[start_idx:end_idx, :, :] = X_values\n",
    "    Y_val_windows[start_idx:end_idx, :, :] = Y_values\n",
    "\n",
    "    # print(f'number of samples copied: {X_train_data_store[start_idx:end_idx, :, :].shape[0]}')\n",
    "\n",
    "    start_idx = end_idx \n",
    "    val_sample_sum += X_values.shape[0]\n",
    "\n",
    "    # except Exception:\n",
    "    #     exception_msg(i)\n",
    "    #     train_excluded_samples.append(i)\n",
    "\n",
    "# print(f\"Completed storage of training windows samples, which contains {X_train_data_store.shape[0]} samples\")\n",
    "X_val_data = X_val_windows[:end_idx, :, :]\n",
    "Y_val_data = Y_val_windows[:end_idx, :, :]\n",
    "\n",
    "\n",
    "print(f'shape of X_val_windows: {X_val_windows.shape}')\n",
    "print(f'shape of Y_val_windows: {Y_val_windows.shape}')\n",
    "\n",
    "print(f'shape of X_val_data: {X_val_data.shape}')\n",
    "print(f'shape of Y_val_data: {Y_val_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trimmed_seqLen: 890\n",
      "trimmed_seqLen_reduced: 690\n",
      "number of slides is: 391\n",
      "trimmed_seqLen: 1001\n",
      "trimmed_seqLen_reduced: 801\n",
      "number of slides is: 502\n",
      "trimmed_seqLen: 831\n",
      "trimmed_seqLen_reduced: 631\n",
      "number of slides is: 332\n",
      "trimmed_seqLen: 1051\n",
      "trimmed_seqLen_reduced: 851\n",
      "number of slides is: 552\n",
      "trimmed_seqLen: 736\n",
      "trimmed_seqLen_reduced: 536\n",
      "number of slides is: 237\n",
      "trimmed_seqLen: 766\n",
      "trimmed_seqLen_reduced: 566\n",
      "number of slides is: 267\n",
      "trimmed_seqLen: 664\n",
      "trimmed_seqLen_reduced: 464\n",
      "number of slides is: 165\n",
      "trimmed_seqLen: 764\n",
      "trimmed_seqLen_reduced: 564\n",
      "number of slides is: 265\n",
      "trimmed_seqLen: 711\n",
      "trimmed_seqLen_reduced: 511\n",
      "number of slides is: 212\n",
      "trimmed_seqLen: 755\n",
      "trimmed_seqLen_reduced: 555\n",
      "number of slides is: 256\n",
      "shape of X_val_lt_windows: (16000, 100, 1)\n",
      "shape of Y_val_lt_windows: (16000, 200, 1)\n",
      "shape of X_val_lt_data: (3179, 100, 1)\n",
      "shape of Y_val_lt_data: (3179, 200, 1)\n"
     ]
    }
   ],
   "source": [
    "# Creating validation datasets (long term predictions)\n",
    "# Selecting the features to be used when creating windows \n",
    "approx_seq_len = 2000 # appoximate the length of the longest sequence that can be encountered \n",
    "# samples_per_file = ((approx_seq_len - (input_window+output_window)) // stride) + 1 #number of window samples generated per file \n",
    "samples_per_file = 1600\n",
    "\n",
    "future_window = 200\n",
    "\n",
    "# create a zero-filled 3D array with shape (number of samples * number of files, window size, number of features)\n",
    "X_val_lt_windows = np.zeros((samples_per_file*val_samples, input_window, len(features)), dtype=np.float32) #size can be reduced by decreasing train_size \n",
    "Y_val_lt_windows = np.zeros((samples_per_file*val_samples, future_window, len(features)), dtype=np.float32) \n",
    "\n",
    "\n",
    "start_idx = 0 #setting start index to equal zero \n",
    "val_lt_sample_sum = 0\n",
    "val_lt_excluded_samples = []\n",
    "# Create training windows \n",
    "\n",
    "#for i in tqdm(range(train_size)): #Use for including all data including outliers \n",
    "for i in range(val_samples): \n",
    "       \n",
    "    X_values, Y_values = window_generator_lt_fltrd(\n",
    "        val_data_np[i,:,:],\n",
    "        input_window=input_window, \n",
    "        future_window=future_window, \n",
    "        stride=stride, \n",
    "        features=features,\n",
    "        labels=labels\n",
    "        )\n",
    "\n",
    "    end_idx = start_idx + X_values.shape[0]\n",
    "\n",
    "    # print(f'file number [{f}] start index: {start_idx}, end index: {end_idx}, number of samples: {X_values.shape[0]}')\n",
    "\n",
    "    X_val_lt_windows[start_idx:end_idx, :, :] = X_values\n",
    "    Y_val_lt_windows[start_idx:end_idx, :, :] = Y_values\n",
    "\n",
    "    # print(f'number of samples copied: {X_train_data_store[start_idx:end_idx, :, :].shape[0]}')\n",
    "\n",
    "    start_idx = end_idx \n",
    "    val_lt_sample_sum += X_values.shape[0]\n",
    "\n",
    "    # except Exception:\n",
    "    #     exception_msg(i)\n",
    "    #     train_excluded_samples.append(i)\n",
    "\n",
    "# print(f\"Completed storage of training windows samples, which contains {X_train_data_store.shape[0]} samples\")\n",
    "X_val_lt_data = X_val_lt_windows[:end_idx, :, :]\n",
    "Y_val_lt_data = Y_val_lt_windows[:end_idx, :, :]\n",
    "\n",
    "\n",
    "print(f'shape of X_val_lt_windows: {X_val_lt_windows.shape}')\n",
    "print(f'shape of Y_val_lt_windows: {Y_val_lt_windows.shape}')\n",
    "\n",
    "print(f'shape of X_val_lt_data: {X_val_lt_data.shape}')\n",
    "print(f'shape of Y_val_lt_data: {Y_val_lt_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 2 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28016/3345126727.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val_lt_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'input'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val_lt_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'200 timesteps in the future'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'target value'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'g'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 2 with size 1"
     ]
    }
   ],
   "source": [
    "#checking the windows\n",
    "x1 = np.arange(0,100)\n",
    "x2 = np.arange(100,300)\n",
    "x3 = 100\n",
    "plt.plot(x1, X_val_lt_data[2,:,1], label='input')\n",
    "plt.plot(x2, Y_val_lt_data[2,:,1], label='200 timesteps in the future')\n",
    "plt.scatter(x3, Y_val_data[2,:,1], s=30, label='target value', color='g')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot histogram of the data \n",
    "rcParams['figure.figsize'] = 10,4\n",
    "\n",
    "for f in range(len(features)):\n",
    "    plt.hist(Y_train_data[:,:,f].reshape(-1,1), label = features[f], bins=50, range=(Y_train_data.min(), Y_train_data.max()))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Data: Normalisation/Standarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-30.96504974],\n",
       "       [ 63.01054001]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_norm, scalars = normalise_fit(X_train_data)\n",
    "Y_train_norm = normalise_transform(Y_train_data, scalars)\n",
    "\n",
    "X_val_norm = normalise_transform(X_val_data, scalars)\n",
    "Y_val_norm = normalise_transform(Y_val_data, scalars)\n",
    "\n",
    "X_val_lt_norm = normalise_transform(X_val_lt_data, scalars)\n",
    "Y_val_lt_norm = normalise_transform(Y_val_lt_data, scalars)\n",
    "\n",
    "\n",
    "scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Use the data unnormalised \n",
    "# X_train_norm = X_train_data\n",
    "# Y_train_norm = Y_train_data\n",
    "\n",
    "\n",
    "# X_val_norm = X_val_data\n",
    "# Y_val_norm = Y_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot histogram of the data \n",
    "\n",
    "rcParams['figure.figsize'] = 10,4\n",
    "\n",
    "for f in range(len(features)):\n",
    "    plt.hist(Y_train_data[:,:,f].reshape(-1,1), bins=50, label = features[f])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot histogram of the data \n",
    "\n",
    "rcParams['figure.figsize'] = 10,4\n",
    "\n",
    "for f in range(len(features)):\n",
    "    plt.hist(Y_train_norm[:,:,f].reshape(-1,1), bins=50, label = features[f])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([40589, 100, 1])\n",
      "Y_train shape: torch.Size([40589, 1, 1])\n",
      "X_val shape: torch.Size([5169, 100, 1])\n",
      "Y_val shape: torch.Size([5169, 1, 1])\n",
      "X_val_lt shape: torch.Size([3179, 100, 1])\n",
      "Y_val_lt shape: torch.Size([3179, 200, 1])\n"
     ]
    }
   ],
   "source": [
    "# Convert to Tensor \n",
    "# do not store on GPU (yet)\n",
    "X_train = torch.from_numpy(X_train_norm).to(torch.float64)\n",
    "Y_train = torch.from_numpy(Y_train_norm).to(torch.float64)\n",
    "\n",
    "# X_val = torch.from_numpy(X_val_norm).float()\n",
    "\n",
    "# X_val_resized = np.expand_dims(Y_val_norm[:,0,:], axis=1)\n",
    "# Y_val = torch.from_numpy(X_val_resized).float()\n",
    "\n",
    "X_val = torch.from_numpy(X_val_norm).to(torch.float64)\n",
    "Y_val = torch.from_numpy(Y_val_norm).to(torch.float64)\n",
    "\n",
    "\n",
    "X_val_lt = torch.from_numpy(X_val_lt_norm).to(torch.float64)\n",
    "Y_val_lt = torch.from_numpy(Y_val_lt_norm).to(torch.float64)\n",
    "\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'Y_train shape: {Y_train.shape}')\n",
    "\n",
    "print(f'X_val shape: {X_val.shape}')\n",
    "print(f'Y_val shape: {Y_val.shape}')\n",
    "\n",
    "print(f'X_val_lt shape: {X_val_lt.shape}') #long term predictions\n",
    "print(f'Y_val_lt shape: {Y_val_lt.shape}') #long term predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to Tensor & ROUND\n",
    "# # do not store on GPU (yet)\n",
    "# X_train = torch.from_numpy(np.round(X_train_norm,2)).float()\n",
    "# Y_train = torch.from_numpy(np.round(Y_train_norm)).float()\n",
    "\n",
    "# X_val = torch.from_numpy(np.round(X_val_norm)).float()\n",
    "# Y_val = torch.from_numpy(np.round(Y_val_norm)).float()\n",
    "\n",
    "# print(f'X_train shape: {X_train.shape}')\n",
    "# print(f'Y_train shape: {Y_train.shape}')\n",
    "\n",
    "# print(f'X_val shape: {X_val.shape}')\n",
    "# print(f'Y_val shape: {Y_val.shape}')\n",
    "\n",
    "\n",
    "# X_train[1,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #only use for predicting a single feautre\n",
    "# Y_train = Y_train[:,:,1].unsqueeze(-1)\n",
    "# Y_val = Y_val[:,:,1].unsqueeze(-1)\n",
    "\n",
    "# print(f'Y_train shape: {Y_train.shape}')\n",
    "\n",
    "# print(f'Y_val shape: {Y_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_window=1\n",
    "# features = ['Knees Flexion-Extension Left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset length: 40589\n",
      "Val Dataset length: 5169\n",
      "Val_lt Dataset length: 3179\n"
     ]
    }
   ],
   "source": [
    "train_dataset = gaitDataset(X_train, Y_train)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle = True)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle = False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle = False)\n",
    "\n",
    "val_dataset = gaitDataset(X_val, Y_val) #ADJUSTED\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "val_lt_dataset = gaitDataset(X_val_lt, Y_val_lt) #long term predictions\n",
    "val_lt_dataloader = DataLoader(val_lt_dataset, batch_size=len(val_lt_dataset), shuffle=False)\n",
    "\n",
    "\n",
    "print(f\"Train Dataset length: {len(train_dataset)}\")\n",
    "print(f\"Val Dataset length: {len(val_dataset)}\")\n",
    "print(f\"Val_lt Dataset length: {len(val_lt_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40589"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Scalars for healthy and CP gait "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-30.96504974],\n",
       "       [ 63.01054001]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Export scalars for healthy gait \n",
    "# import pickle\n",
    "\n",
    "# fname = r'D:\\Study 2 Data\\Healthy Gait' + '\\\\' + 'healthy_scalars.pickle'\n",
    "\n",
    "# # healthy_scalars = scalars.copy()\n",
    "\n",
    "# #export scalars as pickle file \n",
    "# # with open(fname, 'wb') as handle:\n",
    "# #     pickle.dump(healthy_scalars, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # Import pickle file\n",
    "# with open(fname, 'rb') as handle:\n",
    "#     healthy_scalars = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Export scalars for CP gait \n",
    "# import pickle\n",
    "\n",
    "# fname = r'D:\\Study 2 Data\\CP Gait' + '\\\\' + 'cp_scalars.pickle'\n",
    "\n",
    "# # cp_scalars = scalars.copy()\n",
    "\n",
    "# # #export scalars as pickle file \n",
    "# # with open(fname, 'wb') as handle:\n",
    "# #     pickle.dump(cp_scalars, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # # Import pickle file\n",
    "# with open(fname, 'rb') as handle:\n",
    "#     cp_scalars = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # useful resources: https://www.youtube.com/watch?v=8A6TEjG2DNw (LSTM Time Series Prediction Tutorial using PyTorch in Python | Coronavirus Daily Cases Forecasting)\n",
    "# LSTM model \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, in_seq_len, out_seq_len, output_size, params, device):\n",
    "        super(LSTM, self).__init__()\n",
    "        '''\n",
    "        nn.lstm: \n",
    "        input_size = number of features in input, if batch_first = TRUE (batchSize, seq_len, Hin) FEATURES\n",
    "        hidden_size = number of features in hidden state\n",
    "        num_layers\n",
    "        batch_first = if TRUE (batch, seq, Hin) Hin may mean input features\n",
    "        h_0 = (D * num_layers, batchSize, Hout)\n",
    "        c_0 = (D * num_layers, batchSize, Hcell)\n",
    "\n",
    "        output = (batchSize, seq_len, D * Hout) Hout may mean output features, if batch first = TRUE \n",
    "        h_0 = (D*num_layers, N, Hout) final state for each element in the batch \n",
    "        C_0 = (D*num_layers, N, Hcell) final cell state for each element \n",
    "\n",
    "        nn.linear:\n",
    "        input_size = (N,*, Hin) Hin = input_features (equal to hidden size therefore is arbitrarily set)\n",
    "        output_size = (N, *, Hout) Hout = output features (equal to output_size passed in to the model) \n",
    "\n",
    "        '''\n",
    "        # Pytorch documentation: \n",
    "        # >>> rnn = nn.LSTM(10, 20, 2) features, hidden_size, number of layers \n",
    "        # >>> input = torch.randn(5, 3, 10) in_seq_len, batch_size, hidden_size (batch size should not be input to the mode, inferred from the shape of the input and will be the first number if batch_first = TRUE)\n",
    "        # >>> h0 = torch.randn(2, 3, 20)\n",
    "        # >>> c0 = torch.randn(2, 3, 20)\n",
    "        # >>> output, (hn, cn) = rnn(input, (h0, c0))\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = params['hidden_size']\n",
    "        self.num_layers = params['num_layers']\n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.output_size = output_size\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.device = device\n",
    "        \n",
    "        # nn.LSTM(features, hidden_size, number of layers)\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first = True)\n",
    "\n",
    "        #nn.fc1\n",
    "        self.fc1 = nn.Linear(in_features=self.hidden_size, out_features=(self.output_size * self.out_seq_len))\n",
    "        # self.fc1 = nn.Linear(in_features=self.hidden_size, out_features=(self.output_size * self.out_seq_len))\n",
    "\n",
    "\n",
    "    def forward(self, input_data):\n",
    "\n",
    "        # initialise hidden and cell states after passing through each batch (this is skipped in stateless model)\n",
    "        h_0 = torch.zeros(self.num_layers, input_data.shape[0], self.hidden_size).to(self.device) # input_data.shape[0] is equal to batch size\n",
    "        c_0 = torch.zeros(self.num_layers, input_data.shape[0], self.hidden_size).to(self.device)\n",
    "\n",
    "        #propagate through LSTM\n",
    "        lstm_out, (h_out, c_out) = self.lstm(input_data, (h_0, c_0))\n",
    "        # lsmt_out.shape = (batch_size,seq_length, hidden_size)\n",
    "        # print('lstm_out[-1][-1]')\n",
    "        # print(lstm_out[-1][-1])\n",
    "\n",
    "        # print('h_out[-1][-1]')\n",
    "        # print(h_out[-1][-1])\n",
    "        # if lstm_out[-1][-1] == h_out[-1][-1]:\n",
    "        #     print(lstm_out[-1][-1])\n",
    "            \n",
    "        # print(f'lsmt_out: {lstm_out.shape}')\n",
    "        # print(f'h_out: {h_out.shape}')\n",
    "        # print(f'h_out: {h_out.shape}')\n",
    "\n",
    "        # propagate through linear layer \n",
    "        fc1_out = self.fc1(h_out[-1])\n",
    "        # fc1_out = self.fc1(lstm_out)\n",
    "        # print(f'preds.shape (before reshaping): {fc1_out.shape}')\n",
    "        \n",
    "        preds = fc1_out.reshape(input_data.shape[0], self.out_seq_len, self.output_size)\n",
    "        # print(f'preds.shape (after reshaping): {preds.shape}')\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "global trial_number\n",
    "trial_number = 0\n",
    "trial_descrbn = 'LSTM CP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(model, train_dataloader, val_dataloader, num_epochs, params, device):\n",
    "    global trial_number\n",
    "    \n",
    "    MODEL_PATH= r'D:\\Study 2 Results and Models\\Study 2 Optimisation\\2022-07-10 Optimisation LSTM CP 2 (out 1)' + '\\\\'  + str(trial_descrbn) + '-' + str(date.today()) + '-' + 'trial' + str(trial_number) + '.pt'\n",
    "    loss_function = nn.MSELoss(reduction='mean')\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr = params['learning_rate'])\n",
    "\n",
    "    # train_loss, val_loss = np.zeros(num_epochs), np.zeros(num_epochs)\n",
    "    # num_epochs=params['num_epochs']\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    \n",
    "    min_val_loss = 1.\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Loop over batch values \n",
    "        runningLoss_train = 0. \n",
    "\n",
    "        for idx, (batch_inputs, batch_targets) in enumerate(train_dataloader):\n",
    "            \n",
    "            # Save batch on GPU\n",
    "            batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "\n",
    "            # Means we are training the model, so uses techniques such as dropout etc., otherwise model.train(model=False)\n",
    "            model.train() \n",
    "\n",
    "            #set gradients to zero\n",
    "            optimiser.zero_grad()\n",
    "            preds = model(batch_inputs)\n",
    "            # print(f'shape of training predictions: {preds.shape}')\n",
    "        \n",
    "            loss = loss_function(preds, batch_targets)\n",
    "            \n",
    "\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            runningLoss_train += loss.item()\n",
    "\n",
    "        train_loss[epoch] = runningLoss_train / len(train_dataloader)\n",
    "\n",
    "        # print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"Training loss: {runningLoss_train/len(train_dataloader)}\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "\n",
    "        model.eval() # means we are evaluating the model, stops process such as dropout etc. \n",
    "        runningLoss_val = 0.\n",
    "\n",
    "        with torch.no_grad(): # makes sure gradient is not stored \n",
    "            for idx, (batch_inputs, batch_targets) in enumerate(val_dataloader):\n",
    "                \n",
    "                batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "                optimiser.zero_grad() #WHY?\n",
    "                preds = model(batch_inputs)\n",
    "                # print(f'shape of validation predictions: {preds.shape}')\n",
    "\n",
    "                loss = loss_function(preds, batch_targets)\n",
    "                runningLoss_val += loss.item()\n",
    "\n",
    "        val_loss[epoch] = runningLoss_val/len(val_dataloader)\n",
    "\n",
    "        \n",
    "        if val_loss[epoch] < min_val_loss:\n",
    "\n",
    "            # print('YAY, new best value')\n",
    "            min_val_loss = val_loss[epoch]\n",
    "\n",
    "            torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimiser_state_dict': optimiser.state_dict(),\n",
    "                        'loss': val_loss[epoch],\n",
    "                        }, MODEL_PATH)\n",
    "                        \n",
    "\n",
    "        # print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"Validation loss: {runningLoss_val/len(val_dataloader)}\")\n",
    "\n",
    "    # global trial_number\n",
    "    trial_number+=1 #change file path value \n",
    "\n",
    "    return min_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=len(features)\n",
    "in_seq_len=input_window\n",
    "out_seq_len=output_window\n",
    "output_size=len(features)\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_LSTM(trial):\n",
    "\n",
    "    params = {\n",
    "            'learning_rate': trial.suggest_categorical('learning_rate', [0.1, 0.01, 0.001, 0.0001, 0.00001]),\n",
    "            'num_layers': trial.suggest_categorical('num_layers', [1,2,3,4]),\n",
    "            'hidden_size': trial.suggest_categorical('hidden_size', [16, 32, 64, 100, 128, 256])}\n",
    "        \n",
    "    #https://stackoverflow.com/questions/58820574/how-to-sample-parameters-without-duplicates-in-optuna\n",
    "    # Check duplication and skip if it's detected.\n",
    "    for t in trial.study.trials:\n",
    "        # print(t)\n",
    "        if t.state != optuna.structs.TrialState.COMPLETE:\n",
    "            continue\n",
    "\n",
    "        if t.params == trial.params:\n",
    "            global trial_number\n",
    "            trial_number += 1\n",
    "            return t.value\n",
    "            \n",
    "        \n",
    "            \n",
    "\n",
    "    model = LSTM(input_size=input_size, in_seq_len=in_seq_len, out_seq_len=out_seq_len, output_size=output_size, params = params, device=DEVICE).to(DEVICE)\n",
    "    val_loss = train_LSTM(model, train_dataloader, val_dataloader, num_epochs, params, device=DEVICE)\n",
    "\n",
    "\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler())\n",
    "# study.optimize(objective_LSTM, n_trials=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load all models from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'D:\\\\Study 2 Results and Models\\\\Test-example'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28016/3211717992.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'D:\\Study 2 Results and Models\\Test-example'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Changes the working directory to get the data from their location\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'D:\\\\Study 2 Results and Models\\\\Test-example'"
     ]
    }
   ],
   "source": [
    "file_path = r'D:\\Study 2 Results and Models\\Test-example'\n",
    "\n",
    "model_file = os.listdir(file_path) \n",
    "\n",
    "# Changes the working directory to get the data from their location \n",
    "os.chdir(file_path)\n",
    "cwd = os.getcwd()\n",
    "\n",
    "print(f'Current working directory is: {cwd}')\n",
    "print(f\"There are {len(model_file)} files in the specified path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28016/3946434471.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mbest_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mbest_val_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_file' is not defined"
     ]
    }
   ],
   "source": [
    "for f in model_file:\n",
    "    checkpoint = torch.load(f)\n",
    "    best_epoch = checkpoint['epoch']\n",
    "    best_val_loss = checkpoint['loss']\n",
    "\n",
    "    print(f'For file {f}: best_epoch: {best_epoch}, best_val_loss: {best_val_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EarlyStopping Code from: https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "\n",
    "# class EarlyStopping:\n",
    "#     \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "#     def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             patience (int): How long to wait after last time validation loss improved.\n",
    "#                             Default: 7\n",
    "#             verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "#                             Default: False\n",
    "#             delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "#                             Default: 0\n",
    "#             path (str): Path for the checkpoint to be saved to.\n",
    "#                             Default: 'checkpoint.pt'\n",
    "#             trace_func (function): trace print function.\n",
    "#                             Default: print            \n",
    "#         \"\"\"\n",
    "#         self.patience = patience\n",
    "#         self.verbose = verbose\n",
    "#         self.counter = 0\n",
    "#         self.best_score = None\n",
    "#         self.early_stop = False\n",
    "#         self.val_loss_min = np.Inf\n",
    "#         self.delta = delta\n",
    "#         self.path = path\n",
    "#         self.trace_func = trace_func\n",
    "        \n",
    "#     def __call__(self, val_loss, model):\n",
    "\n",
    "#         score = -val_loss\n",
    "\n",
    "#         if self.best_score is None:\n",
    "#             self.best_score = score\n",
    "#             self.save_checkpoint(val_loss, model)\n",
    "#         elif score < self.best_score + self.delta:\n",
    "#             self.counter += 1\n",
    "#             self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "#             if self.counter >= self.patience:\n",
    "#                 self.early_stop = True\n",
    "#         else:\n",
    "#             self.best_score = score\n",
    "#             self.save_checkpoint(val_loss, model)\n",
    "#             self.counter = 0\n",
    "\n",
    "#     def save_checkpoint(self, val_loss, model):\n",
    "#         '''Saves model when validation loss decrease.'''\n",
    "#         if self.verbose:\n",
    "#             self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "#         torch.save(model.state_dict(), self.path)\n",
    "#         self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'LSTM' \n",
    "exp_ID = '208' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH =   r'D:\\Study 2 Results and Models\\Study 2 Model Checkpoints' + '\\\\'  + 'Exp' + str(exp_ID) + '-' + str(date.today()) + '-' + model + '-In' + str(input_window) + '-Out' + str(output_window) + '.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Study 2 Results and Models\\\\Study 2 Model Checkpoints\\\\Exp208-2022-08-19-LSTM-In100-Out1.pt'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # useful resources: https://www.youtube.com/watch?v=8A6TEjG2DNw (LSTM Time Series Prediction Tutorial using PyTorch in Python | Coronavirus Daily Cases Forecasting)\n",
    "# # ORIGINAL\n",
    "# # LSTM model \n",
    "# class LSTM(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, in_seq_len, out_seq_len, output_size, device):\n",
    "#         super(LSTM, self).__init__()\n",
    "#         '''\n",
    "#         nn.lstm: \n",
    "#         input_size = number of features in input, if batch_first = TRUE (batchSize, seq_len, Hin) FEATURES\n",
    "#         hidden_size = number of features in hidden state\n",
    "#         num_layers\n",
    "#         batch_first = if TRUE (batch, seq, Hin) Hin may mean input features\n",
    "#         h_0 = (D * num_layers, batchSize, Hout)\n",
    "#         c_0 = (D * num_layers, batchSize, Hcell)\n",
    "\n",
    "#         output = (batchSize, seq_len, D * Hout) Hout may mean output features, if batch first = TRUE \n",
    "#         h_0 = (D*num_layers, N, Hout) final state for each element in the batch \n",
    "#         C_0 = (D*num_layers, N, Hcell) final cell state for each element \n",
    "\n",
    "#         nn.linear:\n",
    "#         input_size = (N,*, Hin) Hin = input_features (equal to hidden size therefore is arbitrarily set)\n",
    "#         output_size = (N, *, Hout) Hout = output features (equal to output_size passed in to the model) \n",
    "\n",
    "#         '''\n",
    "#         # Pytorch documentation: \n",
    "#         # >>> rnn = nn.LSTM(10, 20, 2) features, hidden_size, number of layers \n",
    "#         # >>> input = torch.randn(5, 3, 10) in_seq_len, batch_size, hidden_size (batch size should not be input to the mode, inferred from the shape of the input and will be the first number if batch_first = TRUE)\n",
    "#         # >>> h0 = torch.randn(2, 3, 20)\n",
    "#         # >>> c0 = torch.randn(2, 3, 20)\n",
    "#         # >>> output, (hn, cn) = rnn(input, (h0, c0))\n",
    "\n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.in_seq_len = in_seq_len\n",
    "#         self.output_size = output_size\n",
    "#         self.out_seq_len = out_seq_len\n",
    "#         self.device = device\n",
    "        \n",
    "#         # nn.LSTM(features, hidden_size, number of layers)\n",
    "#         self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, dropout=0.1, batch_first = True)\n",
    "\n",
    "\n",
    "#         #nn.fc1\n",
    "#         # self.fc1 = nn.Linear(in_features=self.hidden_size, out_features=80)\n",
    "#         self.fc1 = nn.Linear(in_features=21, out_features=(self.output_size * self.out_seq_len))\n",
    "\n",
    "\n",
    "#     def forward(self, input_data):\n",
    "#         # print(f'input_data.shape: {input_data.shape}')\n",
    "#         # initialise hidden and cell states after passing through each batch (this is skipped in stateless model)\n",
    "#         h_0 = torch.zeros(self.num_layers, input_data.shape[0], self.hidden_size).to(self.device) # input_data.shape[0] is equal to batch size\n",
    "#         c_0 = torch.zeros(self.num_layers, input_data.shape[0], self.hidden_size).to(self.device)\n",
    "\n",
    "#         #propagate through LSTM\n",
    "#         lstm_out, (h_out, c_out) = self.lstm(input_data, (h_0, c_0))\n",
    "\n",
    "#         # lsmt_out.shape = (batch_size,seq_length, hidden_size)\n",
    "#         # print('lstm_out[-1][-1]')\n",
    "#         # print(lstm_out[-1][-1])\n",
    "\n",
    "#         # print('h_out[-1][-1]')\n",
    "#         # print(h_out[-1][-1])\n",
    "#         # if lstm_out[-1][-1] == h_out[-1][-1]:\n",
    "#         #     print(lstm_out[-1][-1])\n",
    "            \n",
    "#         # print(f'lsmt_out: {lstm_out.shape}')\n",
    "#         # print(f'h_out: {h_out.shape}')\n",
    "#         # print(f'h_out[-1]: {h_out[-1].shape}')\n",
    "\n",
    "#         h_out = h_out.permute(1,0,2).reshape(input_data.shape[0], -1)\n",
    "#         # print(f'h_out a/f newlook: {h_out.shape}')\n",
    "#         # propagate through linear layer \n",
    "#         fc1_out = self.fc1(h_out)\n",
    "#         # print(f'fc1_out: {fc1_out.shape}')\n",
    "\n",
    "#         # fc2_out = self.fc2(fc1_out)\n",
    "#         # print(f'preds.shape (before reshaping): {fc1_out.shape}')\n",
    "        \n",
    "#         preds = fc1_out.reshape(input_data.shape[0], self.out_seq_len, self.output_size)\n",
    "#         # print(f'preds.shape (after reshaping): {preds.shape}')\n",
    "        \n",
    "#         return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://github.com/QData/spacetimeformer/blob/20ab0b572b6f087964aad8d9f89a886b736bf432/spacetimeformer/time2vec.py \n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Time2Vec(nn.Module):\n",
    "    def __init__(self, input_dim=len(features), embed_dim=512, act_function=torch.sin):\n",
    "        assert embed_dim % input_dim == 0\n",
    "        super(Time2Vec, self).__init__()\n",
    "        self.enabled = embed_dim > 0\n",
    "        if self.enabled:\n",
    "            self.embed_dim = embed_dim // input_dim\n",
    "            self.input_dim = input_dim\n",
    "            self.embed_weight = nn.parameter.Parameter(\n",
    "                torch.randn(self.input_dim, self.embed_dim)\n",
    "            )\n",
    "            self.embed_bias = nn.parameter.Parameter(\n",
    "                torch.randn(self.input_dim, self.embed_dim)\n",
    "            )\n",
    "            self.act_function = act_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.enabled:\n",
    "            x = torch.diag_embed(x)\n",
    "            # x.shape = (bs, sequence_length, input_dim, input_dim)\n",
    "            x_affine = torch.matmul(x, self.embed_weight) + self.embed_bias\n",
    "            # x_affine.shape = (bs, sequence_length, input_dim, time_embed_dim)\n",
    "            x_affine_0, x_affine_remain = torch.split(\n",
    "                x_affine, [1, self.embed_dim - 1], dim=-1\n",
    "            )\n",
    "            x_affine_remain = self.act_function(x_affine_remain)\n",
    "            x_output = torch.cat([x_affine_0, x_affine_remain], dim=-1)\n",
    "            x_output = x_output.view(x_output.size(0), x_output.size(1), -1)\n",
    "            # x_output.shape = (bs, sequence_length, input_dim * time_embed_dim)\n",
    "        else:\n",
    "            x_output = x\n",
    "        return x_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, in_seq_len, out_seq_len, output_size, device):\n",
    "        super(LSTM, self).__init__()\n",
    "        '''\n",
    "        nn.lstm: \n",
    "        input_size = number of features in input, if batch_first = TRUE (batchSize, seq_len, Hin) FEATURES\n",
    "        hidden_size = number of features in hidden state\n",
    "        num_layers\n",
    "        batch_first = if TRUE (batch, seq, Hin) Hin may mean input features\n",
    "        h_0 = (D * num_layers, batchSize, Hout)\n",
    "        c_0 = (D * num_layers, batchSize, Hcell)\n",
    "\n",
    "        output = (batchSize, seq_len, D * Hout) Hout may mean output features, if batch first = TRUE \n",
    "        h_0 = (D*num_layers, N, Hout) final state for each element in the batch \n",
    "        C_0 = (D*num_layers, N, Hcell) final cell state for each element \n",
    "\n",
    "        nn.linear:\n",
    "        input_size = (N,*, Hin) Hin = input_features (equal to hidden size therefore is arbitrarily set)\n",
    "        output_size = (N, *, Hout) Hout = output features (equal to output_size passed in to the model) \n",
    "\n",
    "        '''\n",
    "        # Pytorch documentation: \n",
    "        # >>> rnn = nn.LSTM(10, 20, 2) features, hidden_size, number of layers \n",
    "        # >>> input = torch.randn(5, 3, 10) in_seq_len, batch_size, hidden_size (batch size should not be input to the mode, inferred from the shape of the input and will be the first number if batch_first = TRUE)\n",
    "        # >>> h0 = torch.randn(2, 3, 20)\n",
    "        # >>> c0 = torch.randn(2, 3, 20)\n",
    "        # >>> output, (hn, cn) = rnn(input, (h0, c0))\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.output_size = output_size\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.device = device\n",
    "        \n",
    "        # nn.LSTM(features, hidden_size, number of layers)\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first = True)\n",
    "\n",
    "\n",
    "        #nn.fc1\n",
    "        # self.fc1 = nn.Linear(in_features=self.hidden_size, out_features=80)\n",
    "        self.fc1 = nn.Linear(in_features=200, out_features=(self.output_size * self.out_seq_len))\n",
    "\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # print(f'input_data.shape: {input_data.shape}')\n",
    "        # initialise hidden and cell states after passing through each batch (this is skipped in stateless model)\n",
    "        h_0 = torch.zeros(self.num_layers, input_data.shape[0], self.hidden_size).to(self.device) # input_data.shape[0] is equal to batch size\n",
    "        c_0 = torch.zeros(self.num_layers, input_data.shape[0], self.hidden_size).to(self.device)\n",
    "\n",
    "        #propagate through LSTM\n",
    "        lstm_out, (h_out, c_out) = self.lstm(input_data, (h_0, c_0))\n",
    "\n",
    "        # lsmt_out.shape = (batch_size,seq_length, hidden_size)\n",
    "        # print('lstm_out[-1][-1]')\n",
    "        # print(lstm_out[-1][-1])\n",
    "\n",
    "        # print('h_out[-1][-1]')\n",
    "        # print(h_out[-1][-1])\n",
    "        # if lstm_out[-1][-1] == h_out[-1][-1]:\n",
    "        #     print(lstm_out[-1][-1])\n",
    "            \n",
    "        # print(f'lsmt_out: {lstm_out.shape}')\n",
    "        # print(f'h_out: {h_out.shape}')\n",
    "        # print(f'h_out[-1]: {h_out[-1].shape}')\n",
    "\n",
    "        h_out = h_out.permute(1,0,2).reshape(input_data.shape[0], -1)\n",
    "        # print(f'h_out a/f newlook: {h_out.shape}')\n",
    "        # propagate through linear layer \n",
    "        # fc1_out = self.fc1(h_out[-1])\n",
    "        fc1_out = self.fc1(h_out)\n",
    "\n",
    "        # print(f'fc1_out: {fc1_out.shape}')\n",
    "\n",
    "        # fc2_out = self.fc2(fc1_out)\n",
    "        # print(f'preds.shape (before reshaping): {fc1_out.shape}')\n",
    "        \n",
    "        preds = fc1_out.reshape(input_data.shape[0], self.out_seq_len, self.output_size)\n",
    "        # print(f'preds.shape (after reshaping): {preds.shape}')\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # useful resources: https://www.youtube.com/watch?v=8A6TEjG2DNw (LSTM Time Series Prediction Tutorial using PyTorch in Python | Coronavirus Daily Cases Forecasting)\n",
    "# LSTM model with t2v\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, in_seq_len, out_seq_len, output_size, device):\n",
    "        super(LSTM, self).__init__()\n",
    "        '''\n",
    "        nn.lstm: \n",
    "        input_size = number of features in input, if batch_first = TRUE (batchSize, seq_len, Hin) FEATURES\n",
    "        hidden_size = number of features in hidden state\n",
    "        num_layers\n",
    "        batch_first = if TRUE (batch, seq, Hin) Hin may mean input features\n",
    "        h_0 = (D * num_layers, batchSize, Hout)\n",
    "        c_0 = (D * num_layers, batchSize, Hcell)\n",
    "\n",
    "        output = (batchSize, seq_len, D * Hout) Hout may mean output features, if batch first = TRUE \n",
    "        h_0 = (D*num_layers, N, Hout) final state for each element in the batch \n",
    "        C_0 = (D*num_layers, N, Hcell) final cell state for each element \n",
    "\n",
    "        nn.linear:\n",
    "        input_size = (N,*, Hin) Hin = input_features (equal to hidden size therefore is arbitrarily set)\n",
    "        output_size = (N, *, Hout) Hout = output features (equal to output_size passed in to the model) \n",
    "\n",
    "        '''\n",
    "        # Pytorch documentation: \n",
    "        # >>> rnn = nn.LSTM(10, 20, 2) features, hidden_size, number of layers \n",
    "        # >>> input = torch.randn(5, 3, 10) in_seq_len, batch_size, hidden_size (batch size should not be input to the mode, inferred from the shape of the input and will be the first number if batch_first = TRUE)\n",
    "        # >>> h0 = torch.randn(2, 3, 20)\n",
    "        # >>> c0 = torch.randn(2, 3, 20)\n",
    "        # >>> output, (hn, cn) = rnn(input, (h0, c0))\n",
    "\n",
    "        # self.embed_dim = 300\n",
    "        # self.t2v = Time2Vec(input_dim=len(features), embed_dim=self.embed_dim, act_function=torch.sin)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.output_size = output_size\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.device = device\n",
    "\n",
    "        \n",
    "        # nn.LSTM(features, hidden_size, number of layers)\n",
    "        self.lstm = nn.LSTM(input_size=100, hidden_size=self.hidden_size, batch_first=True)\n",
    "\n",
    "\n",
    "        #nn.fc1\n",
    "        # self.fc1 = nn.Linear(in_features=self.hidden_size, out_features=80)\n",
    "        self.fc1 = nn.Linear(in_features=self.hidden_size, out_features=(self.output_size * self.out_seq_len))\n",
    "\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # print(f'input_data.shape: {input_data.shape}')\n",
    "        # initialise hidden and cell states after passing through each batch (this is skipped in stateless model)\n",
    "\n",
    "        # affine_data = self.t2v(input_data)\n",
    "\n",
    "        # print(f'affine_data.shape: {affine_data.shape}')\n",
    "\n",
    "\n",
    "        # h_0 = torch.zeros(self.num_layers, input_data.shape[0], self.hidden_size).to(self.device).double() # input_data.shape[0] is equal to batch size\n",
    "        # c_0 = torch.zeros(self.num_layers, input_data.shape[0], self.hidden_size).to(self.device).double()\n",
    "\n",
    "        # print(f'shape input: {input_data.shape}')\n",
    "        #propagate through LSTM\n",
    "        lstm_out,  _ = self.lstm(input_data.permute(0,2,1))\n",
    "\n",
    "        # print(f'lsmt_out shape: {lstm_out.shape}')\n",
    "\n",
    "        # h_out = h_out.permute(1,0,2).reshape(input_data.shape[0], -1)\n",
    "\n",
    "        # print(f'h_out shape a/f reshape: {h_out.shape}')\n",
    "\n",
    "        fc1_out = self.fc1(lstm_out[:,-1,:])\n",
    "\n",
    "        # print(f'fc1_out shape: {fc1_out.shape}')\n",
    "        \n",
    "        preds = fc1_out.reshape(input_data.shape[0], self.out_seq_len, self.output_size)\n",
    "\n",
    "        # print(f'preds shape: {preds.shape}')\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, in_seq_len, out_seq_len, output_size, device):\n",
    "        super(GRU, self).__init__()\n",
    "        '''\n",
    "        nn.lstm: \n",
    "        input_size = number of features in input, if batch_first = TRUE (batchSize, seq_len, Hin) FEATURES\n",
    "        hidden_size = number of features in hidden state\n",
    "        num_layers\n",
    "        batch_first = if TRUE (batch, seq, Hin) Hin may mean input features\n",
    "        h_0 = (D * num_layers, batchSize, Hout)\n",
    "        c_0 = (D * num_layers, batchSize, Hcell)\n",
    "\n",
    "        output = (batchSize, seq_len, D * Hout) Hout may mean output features, if batch first = TRUE \n",
    "        h_0 = (D*num_layers, N, Hout) final state for each element in the batch \n",
    "        C_0 = (D*num_layers, N, Hcell) final cell state for each element \n",
    "\n",
    "        nn.linear:\n",
    "        input_size = (N,*, Hin) Hin = input_features (equal to hidden size therefore is arbitrarily set)\n",
    "        output_size = (N, *, Hout) Hout = output features (equal to output_size passed in to the model) \n",
    "\n",
    "        '''\n",
    "        # Pytorch documentation: \n",
    "        # >>> rnn = nn.LSTM(10, 20, 2) features, hidden_size, number of layers \n",
    "        # >>> input = torch.randn(5, 3, 10) in_seq_len, batch_size, hidden_size (batch size should not be input to the mode, inferred from the shape of the input and will be the first number if batch_first = TRUE)\n",
    "        # >>> h0 = torch.randn(2, 3, 20)\n",
    "        # >>> c0 = torch.randn(2, 3, 20)\n",
    "        # >>> output, (hn, cn) = rnn(input, (h0, c0))\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.output_size = output_size\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.device = device\n",
    "        \n",
    "        # nn.LSTM(features, hidden_size, number of layers)\n",
    "        # self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, dropout=0.1, batch_first = True)\n",
    "        self.GRU = nn.GRU(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first = True, dropout=0.1)\n",
    "\n",
    "\n",
    "        #nn.fc1\n",
    "        # self.fc1 = nn.Linear(in_features=self.hidden_size, out_features=80)\n",
    "        self.fc1 = nn.Linear(in_features=7, out_features = 120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=(self.output_size * self.out_seq_len))\n",
    "\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # print(f'input_data.shape: {input_data.shape}')\n",
    "        # initialise hidden and cell states after passing through each batch (this is skipped in stateless model)\n",
    "        h_0 = torch.zeros(self.num_layers, input_data.shape[0], self.hidden_size).to(self.device) # input_data.shape[0] is equal to batch size\n",
    "\n",
    "        lstm_out, h_out = self.GRU(input_data, h_0)\n",
    "\n",
    "        # lsmt_out.shape = (batch_size,seq_length, hidden_size)\n",
    "        # print('lstm_out[-1][-1]')\n",
    "        # print(lstm_out[-1][-1])\n",
    "\n",
    "        # print('h_out[-1][-1]')\n",
    "        # print(h_out[-1][-1])\n",
    "        # if lstm_out[-1][-1] == h_out[-1][-1]:\n",
    "        #     print(lstm_out[-1][-1])\n",
    "            \n",
    "        # print(f'lsmt_out: {lstm_out.shape}')\n",
    "        # print(f'h_out: {h_out.shape}')\n",
    "        # print(f'h_out[-1]: {h_out[-1].shape}')\n",
    "\n",
    "        # h_out = h_out.permute(1,0,2).reshape(input_data.shape[0], -1)\n",
    "        # print(f'h_out a/f newlook: {h_out.shape}')\n",
    "        # propagate through linear layer \n",
    "        fc1_out = self.fc1(h_out[-1])\n",
    "        # print(f'fc1_out: {fc1_out.shape}')\n",
    "\n",
    "        # fc2_out = self.fc2(fc1_out)\n",
    "        # print(f'preds.shape (before reshaping): {fc1_out.shape}')\n",
    "        fc2_out = self.fc2(fc1_out)\n",
    "        preds = fc2_out.reshape(input_data.shape[0], self.out_seq_len, self.output_size)\n",
    "        # print(f'preds.shape (after reshaping): {preds.shape}')\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #code from https://github.com/ojus1/Time2Vec-PyTorch/blob/master/periodic_activations.py\n",
    "\n",
    "# def t2v(tau, f, out_features, w, b, w0, b0, arg=None):\n",
    "#     if arg:\n",
    "#         v1 = f(torch.matmul(tau, w) + b, arg)\n",
    "#     else:\n",
    "#         #print(w.shape, t1.shape, b.shape)\n",
    "#         v1 = f(torch.matmul(tau, w) + b)\n",
    "#     v2 = torch.matmul(tau, w0) + b0\n",
    "#     #print(v1.shape)\n",
    "#     return torch.cat([v1, v2], 1)\n",
    "\n",
    "# class SineActivation(nn.Module):\n",
    "#     def __init__(self, in_features, out_features):\n",
    "#         super(SineActivation, self).__init__()\n",
    "#         self.out_features = out_features\n",
    "#         self.w0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n",
    "#         self.b0 = nn.parameter.Parameter(torch.randn(1))\n",
    "#         self.w = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n",
    "#         self.b = nn.parameter.Parameter(torch.randn(out_features-1))\n",
    "#         self.f = torch.sin\n",
    "\n",
    "#     def forward(self, tau):\n",
    "#         return t2v(tau, self.f, self.out_features, self.w, self.b, self.w0, self.b0)\n",
    "\n",
    "# class CosineActivation(nn.Module):\n",
    "#     def __init__(self, in_features, out_features):\n",
    "#         super(CosineActivation, self).__init__()\n",
    "#         self.out_features = out_features\n",
    "#         self.w0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n",
    "#         self.b0 = nn.parameter.Parameter(torch.randn(1))\n",
    "#         self.w = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n",
    "#         self.b = nn.parameter.Parameter(torch.randn(out_features-1))\n",
    "#         self.f = torch.cos\n",
    "\n",
    "#     def forward(self, tau):\n",
    "#         return t2v(tau, self.f, self.out_features, self.w, self.b, self.w0, self.b0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # useful resources: https://www.youtube.com/watch?v=8A6TEjG2DNw (LSTM Time Series Prediction Tutorial using PyTorch in Python | Coronavirus Daily Cases Forecasting)\n",
    "# # LSTM model \n",
    "# class Attention_LSTM(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, in_seq_len, out_seq_len, output_size, device):\n",
    "#         super(Attention_LSTM, self).__init__()\n",
    "#         '''\n",
    "#         nn.lstm: \n",
    "#         input_size = number of features in input, if batch_first = TRUE (batchSize, seq_len, Hin) FEATURES\n",
    "#         hidden_size = number of features in hidden state\n",
    "#         num_layers\n",
    "#         batch_first = if TRUE (batch, seq, Hin) Hin may mean input features\n",
    "#         h_0 = (D * num_layers, batchSize, Hout)\n",
    "#         c_0 = (D * num_layers, batchSize, Hcell)\n",
    "\n",
    "#         output = (batchSize, seq_len, D * Hout) Hout may mean output features, if batch first = TRUE \n",
    "#         h_0 = (D*num_layers, N, Hout) final state for each element in the batch \n",
    "#         C_0 = (D*num_layers, N, Hcell) final cell state for each element \n",
    "\n",
    "#         nn.linear:\n",
    "#         input_size = (N,*, Hin) Hin = input_features (equal to hidden size therefore is arbitrarily set)\n",
    "#         output_size = (N, *, Hout) Hout = output features (equal to output_size passed in to the model) \n",
    "\n",
    "#         '''\n",
    "#         # Pytorch documentation: \n",
    "#         # >>> rnn = nn.LSTM(10, 20, 2) features, hidden_size, number of layers \n",
    "#         # >>> input = torch.randn(5, 3, 10) in_seq_len, batch_size, hidden_size (batch size should not be input to the mode, inferred from the shape of the input and will be the first number if batch_first = TRUE)\n",
    "#         # >>> h0 = torch.randn(2, 3, 20)\n",
    "#         # >>> c0 = torch.randn(2, 3, 20)\n",
    "#         # >>> output, (hn, cn) = rnn(input, (h0, c0))\n",
    "\n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.in_seq_len = in_seq_len\n",
    "#         self.output_size = output_size\n",
    "#         self.out_seq_len = out_seq_len\n",
    "#         self.device = device\n",
    "        \n",
    "#         # nn.LSTM(features, hidden_size, number of layers)\n",
    "#         self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first = True)\n",
    "\n",
    "#         #nn.fc1\n",
    "#         self.fc1 = nn.Linear(in_features=self.hidden_size, out_features=(self.output_size * self.out_seq_len))\n",
    "        \n",
    "#         # self.fc1 = nn.Linear(in_features=self.hidden_size, out_features=(self.output_size * self.out_seq_len))\n",
    "\n",
    "\n",
    "#     def forward(self, input_data):\n",
    "\n",
    "#         # initialise hidden and cell states after passing through each batch (this is skipped in stateless model)\n",
    "#         h_0 = torch.zeros(self.num_layers, input_data.shape[0], self.hidden_size).to(self.device) # input_data.shape[0] is equal to batch size\n",
    "\n",
    "#         c_0 = torch.zeros(self.num_layers, input_data.shape[0], self.hidden_size).to(self.device)\n",
    "\n",
    "#         #propagate through LSTM\n",
    "#         lstm_out, (h_out, c_out) = self.lstm(input_data, (h_0, c_0))\n",
    "#         # lsmt_out.shape = (batch_size,seq_length, hidden_size)\n",
    "#         # print('lstm_out[-1][-1]')\n",
    "#         # print(lstm_out[-1][-1])\n",
    "\n",
    "#         # print('h_out[-1][-1]')\n",
    "#         # print(h_out[-1][-1])\n",
    "#         # if lstm_out[-1][-1] == h_out[-1][-1]:\n",
    "#         #     print(lstm_out[-1][-1])\n",
    "            \n",
    "#         # print(f'lsmt_out: {lstm_out.shape}')\n",
    "#         # print(f'h_out: {h_out.shape}')\n",
    "#         # print(f'h_out[-1]: {h_out[-1].shape}')\n",
    "\n",
    "#         # propagate through linear layer \n",
    "#         fc1_out = self.fc1(h_out[-1])\n",
    "    \n",
    "#         # fc1_out = self.fc1(lstm_out)\n",
    "#         # print(f'preds.shape (before reshaping): {fc1_out.shape}')\n",
    "        \n",
    "#         preds = fc1_out.reshape(input_data.shape[0], self.out_seq_len, self.output_size)\n",
    "#         # print(f'preds.shape (after reshaping): {preds.shape}')\n",
    "        \n",
    "#         return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the LSTM model using a loss function and a optimiser\n",
    "#ORIGINAL\n",
    "def train_LSTM(model, train_dataloader, val_dataloader, val_lt_dataloader, num_epochs, learning_rate, device):\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    # train_loss, val_loss = np.zeros(num_epochs), np.zeros(num_epochs)\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    lt_loss = np.zeros(num_epochs)\n",
    "\n",
    "    min_val_loss = 1.\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Loop over batch values \n",
    "        runningLoss_train = 0. \n",
    "\n",
    "        for idx, (batch_inputs, batch_targets) in enumerate(train_dataloader):\n",
    "            \n",
    "            # Save batch on GPU\n",
    "            batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "\n",
    "            # Means we are training the model, so uses techniques such as dropout etc., otherwise model.train(model=False)\n",
    "            model.train() \n",
    "\n",
    "            #set gradients to zero\n",
    "            optimiser.zero_grad()\n",
    "            preds = model(batch_inputs)\n",
    "            # print(f'shape of training predictions: {preds.shape}')\n",
    "        \n",
    "            loss = loss_function(preds, batch_targets)\n",
    "            \n",
    "\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            runningLoss_train += loss.item()\n",
    "\n",
    "        train_loss[epoch] = runningLoss_train / len(train_dataloader)\n",
    "\n",
    "        print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"Training loss: {runningLoss_train/len(train_dataloader)}\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "\n",
    "        model.eval() # means we are evaluating the model, stops process such as dropout etc. \n",
    "        runningLoss_val = 0.\n",
    "\n",
    "        with torch.no_grad(): # makes sure gradient is not stored \n",
    "            for idx, (batch_inputs, batch_targets) in enumerate(val_dataloader):\n",
    "                \n",
    "                batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "                optimiser.zero_grad() #WHY?\n",
    "                preds = model(batch_inputs)\n",
    "                # print(f'shape of validation predictions: {preds.shape}')\n",
    "\n",
    "                loss = loss_function(preds, batch_targets)\n",
    "                runningLoss_val += loss.item()\n",
    "\n",
    "        val_loss[epoch] = runningLoss_val/len(val_dataloader)\n",
    "\n",
    "\n",
    "        print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"Validation loss: {runningLoss_val/len(val_dataloader)}\")\n",
    "\n",
    "\n",
    "        #Measure long term error \n",
    "        lt_loss_criterion = nn.L1Loss() #long term loss criterion (l1 loss measures MAE for 99 steps in the future)\n",
    "        future_window=200\n",
    "        runningLoss_val_lt = 0.\n",
    "\n",
    "        with torch.no_grad(): # makes sure gradient is not stored \n",
    "            for idx, (batch_inputs, batch_targets) in enumerate(val_lt_dataloader):\n",
    "\n",
    "                batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "                \n",
    "                future_preds = torch.zeros((batch_inputs.shape[0], 200, len(features))).to(DEVICE) #would store the predicted outputs for 200 timesteps\n",
    "                \n",
    "                # loop to calculate all 200 preds in the future\n",
    "                for t in range(0,future_window):\n",
    "                    optimiser.zero_grad()\n",
    "                    preds = model(batch_inputs)\n",
    "                    # print(f'preds.shape: {preds.shape}')\n",
    "                    # print(f'future_preds[:,t,:].shape: {future_preds[:,t,:].shape}')\n",
    "                    future_preds[:,t,:] = preds.squeeze()\n",
    "                    new_batch_inputs = torch.cat((batch_inputs, preds),1)\n",
    "                    batch_inputs = new_batch_inputs[:,1:,:]\n",
    "                \n",
    "                #Evalute long term predictions\n",
    "                # print(f'future_preds[101:].shape: {future_preds[:,101:,:].shape}')\n",
    "                loss = lt_loss_criterion(future_preds[:,101:,:], batch_targets[:,101:,:])\n",
    "                runningLoss_val_lt += loss.item()\n",
    "\n",
    "        lt_loss[epoch] = runningLoss_val_lt/len(val_lt_dataloader)\n",
    "\n",
    "\n",
    "        print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"Long term Validation loss: {runningLoss_val_lt/len(val_lt_dataloader)}\")\n",
    "\n",
    "\n",
    "        # print(f'val loss: {val_loss[epoch]}')\n",
    "\n",
    "        # print(f'min val loss: {min_val_loss}')\n",
    "        # Save best model so far\n",
    "        if val_loss[epoch] < min_val_loss:\n",
    "\n",
    "            # print('YAY, new best value')\n",
    "            min_val_loss = val_loss[epoch]\n",
    "\n",
    "            torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimiser_state_dict': optimiser.state_dict(),\n",
    "                        'loss': val_loss[epoch],\n",
    "                        }, MODEL_PATH)\n",
    "                        \n",
    "\n",
    "    return train_loss, val_loss, lt_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing function \n",
    "def test_LSTM(model, dataloader, device):\n",
    "    loss_function = nn.MSELoss()\n",
    "    model.eval()\n",
    "    actual_output, pred_output = [], []\n",
    "    running_loss = 0. \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (batch_inputs, batch_targets) in tqdm(enumerate(dataloader)):\n",
    "\n",
    "            batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "\n",
    "            # if idx==0:\n",
    "            #     batch_preds = model(batch_inputs)\n",
    "            #     # print(f'batch shape: {batch_preds.shape}')\n",
    "            #     loss = loss_function(batch_preds, batch_targets)\n",
    "            #     running_loss += loss.item()\n",
    "            #     current_preds = batch_preds\n",
    "            #     all_preds = batch_preds\n",
    "\n",
    "            # else:\n",
    "            #     batch_preds = model(batch_inputs)\n",
    "            #     print(f'batch shape: {batch_preds.shape}')\n",
    "            #     loss = loss_function(batch_preds, batch_targets)\n",
    "            #     running_loss += loss.item()\n",
    "            #     all_preds = torch.cat((current_preds, batch_preds), dim=0)\n",
    "            #     current_preds = batch_preds\n",
    "\n",
    "            batch_preds = model(batch_inputs)\n",
    "            loss = loss_function(batch_preds, batch_targets)\n",
    "            running_loss += loss.item()\n",
    "            actual_output.append(batch_targets)\n",
    "            pred_output.append(batch_preds)\n",
    "\n",
    "\n",
    "            #             lst = []\n",
    "            # print(f'{x.size()}')\n",
    "            # for i in range(10):\n",
    "            #     x += i  # say we do something with x at iteration i\n",
    "            #     lst.append(x)\n",
    "            # # lstt = torch.stack([x for _ in range(10)])\n",
    "            # lstt = torch.stack(lst)\n",
    "            # print(lstt.size())\n",
    "\n",
    "        total_loss = running_loss / len(dataloader)\n",
    "\n",
    "        actual_output_tensor = torch.vstack(actual_output)\n",
    "        pred_output_tensor = torch.vstack(pred_output)\n",
    "    \n",
    "    return pred_output_tensor, actual_output_tensor, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b5675e81e344238f6ee32b6c01a9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10000] Training loss: 0.08393565964130108 / Validation loss: 0.20006610534276636 / Long term Validation loss: 0.2196\n",
      "Epoch: [2/10000] Training loss: 0.07966620172164554 / Validation loss: 0.1938849614786868 / Long term Validation loss: 0.2142\n",
      "Epoch: [3/10000] Training loss: 0.07536744037559284 / Validation loss: 0.18744765423778 / Long term Validation loss: 0.2085\n",
      "Epoch: [4/10000] Training loss: 0.07105085918911215 / Validation loss: 0.1807586111237025 / Long term Validation loss: 0.2025\n",
      "Epoch: [5/10000] Training loss: 0.06672885330269768 / Validation loss: 0.17382751368211571 / Long term Validation loss: 0.1962\n",
      "Epoch: [6/10000] Training loss: 0.062415363128044854 / Validation loss: 0.16667306072281995 / Long term Validation loss: 0.1895\n",
      "Epoch: [7/10000] Training loss: 0.05812741023031352 / Validation loss: 0.15932515009902298 / Long term Validation loss: 0.1824\n",
      "Epoch: [8/10000] Training loss: 0.05388628027153811 / Validation loss: 0.15182350935014416 / Long term Validation loss: 0.1749\n",
      "Epoch: [9/10000] Training loss: 0.049717110459862275 / Validation loss: 0.14421549782971912 / Long term Validation loss: 0.1670\n",
      "Epoch: [10/10000] Training loss: 0.045647729500527194 / Validation loss: 0.13655480812753604 / Long term Validation loss: 0.1587\n",
      "Epoch: [11/10000] Training loss: 0.04170770779182487 / Validation loss: 0.12890065462042904 / Long term Validation loss: 0.1500\n",
      "Epoch: [12/10000] Training loss: 0.037927636883364854 / Validation loss: 0.12131691137874184 / Long term Validation loss: 0.1411\n",
      "Epoch: [13/10000] Training loss: 0.034338434577308306 / Validation loss: 0.11387097731011506 / Long term Validation loss: 0.1319\n",
      "Epoch: [14/10000] Training loss: 0.030970557506809798 / Validation loss: 0.10663230302572758 / Long term Validation loss: 0.1227\n",
      "Epoch: [15/10000] Training loss: 0.02785307269341736 / Validation loss: 0.09967057222648533 / Long term Validation loss: 0.1135\n",
      "Epoch: [16/10000] Training loss: 0.02501257426987666 / Validation loss: 0.0930535596507546 / Long term Validation loss: 0.1045\n",
      "Epoch: [17/10000] Training loss: 0.022471953302012015 / Validation loss: 0.08684471274823106 / Long term Validation loss: 0.0959\n",
      "Epoch: [18/10000] Training loss: 0.020249049324842835 / Validation loss: 0.0811005333775767 / Long term Validation loss: 0.0877\n",
      "Epoch: [19/10000] Training loss: 0.01835523696956868 / Validation loss: 0.07586787255503662 / Long term Validation loss: 0.0803\n",
      "Epoch: [20/10000] Training loss: 0.01679403225457096 / Validation loss: 0.07118129751991317 / Long term Validation loss: 0.0735\n",
      "Epoch: [21/10000] Training loss: 0.015559841101926403 / Validation loss: 0.06706074550097176 / Long term Validation loss: 0.0676\n",
      "Epoch: [22/10000] Training loss: 0.014637014622138957 / Validation loss: 0.06350973653702868 / Long term Validation loss: 0.0626\n",
      "Epoch: [23/10000] Training loss: 0.013999413435259667 / Validation loss: 0.06051446331069787 / Long term Validation loss: 0.0585\n",
      "Epoch: [24/10000] Training loss: 0.013610700971524727 / Validation loss: 0.05804408234821788 / Long term Validation loss: 0.0552\n",
      "Epoch: [25/10000] Training loss: 0.01342556077160357 / Validation loss: 0.056052462688721626 / Long term Validation loss: 0.0527\n",
      "Epoch: [26/10000] Training loss: 0.013391942986680498 / Validation loss: 0.05448147455290893 / Long term Validation loss: 0.0509\n",
      "Epoch: [27/10000] Training loss: 0.013454282505981342 / Validation loss: 0.05326562073595107 / Long term Validation loss: 0.0498\n",
      "Epoch: [28/10000] Training loss: 0.013557417266131017 / Validation loss: 0.052337483924166706 / Long term Validation loss: 0.0491\n",
      "Epoch: [29/10000] Training loss: 0.01365072845314058 / Validation loss: 0.0516332020954383 / Long term Validation loss: 0.0489\n",
      "Epoch: [30/10000] Training loss: 0.013691904750500127 / Validation loss: 0.051097122153264674 / Long term Validation loss: 0.0489\n",
      "Epoch: [31/10000] Training loss: 0.013649765068319502 / Validation loss: 0.05068497137069596 / Long term Validation loss: 0.0491\n",
      "Epoch: [32/10000] Training loss: 0.013505762673146906 / Validation loss: 0.05036524915579022 / Long term Validation loss: 0.0494\n",
      "Epoch: [33/10000] Training loss: 0.013254069205072062 / Validation loss: 0.05011892067412753 / Long term Validation loss: 0.0498\n",
      "Epoch: [34/10000] Training loss: 0.012900396713628503 / Validation loss: 0.04993775949806036 / Long term Validation loss: 0.0503\n",
      "Epoch: [35/10000] Training loss: 0.012459883937033015 / Validation loss: 0.04982179886535677 / Long term Validation loss: 0.0510\n",
      "Epoch: [36/10000] Training loss: 0.011954431853629282 / Validation loss: 0.049776343345200674 / Long term Validation loss: 0.0518\n",
      "Epoch: [37/10000] Training loss: 0.011409849476112285 / Validation loss: 0.049808921041307415 / Long term Validation loss: 0.0528\n",
      "Epoch: [38/10000] Training loss: 0.010853103471636557 / Validation loss: 0.049926465559194944 / Long term Validation loss: 0.0542\n",
      "Epoch: [39/10000] Training loss: 0.010309885540031764 / Validation loss: 0.05013293052850444 / Long term Validation loss: 0.0559\n",
      "Epoch: [40/10000] Training loss: 0.009802636431369862 / Validation loss: 0.050427465923128016 / Long term Validation loss: 0.0581\n",
      "Epoch: [41/10000] Training loss: 0.009349101382794137 / Validation loss: 0.05080322642015502 / Long term Validation loss: 0.0607\n",
      "Epoch: [42/10000] Training loss: 0.008961440024780741 / Validation loss: 0.05124683751335005 / Long term Validation loss: 0.0637\n",
      "Epoch: [43/10000] Training loss: 0.008645874401662564 / Validation loss: 0.051738513738824654 / Long term Validation loss: 0.0671\n",
      "Epoch: [44/10000] Training loss: 0.008402830851002784 / Validation loss: 0.052252800806186744 / Long term Validation loss: 0.0706\n",
      "Epoch: [45/10000] Training loss: 0.008227512812029821 / Validation loss: 0.052759891491300316 / Long term Validation loss: 0.0744\n",
      "Epoch: [46/10000] Training loss: 0.008110828242094272 / Validation loss: 0.05322743452288587 / Long term Validation loss: 0.0780\n",
      "Epoch: [47/10000] Training loss: 0.008040582900248935 / Validation loss: 0.05362271139511588 / Long term Validation loss: 0.0816\n",
      "Epoch: [48/10000] Training loss: 0.008002836945242798 / Validation loss: 0.05391500243955608 / Long term Validation loss: 0.0849\n",
      "Epoch: [49/10000] Training loss: 0.007983308496131906 / Validation loss: 0.05407791508110697 / Long term Validation loss: 0.0879\n",
      "Epoch: [50/10000] Training loss: 0.00796869909659577 / Validation loss: 0.05409142337754392 / Long term Validation loss: 0.0905\n",
      "Epoch: [51/10000] Training loss: 0.007947818871148591 / Validation loss: 0.05394338400523446 / Long term Validation loss: 0.0926\n",
      "Epoch: [52/10000] Training loss: 0.007912408024888686 / Validation loss: 0.05363035320302697 / Long term Validation loss: 0.0943\n",
      "Epoch: [53/10000] Training loss: 0.007857586065361325 / Validation loss: 0.05315762118438043 / Long term Validation loss: 0.0955\n",
      "Epoch: [54/10000] Training loss: 0.007781905546848728 / Validation loss: 0.05253848498989858 / Long term Validation loss: 0.0963\n",
      "Epoch: [55/10000] Training loss: 0.007687034988166751 / Validation loss: 0.05179287598664158 / Long term Validation loss: 0.0967\n",
      "Epoch: [56/10000] Training loss: 0.007577137544783616 / Validation loss: 0.05094552806404413 / Long term Validation loss: 0.0967\n",
      "Epoch: [57/10000] Training loss: 0.007458042065316802 / Validation loss: 0.050023909506333133 / Long term Validation loss: 0.0964\n",
      "Epoch: [58/10000] Training loss: 0.0073363187151211096 / Validation loss: 0.049056145806732475 / Long term Validation loss: 0.0958\n",
      "Epoch: [59/10000] Training loss: 0.007218372628246039 / Validation loss: 0.04806913747425799 / Long term Validation loss: 0.0951\n",
      "Epoch: [60/10000] Training loss: 0.007109657911696463 / Validation loss: 0.04708703328772248 / Long term Validation loss: 0.0942\n",
      "Epoch: [61/10000] Training loss: 0.00701409319024928 / Validation loss: 0.046130162819865976 / Long term Validation loss: 0.0932\n",
      "Epoch: [62/10000] Training loss: 0.006933731235480323 / Validation loss: 0.045214469444409024 / Long term Validation loss: 0.0923\n",
      "Epoch: [63/10000] Training loss: 0.0068687017408400235 / Validation loss: 0.044351423356414295 / Long term Validation loss: 0.0914\n",
      "Epoch: [64/10000] Training loss: 0.006817411232251496 / Validation loss: 0.04354834022577048 / Long term Validation loss: 0.0906\n",
      "Epoch: [65/10000] Training loss: 0.006776951498472991 / Validation loss: 0.042808991318262934 / Long term Validation loss: 0.0900\n",
      "Epoch: [66/10000] Training loss: 0.006743642380525282 / Validation loss: 0.042134370123036995 / Long term Validation loss: 0.0894\n",
      "Epoch: [67/10000] Training loss: 0.0067136205241620705 / Validation loss: 0.04152348088977143 / Long term Validation loss: 0.0891\n",
      "Epoch: [68/10000] Training loss: 0.006683385454419346 / Validation loss: 0.04097403465552949 / Long term Validation loss: 0.0890\n",
      "Epoch: [69/10000] Training loss: 0.006650228145534445 / Validation loss: 0.04048297351481644 / Long term Validation loss: 0.0890\n",
      "Epoch: [70/10000] Training loss: 0.0066124922640126765 / Validation loss: 0.040046786816612064 / Long term Validation loss: 0.0892\n",
      "Epoch: [71/10000] Training loss: 0.006569649280104803 / Validation loss: 0.03966162568334327 / Long term Validation loss: 0.0895\n",
      "Epoch: [72/10000] Training loss: 0.006522199559130556 / Validation loss: 0.03932325775316237 / Long term Validation loss: 0.0900\n",
      "Epoch: [73/10000] Training loss: 0.006471436872936421 / Validation loss: 0.039026927564658434 / Long term Validation loss: 0.0906\n",
      "Epoch: [74/10000] Training loss: 0.0064191298408050185 / Validation loss: 0.03876719741009606 / Long term Validation loss: 0.0912\n",
      "Epoch: [75/10000] Training loss: 0.006367179244873769 / Validation loss: 0.03853783912932932 / Long term Validation loss: 0.0920\n",
      "Epoch: [76/10000] Training loss: 0.0063173057030129695 / Validation loss: 0.03833183147414937 / Long term Validation loss: 0.0927\n",
      "Epoch: [77/10000] Training loss: 0.006270810058247149 / Validation loss: 0.038141493903900224 / Long term Validation loss: 0.0935\n",
      "Epoch: [78/10000] Training loss: 0.006228432072020243 / Validation loss: 0.03795876025287484 / Long term Validation loss: 0.0942\n",
      "Epoch: [79/10000] Training loss: 0.006190314753401553 / Validation loss: 0.03777556910799853 / Long term Validation loss: 0.0949\n",
      "Epoch: [80/10000] Training loss: 0.006156064779653474 / Validation loss: 0.037584326093196944 / Long term Validation loss: 0.0954\n",
      "Epoch: [81/10000] Training loss: 0.006124886229249497 / Validation loss: 0.03737837983306968 / Long term Validation loss: 0.0959\n",
      "Epoch: [82/10000] Training loss: 0.006095756737775413 / Validation loss: 0.03715245001366406 / Long term Validation loss: 0.0962\n",
      "Epoch: [83/10000] Training loss: 0.006067612798279223 / Validation loss: 0.03690295276340546 / Long term Validation loss: 0.0964\n",
      "Epoch: [84/10000] Training loss: 0.006039513964215761 / Validation loss: 0.036628183846638886 / Long term Validation loss: 0.0965\n",
      "Epoch: [85/10000] Training loss: 0.006010763085377972 / Validation loss: 0.036328340777502695 / Long term Validation loss: 0.0965\n",
      "Epoch: [86/10000] Training loss: 0.00598096975402227 / Validation loss: 0.036005387070581416 / Long term Validation loss: 0.0963\n",
      "Epoch: [87/10000] Training loss: 0.0059500549431286045 / Validation loss: 0.03566278168311093 / Long term Validation loss: 0.0960\n",
      "Epoch: [88/10000] Training loss: 0.005918204555231744 / Validation loss: 0.035305111323583695 / Long term Validation loss: 0.0957\n",
      "Epoch: [89/10000] Training loss: 0.005885786832623872 / Validation loss: 0.034937671027280424 / Long term Validation loss: 0.0953\n",
      "Epoch: [90/10000] Training loss: 0.005853252455847189 / Validation loss: 0.03456603894388699 / Long term Validation loss: 0.0948\n",
      "Epoch: [91/10000] Training loss: 0.0058210364723031 / Validation loss: 0.034195685571883676 / Long term Validation loss: 0.0943\n",
      "Epoch: [92/10000] Training loss: 0.005789478338346054 / Validation loss: 0.033831647483657586 / Long term Validation loss: 0.0938\n",
      "Epoch: [93/10000] Training loss: 0.005758771169274401 / Validation loss: 0.03347828310541849 / Long term Validation loss: 0.0934\n",
      "Epoch: [94/10000] Training loss: 0.00572894488705918 / Validation loss: 0.033139115544055964 / Long term Validation loss: 0.0930\n",
      "Epoch: [95/10000] Training loss: 0.0056998815253108725 / Validation loss: 0.03281675665802183 / Long term Validation loss: 0.0926\n",
      "Epoch: [96/10000] Training loss: 0.005671355571597214 / Validation loss: 0.03251289885976221 / Long term Validation loss: 0.0923\n",
      "Epoch: [97/10000] Training loss: 0.005643088702664363 / Validation loss: 0.03222835714040697 / Long term Validation loss: 0.0921\n",
      "Epoch: [98/10000] Training loss: 0.005614807017591705 / Validation loss: 0.031963143460353935 / Long term Validation loss: 0.0920\n",
      "Epoch: [99/10000] Training loss: 0.005586289886246691 / Validation loss: 0.031716558293938206 / Long term Validation loss: 0.0919\n",
      "Epoch: [100/10000] Training loss: 0.0055574023946905375 / Validation loss: 0.031487288678177704 / Long term Validation loss: 0.0920\n",
      "Epoch: [101/10000] Training loss: 0.005528107377434381 / Validation loss: 0.031273507333599786 / Long term Validation loss: 0.0921\n",
      "Epoch: [102/10000] Training loss: 0.0054984573268941185 / Validation loss: 0.03107297209106716 / Long term Validation loss: 0.0922\n",
      "Epoch: [103/10000] Training loss: 0.005468570237850834 / Validation loss: 0.030883128025657805 / Long term Validation loss: 0.0924\n",
      "Epoch: [104/10000] Training loss: 0.005438596031921188 / Validation loss: 0.03070121582893215 / Long term Validation loss: 0.0927\n",
      "Epoch: [105/10000] Training loss: 0.005408681248463976 / Validation loss: 0.030524388981476868 / Long term Validation loss: 0.0930\n",
      "Epoch: [106/10000] Training loss: 0.005378939141786176 / Validation loss: 0.030349839611897435 / Long term Validation loss: 0.0932\n",
      "Epoch: [107/10000] Training loss: 0.005349430449891534 / Validation loss: 0.030174929297881434 / Long term Validation loss: 0.0935\n",
      "Epoch: [108/10000] Training loss: 0.005320157386085164 / Validation loss: 0.02999731742730098 / Long term Validation loss: 0.0937\n",
      "Epoch: [109/10000] Training loss: 0.005291070460694215 / Validation loss: 0.029815077037440036 / Long term Validation loss: 0.0939\n",
      "Epoch: [110/10000] Training loss: 0.005262085172306023 / Validation loss: 0.029626787029145552 / Long term Validation loss: 0.0940\n",
      "Epoch: [111/10000] Training loss: 0.00523310390271355 / Validation loss: 0.029431590684667948 / Long term Validation loss: 0.0941\n",
      "Epoch: [112/10000] Training loss: 0.0052040377836943584 / Validation loss: 0.029229213427012072 / Long term Validation loss: 0.0941\n",
      "Epoch: [113/10000] Training loss: 0.00517482389751406 / Validation loss: 0.029019937234595547 / Long term Validation loss: 0.0941\n",
      "Epoch: [114/10000] Training loss: 0.005145434699896538 / Validation loss: 0.028804534241709204 / Long term Validation loss: 0.0940\n",
      "Epoch: [115/10000] Training loss: 0.005115878601626536 / Validation loss: 0.028584166861809344 / Long term Validation loss: 0.0939\n",
      "Epoch: [116/10000] Training loss: 0.005086192715915753 / Validation loss: 0.02836026540748335 / Long term Validation loss: 0.0937\n",
      "Epoch: [117/10000] Training loss: 0.005056430408533953 / Validation loss: 0.02813439606519221 / Long term Validation loss: 0.0935\n",
      "Epoch: [118/10000] Training loss: 0.005026647148514467 / Validation loss: 0.02790813201819743 / Long term Validation loss: 0.0933\n",
      "Epoch: [119/10000] Training loss: 0.004996888124500416 / Validation loss: 0.02768293869910451 / Long term Validation loss: 0.0930\n",
      "Epoch: [120/10000] Training loss: 0.004967180259183877 / Validation loss: 0.02746008111267378 / Long term Validation loss: 0.0928\n",
      "Epoch: [121/10000] Training loss: 0.0049375298953176034 / Validation loss: 0.027240557589284405 / Long term Validation loss: 0.0925\n",
      "Epoch: [122/10000] Training loss: 0.0049079259158213 / Validation loss: 0.027025060900715735 / Long term Validation loss: 0.0923\n",
      "Epoch: [123/10000] Training loss: 0.004878346774945027 / Validation loss: 0.02681396493568968 / Long term Validation loss: 0.0921\n",
      "Epoch: [124/10000] Training loss: 0.004848769146109109 / Validation loss: 0.026607333390405462 / Long term Validation loss: 0.0919\n",
      "Epoch: [125/10000] Training loss: 0.004819175771956772 / Validation loss: 0.026404946203023246 / Long term Validation loss: 0.0918\n",
      "Epoch: [126/10000] Training loss: 0.004789560597457844 / Validation loss: 0.026206339540841304 / Long term Validation loss: 0.0917\n",
      "Epoch: [127/10000] Training loss: 0.004759930192046494 / Validation loss: 0.02601085568170403 / Long term Validation loss: 0.0916\n",
      "Epoch: [128/10000] Training loss: 0.004730301545247115 / Validation loss: 0.025817699735196355 / Long term Validation loss: 0.0915\n",
      "Epoch: [129/10000] Training loss: 0.004700697261499952 / Validation loss: 0.025626000522332562 / Long term Validation loss: 0.0914\n",
      "Epoch: [130/10000] Training loss: 0.004671139754073863 / Validation loss: 0.025434872928871526 / Long term Validation loss: 0.0913\n",
      "Epoch: [131/10000] Training loss: 0.004641646129826299 / Validation loss: 0.02524347870543348 / Long term Validation loss: 0.0913\n",
      "Epoch: [132/10000] Training loss: 0.004612225086677256 / Validation loss: 0.025051082204816527 / Long term Validation loss: 0.0912\n",
      "Epoch: [133/10000] Training loss: 0.004582876455163019 / Validation loss: 0.02485709721048911 / Long term Validation loss: 0.0911\n",
      "Epoch: [134/10000] Training loss: 0.0045535932203914315 / Validation loss: 0.02466112110224479 / Long term Validation loss: 0.0910\n",
      "Epoch: [135/10000] Training loss: 0.0045243651889328015 / Validation loss: 0.024462953308993344 / Long term Validation loss: 0.0908\n",
      "Epoch: [136/10000] Training loss: 0.004495183092930073 / Validation loss: 0.024262596334289493 / Long term Validation loss: 0.0907\n",
      "Epoch: [137/10000] Training loss: 0.00446604192970318 / Validation loss: 0.024060239445687432 / Long term Validation loss: 0.0905\n",
      "Epoch: [138/10000] Training loss: 0.004436942684139285 / Validation loss: 0.023856227087452088 / Long term Validation loss: 0.0903\n",
      "Epoch: [139/10000] Training loss: 0.004407892141405634 / Validation loss: 0.023651015835397707 / Long term Validation loss: 0.0901\n",
      "Epoch: [140/10000] Training loss: 0.004378901085566175 / Validation loss: 0.023445124924533454 / Long term Validation loss: 0.0899\n",
      "Epoch: [141/10000] Training loss: 0.004349981618570307 / Validation loss: 0.023239085829124546 / Long term Validation loss: 0.0897\n",
      "Epoch: [142/10000] Training loss: 0.004321144506277063 / Validation loss: 0.023033396018831507 / Long term Validation loss: 0.0894\n",
      "Epoch: [143/10000] Training loss: 0.0042923973371752884 / Validation loss: 0.022828480986039266 / Long term Validation loss: 0.0892\n",
      "Epoch: [144/10000] Training loss: 0.004263743931215345 / Validation loss: 0.02262466719720422 / Long term Validation loss: 0.0890\n",
      "Epoch: [145/10000] Training loss: 0.004235184991525111 / Validation loss: 0.02242216707257178 / Long term Validation loss: 0.0888\n",
      "Epoch: [146/10000] Training loss: 0.004206719600298406 / Validation loss: 0.022221075717479118 / Long term Validation loss: 0.0887\n",
      "Epoch: [147/10000] Training loss: 0.004178346941123671 / Validation loss: 0.0220213780916118 / Long term Validation loss: 0.0885\n",
      "Epoch: [148/10000] Training loss: 0.004150067636642185 / Validation loss: 0.02182296466439756 / Long term Validation loss: 0.0884\n",
      "Epoch: [149/10000] Training loss: 0.00412188429771575 / Validation loss: 0.021625653311152765 / Long term Validation loss: 0.0883\n",
      "Epoch: [150/10000] Training loss: 0.004093801199422482 / Validation loss: 0.021429215138375868 / Long term Validation loss: 0.0881\n",
      "Epoch: [151/10000] Training loss: 0.004065823311892651 / Validation loss: 0.02123340196524218 / Long term Validation loss: 0.0880\n",
      "Epoch: [152/10000] Training loss: 0.004037955114401776 / Validation loss: 0.021037973254252433 / Long term Validation loss: 0.0879\n",
      "Epoch: [153/10000] Training loss: 0.004010199650889197 / Validation loss: 0.020842720369460107 / Long term Validation loss: 0.0878\n",
      "Epoch: [154/10000] Training loss: 0.003982558149882755 / Validation loss: 0.02064748620092991 / Long term Validation loss: 0.0877\n",
      "Epoch: [155/10000] Training loss: 0.003955030294858587 / Validation loss: 0.020452178511950744 / Long term Validation loss: 0.0876\n",
      "Epoch: [156/10000] Training loss: 0.003927614986524108 / Validation loss: 0.020256775903128185 / Long term Validation loss: 0.0875\n",
      "Epoch: [157/10000] Training loss: 0.003900311275557931 / Validation loss: 0.0200613260439389 / Long term Validation loss: 0.0874\n",
      "Epoch: [158/10000] Training loss: 0.003873119115059985 / Validation loss: 0.01986593671555367 / Long term Validation loss: 0.0873\n",
      "Epoch: [159/10000] Training loss: 0.003846039684811642 / Validation loss: 0.019670761089500822 / Long term Validation loss: 0.0871\n",
      "Epoch: [160/10000] Training loss: 0.0038190752237210507 / Validation loss: 0.019475979360762855 / Long term Validation loss: 0.0870\n",
      "Epoch: [161/10000] Training loss: 0.003792228495481514 / Validation loss: 0.019281779216324162 / Long term Validation loss: 0.0868\n",
      "Epoch: [162/10000] Training loss: 0.0037655021335167627 / Validation loss: 0.019088337581868446 / Long term Validation loss: 0.0867\n",
      "Epoch: [163/10000] Training loss: 0.003738898125172899 / Validation loss: 0.018895805677577326 / Long term Validation loss: 0.0865\n",
      "Epoch: [164/10000] Training loss: 0.003712417607519657 / Validation loss: 0.018704298740860423 / Long term Validation loss: 0.0864\n",
      "Epoch: [165/10000] Training loss: 0.0036860610029968025 / Validation loss: 0.018513890997659665 / Long term Validation loss: 0.0863\n",
      "Epoch: [166/10000] Training loss: 0.003659828385566851 / Validation loss: 0.018324615739672923 / Long term Validation loss: 0.0862\n",
      "Epoch: [167/10000] Training loss: 0.003633719891377333 / Validation loss: 0.018136469802017142 / Long term Validation loss: 0.0861\n",
      "Epoch: [168/10000] Training loss: 0.0036077359965715977 / Validation loss: 0.01794942137849994 / Long term Validation loss: 0.0860\n",
      "Epoch: [169/10000] Training loss: 0.0035818775658389987 / Validation loss: 0.017763419944252908 / Long term Validation loss: 0.0859\n",
      "Epoch: [170/10000] Training loss: 0.0035561456868683466 / Validation loss: 0.017578407028879105 / Long term Validation loss: 0.0858\n",
      "Epoch: [171/10000] Training loss: 0.003530541397604232 / Validation loss: 0.017394326646986626 / Long term Validation loss: 0.0857\n",
      "Epoch: [172/10000] Training loss: 0.0035050654479910587 / Validation loss: 0.01721113431881148 / Long term Validation loss: 0.0856\n",
      "Epoch: [173/10000] Training loss: 0.0034797182066673275 / Validation loss: 0.01702880380122411 / Long term Validation loss: 0.0855\n",
      "Epoch: [174/10000] Training loss: 0.003454499746326576 / Validation loss: 0.016847330913290104 / Long term Validation loss: 0.0855\n",
      "Epoch: [175/10000] Training loss: 0.003429410057705843 / Validation loss: 0.016666734187003488 / Long term Validation loss: 0.0854\n",
      "Epoch: [176/10000] Training loss: 0.0034044492893690464 / Validation loss: 0.01648705248065218 / Long term Validation loss: 0.0853\n",
      "Epoch: [177/10000] Training loss: 0.0033796179091488483 / Validation loss: 0.01630834010289739 / Long term Validation loss: 0.0852\n",
      "Epoch: [178/10000] Training loss: 0.003354916728649513 / Validation loss: 0.016130660331488522 / Long term Validation loss: 0.0851\n",
      "Epoch: [179/10000] Training loss: 0.0033303467991013893 / Validation loss: 0.015954078397055923 / Long term Validation loss: 0.0850\n",
      "Epoch: [180/10000] Training loss: 0.0033059092423163763 / Validation loss: 0.015778654996910897 / Long term Validation loss: 0.0849\n",
      "Epoch: [181/10000] Training loss: 0.003281605100297059 / Validation loss: 0.015604441212291218 / Long term Validation loss: 0.0848\n",
      "Epoch: [182/10000] Training loss: 0.0032574352660570123 / Validation loss: 0.015431475378358912 / Long term Validation loss: 0.0848\n",
      "Epoch: [183/10000] Training loss: 0.0032334005109899023 / Validation loss: 0.015259782081168186 / Long term Validation loss: 0.0847\n",
      "Epoch: [184/10000] Training loss: 0.00320950157615851 / Validation loss: 0.01508937311212456 / Long term Validation loss: 0.0846\n",
      "Epoch: [185/10000] Training loss: 0.0031857392692501135 / Validation loss: 0.014920249956738874 / Long term Validation loss: 0.0846\n",
      "Epoch: [186/10000] Training loss: 0.0031621145153898685 / Validation loss: 0.014752407253843186 / Long term Validation loss: 0.0845\n",
      "Epoch: [187/10000] Training loss: 0.0031386283414288896 / Validation loss: 0.014585836625233453 / Long term Validation loss: 0.0845\n",
      "Epoch: [188/10000] Training loss: 0.0031152818109894263 / Validation loss: 0.014420530318131916 / Long term Validation loss: 0.0844\n",
      "Epoch: [189/10000] Training loss: 0.0030920759515985934 / Validation loss: 0.014256484197420025 / Long term Validation loss: 0.0844\n",
      "Epoch: [190/10000] Training loss: 0.0030690117148598596 / Validation loss: 0.014093699753956941 / Long term Validation loss: 0.0844\n",
      "Epoch: [191/10000] Training loss: 0.0030460899886784834 / Validation loss: 0.013932184950932196 / Long term Validation loss: 0.0843\n",
      "Epoch: [192/10000] Training loss: 0.003023311650841893 / Validation loss: 0.013771953904252236 / Long term Validation loss: 0.0843\n",
      "Epoch: [193/10000] Training loss: 0.003000677632197713 / Validation loss: 0.013613025569301303 / Long term Validation loss: 0.0843\n",
      "Epoch: [194/10000] Training loss: 0.002978188955486793 / Validation loss: 0.013455421757740782 / Long term Validation loss: 0.0842\n",
      "Epoch: [195/10000] Training loss: 0.0029558467317538847 / Validation loss: 0.013299164901971932 / Long term Validation loss: 0.0842\n",
      "Epoch: [196/10000] Training loss: 0.0029336521198175367 / Validation loss: 0.013144275996602253 / Long term Validation loss: 0.0842\n",
      "Epoch: [197/10000] Training loss: 0.0029116062721401127 / Validation loss: 0.012990773070473785 / Long term Validation loss: 0.0842\n",
      "Epoch: [198/10000] Training loss: 0.002889710293723679 / Validation loss: 0.012838670398871795 / Long term Validation loss: 0.0842\n",
      "Epoch: [199/10000] Training loss: 0.0028679652293976195 / Validation loss: 0.012687978491989799 / Long term Validation loss: 0.0841\n",
      "Epoch: [200/10000] Training loss: 0.002846372077056929 / Validation loss: 0.012538704736952115 / Long term Validation loss: 0.0841\n",
      "Epoch: [201/10000] Training loss: 0.0028249318107421225 / Validation loss: 0.012390854461792817 / Long term Validation loss: 0.0841\n",
      "Epoch: [202/10000] Training loss: 0.00280364539501637 / Validation loss: 0.012244432146686812 / Long term Validation loss: 0.0842\n",
      "Epoch: [203/10000] Training loss: 0.0027825137807607155 / Validation loss: 0.012099442526119738 / Long term Validation loss: 0.0842\n",
      "Epoch: [204/10000] Training loss: 0.0027615378857727105 / Validation loss: 0.011955891387761746 / Long term Validation loss: 0.0842\n",
      "Epoch: [205/10000] Training loss: 0.002740718572999363 / Validation loss: 0.011813785958030018 / Long term Validation loss: 0.0842\n",
      "Epoch: [206/10000] Training loss: 0.0027200566396619126 / Validation loss: 0.011673134852835736 / Long term Validation loss: 0.0842\n",
      "Epoch: [207/10000] Training loss: 0.0026995528226940375 / Validation loss: 0.011533947652850585 / Long term Validation loss: 0.0843\n",
      "Epoch: [208/10000] Training loss: 0.002679207815461259 / Validation loss: 0.011396234227506274 / Long term Validation loss: 0.0843\n",
      "Epoch: [209/10000] Training loss: 0.002659022284293481 / Validation loss: 0.011260003973092058 / Long term Validation loss: 0.0843\n",
      "Epoch: [210/10000] Training loss: 0.0026389968744503405 / Validation loss: 0.011125265139889968 / Long term Validation loss: 0.0844\n",
      "Epoch: [211/10000] Training loss: 0.0026191322023060358 / Validation loss: 0.010992024396347687 / Long term Validation loss: 0.0844\n",
      "Epoch: [212/10000] Training loss: 0.0025994288387844035 / Validation loss: 0.010860286717420486 / Long term Validation loss: 0.0845\n",
      "Epoch: [213/10000] Training loss: 0.002579887293288586 / Validation loss: 0.010730055602156617 / Long term Validation loss: 0.0845\n",
      "Epoch: [214/10000] Training loss: 0.002560508005515009 / Validation loss: 0.010601333543233836 / Long term Validation loss: 0.0846\n",
      "Epoch: [215/10000] Training loss: 0.002541291346583548 / Validation loss: 0.010474122610966398 / Long term Validation loss: 0.0846\n",
      "Epoch: [216/10000] Training loss: 0.0025222376251344803 / Validation loss: 0.01034842499231175 / Long term Validation loss: 0.0847\n",
      "Epoch: [217/10000] Training loss: 0.0025033470920499787 / Validation loss: 0.010224243345544094 / Long term Validation loss: 0.0848\n",
      "Epoch: [218/10000] Training loss: 0.002484619939960575 / Validation loss: 0.010101580884833093 / Long term Validation loss: 0.0848\n",
      "Epoch: [219/10000] Training loss: 0.0024660562983824627 / Validation loss: 0.00998044117901038 / Long term Validation loss: 0.0849\n",
      "Epoch: [220/10000] Training loss: 0.002447656228739846 / Validation loss: 0.009860827716705498 / Long term Validation loss: 0.0850\n",
      "Epoch: [221/10000] Training loss: 0.002429419723480935 / Validation loss: 0.009742743341036062 / Long term Validation loss: 0.0851\n",
      "Epoch: [222/10000] Training loss: 0.0024113467104184775 / Validation loss: 0.009626189682726317 / Long term Validation loss: 0.0852\n",
      "Epoch: [223/10000] Training loss: 0.002393437059765476 / Validation loss: 0.009511166718418717 / Long term Validation loss: 0.0853\n",
      "Epoch: [224/10000] Training loss: 0.0023756905896838037 / Validation loss: 0.009397672552963456 / Long term Validation loss: 0.0854\n",
      "Epoch: [225/10000] Training loss: 0.002358107067459075 / Validation loss: 0.009285703476006438 / Long term Validation loss: 0.0855\n",
      "Epoch: [226/10000] Training loss: 0.0023406862064795325 / Validation loss: 0.00917525428306658 / Long term Validation loss: 0.0856\n",
      "Epoch: [227/10000] Training loss: 0.002323427661769898 / Validation loss: 0.009066318791613327 / Long term Validation loss: 0.0858\n",
      "Epoch: [228/10000] Training loss: 0.0023063310272541963 / Validation loss: 0.008958890437212522 / Long term Validation loss: 0.0859\n",
      "Epoch: [229/10000] Training loss: 0.002289395836217898 / Validation loss: 0.008852962815580994 / Long term Validation loss: 0.0860\n",
      "Epoch: [230/10000] Training loss: 0.0022726215640969136 / Validation loss: 0.008748530049400518 / Long term Validation loss: 0.0861\n",
      "Epoch: [231/10000] Training loss: 0.002256007631462806 / Validation loss: 0.008645586901334002 / Long term Validation loss: 0.0863\n",
      "Epoch: [232/10000] Training loss: 0.0022395534056052786 / Validation loss: 0.008544128615857495 / Long term Validation loss: 0.0864\n",
      "Epoch: [233/10000] Training loss: 0.002223258200745911 / Validation loss: 0.008444150536358028 / Long term Validation loss: 0.0866\n",
      "Epoch: [234/10000] Training loss: 0.0022071212782496454 / Validation loss: 0.008345647594631647 / Long term Validation loss: 0.0867\n",
      "Epoch: [235/10000] Training loss: 0.0021911418482427774 / Validation loss: 0.008248613796131687 / Long term Validation loss: 0.0869\n",
      "Epoch: [236/10000] Training loss: 0.0021753190729101304 / Validation loss: 0.008153041821537813 / Long term Validation loss: 0.0870\n",
      "Epoch: [237/10000] Training loss: 0.002159652070431341 / Validation loss: 0.008058922835688717 / Long term Validation loss: 0.0872\n",
      "Epoch: [238/10000] Training loss: 0.002144139918059103 / Validation loss: 0.00796624654619284 / Long term Validation loss: 0.0874\n",
      "Epoch: [239/10000] Training loss: 0.0021287816535154064 / Validation loss: 0.007875001497023556 / Long term Validation loss: 0.0876\n",
      "Epoch: [240/10000] Training loss: 0.002113576275074883 / Validation loss: 0.007785175529487443 / Long term Validation loss: 0.0877\n",
      "Epoch: [241/10000] Training loss: 0.0020985227414851686 / Validation loss: 0.0076967563058620014 / Long term Validation loss: 0.0879\n",
      "Epoch: [242/10000] Training loss: 0.002083619972734189 / Validation loss: 0.00760973177842558 / Long term Validation loss: 0.0881\n",
      "Epoch: [243/10000] Training loss: 0.0020688668518638307 / Validation loss: 0.007524090501808946 / Long term Validation loss: 0.0883\n",
      "Epoch: [244/10000] Training loss: 0.0020542622272667426 / Validation loss: 0.007439821726005562 / Long term Validation loss: 0.0885\n",
      "Epoch: [245/10000] Training loss: 0.0020398049147558545 / Validation loss: 0.007356915261053387 / Long term Validation loss: 0.0887\n",
      "Epoch: [246/10000] Training loss: 0.002025493699159716 / Validation loss: 0.007275361158666635 / Long term Validation loss: 0.0889\n",
      "Epoch: [247/10000] Training loss: 0.0020113273357780267 / Validation loss: 0.007195149297426096 / Long term Validation loss: 0.0891\n",
      "Epoch: [248/10000] Training loss: 0.0019973045522146296 / Validation loss: 0.007116268976902338 / Long term Validation loss: 0.0894\n",
      "Epoch: [249/10000] Training loss: 0.0019834240507757353 / Validation loss: 0.007038708618847252 / Long term Validation loss: 0.0896\n",
      "Epoch: [250/10000] Training loss: 0.0019696845111122703 / Validation loss: 0.006962455643415689 / Long term Validation loss: 0.0898\n",
      "Epoch: [251/10000] Training loss: 0.0019560845925483844 / Validation loss: 0.00688749654351327 / Long term Validation loss: 0.0900\n",
      "Epoch: [252/10000] Training loss: 0.0019426229357536625 / Validation loss: 0.006813817131958051 / Long term Validation loss: 0.0903\n",
      "Epoch: [253/10000] Training loss: 0.001929298163873304 / Validation loss: 0.006741402895689764 / Long term Validation loss: 0.0905\n",
      "Epoch: [254/10000] Training loss: 0.0019161088835442362 / Validation loss: 0.006670239368142314 / Long term Validation loss: 0.0907\n",
      "Epoch: [255/10000] Training loss: 0.001903053686175292 / Validation loss: 0.006600312430280634 / Long term Validation loss: 0.0910\n",
      "Epoch: [256/10000] Training loss: 0.0018901311495593755 / Validation loss: 0.006531608472317967 / Long term Validation loss: 0.0912\n",
      "Epoch: [257/10000] Training loss: 0.001877339839611108 / Validation loss: 0.006464114385796386 / Long term Validation loss: 0.0914\n",
      "Epoch: [258/10000] Training loss: 0.001864678311998764 / Validation loss: 0.00639781739943262 / Long term Validation loss: 0.0917\n",
      "Epoch: [259/10000] Training loss: 0.0018521451136284737 / Validation loss: 0.006332704810446173 / Long term Validation loss: 0.0919\n",
      "Epoch: [260/10000] Training loss: 0.001839738784125325 / Validation loss: 0.0062687636863801325 / Long term Validation loss: 0.0922\n",
      "Epoch: [261/10000] Training loss: 0.0018274578574602285 / Validation loss: 0.006205980615486252 / Long term Validation loss: 0.0924\n",
      "Epoch: [262/10000] Training loss: 0.0018153008637054296 / Validation loss: 0.006144341566907665 / Long term Validation loss: 0.0927\n",
      "Epoch: [263/10000] Training loss: 0.0018032663307352517 / Validation loss: 0.006083831890553989 / Long term Validation loss: 0.0929\n",
      "Epoch: [264/10000] Training loss: 0.0017913527856779075 / Validation loss: 0.006024436449450561 / Long term Validation loss: 0.0932\n",
      "Epoch: [265/10000] Training loss: 0.0017795587560716897 / Validation loss: 0.005966139844138915 / Long term Validation loss: 0.0935\n",
      "Epoch: [266/10000] Training loss: 0.0017678827708478338 / Validation loss: 0.005908926667644382 / Long term Validation loss: 0.0937\n",
      "Epoch: [267/10000] Training loss: 0.0017563233613136272 / Validation loss: 0.005852781725439028 / Long term Validation loss: 0.0940\n",
      "Epoch: [268/10000] Training loss: 0.0017448790622232382 / Validation loss: 0.005797690168007135 / Long term Validation loss: 0.0942\n",
      "Epoch: [269/10000] Training loss: 0.0017335484129000244 / Validation loss: 0.005743637509816391 / Long term Validation loss: 0.0945\n",
      "Epoch: [270/10000] Training loss: 0.001722329958325135 / Validation loss: 0.005690609540123907 / Long term Validation loss: 0.0948\n",
      "Epoch: [271/10000] Training loss: 0.0017112222501557402 / Validation loss: 0.00563859215934267 / Long term Validation loss: 0.0950\n",
      "Epoch: [272/10000] Training loss: 0.0017002238477098436 / Validation loss: 0.00558757119221967 / Long term Validation loss: 0.0953\n",
      "Epoch: [273/10000] Training loss: 0.0016893333189698222 / Validation loss: 0.005537532231742472 / Long term Validation loss: 0.0956\n",
      "Epoch: [274/10000] Training loss: 0.0016785492416006961 / Validation loss: 0.005488460555708928 / Long term Validation loss: 0.0958\n",
      "Epoch: [275/10000] Training loss: 0.0016678702039133741 / Validation loss: 0.005440341135437582 / Long term Validation loss: 0.0961\n",
      "Epoch: [276/10000] Training loss: 0.0016572948056954606 / Validation loss: 0.005393158729889887 / Long term Validation loss: 0.0963\n",
      "Epoch: [277/10000] Training loss: 0.0016468216588898488 / Validation loss: 0.0053468980358270446 / Long term Validation loss: 0.0966\n",
      "Epoch: [278/10000] Training loss: 0.0016364493881701647 / Validation loss: 0.005301543851449049 / Long term Validation loss: 0.0969\n",
      "Epoch: [279/10000] Training loss: 0.001626176631483829 / Validation loss: 0.0052570812102831314 / Long term Validation loss: 0.0971\n",
      "Epoch: [280/10000] Training loss: 0.001616002040600052 / Validation loss: 0.005213495453346829 / Long term Validation loss: 0.0974\n",
      "Epoch: [281/10000] Training loss: 0.0016059242816529842 / Validation loss: 0.005170772226967965 / Long term Validation loss: 0.0977\n",
      "Epoch: [282/10000] Training loss: 0.001595942035653655 / Validation loss: 0.005128897415184388 / Long term Validation loss: 0.0979\n",
      "Epoch: [283/10000] Training loss: 0.0015860539989631685 / Validation loss: 0.0050878570331609335 / Long term Validation loss: 0.0982\n",
      "Epoch: [284/10000] Training loss: 0.0015762588837422622 / Validation loss: 0.005047637116858327 / Long term Validation loss: 0.0984\n",
      "Epoch: [285/10000] Training loss: 0.001566555418389386 / Validation loss: 0.005008223642375299 / Long term Validation loss: 0.0987\n",
      "Epoch: [286/10000] Training loss: 0.0015569423479539115 / Validation loss: 0.004969602497233117 / Long term Validation loss: 0.0989\n",
      "Epoch: [287/10000] Training loss: 0.0015474184344916415 / Validation loss: 0.004931759509233223 / Long term Validation loss: 0.0992\n",
      "Epoch: [288/10000] Training loss: 0.0015379824573382008 / Validation loss: 0.004894680521537516 / Long term Validation loss: 0.0994\n",
      "Epoch: [289/10000] Training loss: 0.001528633213304995 / Validation loss: 0.004858351490179863 / Long term Validation loss: 0.0997\n",
      "Epoch: [290/10000] Training loss: 0.0015193695168258066 / Validation loss: 0.004822758575569563 / Long term Validation loss: 0.0999\n",
      "Epoch: [291/10000] Training loss: 0.0015101902000817903 / Validation loss: 0.0047878882034871895 / Long term Validation loss: 0.1002\n",
      "Epoch: [292/10000] Training loss: 0.0015010941131149928 / Validation loss: 0.004753727081849487 / Long term Validation loss: 0.1004\n",
      "Epoch: [293/10000] Training loss: 0.0014920801239255838 / Validation loss: 0.0047202621734873055 / Long term Validation loss: 0.1007\n",
      "Epoch: [294/10000] Training loss: 0.001483147118547642 / Validation loss: 0.004687480637987227 / Long term Validation loss: 0.1009\n",
      "Epoch: [295/10000] Training loss: 0.0014742940011061682 / Validation loss: 0.0046553697635809506 / Long term Validation loss: 0.1012\n",
      "Epoch: [296/10000] Training loss: 0.0014655196938602082 / Validation loss: 0.00462391691113565 / Long term Validation loss: 0.1014\n",
      "Epoch: [297/10000] Training loss: 0.0014568231372289074 / Validation loss: 0.004593109486717661 / Long term Validation loss: 0.1016\n",
      "Epoch: [298/10000] Training loss: 0.0014482032897879383 / Validation loss: 0.004562934949165443 / Long term Validation loss: 0.1019\n",
      "Epoch: [299/10000] Training loss: 0.0014396591282241676 / Validation loss: 0.004533380847979899 / Long term Validation loss: 0.1021\n",
      "Epoch: [300/10000] Training loss: 0.0014311896472476058 / Validation loss: 0.004504434878054204 / Long term Validation loss: 0.1023\n",
      "Epoch: [301/10000] Training loss: 0.0014227938594716617 / Validation loss: 0.0044760849337890235 / Long term Validation loss: 0.1026\n",
      "Epoch: [302/10000] Training loss: 0.0014144707952759255 / Validation loss: 0.00444831914679901 / Long term Validation loss: 0.1028\n",
      "Epoch: [303/10000] Training loss: 0.001406219502659908 / Validation loss: 0.004421125897761114 / Long term Validation loss: 0.1030\n",
      "Epoch: [304/10000] Training loss: 0.001398039047089101 / Validation loss: 0.004394493801664742 / Long term Validation loss: 0.1032\n",
      "Epoch: [305/10000] Training loss: 0.0013899285113325977 / Validation loss: 0.004368411673919227 / Long term Validation loss: 0.1034\n",
      "Epoch: [306/10000] Training loss: 0.0013818869952931887 / Validation loss: 0.004342868489954794 / Long term Validation loss: 0.1037\n",
      "Epoch: [307/10000] Training loss: 0.001373913615831137 / Validation loss: 0.004317853351755206 / Long term Validation loss: 0.1039\n",
      "Epoch: [308/10000] Training loss: 0.0013660075065794865 / Validation loss: 0.004293355471301078 / Long term Validation loss: 0.1041\n",
      "Epoch: [309/10000] Training loss: 0.001358167817745128 / Validation loss: 0.004269364174629024 / Long term Validation loss: 0.1043\n",
      "Epoch: [310/10000] Training loss: 0.0013503937158905082 / Validation loss: 0.004245868923335109 / Long term Validation loss: 0.1045\n",
      "Epoch: [311/10000] Training loss: 0.0013426843836960491 / Validation loss: 0.0042228593450914205 / Long term Validation loss: 0.1047\n",
      "Epoch: [312/10000] Training loss: 0.0013350390197087857 / Validation loss: 0.004200325262627138 / Long term Validation loss: 0.1049\n",
      "Epoch: [313/10000] Training loss: 0.001327456838084246 / Validation loss: 0.004178256712054152 / Long term Validation loss: 0.1051\n",
      "Epoch: [314/10000] Training loss: 0.0013199370683262832 / Validation loss: 0.004156643945631658 / Long term Validation loss: 0.1053\n",
      "Epoch: [315/10000] Training loss: 0.0013124789550265565 / Validation loss: 0.004135477419471058 / Long term Validation loss: 0.1055\n",
      "Epoch: [316/10000] Training loss: 0.001305081757604123 / Validation loss: 0.004114747771434578 / Long term Validation loss: 0.1057\n",
      "Epoch: [317/10000] Training loss: 0.0012977447500455627 / Validation loss: 0.004094445797096588 / Long term Validation loss: 0.1059\n",
      "Epoch: [318/10000] Training loss: 0.001290467220645351 / Validation loss: 0.004074562431455705 / Long term Validation loss: 0.1061\n",
      "Epoch: [319/10000] Training loss: 0.0012832484717444504 / Validation loss: 0.004055088741419282 / Long term Validation loss: 0.1063\n",
      "Epoch: [320/10000] Training loss: 0.001276087819464084 / Validation loss: 0.004036015930028191 / Long term Validation loss: 0.1065\n",
      "Epoch: [321/10000] Training loss: 0.0012689845934328516 / Validation loss: 0.004017335349405957 / Long term Validation loss: 0.1067\n",
      "Epoch: [322/10000] Training loss: 0.0012619381365081461 / Validation loss: 0.00399903851680797 / Long term Validation loss: 0.1069\n",
      "Epoch: [323/10000] Training loss: 0.001254947804495084 / Validation loss: 0.0039811171276502 / Long term Validation loss: 0.1070\n",
      "Epoch: [324/10000] Training loss: 0.001248012965866425 / Validation loss: 0.003963563060983384 / Long term Validation loss: 0.1072\n",
      "Epoch: [325/10000] Training loss: 0.0012411330014857418 / Validation loss: 0.003946368375827631 / Long term Validation loss: 0.1074\n",
      "Epoch: [326/10000] Training loss: 0.0012343073043348228 / Validation loss: 0.00392952529997976 / Long term Validation loss: 0.1076\n",
      "Epoch: [327/10000] Training loss: 0.0012275352792456117 / Validation loss: 0.003913026215252582 / Long term Validation loss: 0.1078\n",
      "Epoch: [328/10000] Training loss: 0.0012208163426365153 / Validation loss: 0.0038968636438888532 / Long term Validation loss: 0.1079\n",
      "Epoch: [329/10000] Training loss: 0.0012141499222521346 / Validation loss: 0.003881030239995349 / Long term Validation loss: 0.1081\n",
      "Epoch: [330/10000] Training loss: 0.0012075354569047693 / Validation loss: 0.003865518787729552 / Long term Validation loss: 0.1083\n",
      "Epoch: [331/10000] Training loss: 0.0012009723962162224 / Validation loss: 0.0038503222054803365 / Long term Validation loss: 0.1085\n",
      "Epoch: [332/10000] Training loss: 0.001194460200359617 / Validation loss: 0.0038354335533030614 / Long term Validation loss: 0.1086\n",
      "Epoch: [333/10000] Training loss: 0.001187998339802374 / Validation loss: 0.0038208460400293957 / Long term Validation loss: 0.1088\n",
      "Epoch: [334/10000] Training loss: 0.0011815862950522072 / Validation loss: 0.003806553026951657 / Long term Validation loss: 0.1090\n",
      "Epoch: [335/10000] Training loss: 0.0011752235564077616 / Validation loss: 0.003792548026486069 / Long term Validation loss: 0.1091\n",
      "Epoch: [336/10000] Training loss: 0.0011689096237148193 / Validation loss: 0.003778824696126083 / Long term Validation loss: 0.1093\n",
      "Epoch: [337/10000] Training loss: 0.0011626440061284046 / Validation loss: 0.0037653768296008515 / Long term Validation loss: 0.1095\n",
      "Epoch: [338/10000] Training loss: 0.001156426221880636 / Validation loss: 0.0037521983479231343 / Long term Validation loss: 0.1096\n",
      "Epoch: [339/10000] Training loss: 0.0011502557980537086 / Validation loss: 0.0037392832927580807 / Long term Validation loss: 0.1098\n",
      "Epoch: [340/10000] Training loss: 0.0011441322703569822 / Validation loss: 0.0037266258234611087 / Long term Validation loss: 0.1099\n",
      "Epoch: [341/10000] Training loss: 0.0011380551829071632 / Validation loss: 0.0037142202176863232 / Long term Validation loss: 0.1101\n",
      "Epoch: [342/10000] Training loss: 0.0011320240880111345 / Validation loss: 0.0037020608742071425 / Long term Validation loss: 0.1103\n",
      "Epoch: [343/10000] Training loss: 0.0011260385459517888 / Validation loss: 0.0036901423159474097 / Long term Validation loss: 0.1104\n",
      "Epoch: [344/10000] Training loss: 0.0011200981247777655 / Validation loss: 0.0036784591913617114 / Long term Validation loss: 0.1106\n",
      "Epoch: [345/10000] Training loss: 0.0011142024000980388 / Validation loss: 0.003667006273097109 / Long term Validation loss: 0.1107\n",
      "Epoch: [346/10000] Training loss: 0.0011083509548819872 / Validation loss: 0.0036557784539625634 / Long term Validation loss: 0.1109\n",
      "Epoch: [347/10000] Training loss: 0.0011025433792651728 / Validation loss: 0.0036447707412003596 / Long term Validation loss: 0.1110\n",
      "Epoch: [348/10000] Training loss: 0.0010967792703606815 / Validation loss: 0.003633978250557775 / Long term Validation loss: 0.1112\n",
      "Epoch: [349/10000] Training loss: 0.001091058232075558 / Validation loss: 0.003623396201562337 / Long term Validation loss: 0.1114\n",
      "Epoch: [350/10000] Training loss: 0.0010853798749316192 / Validation loss: 0.0036130199148078006 / Long term Validation loss: 0.1115\n",
      "Epoch: [351/10000] Training loss: 0.0010797438158899686 / Validation loss: 0.003602844811232445 / Long term Validation loss: 0.1117\n",
      "Epoch: [352/10000] Training loss: 0.0010741496781788472 / Validation loss: 0.0035928664126440306 / Long term Validation loss: 0.1118\n",
      "Epoch: [353/10000] Training loss: 0.001068597091124926 / Validation loss: 0.003583080342374982 / Long term Validation loss: 0.1120\n",
      "Epoch: [354/10000] Training loss: 0.0010630856899884807 / Validation loss: 0.003573482325035877 / Long term Validation loss: 0.1121\n",
      "Epoch: [355/10000] Training loss: 0.0010576151158029686 / Validation loss: 0.003564068184794816 / Long term Validation loss: 0.1123\n",
      "Epoch: [356/10000] Training loss: 0.0010521850152193557 / Validation loss: 0.003554833842232821 / Long term Validation loss: 0.1124\n",
      "Epoch: [357/10000] Training loss: 0.0010467950403552956 / Validation loss: 0.0035457753103601344 / Long term Validation loss: 0.1126\n",
      "Epoch: [358/10000] Training loss: 0.0010414448486490003 / Validation loss: 0.0035368886906339734 / Long term Validation loss: 0.1127\n",
      "Epoch: [359/10000] Training loss: 0.001036134102717413 / Validation loss: 0.003528170169729031 / Long term Validation loss: 0.1129\n",
      "Epoch: [360/10000] Training loss: 0.0010308624702181806 / Validation loss: 0.0035196160174496855 / Long term Validation loss: 0.1130\n",
      "Epoch: [361/10000] Training loss: 0.0010256296237149696 / Validation loss: 0.0035112225857054126 / Long term Validation loss: 0.1132\n",
      "Epoch: [362/10000] Training loss: 0.0010204352405458906 / Validation loss: 0.0035029863080876197 / Long term Validation loss: 0.1133\n",
      "Epoch: [363/10000] Training loss: 0.0010152790026950712 / Validation loss: 0.00349490369942424 / Long term Validation loss: 0.1135\n",
      "Epoch: [364/10000] Training loss: 0.0010101605966676163 / Validation loss: 0.0034869713547843943 / Long term Validation loss: 0.1137\n",
      "Epoch: [365/10000] Training loss: 0.0010050797133682313 / Validation loss: 0.003479185947693347 / Long term Validation loss: 0.1138\n",
      "Epoch: [366/10000] Training loss: 0.0010000360479836675 / Validation loss: 0.003471544227664373 / Long term Validation loss: 0.1140\n",
      "Epoch: [367/10000] Training loss: 0.0009950292998689987 / Validation loss: 0.0034640430174181598 / Long term Validation loss: 0.1141\n",
      "Epoch: [368/10000] Training loss: 0.0009900591724375427 / Validation loss: 0.003456679210248706 / Long term Validation loss: 0.1143\n",
      "Epoch: [369/10000] Training loss: 0.0009851253730541211 / Validation loss: 0.003449449767893307 / Long term Validation loss: 0.1144\n",
      "Epoch: [370/10000] Training loss: 0.000980227612931302 / Validation loss: 0.0034423517190349993 / Long term Validation loss: 0.1146\n",
      "Epoch: [371/10000] Training loss: 0.0009753656070283387 / Validation loss: 0.0034353821583139195 / Long term Validation loss: 0.1147\n",
      "Epoch: [372/10000] Training loss: 0.0009705390739526801 / Validation loss: 0.003428538245551271 / Long term Validation loss: 0.1149\n",
      "Epoch: [373/10000] Training loss: 0.0009657477358640942 / Validation loss: 0.003421817204855054 / Long term Validation loss: 0.1151\n",
      "Epoch: [374/10000] Training loss: 0.0009609913183815417 / Validation loss: 0.003415216323377245 / Long term Validation loss: 0.1152\n",
      "Epoch: [375/10000] Training loss: 0.0009562695504929382 / Validation loss: 0.0034087329496716333 / Long term Validation loss: 0.1154\n",
      "Epoch: [376/10000] Training loss: 0.0009515821644678571 / Validation loss: 0.003402364491778394 / Long term Validation loss: 0.1155\n",
      "Epoch: [377/10000] Training loss: 0.0009469288957731076 / Validation loss: 0.0033961084152652607 / Long term Validation loss: 0.1157\n",
      "Epoch: [378/10000] Training loss: 0.0009423094829910152 / Validation loss: 0.003389962241453027 / Long term Validation loss: 0.1159\n",
      "Epoch: [379/10000] Training loss: 0.0009377236677401607 / Validation loss: 0.003383923545959171 / Long term Validation loss: 0.1160\n",
      "Epoch: [380/10000] Training loss: 0.0009331711945983476 / Validation loss: 0.00337798995755737 / Long term Validation loss: 0.1162\n",
      "Epoch: [381/10000] Training loss: 0.0009286518110276376 / Validation loss: 0.003372159157233306 / Long term Validation loss: 0.1163\n",
      "Epoch: [382/10000] Training loss: 0.0009241652673014094 / Validation loss: 0.0033664288772640226 / Long term Validation loss: 0.1165\n",
      "Epoch: [383/10000] Training loss: 0.0009197113164334852 / Validation loss: 0.003360796900172884 / Long term Validation loss: 0.1167\n",
      "Epoch: [384/10000] Training loss: 0.0009152897141094075 / Validation loss: 0.0033552610574944823 / Long term Validation loss: 0.1168\n",
      "Epoch: [385/10000] Training loss: 0.0009109002186199184 / Validation loss: 0.003349819228382174 / Long term Validation loss: 0.1170\n",
      "Epoch: [386/10000] Training loss: 0.000906542590796626 / Validation loss: 0.003344469338162401 / Long term Validation loss: 0.1172\n",
      "Epoch: [387/10000] Training loss: 0.0009022165939497618 / Validation loss: 0.0033392093569581693 / Long term Validation loss: 0.1173\n",
      "Epoch: [388/10000] Training loss: 0.0008979219938078771 / Validation loss: 0.003334037298468806 / Long term Validation loss: 0.1175\n",
      "Epoch: [389/10000] Training loss: 0.000893658558459306 / Validation loss: 0.003328951218926112 / Long term Validation loss: 0.1176\n",
      "Epoch: [390/10000] Training loss: 0.0008894260582952615 / Validation loss: 0.0033239492161812848 / Long term Validation loss: 0.1178\n",
      "Epoch: [391/10000] Training loss: 0.0008852242659544951 / Validation loss: 0.0033190294288398146 / Long term Validation loss: 0.1180\n",
      "Epoch: [392/10000] Training loss: 0.0008810529562695181 / Validation loss: 0.0033141900353651347 / Long term Validation loss: 0.1181\n",
      "Epoch: [393/10000] Training loss: 0.0008769119062144301 / Validation loss: 0.003309429253108509 / Long term Validation loss: 0.1183\n",
      "Epoch: [394/10000] Training loss: 0.000872800894854383 / Validation loss: 0.0033047453372721377 / Long term Validation loss: 0.1185\n",
      "Epoch: [395/10000] Training loss: 0.0008687197032966891 / Validation loss: 0.003300136579851343 / Long term Validation loss: 0.1186\n",
      "Epoch: [396/10000] Training loss: 0.0008646681146435138 / Validation loss: 0.003295601308614533 / Long term Validation loss: 0.1188\n",
      "Epoch: [397/10000] Training loss: 0.0008606459139460576 / Validation loss: 0.0032911378861647353 / Long term Validation loss: 0.1190\n",
      "Epoch: [398/10000] Training loss: 0.0008566528881601089 / Validation loss: 0.0032867447090943968 / Long term Validation loss: 0.1191\n",
      "Epoch: [399/10000] Training loss: 0.0008526888261028684 / Validation loss: 0.0032824202072127178 / Long term Validation loss: 0.1193\n",
      "Epoch: [400/10000] Training loss: 0.0008487535184109752 / Validation loss: 0.0032781628428065206 / Long term Validation loss: 0.1195\n",
      "Epoch: [401/10000] Training loss: 0.0008448467574997266 / Validation loss: 0.003273971109897562 / Long term Validation loss: 0.1196\n",
      "Epoch: [402/10000] Training loss: 0.0008409683375235053 / Validation loss: 0.0032698435334772573 / Long term Validation loss: 0.1198\n",
      "Epoch: [403/10000] Training loss: 0.0008371180543374358 / Validation loss: 0.0032657786687232686 / Long term Validation loss: 0.1200\n",
      "Epoch: [404/10000] Training loss: 0.0008332957054602708 / Validation loss: 0.003261775100219508 / Long term Validation loss: 0.1201\n",
      "Epoch: [405/10000] Training loss: 0.0008295010900384779 / Validation loss: 0.0032578314412050656 / Long term Validation loss: 0.1203\n",
      "Epoch: [406/10000] Training loss: 0.0008257340088114603 / Validation loss: 0.0032539463328684617 / Long term Validation loss: 0.1205\n",
      "Epoch: [407/10000] Training loss: 0.0008219942640778287 / Validation loss: 0.003250118443688053 / Long term Validation loss: 0.1206\n",
      "Epoch: [408/10000] Training loss: 0.0008182816596626581 / Validation loss: 0.0032463464688058043 / Long term Validation loss: 0.1208\n",
      "Epoch: [409/10000] Training loss: 0.0008145960008856738 / Validation loss: 0.0032426291294164727 / Long term Validation loss: 0.1210\n",
      "Epoch: [410/10000] Training loss: 0.0008109370945303546 / Validation loss: 0.0032389651721586184 / Long term Validation loss: 0.1211\n",
      "Epoch: [411/10000] Training loss: 0.000807304748813959 / Validation loss: 0.003235353368504235 / Long term Validation loss: 0.1213\n",
      "Epoch: [412/10000] Training loss: 0.0008036987733584822 / Validation loss: 0.003231792514153794 / Long term Validation loss: 0.1214\n",
      "Epoch: [413/10000] Training loss: 0.0008001189791625491 / Validation loss: 0.0032282814284482365 / Long term Validation loss: 0.1216\n",
      "Epoch: [414/10000] Training loss: 0.000796565178574215 / Validation loss: 0.0032248189538068876 / Long term Validation loss: 0.1218\n",
      "Epoch: [415/10000] Training loss: 0.0007930371852646342 / Validation loss: 0.003221403955193093 / Long term Validation loss: 0.1219\n",
      "Epoch: [416/10000] Training loss: 0.0007895348142025342 / Validation loss: 0.0032180353196018152 / Long term Validation loss: 0.1221\n",
      "Epoch: [417/10000] Training loss: 0.0007860578816294535 / Validation loss: 0.0032147119555599136 / Long term Validation loss: 0.1223\n",
      "Epoch: [418/10000] Training loss: 0.0007826062050357028 / Validation loss: 0.003211432792631863 / Long term Validation loss: 0.1224\n",
      "Epoch: [419/10000] Training loss: 0.0007791796031370385 / Validation loss: 0.0032081967809295853 / Long term Validation loss: 0.1226\n",
      "Epoch: [420/10000] Training loss: 0.0007757778958520512 / Validation loss: 0.003205002890631254 / Long term Validation loss: 0.1228\n",
      "Epoch: [421/10000] Training loss: 0.0007724009042802722 / Validation loss: 0.003201850111516786 / Long term Validation loss: 0.1229\n",
      "Epoch: [422/10000] Training loss: 0.0007690484506809966 / Validation loss: 0.0031987374525257444 / Long term Validation loss: 0.1231\n",
      "Epoch: [423/10000] Training loss: 0.000765720358452803 / Validation loss: 0.0031956639413378645 / Long term Validation loss: 0.1233\n",
      "Epoch: [424/10000] Training loss: 0.0007624164521137398 / Validation loss: 0.0031926286239707205 / Long term Validation loss: 0.1234\n",
      "Epoch: [425/10000] Training loss: 0.0007591365572821338 / Validation loss: 0.003189630564386227 / Long term Validation loss: 0.1236\n",
      "Epoch: [426/10000] Training loss: 0.0007558805006579953 / Validation loss: 0.003186668844099537 / Long term Validation loss: 0.1237\n",
      "Epoch: [427/10000] Training loss: 0.0007526481100049877 / Validation loss: 0.0031837425617890804 / Long term Validation loss: 0.1239\n",
      "Epoch: [428/10000] Training loss: 0.0007494392141329615 / Validation loss: 0.003180850832912322 / Long term Validation loss: 0.1241\n",
      "Epoch: [429/10000] Training loss: 0.0007462536428810464 / Validation loss: 0.0031779927893348537 / Long term Validation loss: 0.1242\n",
      "Epoch: [430/10000] Training loss: 0.0007430912271013065 / Validation loss: 0.0031751675789791324 / Long term Validation loss: 0.1244\n",
      "Epoch: [431/10000] Training loss: 0.000739951798642951 / Validation loss: 0.0031723743654942 / Long term Validation loss: 0.1246\n",
      "Epoch: [432/10000] Training loss: 0.0007368351903370847 / Validation loss: 0.00316961232794182 / Long term Validation loss: 0.1247\n",
      "Epoch: [433/10000] Training loss: 0.0007337412359819744 / Validation loss: 0.003166880660490835 / Long term Validation loss: 0.1249\n",
      "Epoch: [434/10000] Training loss: 0.0007306697703288031 / Validation loss: 0.0031641785721120996 / Long term Validation loss: 0.1250\n",
      "Epoch: [435/10000] Training loss: 0.0007276206290678893 / Validation loss: 0.0031615052862708254 / Long term Validation loss: 0.1252\n",
      "Epoch: [436/10000] Training loss: 0.0007245936488153584 / Validation loss: 0.003158860040619098 / Long term Validation loss: 0.1254\n",
      "Epoch: [437/10000] Training loss: 0.0007215886671002587 / Validation loss: 0.003156242086695567 / Long term Validation loss: 0.1255\n",
      "Epoch: [438/10000] Training loss: 0.0007186055223521248 / Validation loss: 0.003153650689639717 / Long term Validation loss: 0.1257\n",
      "Epoch: [439/10000] Training loss: 0.0007156440538889818 / Validation loss: 0.003151085127924329 / Long term Validation loss: 0.1258\n",
      "Epoch: [440/10000] Training loss: 0.0007127041019057869 / Validation loss: 0.0031485446931042004 / Long term Validation loss: 0.1260\n",
      "Epoch: [441/10000] Training loss: 0.0007097855074632924 / Validation loss: 0.0031460286895743502 / Long term Validation loss: 0.1261\n",
      "Epoch: [442/10000] Training loss: 0.0007068881124773111 / Validation loss: 0.003143536434329945 / Long term Validation loss: 0.1263\n",
      "Epoch: [443/10000] Training loss: 0.000704011759708366 / Validation loss: 0.0031410672567228097 / Long term Validation loss: 0.1265\n",
      "Epoch: [444/10000] Training loss: 0.0007011562927517083 / Validation loss: 0.00313862049821485 / Long term Validation loss: 0.1266\n",
      "Epoch: [445/10000] Training loss: 0.0006983215560276968 / Validation loss: 0.003136195512133531 / Long term Validation loss: 0.1268\n",
      "Epoch: [446/10000] Training loss: 0.0006955073947725333 / Validation loss: 0.003133791663436552 / Long term Validation loss: 0.1269\n",
      "Epoch: [447/10000] Training loss: 0.0006927136550293535 / Validation loss: 0.003131408328491 / Long term Validation loss: 0.1271\n",
      "Epoch: [448/10000] Training loss: 0.0006899401836396697 / Validation loss: 0.003129044894867511 / Long term Validation loss: 0.1272\n",
      "Epoch: [449/10000] Training loss: 0.0006871868282351544 / Validation loss: 0.0031267007611451296 / Long term Validation loss: 0.1274\n",
      "Epoch: [450/10000] Training loss: 0.0006844534372297565 / Validation loss: 0.003124375336719973 / Long term Validation loss: 0.1275\n",
      "Epoch: [451/10000] Training loss: 0.0006817398598121309 / Validation loss: 0.0031220680416118615 / Long term Validation loss: 0.1277\n",
      "Epoch: [452/10000] Training loss: 0.0006790459459383727 / Validation loss: 0.003119778306267112 / Long term Validation loss: 0.1279\n",
      "Epoch: [453/10000] Training loss: 0.000676371546325043 / Validation loss: 0.0031175055713603536 / Long term Validation loss: 0.1280\n",
      "Epoch: [454/10000] Training loss: 0.0006737165124424843 / Validation loss: 0.0031152492876012094 / Long term Validation loss: 0.1282\n",
      "Epoch: [455/10000] Training loss: 0.0006710806965084206 / Validation loss: 0.0031130089155513557 / Long term Validation loss: 0.1283\n",
      "Epoch: [456/10000] Training loss: 0.000668463951481838 / Validation loss: 0.0031107839254541454 / Long term Validation loss: 0.1285\n",
      "Epoch: [457/10000] Training loss: 0.0006658661310571445 / Validation loss: 0.0031085737970746275 / Long term Validation loss: 0.1286\n",
      "Epoch: [458/10000] Training loss: 0.0006632870896585955 / Validation loss: 0.0031063780195446757 / Long term Validation loss: 0.1288\n",
      "Epoch: [459/10000] Training loss: 0.0006607266824349776 / Validation loss: 0.003104196091207607 / Long term Validation loss: 0.1289\n",
      "Epoch: [460/10000] Training loss: 0.0006581847652545376 / Validation loss: 0.003102027519459404 / Long term Validation loss: 0.1291\n",
      "Epoch: [461/10000] Training loss: 0.0006556611947001515 / Validation loss: 0.0030998718205875703 / Long term Validation loss: 0.1292\n",
      "Epoch: [462/10000] Training loss: 0.0006531558280647229 / Validation loss: 0.0030977285196118047 / Long term Validation loss: 0.1294\n",
      "Epoch: [463/10000] Training loss: 0.0006506685233468099 / Validation loss: 0.003095597150131442 / Long term Validation loss: 0.1295\n",
      "Epoch: [464/10000] Training loss: 0.0006481991392464779 / Validation loss: 0.003093477254182567 / Long term Validation loss: 0.1296\n",
      "Epoch: [465/10000] Training loss: 0.0006457475351613716 / Validation loss: 0.0030913683821041076 / Long term Validation loss: 0.1298\n",
      "Epoch: [466/10000] Training loss: 0.0006433135711830008 / Validation loss: 0.0030892700924092503 / Long term Validation loss: 0.1299\n",
      "Epoch: [467/10000] Training loss: 0.0006408971080932338 / Validation loss: 0.003087181951657464 / Long term Validation loss: 0.1301\n",
      "Epoch: [468/10000] Training loss: 0.0006384980073609853 / Validation loss: 0.003085103534323967 / Long term Validation loss: 0.1302\n",
      "Epoch: [469/10000] Training loss: 0.0006361161311390975 / Validation loss: 0.003083034422666583 / Long term Validation loss: 0.1304\n",
      "Epoch: [470/10000] Training loss: 0.0006337513422614005 / Validation loss: 0.0030809742065928533 / Long term Validation loss: 0.1305\n",
      "Epoch: [471/10000] Training loss: 0.0006314035042399564 / Validation loss: 0.003078922483531404 / Long term Validation loss: 0.1306\n",
      "Epoch: [472/10000] Training loss: 0.0006290724812624753 / Validation loss: 0.0030768788583105233 / Long term Validation loss: 0.1308\n",
      "Epoch: [473/10000] Training loss: 0.0006267581381899072 / Validation loss: 0.003074842943044133 / Long term Validation loss: 0.1309\n",
      "Epoch: [474/10000] Training loss: 0.0006244603405541978 / Validation loss: 0.0030728143570227214 / Long term Validation loss: 0.1311\n",
      "Epoch: [475/10000] Training loss: 0.0006221789545562057 / Validation loss: 0.003070792726605525 / Long term Validation loss: 0.1312\n",
      "Epoch: [476/10000] Training loss: 0.0006199138470637742 / Validation loss: 0.003068777685111095 / Long term Validation loss: 0.1313\n",
      "Epoch: [477/10000] Training loss: 0.0006176648856099481 / Validation loss: 0.003066768872705654 / Long term Validation loss: 0.1315\n",
      "Epoch: [478/10000] Training loss: 0.000615431938391332 / Validation loss: 0.003064765936291168 / Long term Validation loss: 0.1316\n",
      "Epoch: [479/10000] Training loss: 0.0006132148742665872 / Validation loss: 0.0030627685293962365 / Long term Validation loss: 0.1317\n",
      "Epoch: [480/10000] Training loss: 0.00061101356275506 / Validation loss: 0.0030607763120723513 / Long term Validation loss: 0.1319\n",
      "Epoch: [481/10000] Training loss: 0.000608827874035541 / Validation loss: 0.003058788950796106 / Long term Validation loss: 0.1320\n",
      "Epoch: [482/10000] Training loss: 0.0006066576789451492 / Validation loss: 0.0030568061183756583 / Long term Validation loss: 0.1321\n",
      "Epoch: [483/10000] Training loss: 0.000604502848978336 / Validation loss: 0.0030548274938586484 / Long term Validation loss: 0.1323\n",
      "Epoch: [484/10000] Training loss: 0.0006023632562860036 / Validation loss: 0.003052852762439136 / Long term Validation loss: 0.1324\n",
      "Epoch: [485/10000] Training loss: 0.0006002387736747308 / Validation loss: 0.0030508816153628614 / Long term Validation loss: 0.1325\n",
      "Epoch: [486/10000] Training loss: 0.0005981292746061041 / Validation loss: 0.0030489137498321264 / Long term Validation loss: 0.1327\n",
      "Epoch: [487/10000] Training loss: 0.0005960346331961463 / Validation loss: 0.003046948868912659 / Long term Validation loss: 0.1328\n",
      "Epoch: [488/10000] Training loss: 0.0005939547242148416 / Validation loss: 0.0030449866814445227 / Long term Validation loss: 0.1329\n",
      "Epoch: [489/10000] Training loss: 0.0005918894230857544 / Validation loss: 0.0030430269019576434 / Long term Validation loss: 0.1330\n",
      "Epoch: [490/10000] Training loss: 0.0005898386058857344 / Validation loss: 0.0030410692505907604 / Long term Validation loss: 0.1332\n",
      "Epoch: [491/10000] Training loss: 0.0005878021493447071 / Validation loss: 0.00303911345301165 / Long term Validation loss: 0.1333\n",
      "Epoch: [492/10000] Training loss: 0.0005857799308455425 / Validation loss: 0.00303715924033671 / Long term Validation loss: 0.1334\n",
      "Epoch: [493/10000] Training loss: 0.0005837718284239982 / Validation loss: 0.0030352063490493076 / Long term Validation loss: 0.1335\n",
      "Epoch: [494/10000] Training loss: 0.000581777720768732 / Validation loss: 0.0030332545209178605 / Long term Validation loss: 0.1336\n",
      "Epoch: [495/10000] Training loss: 0.0005797974872213812 / Validation loss: 0.0030313035029154362 / Long term Validation loss: 0.1338\n",
      "Epoch: [496/10000] Training loss: 0.0005778310077767035 / Validation loss: 0.00302935304714247 / Long term Validation loss: 0.1339\n",
      "Epoch: [497/10000] Training loss: 0.0005758781630827796 / Validation loss: 0.003027402910753045 / Long term Validation loss: 0.1340\n",
      "Epoch: [498/10000] Training loss: 0.0005739388344412696 / Validation loss: 0.003025452855883797 / Long term Validation loss: 0.1341\n",
      "Epoch: [499/10000] Training loss: 0.0005720129038077226 / Validation loss: 0.003023502649583805 / Long term Validation loss: 0.1342\n",
      "Epoch: [500/10000] Training loss: 0.000570100253791934 / Validation loss: 0.00302155206374404 / Long term Validation loss: 0.1343\n",
      "Epoch: [501/10000] Training loss: 0.000568200767658347 / Validation loss: 0.0030196008750259812 / Long term Validation loss: 0.1345\n",
      "Epoch: [502/10000] Training loss: 0.0005663143293264933 / Validation loss: 0.0030176488647901804 / Long term Validation loss: 0.1346\n",
      "Epoch: [503/10000] Training loss: 0.0005644408233714723 / Validation loss: 0.00301569581902616 / Long term Validation loss: 0.1347\n",
      "Epoch: [504/10000] Training loss: 0.0005625801350244629 / Validation loss: 0.0030137415282848385 / Long term Validation loss: 0.1348\n",
      "Epoch: [505/10000] Training loss: 0.0005607321501732669 / Validation loss: 0.003011785787613688 / Long term Validation loss: 0.1349\n",
      "Epoch: [506/10000] Training loss: 0.000558896755362881 / Validation loss: 0.003009828396493878 / Long term Validation loss: 0.1350\n",
      "Epoch: [507/10000] Training loss: 0.0005570738377960924 / Validation loss: 0.0030078691587781224 / Long term Validation loss: 0.1351\n",
      "Epoch: [508/10000] Training loss: 0.0005552632853340958 / Validation loss: 0.0030059078826282152 / Long term Validation loss: 0.1352\n",
      "Epoch: [509/10000] Training loss: 0.0005534649864971291 / Validation loss: 0.003003944380452073 / Long term Validation loss: 0.1353\n",
      "Epoch: [510/10000] Training loss: 0.0005516788304651222 / Validation loss: 0.0030019784688410005 / Long term Validation loss: 0.1354\n",
      "Epoch: [511/10000] Training loss: 0.0005499047070783593 / Validation loss: 0.0030000099685081856 / Long term Validation loss: 0.1355\n",
      "Epoch: [512/10000] Training loss: 0.0005481425068381506 / Validation loss: 0.0029980387042293174 / Long term Validation loss: 0.1356\n",
      "Epoch: [513/10000] Training loss: 0.0005463921209075102 / Validation loss: 0.002996064504785262 / Long term Validation loss: 0.1357\n",
      "Epoch: [514/10000] Training loss: 0.0005446534411118381 / Validation loss: 0.0029940872029061965 / Long term Validation loss: 0.1358\n",
      "Epoch: [515/10000] Training loss: 0.0005429263599396058 / Validation loss: 0.00299210663521617 / Long term Validation loss: 0.1359\n",
      "Epoch: [516/10000] Training loss: 0.0005412107705430372 / Validation loss: 0.0029901226421774605 / Long term Validation loss: 0.1360\n",
      "Epoch: [517/10000] Training loss: 0.0005395065667387896 / Validation loss: 0.0029881350680347266 / Long term Validation loss: 0.1361\n",
      "Epoch: [518/10000] Training loss: 0.0005378136430086251 / Validation loss: 0.002986143760759557 / Long term Validation loss: 0.1362\n",
      "Epoch: [519/10000] Training loss: 0.0005361318945000746 / Validation loss: 0.0029841485719962536 / Long term Validation loss: 0.1363\n",
      "Epoch: [520/10000] Training loss: 0.0005344612170270921 / Validation loss: 0.002982149357009308 / Long term Validation loss: 0.1364\n",
      "Epoch: [521/10000] Training loss: 0.0005328015070706943 / Validation loss: 0.0029801459746324685 / Long term Validation loss: 0.1365\n",
      "Epoch: [522/10000] Training loss: 0.0005311526617795874 / Validation loss: 0.00297813828721877 / Long term Validation loss: 0.1366\n",
      "Epoch: [523/10000] Training loss: 0.0005295145789707755 / Validation loss: 0.00297612616059082 / Long term Validation loss: 0.1367\n",
      "Epoch: [524/10000] Training loss: 0.0005278871571301497 / Validation loss: 0.0029741094639909793 / Long term Validation loss: 0.1368\n",
      "Epoch: [525/10000] Training loss: 0.000526270295413057 / Validation loss: 0.002972088070031572 / Long term Validation loss: 0.1369\n",
      "Epoch: [526/10000] Training loss: 0.000524663893644845 / Validation loss: 0.0029700618546456806 / Long term Validation loss: 0.1369\n",
      "Epoch: [527/10000] Training loss: 0.0005230678523213823 / Validation loss: 0.0029680306970391135 / Long term Validation loss: 0.1370\n",
      "Epoch: [528/10000] Training loss: 0.000521482072609553 / Validation loss: 0.0029659944796437583 / Long term Validation loss: 0.1371\n",
      "Epoch: [529/10000] Training loss: 0.0005199064563477219 / Validation loss: 0.00296395308807209 / Long term Validation loss: 0.1372\n",
      "Epoch: [530/10000] Training loss: 0.0005183409060461711 / Validation loss: 0.0029619064110723288 / Long term Validation loss: 0.1373\n",
      "Epoch: [531/10000] Training loss: 0.0005167853248875049 / Validation loss: 0.0029598543404837554 / Long term Validation loss: 0.1374\n",
      "Epoch: [532/10000] Training loss: 0.000515239616727022 / Validation loss: 0.002957796771192037 / Long term Validation loss: 0.1374\n",
      "Epoch: [533/10000] Training loss: 0.0005137036860930531 / Validation loss: 0.0029557336010848276 / Long term Validation loss: 0.1375\n",
      "Epoch: [534/10000] Training loss: 0.0005121774381872633 / Validation loss: 0.0029536647310080756 / Long term Validation loss: 0.1376\n",
      "Epoch: [535/10000] Training loss: 0.0005106607788849178 / Validation loss: 0.0029515900647234202 / Long term Validation loss: 0.1377\n",
      "Epoch: [536/10000] Training loss: 0.0005091536147351086 / Validation loss: 0.0029495095088666982 / Long term Validation loss: 0.1378\n",
      "Epoch: [537/10000] Training loss: 0.0005076558529609433 / Validation loss: 0.00294742297290729 / Long term Validation loss: 0.1378\n",
      "Epoch: [538/10000] Training loss: 0.0005061674014596938 / Validation loss: 0.002945330369107869 / Long term Validation loss: 0.1379\n",
      "Epoch: [539/10000] Training loss: 0.000504688168802902 / Validation loss: 0.0029432316124843034 / Long term Validation loss: 0.1380\n",
      "Epoch: [540/10000] Training loss: 0.0005032180642364453 / Validation loss: 0.0029411266207657397 / Long term Validation loss: 0.1381\n",
      "Epoch: [541/10000] Training loss: 0.0005017569976805575 / Validation loss: 0.0029390153143551124 / Long term Validation loss: 0.1381\n",
      "Epoch: [542/10000] Training loss: 0.0005003048797298056 / Validation loss: 0.002936897616290452 / Long term Validation loss: 0.1382\n",
      "Epoch: [543/10000] Training loss: 0.0004988616216530225 / Validation loss: 0.0029347734522071588 / Long term Validation loss: 0.1383\n",
      "Epoch: [544/10000] Training loss: 0.0004974271353931942 / Validation loss: 0.0029326427503011086 / Long term Validation loss: 0.1384\n",
      "Epoch: [545/10000] Training loss: 0.0004960013335672989 / Validation loss: 0.0029305054412923693 / Long term Validation loss: 0.1384\n",
      "Epoch: [546/10000] Training loss: 0.0004945841294661027 / Validation loss: 0.0029283614583891894 / Long term Validation loss: 0.1385\n",
      "Epoch: [547/10000] Training loss: 0.0004931754370539043 / Validation loss: 0.0029262107372522 / Long term Validation loss: 0.1386\n",
      "Epoch: [548/10000] Training loss: 0.0004917751709682335 / Validation loss: 0.0029240532159589255 / Long term Validation loss: 0.1386\n",
      "Epoch: [549/10000] Training loss: 0.0004903832465195006 / Validation loss: 0.002921888834968872 / Long term Validation loss: 0.1387\n",
      "Epoch: [550/10000] Training loss: 0.0004889995796905978 / Validation loss: 0.0029197175370894047 / Long term Validation loss: 0.1388\n",
      "Epoch: [551/10000] Training loss: 0.0004876240871364491 / Validation loss: 0.0029175392674424113 / Long term Validation loss: 0.1388\n",
      "Epoch: [552/10000] Training loss: 0.00048625668618351367 / Validation loss: 0.0029153539734316338 / Long term Validation loss: 0.1389\n",
      "Epoch: [553/10000] Training loss: 0.00048489729482923625 / Validation loss: 0.002913161604710389 / Long term Validation loss: 0.1390\n",
      "Epoch: [554/10000] Training loss: 0.0004835458317414498 / Validation loss: 0.0029109621131495687 / Long term Validation loss: 0.1390\n",
      "Epoch: [555/10000] Training loss: 0.00048220221625772645 / Validation loss: 0.002908755452805917 / Long term Validation loss: 0.1391\n",
      "Epoch: [556/10000] Training loss: 0.00048086636838467795 / Validation loss: 0.0029065415798907777 / Long term Validation loss: 0.1392\n",
      "Epoch: [557/10000] Training loss: 0.00047953820879720647 / Validation loss: 0.00290432045273945 / Long term Validation loss: 0.1392\n",
      "Epoch: [558/10000] Training loss: 0.00047821765883770354 / Validation loss: 0.0029020920317812888 / Long term Validation loss: 0.1393\n",
      "Epoch: [559/10000] Training loss: 0.00047690464051519866 / Validation loss: 0.002899856279510438 / Long term Validation loss: 0.1393\n",
      "Epoch: [560/10000] Training loss: 0.00047559907650445766 / Validation loss: 0.002897613160457035 / Long term Validation loss: 0.1394\n",
      "Epoch: [561/10000] Training loss: 0.00047430089014502993 / Validation loss: 0.00289536264115877 / Long term Validation loss: 0.1395\n",
      "Epoch: [562/10000] Training loss: 0.00047301000544024515 / Validation loss: 0.002893104690132716 / Long term Validation loss: 0.1395\n",
      "Epoch: [563/10000] Training loss: 0.0004717263470561598 / Validation loss: 0.0028908392778475697 / Long term Validation loss: 0.1396\n",
      "Epoch: [564/10000] Training loss: 0.00047044984032045234 / Validation loss: 0.002888566376696421 / Long term Validation loss: 0.1396\n",
      "Epoch: [565/10000] Training loss: 0.00046918041122126963 / Validation loss: 0.002886285960970149 / Long term Validation loss: 0.1397\n",
      "Epoch: [566/10000] Training loss: 0.0004679179864060225 / Validation loss: 0.002883998006831426 / Long term Validation loss: 0.1397\n",
      "Epoch: [567/10000] Training loss: 0.0004666624931801315 / Validation loss: 0.0028817024922892388 / Long term Validation loss: 0.1398\n",
      "Epoch: [568/10000] Training loss: 0.00046541385950572416 / Validation loss: 0.002879399397173756 / Long term Validation loss: 0.1398\n",
      "Epoch: [569/10000] Training loss: 0.00046417201400028184 / Validation loss: 0.0028770887031115597 / Long term Validation loss: 0.1399\n",
      "Epoch: [570/10000] Training loss: 0.00046293688593523837 / Validation loss: 0.0028747703935011948 / Long term Validation loss: 0.1399\n",
      "Epoch: [571/10000] Training loss: 0.00046170840523453047 / Validation loss: 0.002872444453489203 / Long term Validation loss: 0.1400\n",
      "Epoch: [572/10000] Training loss: 0.00046048650247309966 / Validation loss: 0.0028701108699467036 / Long term Validation loss: 0.1400\n",
      "Epoch: [573/10000] Training loss: 0.00045927110887534567 / Validation loss: 0.0028677696314465427 / Long term Validation loss: 0.1401\n",
      "Epoch: [574/10000] Training loss: 0.00045806215631353413 / Validation loss: 0.002865420728240912 / Long term Validation loss: 0.1401\n",
      "Epoch: [575/10000] Training loss: 0.000456859577306156 / Validation loss: 0.002863064152239397 / Long term Validation loss: 0.1402\n",
      "Epoch: [576/10000] Training loss: 0.0004556633050162407 / Validation loss: 0.002860699896987326 / Long term Validation loss: 0.1402\n",
      "Epoch: [577/10000] Training loss: 0.0004544732732496241 / Validation loss: 0.00285832795764447 / Long term Validation loss: 0.1403\n",
      "Epoch: [578/10000] Training loss: 0.00045328941645316964 / Validation loss: 0.0028559483309641676 / Long term Validation loss: 0.1403\n",
      "Epoch: [579/10000] Training loss: 0.0004521116697129454 / Validation loss: 0.0028535610152729102 / Long term Validation loss: 0.1404\n",
      "Epoch: [580/10000] Training loss: 0.0004509399687523559 / Validation loss: 0.0028511660104504613 / Long term Validation loss: 0.1404\n",
      "Epoch: [581/10000] Training loss: 0.0004497742499302303 / Validation loss: 0.002848763317910408 / Long term Validation loss: 0.1405\n",
      "Epoch: [582/10000] Training loss: 0.0004486144502388671 / Validation loss: 0.0028463529405811155 / Long term Validation loss: 0.1405\n",
      "Epoch: [583/10000] Training loss: 0.0004474605073020354 / Validation loss: 0.0028439348828870146 / Long term Validation loss: 0.1406\n",
      "Epoch: [584/10000] Training loss: 0.0004463123593729341 / Validation loss: 0.0028415091507301886 / Long term Validation loss: 0.1406\n",
      "Epoch: [585/10000] Training loss: 0.00044516994533210934 / Validation loss: 0.0028390757514723548 / Long term Validation loss: 0.1407\n",
      "Epoch: [586/10000] Training loss: 0.0004440332046853298 / Validation loss: 0.0028366346939172576 / Long term Validation loss: 0.1407\n",
      "Epoch: [587/10000] Training loss: 0.0004429020775614227 / Validation loss: 0.0028341859882935034 / Long term Validation loss: 0.1407\n",
      "Epoch: [588/10000] Training loss: 0.0004417765047100677 / Validation loss: 0.0028317296462378088 / Long term Validation loss: 0.1408\n",
      "Epoch: [589/10000] Training loss: 0.00044065642749955305 / Validation loss: 0.0028292656807786176 / Long term Validation loss: 0.1408\n",
      "Epoch: [590/10000] Training loss: 0.00043954178791449184 / Validation loss: 0.002826794106320015 / Long term Validation loss: 0.1409\n",
      "Epoch: [591/10000] Training loss: 0.00043843252855349946 / Validation loss: 0.002824314938625928 / Long term Validation loss: 0.1409\n",
      "Epoch: [592/10000] Training loss: 0.00043732859262683455 / Validation loss: 0.0028218281948046704 / Long term Validation loss: 0.1409\n",
      "Epoch: [593/10000] Training loss: 0.0004362299239540007 / Validation loss: 0.002819333893293837 / Long term Validation loss: 0.1410\n",
      "Epoch: [594/10000] Training loss: 0.0004351364669613136 / Validation loss: 0.002816832053845566 / Long term Validation loss: 0.1410\n",
      "Epoch: [595/10000] Training loss: 0.0004340481666794307 / Validation loss: 0.0028143226975121728 / Long term Validation loss: 0.1410\n",
      "Epoch: [596/10000] Training loss: 0.00043296496874084637 / Validation loss: 0.0028118058466320866 / Long term Validation loss: 0.1411\n",
      "Epoch: [597/10000] Training loss: 0.0004318868193773514 / Validation loss: 0.0028092815248160524 / Long term Validation loss: 0.1411\n",
      "Epoch: [598/10000] Training loss: 0.0004308136654174585 / Validation loss: 0.0028067497569335934 / Long term Validation loss: 0.1411\n",
      "Epoch: [599/10000] Training loss: 0.0004297454542837945 / Validation loss: 0.0028042105690997293 / Long term Validation loss: 0.1412\n",
      "Epoch: [600/10000] Training loss: 0.0004286821339904592 / Validation loss: 0.0028016639886620062 / Long term Validation loss: 0.1412\n",
      "Epoch: [601/10000] Training loss: 0.0004276236531403515 / Validation loss: 0.0027991100441878214 / Long term Validation loss: 0.1412\n",
      "Epoch: [602/10000] Training loss: 0.00042656996092246363 / Validation loss: 0.002796548765452032 / Long term Validation loss: 0.1413\n",
      "Epoch: [603/10000] Training loss: 0.000425521007109145 / Validation loss: 0.002793980183424808 / Long term Validation loss: 0.1413\n",
      "Epoch: [604/10000] Training loss: 0.0004244767420533341 / Validation loss: 0.002791404330259721 / Long term Validation loss: 0.1413\n",
      "Epoch: [605/10000] Training loss: 0.00042343711668576026 / Validation loss: 0.0027888212392819874 / Long term Validation loss: 0.1414\n",
      "Epoch: [606/10000] Training loss: 0.0004224020825121167 / Validation loss: 0.0027862309449769784 / Long term Validation loss: 0.1414\n",
      "Epoch: [607/10000] Training loss: 0.00042137159161020416 / Validation loss: 0.0027836334829788906 / Long term Validation loss: 0.1414\n",
      "Epoch: [608/10000] Training loss: 0.00042034559662704527 / Validation loss: 0.0027810288900596758 / Long term Validation loss: 0.1414\n",
      "Epoch: [609/10000] Training loss: 0.0004193240507759726 / Validation loss: 0.00277841720411817 / Long term Validation loss: 0.1415\n",
      "Epoch: [610/10000] Training loss: 0.0004183069078336871 / Validation loss: 0.0027757984641693986 / Long term Validation loss: 0.1415\n",
      "Epoch: [611/10000] Training loss: 0.00041729412213729216 / Validation loss: 0.002773172710334018 / Long term Validation loss: 0.1415\n",
      "Epoch: [612/10000] Training loss: 0.00041628564858129925 / Validation loss: 0.002770539983827905 / Long term Validation loss: 0.1416\n",
      "Epoch: [613/10000] Training loss: 0.00041528144261460906 / Validation loss: 0.0027679003269518603 / Long term Validation loss: 0.1416\n",
      "Epoch: [614/10000] Training loss: 0.0004142814602374673 / Validation loss: 0.0027652537830814734 / Long term Validation loss: 0.1416\n",
      "Epoch: [615/10000] Training loss: 0.00041328565799839427 / Validation loss: 0.002762600396657107 / Long term Validation loss: 0.1416\n",
      "Epoch: [616/10000] Training loss: 0.00041229399299109315 / Validation loss: 0.0027599402131740225 / Long term Validation loss: 0.1416\n",
      "Epoch: [617/10000] Training loss: 0.00041130642285133094 / Validation loss: 0.0027572732791725867 / Long term Validation loss: 0.1417\n",
      "Epoch: [618/10000] Training loss: 0.0004103229057538 / Validation loss: 0.0027545996422285507 / Long term Validation loss: 0.1417\n",
      "Epoch: [619/10000] Training loss: 0.0004093434004089529 / Validation loss: 0.0027519193509433905 / Long term Validation loss: 0.1417\n",
      "Epoch: [620/10000] Training loss: 0.00040836786605981883 / Validation loss: 0.0027492324549346966 / Long term Validation loss: 0.1417\n",
      "Epoch: [621/10000] Training loss: 0.00040739626247879545 / Validation loss: 0.0027465390048266275 / Long term Validation loss: 0.1418\n",
      "Epoch: [622/10000] Training loss: 0.0004064285499644209 / Validation loss: 0.002743839052240403 / Long term Validation loss: 0.1418\n",
      "Epoch: [623/10000] Training loss: 0.0004054646893381248 / Validation loss: 0.002741132649784862 / Long term Validation loss: 0.1418\n",
      "Epoch: [624/10000] Training loss: 0.00040450464194095794 / Validation loss: 0.0027384198510469987 / Long term Validation loss: 0.1418\n",
      "Epoch: [625/10000] Training loss: 0.00040354836963030417 / Validation loss: 0.002735700710582503 / Long term Validation loss: 0.1418\n",
      "Epoch: [626/10000] Training loss: 0.00040259583477657073 / Validation loss: 0.0027329752839062855 / Long term Validation loss: 0.1418\n",
      "Epoch: [627/10000] Training loss: 0.0004016470002598605 / Validation loss: 0.0027302436274829695 / Long term Validation loss: 0.1419\n",
      "Epoch: [628/10000] Training loss: 0.00040070182946662624 / Validation loss: 0.0027275057987173673 / Long term Validation loss: 0.1419\n",
      "Epoch: [629/10000] Training loss: 0.00039976028628630584 / Validation loss: 0.0027247618559449183 / Long term Validation loss: 0.1419\n",
      "Epoch: [630/10000] Training loss: 0.0003988223351079403 / Validation loss: 0.0027220118584220868 / Long term Validation loss: 0.1419\n",
      "Epoch: [631/10000] Training loss: 0.0003978879408167751 / Validation loss: 0.0027192558663166797 / Long term Validation loss: 0.1419\n",
      "Epoch: [632/10000] Training loss: 0.0003969570687908434 / Validation loss: 0.002716493940698091 / Long term Validation loss: 0.1419\n",
      "Epoch: [633/10000] Training loss: 0.0003960296848975338 / Validation loss: 0.0027137261435274426 / Long term Validation loss: 0.1420\n",
      "Epoch: [634/10000] Training loss: 0.0003951057554901431 / Validation loss: 0.002710952537647635 / Long term Validation loss: 0.1420\n",
      "Epoch: [635/10000] Training loss: 0.0003941852474044107 / Validation loss: 0.0027081731867732886 / Long term Validation loss: 0.1420\n",
      "Epoch: [636/10000] Training loss: 0.00039326812795504093 / Validation loss: 0.0027053881554806028 / Long term Validation loss: 0.1420\n",
      "Epoch: [637/10000] Training loss: 0.0003923543649322094 / Validation loss: 0.0027025975091970497 / Long term Validation loss: 0.1420\n",
      "Epoch: [638/10000] Training loss: 0.00039144392659805366 / Validation loss: 0.002699801314190977 / Long term Validation loss: 0.1420\n",
      "Epoch: [639/10000] Training loss: 0.0003905367816831529 / Validation loss: 0.002696999637561018 / Long term Validation loss: 0.1420\n",
      "Epoch: [640/10000] Training loss: 0.0003896328993829919 / Validation loss: 0.0026941925472253733 / Long term Validation loss: 0.1420\n",
      "Epoch: [641/10000] Training loss: 0.0003887322493544118 / Validation loss: 0.002691380111910919 / Long term Validation loss: 0.1421\n",
      "Epoch: [642/10000] Training loss: 0.0003878348017120502 / Validation loss: 0.0026885624011421497 / Long term Validation loss: 0.1421\n",
      "Epoch: [643/10000] Training loss: 0.00038694052702476634 / Validation loss: 0.002685739485229977 / Long term Validation loss: 0.1421\n",
      "Epoch: [644/10000] Training loss: 0.0003860493963120563 / Validation loss: 0.002682911435260308 / Long term Validation loss: 0.1421\n",
      "Epoch: [645/10000] Training loss: 0.0003851613810404555 / Validation loss: 0.0026800783230824603 / Long term Validation loss: 0.1421\n",
      "Epoch: [646/10000] Training loss: 0.0003842764531199307 / Validation loss: 0.0026772402212973598 / Long term Validation loss: 0.1421\n",
      "Epoch: [647/10000] Training loss: 0.00038339458490026067 / Validation loss: 0.0026743972032455435 / Long term Validation loss: 0.1421\n",
      "Epoch: [648/10000] Training loss: 0.0003825157491674071 / Validation loss: 0.0026715493429949567 / Long term Validation loss: 0.1421\n",
      "Epoch: [649/10000] Training loss: 0.00038163991913987453 / Validation loss: 0.00266869671532853 / Long term Validation loss: 0.1421\n",
      "Epoch: [650/10000] Training loss: 0.00038076706846506175 / Validation loss: 0.0026658393957315683 / Long term Validation loss: 0.1421\n",
      "Epoch: [651/10000] Training loss: 0.0003798971712156033 / Validation loss: 0.0026629774603788973 / Long term Validation loss: 0.1421\n",
      "Epoch: [652/10000] Training loss: 0.00037903020188570133 / Validation loss: 0.0026601109861217912 / Long term Validation loss: 0.1421\n",
      "Epoch: [653/10000] Training loss: 0.0003781661353874511 / Validation loss: 0.0026572400504746717 / Long term Validation loss: 0.1421\n",
      "Epoch: [654/10000] Training loss: 0.00037730494704715626 / Validation loss: 0.0026543647316015725 / Long term Validation loss: 0.1422\n",
      "Epoch: [655/10000] Training loss: 0.00037644661260163674 / Validation loss: 0.0026514851083023787 / Long term Validation loss: 0.1422\n",
      "Epoch: [656/10000] Training loss: 0.0003755911081945304 / Validation loss: 0.002648601259998835 / Long term Validation loss: 0.1422\n",
      "Epoch: [657/10000] Training loss: 0.00037473841037258645 / Validation loss: 0.002645713266720325 / Long term Validation loss: 0.1422\n",
      "Epoch: [658/10000] Training loss: 0.0003738884960819527 / Validation loss: 0.0026428212090894134 / Long term Validation loss: 0.1422\n",
      "Epoch: [659/10000] Training loss: 0.00037304134266445644 / Validation loss: 0.002639925168307143 / Long term Validation loss: 0.1422\n",
      "Epoch: [660/10000] Training loss: 0.0003721969278538797 / Validation loss: 0.002637025226138111 / Long term Validation loss: 0.1422\n",
      "Epoch: [661/10000] Training loss: 0.00037135522977222845 / Validation loss: 0.0026341214648952973 / Long term Validation loss: 0.1422\n",
      "Epoch: [662/10000] Training loss: 0.00037051622692599724 / Validation loss: 0.0026312139674246806 / Long term Validation loss: 0.1422\n",
      "Epoch: [663/10000] Training loss: 0.0003696798982024286 / Validation loss: 0.002628302817089611 / Long term Validation loss: 0.1422\n",
      "Epoch: [664/10000] Training loss: 0.00036884622286576836 / Validation loss: 0.0026253880977549666 / Long term Validation loss: 0.1422\n",
      "Epoch: [665/10000] Training loss: 0.0003680151805535168 / Validation loss: 0.002622469893771078 / Long term Validation loss: 0.1422\n",
      "Epoch: [666/10000] Training loss: 0.000367186751272676 / Validation loss: 0.0026195482899574437 / Long term Validation loss: 0.1422\n",
      "Epoch: [667/10000] Training loss: 0.00036636091539599445 / Validation loss: 0.0026166233715862065 / Long term Validation loss: 0.1422\n",
      "Epoch: [668/10000] Training loss: 0.00036553765365820805 / Validation loss: 0.0026136952243654347 / Long term Validation loss: 0.1422\n",
      "Epoch: [669/10000] Training loss: 0.0003647169471522789 / Validation loss: 0.002610763934422205 / Long term Validation loss: 0.1422\n",
      "Epoch: [670/10000] Training loss: 0.00036389877732563217 / Validation loss: 0.002607829588285455 / Long term Validation loss: 0.1422\n",
      "Epoch: [671/10000] Training loss: 0.0003630831259763909 / Validation loss: 0.0026048922728686805 / Long term Validation loss: 0.1422\n",
      "Epoch: [672/10000] Training loss: 0.00036226997524960934 / Validation loss: 0.002601952075452398 / Long term Validation loss: 0.1422\n",
      "Epoch: [673/10000] Training loss: 0.0003614593076335057 / Validation loss: 0.0025990090836664715 / Long term Validation loss: 0.1422\n",
      "Epoch: [674/10000] Training loss: 0.0003606511059556938 / Validation loss: 0.0025960633854722076 / Long term Validation loss: 0.1422\n",
      "Epoch: [675/10000] Training loss: 0.00035984535337941413 / Validation loss: 0.002593115069144329 / Long term Validation loss: 0.1422\n",
      "Epoch: [676/10000] Training loss: 0.0003590420333997665 / Validation loss: 0.0025901642232527733 / Long term Validation loss: 0.1422\n",
      "Epoch: [677/10000] Training loss: 0.00035824112983994113 / Validation loss: 0.0025872109366443254 / Long term Validation loss: 0.1422\n",
      "Epoch: [678/10000] Training loss: 0.00035744262684745217 / Validation loss: 0.002584255298424126 / Long term Validation loss: 0.1422\n",
      "Epoch: [679/10000] Training loss: 0.0003566465088903717 / Validation loss: 0.002581297397937039 / Long term Validation loss: 0.1422\n",
      "Epoch: [680/10000] Training loss: 0.00035585276075356554 / Validation loss: 0.0025783373247488706 / Long term Validation loss: 0.1422\n",
      "Epoch: [681/10000] Training loss: 0.0003550613675349313 / Validation loss: 0.0025753751686275017 / Long term Validation loss: 0.1422\n",
      "Epoch: [682/10000] Training loss: 0.000354272314641638 / Validation loss: 0.0025724110195238974 / Long term Validation loss: 0.1422\n",
      "Epoch: [683/10000] Training loss: 0.0003534855877863692 / Validation loss: 0.0025694449675530287 / Long term Validation loss: 0.1422\n",
      "Epoch: [684/10000] Training loss: 0.00035270117298356933 / Validation loss: 0.002566477102974694 / Long term Validation loss: 0.1422\n",
      "Epoch: [685/10000] Training loss: 0.00035191905654569257 / Validation loss: 0.0025635075161742726 / Long term Validation loss: 0.1422\n",
      "Epoch: [686/10000] Training loss: 0.00035113922507945663 / Validation loss: 0.0025605362976434116 / Long term Validation loss: 0.1422\n",
      "Epoch: [687/10000] Training loss: 0.00035036166548209997 / Validation loss: 0.0025575635379606453 / Long term Validation loss: 0.1422\n",
      "Epoch: [688/10000] Training loss: 0.00034958636493764443 / Validation loss: 0.002554589327771996 / Long term Validation loss: 0.1422\n",
      "Epoch: [689/10000] Training loss: 0.0003488133109131622 / Validation loss: 0.002551613757771509 / Long term Validation loss: 0.1422\n",
      "Epoch: [690/10000] Training loss: 0.00034804249115504834 / Validation loss: 0.0025486369186817947 / Long term Validation loss: 0.1422\n",
      "Epoch: [691/10000] Training loss: 0.0003472738936852989 / Validation loss: 0.0025456589012345355 / Long term Validation loss: 0.1422\n",
      "Epoch: [692/10000] Training loss: 0.00034650750679779597 / Validation loss: 0.0025426797961510096 / Long term Validation loss: 0.1422\n",
      "Epoch: [693/10000] Training loss: 0.0003457433190545978 / Validation loss: 0.002539699694122608 / Long term Validation loss: 0.1422\n",
      "Epoch: [694/10000] Training loss: 0.00034498131928223727 / Validation loss: 0.0025367186857914027 / Long term Validation loss: 0.1422\n",
      "Epoch: [695/10000] Training loss: 0.0003442214965680262 / Validation loss: 0.002533736861730716 / Long term Validation loss: 0.1422\n",
      "Epoch: [696/10000] Training loss: 0.0003434638402563684 / Validation loss: 0.0025307543124257594 / Long term Validation loss: 0.1422\n",
      "Epoch: [697/10000] Training loss: 0.00034270833994508017 / Validation loss: 0.0025277711282543206 / Long term Validation loss: 0.1422\n",
      "Epoch: [698/10000] Training loss: 0.00034195498548171923 / Validation loss: 0.002524787399467516 / Long term Validation loss: 0.1422\n",
      "Epoch: [699/10000] Training loss: 0.0003412037669599224 / Validation loss: 0.0025218032161706304 / Long term Validation loss: 0.1422\n",
      "Epoch: [700/10000] Training loss: 0.00034045467471575177 / Validation loss: 0.002518818668304032 / Long term Validation loss: 0.1422\n",
      "Epoch: [701/10000] Training loss: 0.00033970769932405165 / Validation loss: 0.0025158338456242172 / Long term Validation loss: 0.1422\n",
      "Epoch: [702/10000] Training loss: 0.00033896283159481417 / Validation loss: 0.0025128488376849386 / Long term Validation loss: 0.1422\n",
      "Epoch: [703/10000] Training loss: 0.00033822006256955536 / Validation loss: 0.002509863733818472 / Long term Validation loss: 0.1422\n",
      "Epoch: [704/10000] Training loss: 0.00033747938351770215 / Validation loss: 0.002506878623117028 / Long term Validation loss: 0.1421\n",
      "Epoch: [705/10000] Training loss: 0.00033674078593298956 / Validation loss: 0.0025038935944142744 / Long term Validation loss: 0.1421\n",
      "Epoch: [706/10000] Training loss: 0.00033600426152987 / Validation loss: 0.0025009087362670526 / Long term Validation loss: 0.1421\n",
      "Epoch: [707/10000] Training loss: 0.0003352698022399335 / Validation loss: 0.0024979241369372323 / Long term Validation loss: 0.1421\n",
      "Epoch: [708/10000] Training loss: 0.00033453740020834047 / Validation loss: 0.002494939884373772 / Long term Validation loss: 0.1421\n",
      "Epoch: [709/10000] Training loss: 0.0003338070477902664 / Validation loss: 0.0024919560661949298 / Long term Validation loss: 0.1421\n",
      "Epoch: [710/10000] Training loss: 0.0003330787375473593 / Validation loss: 0.0024889727696706886 / Long term Validation loss: 0.1421\n",
      "Epoch: [711/10000] Training loss: 0.00033235246224421074 / Validation loss: 0.0024859900817053903 / Long term Validation loss: 0.1421\n",
      "Epoch: [712/10000] Training loss: 0.0003316282148448396 / Validation loss: 0.002483008088820572 / Long term Validation loss: 0.1421\n",
      "Epoch: [713/10000] Training loss: 0.0003309059885091898 / Validation loss: 0.0024800268771380347 / Long term Validation loss: 0.1421\n",
      "Epoch: [714/10000] Training loss: 0.0003301857765896426 / Validation loss: 0.002477046532363139 / Long term Validation loss: 0.1421\n",
      "Epoch: [715/10000] Training loss: 0.0003294675726275427 / Validation loss: 0.002474067139768338 / Long term Validation loss: 0.1421\n",
      "Epoch: [716/10000] Training loss: 0.00032875137034973976 / Validation loss: 0.002471088784176968 / Long term Validation loss: 0.1421\n",
      "Epoch: [717/10000] Training loss: 0.0003280371636651446 / Validation loss: 0.0024681115499472803 / Long term Validation loss: 0.1421\n",
      "Epoch: [718/10000] Training loss: 0.0003273249466613011 / Validation loss: 0.00246513552095675 / Long term Validation loss: 0.1421\n",
      "Epoch: [719/10000] Training loss: 0.00032661471360097413 / Validation loss: 0.0024621607805866514 / Long term Validation loss: 0.1421\n",
      "Epoch: [720/10000] Training loss: 0.00032590645891875347 / Validation loss: 0.0024591874117069023 / Long term Validation loss: 0.1421\n",
      "Epoch: [721/10000] Training loss: 0.0003252001772176742 / Validation loss: 0.0024562154966612116 / Long term Validation loss: 0.1421\n",
      "Epoch: [722/10000] Training loss: 0.0003244958632658544 / Validation loss: 0.00245324511725249 / Long term Validation loss: 0.1421\n",
      "Epoch: [723/10000] Training loss: 0.0003237935119931494 / Validation loss: 0.0024502763547285845 / Long term Validation loss: 0.1420\n",
      "Epoch: [724/10000] Training loss: 0.0003230931184878239 / Validation loss: 0.0024473092897683 / Long term Validation loss: 0.1420\n",
      "Epoch: [725/10000] Training loss: 0.0003223946779932423 / Validation loss: 0.0024443440024677233 / Long term Validation loss: 0.1420\n",
      "Epoch: [726/10000] Training loss: 0.0003216981859045767 / Validation loss: 0.002441380572326882 / Long term Validation loss: 0.1420\n",
      "Epoch: [727/10000] Training loss: 0.0003210036377655335 / Validation loss: 0.0024384190782366947 / Long term Validation loss: 0.1420\n",
      "Epoch: [728/10000] Training loss: 0.0003203110292650992 / Validation loss: 0.0024354595984662566 / Long term Validation loss: 0.1420\n",
      "Epoch: [729/10000] Training loss: 0.00031962035623430524 / Validation loss: 0.00243250221065045 / Long term Validation loss: 0.1420\n",
      "Epoch: [730/10000] Training loss: 0.00031893161464301205 / Validation loss: 0.0024295469917778845 / Long term Validation loss: 0.1420\n",
      "Epoch: [731/10000] Training loss: 0.000318244800596713 / Validation loss: 0.002426594018179161 / Long term Validation loss: 0.1420\n",
      "Epoch: [732/10000] Training loss: 0.00031755991033335885 / Validation loss: 0.002423643365515487 / Long term Validation loss: 0.1420\n",
      "Epoch: [733/10000] Training loss: 0.00031687694022020254 / Validation loss: 0.0024206951087676182 / Long term Validation loss: 0.1420\n",
      "Epoch: [734/10000] Training loss: 0.000316195886750664 / Validation loss: 0.0024177493222251466 / Long term Validation loss: 0.1420\n",
      "Epoch: [735/10000] Training loss: 0.00031551674654121736 / Validation loss: 0.0024148060794761334 / Long term Validation loss: 0.1420\n",
      "Epoch: [736/10000] Training loss: 0.0003148395163282978 / Validation loss: 0.0024118654533970807 / Long term Validation loss: 0.1420\n",
      "Epoch: [737/10000] Training loss: 0.00031416419296523187 / Validation loss: 0.002408927516143254 / Long term Validation loss: 0.1419\n",
      "Epoch: [738/10000] Training loss: 0.00031349077341918827 / Validation loss: 0.0024059923391393555 / Long term Validation loss: 0.1419\n",
      "Epoch: [739/10000] Training loss: 0.00031281925476815215 / Validation loss: 0.0024030599930705476 / Long term Validation loss: 0.1419\n",
      "Epoch: [740/10000] Training loss: 0.0003121496341979204 / Validation loss: 0.002400130547873823 / Long term Validation loss: 0.1419\n",
      "Epoch: [741/10000] Training loss: 0.00031148190899912175 / Validation loss: 0.0023972040727297133 / Long term Validation loss: 0.1419\n",
      "Epoch: [742/10000] Training loss: 0.0003108160765642581 / Validation loss: 0.0023942806360543766 / Long term Validation loss: 0.1419\n",
      "Epoch: [743/10000] Training loss: 0.0003101521343847703 / Validation loss: 0.002391360305492004 / Long term Validation loss: 0.1419\n",
      "Epoch: [744/10000] Training loss: 0.00030949008004812746 / Validation loss: 0.002388443147907596 / Long term Validation loss: 0.1419\n",
      "Epoch: [745/10000] Training loss: 0.00030882991123494015 / Validation loss: 0.002385529229380081 / Long term Validation loss: 0.1419\n",
      "Epoch: [746/10000] Training loss: 0.00030817162571609747 / Validation loss: 0.0023826186151957906 / Long term Validation loss: 0.1419\n",
      "Epoch: [747/10000] Training loss: 0.00030751522134992945 / Validation loss: 0.0023797113698422632 / Long term Validation loss: 0.1419\n",
      "Epoch: [748/10000] Training loss: 0.0003068606960793931 / Validation loss: 0.002376807557002408 / Long term Validation loss: 0.1418\n",
      "Epoch: [749/10000] Training loss: 0.00030620804792928454 / Validation loss: 0.0023739072395490128 / Long term Validation loss: 0.1418\n",
      "Epoch: [750/10000] Training loss: 0.00030555727500347526 / Validation loss: 0.002371010479539586 / Long term Validation loss: 0.1418\n",
      "Epoch: [751/10000] Training loss: 0.00030490837548217563 / Validation loss: 0.0023681173382115446 / Long term Validation loss: 0.1418\n",
      "Epoch: [752/10000] Training loss: 0.00030426134761922166 / Validation loss: 0.002365227875977739 / Long term Validation loss: 0.1418\n",
      "Epoch: [753/10000] Training loss: 0.00030361618973939037 / Validation loss: 0.0023623421524223047 / Long term Validation loss: 0.1418\n",
      "Epoch: [754/10000] Training loss: 0.00030297290023573935 / Validation loss: 0.002359460226296865 / Long term Validation loss: 0.1418\n",
      "Epoch: [755/10000] Training loss: 0.00030233147756697455 / Validation loss: 0.0023565821555170405 / Long term Validation loss: 0.1418\n",
      "Epoch: [756/10000] Training loss: 0.000301691920254843 / Validation loss: 0.0023537079971593005 / Long term Validation loss: 0.1418\n",
      "Epoch: [757/10000] Training loss: 0.00030105422688155394 / Validation loss: 0.0023508378074581283 / Long term Validation loss: 0.1418\n",
      "Epoch: [758/10000] Training loss: 0.0003004183960872261 / Validation loss: 0.0023479716418035143 / Long term Validation loss: 0.1417\n",
      "Epoch: [759/10000] Training loss: 0.0002997844265673632 / Validation loss: 0.0023451095547387573 / Long term Validation loss: 0.1417\n",
      "Epoch: [760/10000] Training loss: 0.0002991523170703561 / Validation loss: 0.0023422515999585764 / Long term Validation loss: 0.1417\n",
      "Epoch: [761/10000] Training loss: 0.00029852206639501374 / Validation loss: 0.00233939783030755 / Long term Validation loss: 0.1417\n",
      "Epoch: [762/10000] Training loss: 0.000297893673388121 / Validation loss: 0.0023365482977788276 / Long term Validation loss: 0.1417\n",
      "Epoch: [763/10000] Training loss: 0.0002972671369420257 / Validation loss: 0.0023337030535131626 / Long term Validation loss: 0.1417\n",
      "Epoch: [764/10000] Training loss: 0.0002966424559922531 / Validation loss: 0.002330862147798235 / Long term Validation loss: 0.1417\n",
      "Epoch: [765/10000] Training loss: 0.0002960196295151499 / Validation loss: 0.0023280256300682667 / Long term Validation loss: 0.1417\n",
      "Epoch: [766/10000] Training loss: 0.00029539865652555597 / Validation loss: 0.002325193548903926 / Long term Validation loss: 0.1417\n",
      "Epoch: [767/10000] Training loss: 0.0002947795360745057 / Validation loss: 0.0023223659520324996 / Long term Validation loss: 0.1416\n",
      "Epoch: [768/10000] Training loss: 0.00029416226724695817 / Validation loss: 0.0023195428863283575 / Long term Validation loss: 0.1416\n",
      "Epoch: [769/10000] Training loss: 0.0002935468491595566 / Validation loss: 0.002316724397813678 / Long term Validation loss: 0.1416\n",
      "Epoch: [770/10000] Training loss: 0.00029293328095841685 / Validation loss: 0.0023139105316594523 / Long term Validation loss: 0.1416\n",
      "Epoch: [771/10000] Training loss: 0.000292321561816946 / Validation loss: 0.0023111013321867337 / Long term Validation loss: 0.1416\n",
      "Epoch: [772/10000] Training loss: 0.00029171169093369025 / Validation loss: 0.0023082968428681607 / Long term Validation loss: 0.1416\n",
      "Epoch: [773/10000] Training loss: 0.00029110366753021293 / Validation loss: 0.002305497106329718 / Long term Validation loss: 0.1416\n",
      "Epoch: [774/10000] Training loss: 0.0002904974908490018 / Validation loss: 0.002302702164352757 / Long term Validation loss: 0.1416\n",
      "Epoch: [775/10000] Training loss: 0.0002898931601514073 / Validation loss: 0.0022999120578762414 / Long term Validation loss: 0.1415\n",
      "Epoch: [776/10000] Training loss: 0.0002892906747156105 / Validation loss: 0.002297126826999245 / Long term Validation loss: 0.1415\n",
      "Epoch: [777/10000] Training loss: 0.0002886900338346211 / Validation loss: 0.0022943465109836565 / Long term Validation loss: 0.1415\n",
      "Epoch: [778/10000] Training loss: 0.00028809123681430675 / Validation loss: 0.00229157114825714 / Long term Validation loss: 0.1415\n",
      "Epoch: [779/10000] Training loss: 0.00028749428297145135 / Validation loss: 0.002288800776416293 / Long term Validation loss: 0.1415\n",
      "Epoch: [780/10000] Training loss: 0.000286899171631846 / Validation loss: 0.002286035432230018 / Long term Validation loss: 0.1415\n",
      "Epoch: [781/10000] Training loss: 0.0002863059021284085 / Validation loss: 0.0022832751516431134 / Long term Validation loss: 0.1415\n",
      "Epoch: [782/10000] Training loss: 0.00028571447379933533 / Validation loss: 0.0022805199697800596 / Long term Validation loss: 0.1415\n",
      "Epoch: [783/10000] Training loss: 0.00028512488598628346 / Validation loss: 0.002277769920948997 / Long term Validation loss: 0.1414\n",
      "Epoch: [784/10000] Training loss: 0.00028453713803258337 / Validation loss: 0.002275025038645914 / Long term Validation loss: 0.1414\n",
      "Epoch: [785/10000] Training loss: 0.000283951229281483 / Validation loss: 0.0022722853555589943 / Long term Validation loss: 0.1414\n",
      "Epoch: [786/10000] Training loss: 0.00028336715907442304 / Validation loss: 0.002269550903573174 / Long term Validation loss: 0.1414\n",
      "Epoch: [787/10000] Training loss: 0.00028278492674934237 / Validation loss: 0.0022668217137748496 / Long term Validation loss: 0.1414\n",
      "Epoch: [788/10000] Training loss: 0.00028220453163901583 / Validation loss: 0.0022640978164567655 / Long term Validation loss: 0.1414\n",
      "Epoch: [789/10000] Training loss: 0.00028162597306942216 / Validation loss: 0.0022613792411230647 / Long term Validation loss: 0.1414\n",
      "Epoch: [790/10000] Training loss: 0.0002810492503581433 / Validation loss: 0.002258666016494496 / Long term Validation loss: 0.1413\n",
      "Epoch: [791/10000] Training loss: 0.0002804743628127954 / Validation loss: 0.0022559581705137747 / Long term Validation loss: 0.1413\n",
      "Epoch: [792/10000] Training loss: 0.00027990130972949004 / Validation loss: 0.0022532557303510943 / Long term Validation loss: 0.1413\n",
      "Epoch: [793/10000] Training loss: 0.0002793300903913276 / Validation loss: 0.002250558722409771 / Long term Validation loss: 0.1413\n",
      "Epoch: [794/10000] Training loss: 0.00027876070406692094 / Validation loss: 0.0022478671723320403 / Long term Validation loss: 0.1413\n",
      "Epoch: [795/10000] Training loss: 0.0002781931500089512 / Validation loss: 0.002245181105004961 / Long term Validation loss: 0.1413\n",
      "Epoch: [796/10000] Training loss: 0.0002776274274527536 / Validation loss: 0.002242500544566469 / Long term Validation loss: 0.1413\n",
      "Epoch: [797/10000] Training loss: 0.0002770635356149352 / Validation loss: 0.0022398255144115385 / Long term Validation loss: 0.1412\n",
      "Epoch: [798/10000] Training loss: 0.0002765014736920236 / Validation loss: 0.0022371560371984654 / Long term Validation loss: 0.1412\n",
      "Epoch: [799/10000] Training loss: 0.00027594124085914635 / Validation loss: 0.002234492134855249 / Long term Validation loss: 0.1412\n",
      "Epoch: [800/10000] Training loss: 0.00027538283626874144 / Validation loss: 0.0022318338285861015 / Long term Validation loss: 0.1412\n",
      "Epoch: [801/10000] Training loss: 0.0002748262590492992 / Validation loss: 0.002229181138878032 / Long term Validation loss: 0.1412\n",
      "Epoch: [802/10000] Training loss: 0.0002742715083041344 / Validation loss: 0.0022265340855075373 / Long term Validation loss: 0.1412\n",
      "Epoch: [803/10000] Training loss: 0.00027371858311018915 / Validation loss: 0.0022238926875473913 / Long term Validation loss: 0.1412\n",
      "Epoch: [804/10000] Training loss: 0.00027316748251686744 / Validation loss: 0.0022212569633735064 / Long term Validation loss: 0.1411\n",
      "Epoch: [805/10000] Training loss: 0.0002726182055448992 / Validation loss: 0.0022186269306718864 / Long term Validation loss: 0.1411\n",
      "Epoch: [806/10000] Training loss: 0.00027207075118523523 / Validation loss: 0.0022160026064456397 / Long term Validation loss: 0.1411\n",
      "Epoch: [807/10000] Training loss: 0.00027152511839797307 / Validation loss: 0.0022133840070220857 / Long term Validation loss: 0.1411\n",
      "Epoch: [808/10000] Training loss: 0.0002709813061113124 / Validation loss: 0.002210771148059906 / Long term Validation loss: 0.1411\n",
      "Epoch: [809/10000] Training loss: 0.00027043931322054133 / Validation loss: 0.0022081640445563763 / Long term Validation loss: 0.1411\n",
      "Epoch: [810/10000] Training loss: 0.0002698991385870523 / Validation loss: 0.002205562710854643 / Long term Validation loss: 0.1410\n",
      "Epoch: [811/10000] Training loss: 0.0002693607810373884 / Validation loss: 0.0022029671606510603 / Long term Validation loss: 0.1410\n",
      "Epoch: [812/10000] Training loss: 0.00026882423936231895 / Validation loss: 0.002200377407002573 / Long term Validation loss: 0.1410\n",
      "Epoch: [813/10000] Training loss: 0.000268289512315945 / Validation loss: 0.0021977934623341484 / Long term Validation loss: 0.1410\n",
      "Epoch: [814/10000] Training loss: 0.00026775659861483507 / Validation loss: 0.002195215338446238 / Long term Validation loss: 0.1410\n",
      "Epoch: [815/10000] Training loss: 0.00026722549693718845 / Validation loss: 0.0021926430465222945 / Long term Validation loss: 0.1410\n",
      "Epoch: [816/10000] Training loss: 0.0002666962059220305 / Validation loss: 0.002190076597136308 / Long term Validation loss: 0.1410\n",
      "Epoch: [817/10000] Training loss: 0.0002661687241684334 / Validation loss: 0.002187516000260364 / Long term Validation loss: 0.1409\n",
      "Epoch: [818/10000] Training loss: 0.0002656430502347694 / Validation loss: 0.0021849612652722517 / Long term Validation loss: 0.1409\n",
      "Epoch: [819/10000] Training loss: 0.0002651191826379904 / Validation loss: 0.0021824124009630647 / Long term Validation loss: 0.1409\n",
      "Epoch: [820/10000] Training loss: 0.0002645971198529366 / Validation loss: 0.0021798694155448495 / Long term Validation loss: 0.1409\n",
      "Epoch: [821/10000] Training loss: 0.00026407686031167376 / Validation loss: 0.0021773323166582354 / Long term Validation loss: 0.1409\n",
      "Epoch: [822/10000] Training loss: 0.00026355840240285816 / Validation loss: 0.002174801111380095 / Long term Validation loss: 0.1409\n",
      "Epoch: [823/10000] Training loss: 0.0002630417444711298 / Validation loss: 0.002172275806231215 / Long term Validation loss: 0.1409\n",
      "Epoch: [824/10000] Training loss: 0.00026252688481653294 / Validation loss: 0.0021697564071839564 / Long term Validation loss: 0.1408\n",
      "Epoch: [825/10000] Training loss: 0.00026201382169396435 / Validation loss: 0.002167242919669934 / Long term Validation loss: 0.1408\n",
      "Epoch: [826/10000] Training loss: 0.00026150255331264857 / Validation loss: 0.0021647353485876666 / Long term Validation loss: 0.1408\n",
      "Epoch: [827/10000] Training loss: 0.0002609930778356402 / Validation loss: 0.0021622336983102416 / Long term Validation loss: 0.1408\n",
      "Epoch: [828/10000] Training loss: 0.00026048539337935304 / Validation loss: 0.002159737972692977 / Long term Validation loss: 0.1408\n",
      "Epoch: [829/10000] Training loss: 0.0002599794980131156 / Validation loss: 0.0021572481750810487 / Long term Validation loss: 0.1408\n",
      "Epoch: [830/10000] Training loss: 0.000259475389758753 / Validation loss: 0.0021547643083171236 / Long term Validation loss: 0.1408\n",
      "Epoch: [831/10000] Training loss: 0.0002589730665901949 / Validation loss: 0.0021522863747489643 / Long term Validation loss: 0.1407\n",
      "Epoch: [832/10000] Training loss: 0.00025847252643310863 / Validation loss: 0.002149814376237018 / Long term Validation loss: 0.1407\n",
      "Epoch: [833/10000] Training loss: 0.0002579737671645586 / Validation loss: 0.0021473483141619823 / Long term Validation loss: 0.1407\n",
      "Epoch: [834/10000] Training loss: 0.00025747678661269054 / Validation loss: 0.0021448881894323446 / Long term Validation loss: 0.1407\n",
      "Epoch: [835/10000] Training loss: 0.0002569815825564402 / Validation loss: 0.002142434002491899 / Long term Validation loss: 0.1407\n",
      "Epoch: [836/10000] Training loss: 0.0002564881527252676 / Validation loss: 0.002139985753327229 / Long term Validation loss: 0.1407\n",
      "Epoch: [837/10000] Training loss: 0.0002559964947989146 / Validation loss: 0.0021375434414751555 / Long term Validation loss: 0.1407\n",
      "Epoch: [838/10000] Training loss: 0.0002555066064071881 / Validation loss: 0.0021351070660301594 / Long term Validation loss: 0.1406\n",
      "Epoch: [839/10000] Training loss: 0.0002550184851297647 / Validation loss: 0.002132676625651752 / Long term Validation loss: 0.1406\n",
      "Epoch: [840/10000] Training loss: 0.00025453212849602076 / Validation loss: 0.0021302521185718295 / Long term Validation loss: 0.1406\n",
      "Epoch: [841/10000] Training loss: 0.0002540475339848854 / Validation loss: 0.002127833542601965 / Long term Validation loss: 0.1406\n",
      "Epoch: [842/10000] Training loss: 0.0002535646990247152 / Validation loss: 0.002125420895140677 / Long term Validation loss: 0.1406\n",
      "Epoch: [843/10000] Training loss: 0.0002530836209931924 / Validation loss: 0.0021230141731806364 / Long term Validation loss: 0.1406\n",
      "Epoch: [844/10000] Training loss: 0.0002526042972172453 / Validation loss: 0.002120613373315837 / Long term Validation loss: 0.1406\n",
      "Epoch: [845/10000] Training loss: 0.0002521267249729904 / Validation loss: 0.002118218491748714 / Long term Validation loss: 0.1405\n",
      "Epoch: [846/10000] Training loss: 0.0002516509014856959 / Validation loss: 0.0021158295242972146 / Long term Validation loss: 0.1405\n",
      "Epoch: [847/10000] Training loss: 0.0002511768239297666 / Validation loss: 0.0021134464664018322 / Long term Validation loss: 0.1405\n",
      "Epoch: [848/10000] Training loss: 0.00025070448942875043 / Validation loss: 0.002111069313132558 / Long term Validation loss: 0.1405\n",
      "Epoch: [849/10000] Training loss: 0.00025023389505536423 / Validation loss: 0.0021086980591958 / Long term Validation loss: 0.1405\n",
      "Epoch: [850/10000] Training loss: 0.0002497650378315411 / Validation loss: 0.002106332698941242 / Long term Validation loss: 0.1405\n",
      "Epoch: [851/10000] Training loss: 0.00024929791472849683 / Validation loss: 0.0021039732263686403 / Long term Validation loss: 0.1405\n",
      "Epoch: [852/10000] Training loss: 0.00024883252266681645 / Validation loss: 0.0021016196351345807 / Long term Validation loss: 0.1404\n",
      "Epoch: [853/10000] Training loss: 0.0002483688585165595 / Validation loss: 0.0020992719185591543 / Long term Validation loss: 0.1404\n",
      "Epoch: [854/10000] Training loss: 0.00024790691909738505 / Validation loss: 0.0020969300696325765 / Long term Validation loss: 0.1404\n",
      "Epoch: [855/10000] Training loss: 0.0002474467011786944 / Validation loss: 0.0020945940810217652 / Long term Validation loss: 0.1404\n",
      "Epoch: [856/10000] Training loss: 0.0002469882014797923 / Validation loss: 0.0020922639450768245 / Long term Validation loss: 0.1404\n",
      "Epoch: [857/10000] Training loss: 0.0002465314166700663 / Validation loss: 0.002089939653837499 / Long term Validation loss: 0.1404\n",
      "Epoch: [858/10000] Training loss: 0.00024607634336918364 / Validation loss: 0.00208762119903954 / Long term Validation loss: 0.1404\n",
      "Epoch: [859/10000] Training loss: 0.00024562297814730445 / Validation loss: 0.002085308572121012 / Long term Validation loss: 0.1403\n",
      "Epoch: [860/10000] Training loss: 0.00024517131752531325 / Validation loss: 0.0020830017642285404 / Long term Validation loss: 0.1403\n",
      "Epoch: [861/10000] Training loss: 0.00024472135797506516 / Validation loss: 0.002080700766223489 / Long term Validation loss: 0.1403\n",
      "Epoch: [862/10000] Training loss: 0.0002442730959196499 / Validation loss: 0.002078405568688062 / Long term Validation loss: 0.1403\n",
      "Epoch: [863/10000] Training loss: 0.0002438265277336705 / Validation loss: 0.002076116161931357 / Long term Validation loss: 0.1403\n",
      "Epoch: [864/10000] Training loss: 0.00024338164974353722 / Validation loss: 0.0020738325359953287 / Long term Validation loss: 0.1403\n",
      "Epoch: [865/10000] Training loss: 0.00024293845822777766 / Validation loss: 0.0020715546806607046 / Long term Validation loss: 0.1403\n",
      "Epoch: [866/10000] Training loss: 0.0002424969494173603 / Validation loss: 0.0020692825854528047 / Long term Validation loss: 0.1402\n",
      "Epoch: [867/10000] Training loss: 0.0002420571194960332 / Validation loss: 0.0020670162396473272 / Long term Validation loss: 0.1402\n",
      "Epoch: [868/10000] Training loss: 0.00024161896460067595 / Validation loss: 0.002064755632276029 / Long term Validation loss: 0.1402\n",
      "Epoch: [869/10000] Training loss: 0.00024118248082166612 / Validation loss: 0.0020625007521323574 / Long term Validation loss: 0.1402\n",
      "Epoch: [870/10000] Training loss: 0.0002407476642032581 / Validation loss: 0.0020602515877770036 / Long term Validation loss: 0.1402\n",
      "Epoch: [871/10000] Training loss: 0.00024031451074397507 / Validation loss: 0.0020580081275433882 / Long term Validation loss: 0.1402\n",
      "Epoch: [872/10000] Training loss: 0.00023988301639701418 / Validation loss: 0.0020557703595430687 / Long term Validation loss: 0.1402\n",
      "Epoch: [873/10000] Training loss: 0.00023945317707066248 / Validation loss: 0.0020535382716710873 / Long term Validation loss: 0.1402\n",
      "Epoch: [874/10000] Training loss: 0.00023902498862872644 / Validation loss: 0.002051311851611238 / Long term Validation loss: 0.1401\n",
      "Epoch: [875/10000] Training loss: 0.00023859844689097135 / Validation loss: 0.002049091086841263 / Long term Validation loss: 0.1401\n",
      "Epoch: [876/10000] Training loss: 0.00023817354763357258 / Validation loss: 0.0020468759646379863 / Long term Validation loss: 0.1401\n",
      "Epoch: [877/10000] Training loss: 0.000237750286589578 / Validation loss: 0.00204466647208236 / Long term Validation loss: 0.1401\n",
      "Epoch: [878/10000] Training loss: 0.00023732865944937965 / Validation loss: 0.002042462596064453 / Long term Validation loss: 0.1401\n",
      "Epoch: [879/10000] Training loss: 0.00023690866186119693 / Validation loss: 0.0020402643232883644 / Long term Validation loss: 0.1401\n",
      "Epoch: [880/10000] Training loss: 0.000236490289431568 / Validation loss: 0.002038071640277058 / Long term Validation loss: 0.1400\n",
      "Epoch: [881/10000] Training loss: 0.0002360735377258519 / Validation loss: 0.0020358845333771415 / Long term Validation loss: 0.1400\n",
      "Epoch: [882/10000] Training loss: 0.00023565840226873883 / Validation loss: 0.0020337029887635572 / Long term Validation loss: 0.1400\n",
      "Epoch: [883/10000] Training loss: 0.00023524487854476934 / Validation loss: 0.0020315269924442146 / Long term Validation loss: 0.1400\n",
      "Epoch: [884/10000] Training loss: 0.00023483296199886225 / Validation loss: 0.0020293565302645445 / Long term Validation loss: 0.1400\n",
      "Epoch: [885/10000] Training loss: 0.0002344226480368498 / Validation loss: 0.002027191587911981 / Long term Validation loss: 0.1400\n",
      "Epoch: [886/10000] Training loss: 0.0002340139320260216 / Validation loss: 0.0020250321509203877 / Long term Validation loss: 0.1400\n",
      "Epoch: [887/10000] Training loss: 0.00023360680929567472 / Validation loss: 0.0020228782046743964 / Long term Validation loss: 0.1399\n",
      "Epoch: [888/10000] Training loss: 0.00023320127513767137 / Validation loss: 0.002020729734413687 / Long term Validation loss: 0.1399\n",
      "Epoch: [889/10000] Training loss: 0.00023279732480700394 / Validation loss: 0.0020185867252372016 / Long term Validation loss: 0.1399\n",
      "Epoch: [890/10000] Training loss: 0.00023239495352236512 / Validation loss: 0.0020164491621072643 / Long term Validation loss: 0.1399\n",
      "Epoch: [891/10000] Training loss: 0.00023199415646672498 / Validation loss: 0.0020143170298536706 / Long term Validation loss: 0.1399\n",
      "Epoch: [892/10000] Training loss: 0.00023159492878791358 / Validation loss: 0.0020121903131776805 / Long term Validation loss: 0.1399\n",
      "Epoch: [893/10000] Training loss: 0.00023119726559920885 / Validation loss: 0.002010068996655957 / Long term Validation loss: 0.1398\n",
      "Epoch: [894/10000] Training loss: 0.00023080116197992965 / Validation loss: 0.0020079530647444392 / Long term Validation loss: 0.1398\n",
      "Epoch: [895/10000] Training loss: 0.0002304066129760342 / Validation loss: 0.0020058425017821257 / Long term Validation loss: 0.1398\n",
      "Epoch: [896/10000] Training loss: 0.00023001361360072166 / Validation loss: 0.002003737291994832 / Long term Validation loss: 0.1398\n",
      "Epoch: [897/10000] Training loss: 0.00022962215883503949 / Validation loss: 0.002001637419498844 / Long term Validation loss: 0.1398\n",
      "Epoch: [898/10000] Training loss: 0.00022923224362849385 / Validation loss: 0.0019995428683045316 / Long term Validation loss: 0.1398\n",
      "Epoch: [899/10000] Training loss: 0.00022884386289966362 / Validation loss: 0.001997453622319881 / Long term Validation loss: 0.1397\n",
      "Epoch: [900/10000] Training loss: 0.000228457011536818 / Validation loss: 0.0019953696653539814 / Long term Validation loss: 0.1397\n",
      "Epoch: [901/10000] Training loss: 0.00022807168439853708 / Validation loss: 0.0019932909811204336 / Long term Validation loss: 0.1397\n",
      "Epoch: [902/10000] Training loss: 0.0002276878763143352 / Validation loss: 0.0019912175532406937 / Long term Validation loss: 0.1397\n",
      "Epoch: [903/10000] Training loss: 0.0002273055820852867 / Validation loss: 0.0019891493652473705 / Long term Validation loss: 0.1397\n",
      "Epoch: [904/10000] Training loss: 0.0002269247964846541 / Validation loss: 0.0019870864005874503 / Long term Validation loss: 0.1397\n",
      "Epoch: [905/10000] Training loss: 0.00022654551425851786 / Validation loss: 0.0019850286426254583 / Long term Validation loss: 0.1396\n",
      "Epoch: [906/10000] Training loss: 0.00022616773012640822 / Validation loss: 0.0019829760746465745 / Long term Validation loss: 0.1396\n",
      "Epoch: [907/10000] Training loss: 0.0002257914387819384 / Validation loss: 0.0019809286798596636 / Long term Validation loss: 0.1396\n",
      "Epoch: [908/10000] Training loss: 0.00022541663489343843 / Validation loss: 0.001978886441400283 / Long term Validation loss: 0.1396\n",
      "Epoch: [909/10000] Training loss: 0.00022504331310459075 / Validation loss: 0.001976849342333598 / Long term Validation loss: 0.1396\n",
      "Epoch: [910/10000] Training loss: 0.00022467146803506596 / Validation loss: 0.0019748173656572564 / Long term Validation loss: 0.1396\n",
      "Epoch: [911/10000] Training loss: 0.00022430109428115915 / Validation loss: 0.0019727904943042107 / Long term Validation loss: 0.1395\n",
      "Epoch: [912/10000] Training loss: 0.00022393218641642612 / Validation loss: 0.0019707687111454692 / Long term Validation loss: 0.1395\n",
      "Epoch: [913/10000] Training loss: 0.0002235647389923197 / Validation loss: 0.0019687519989928025 / Long term Validation loss: 0.1395\n",
      "Epoch: [914/10000] Training loss: 0.00022319874653882653 / Validation loss: 0.0019667403406013967 / Long term Validation loss: 0.1395\n",
      "Epoch: [915/10000] Training loss: 0.00022283420356510196 / Validation loss: 0.001964733718672447 / Long term Validation loss: 0.1395\n",
      "Epoch: [916/10000] Training loss: 0.00022247110456010547 / Validation loss: 0.001962732115855695 / Long term Validation loss: 0.1395\n",
      "Epoch: [917/10000] Training loss: 0.0002221094439932342 / Validation loss: 0.0019607355147519268 / Long term Validation loss: 0.1394\n",
      "Epoch: [918/10000] Training loss: 0.00022174921631495635 / Validation loss: 0.0019587438979154063 / Long term Validation loss: 0.1394\n",
      "Epoch: [919/10000] Training loss: 0.00022139041595744188 / Validation loss: 0.001956757247856268 / Long term Validation loss: 0.1394\n",
      "Epoch: [920/10000] Training loss: 0.000221033037335193 / Validation loss: 0.0019547755470428418 / Long term Validation loss: 0.1394\n",
      "Epoch: [921/10000] Training loss: 0.00022067707484567213 / Validation loss: 0.0019527987779039602 / Long term Validation loss: 0.1394\n",
      "Epoch: [922/10000] Training loss: 0.00022032252286992803 / Validation loss: 0.0019508269228311807 / Long term Validation loss: 0.1394\n",
      "Epoch: [923/10000] Training loss: 0.00021996937577321976 / Validation loss: 0.0019488599641809806 / Long term Validation loss: 0.1394\n",
      "Epoch: [924/10000] Training loss: 0.00021961762790563872 / Validation loss: 0.001946897884276905 / Long term Validation loss: 0.1393\n",
      "Epoch: [925/10000] Training loss: 0.0002192672736027275 / Validation loss: 0.0019449406654116676 / Long term Validation loss: 0.1393\n",
      "Epoch: [926/10000] Training loss: 0.00021891830718609665 / Validation loss: 0.0019429882898491925 / Long term Validation loss: 0.1393\n",
      "Epoch: [927/10000] Training loss: 0.00021857072296403813 / Validation loss: 0.0019410407398266247 / Long term Validation loss: 0.1393\n",
      "Epoch: [928/10000] Training loss: 0.00021822451523213638 / Validation loss: 0.0019390979975562937 / Long term Validation loss: 0.1393\n",
      "Epoch: [929/10000] Training loss: 0.0002178796782738753 / Validation loss: 0.0019371600452276392 / Long term Validation loss: 0.1393\n",
      "Epoch: [930/10000] Training loss: 0.0002175362063612429 / Validation loss: 0.0019352268650090756 / Long term Validation loss: 0.1393\n",
      "Epoch: [931/10000] Training loss: 0.00021719409375533147 / Validation loss: 0.0019332984390498344 / Long term Validation loss: 0.1392\n",
      "Epoch: [932/10000] Training loss: 0.00021685333470693497 / Validation loss: 0.0019313747494817534 / Long term Validation loss: 0.1392\n",
      "Epoch: [933/10000] Training loss: 0.00021651392345714242 / Validation loss: 0.00192945577842103 / Long term Validation loss: 0.1392\n",
      "Epoch: [934/10000] Training loss: 0.0002161758542379267 / Validation loss: 0.001927541507969936 / Long term Validation loss: 0.1392\n",
      "Epoch: [935/10000] Training loss: 0.00021583912127273036 / Validation loss: 0.0019256319202184905 / Long term Validation loss: 0.1392\n",
      "Epoch: [936/10000] Training loss: 0.0002155037187770464 / Validation loss: 0.0019237269972460945 / Long term Validation loss: 0.1392\n",
      "Epoch: [937/10000] Training loss: 0.00021516964095899538 / Validation loss: 0.0019218267211231314 / Long term Validation loss: 0.1392\n",
      "Epoch: [938/10000] Training loss: 0.00021483688201989756 / Validation loss: 0.0019199310739125167 / Long term Validation loss: 0.1391\n",
      "Epoch: [939/10000] Training loss: 0.00021450543615484082 / Validation loss: 0.0019180400376712374 / Long term Validation loss: 0.1391\n",
      "Epoch: [940/10000] Training loss: 0.00021417529755324397 / Validation loss: 0.0019161535944518357 / Long term Validation loss: 0.1391\n",
      "Epoch: [941/10000] Training loss: 0.00021384646039941492 / Validation loss: 0.001914271726303862 / Long term Validation loss: 0.1391\n",
      "Epoch: [942/10000] Training loss: 0.00021351891887310442 / Validation loss: 0.0019123944152752918 / Long term Validation loss: 0.1391\n",
      "Epoch: [943/10000] Training loss: 0.00021319266715005462 / Validation loss: 0.0019105216434139164 / Long term Validation loss: 0.1391\n",
      "Epoch: [944/10000] Training loss: 0.00021286769940254257 / Validation loss: 0.0019086533927686964 / Long term Validation loss: 0.1390\n",
      "Epoch: [945/10000] Training loss: 0.00021254400979991883 / Validation loss: 0.0019067896453910866 / Long term Validation loss: 0.1390\n",
      "Epoch: [946/10000] Training loss: 0.00021222159250913996 / Validation loss: 0.0019049303833363217 / Long term Validation loss: 0.1390\n",
      "Epoch: [947/10000] Training loss: 0.00021190044169529687 / Validation loss: 0.0019030755886646738 / Long term Validation loss: 0.1390\n",
      "Epoch: [948/10000] Training loss: 0.00021158055152213734 / Validation loss: 0.0019012252434426839 / Long term Validation loss: 0.1390\n",
      "Epoch: [949/10000] Training loss: 0.00021126191615258237 / Validation loss: 0.0018993793297443623 / Long term Validation loss: 0.1390\n",
      "Epoch: [950/10000] Training loss: 0.00021094452974923812 / Validation loss: 0.0018975378296523665 / Long term Validation loss: 0.1390\n",
      "Epoch: [951/10000] Training loss: 0.00021062838647490108 / Validation loss: 0.0018957007252591322 / Long term Validation loss: 0.1389\n",
      "Epoch: [952/10000] Training loss: 0.00021031348049305857 / Validation loss: 0.0018938679986679977 / Long term Validation loss: 0.1389\n",
      "Epoch: [953/10000] Training loss: 0.000209999805968383 / Validation loss: 0.0018920396319942793 / Long term Validation loss: 0.1389\n",
      "Epoch: [954/10000] Training loss: 0.00020968735706722 / Validation loss: 0.0018902156073663456 / Long term Validation loss: 0.1389\n",
      "Epoch: [955/10000] Training loss: 0.00020937612795807157 / Validation loss: 0.0018883959069266498 / Long term Validation loss: 0.1389\n",
      "Epoch: [956/10000] Training loss: 0.0002090661128120724 / Validation loss: 0.001886580512832745 / Long term Validation loss: 0.1389\n",
      "Epoch: [957/10000] Training loss: 0.00020875730580346082 / Validation loss: 0.0018847694072582544 / Long term Validation loss: 0.1389\n",
      "Epoch: [958/10000] Training loss: 0.00020844970111004378 / Validation loss: 0.0018829625723938465 / Long term Validation loss: 0.1388\n",
      "Epoch: [959/10000] Training loss: 0.00020814329291365518 / Validation loss: 0.0018811599904481734 / Long term Validation loss: 0.1388\n",
      "Epoch: [960/10000] Training loss: 0.00020783807540060857 / Validation loss: 0.001879361643648783 / Long term Validation loss: 0.1388\n",
      "Epoch: [961/10000] Training loss: 0.00020753404276214433 / Validation loss: 0.0018775675142430055 / Long term Validation loss: 0.1388\n",
      "Epoch: [962/10000] Training loss: 0.0002072311891948694 / Validation loss: 0.0018757775844988368 / Long term Validation loss: 0.1388\n",
      "Epoch: [963/10000] Training loss: 0.00020692950890119165 / Validation loss: 0.0018739918367057782 / Long term Validation loss: 0.1388\n",
      "Epoch: [964/10000] Training loss: 0.0002066289960897483 / Validation loss: 0.001872210253175672 / Long term Validation loss: 0.1388\n",
      "Epoch: [965/10000] Training loss: 0.00020632964497582738 / Validation loss: 0.001870432816243504 / Long term Validation loss: 0.1387\n",
      "Epoch: [966/10000] Training loss: 0.00020603144978178364 / Validation loss: 0.0018686595082681944 / Long term Validation loss: 0.1387\n",
      "Epoch: [967/10000] Training loss: 0.00020573440473744775 / Validation loss: 0.0018668903116333684 / Long term Validation loss: 0.1387\n",
      "Epoch: [968/10000] Training loss: 0.0002054385040805294 / Validation loss: 0.0018651252087481024 / Long term Validation loss: 0.1387\n",
      "Epoch: [969/10000] Training loss: 0.00020514374205701425 / Validation loss: 0.0018633641820476599 / Long term Validation loss: 0.1387\n",
      "Epoch: [970/10000] Training loss: 0.0002048501129215548 / Validation loss: 0.0018616072139942047 / Long term Validation loss: 0.1387\n",
      "Epoch: [971/10000] Training loss: 0.00020455761093785406 / Validation loss: 0.0018598542870774924 / Long term Validation loss: 0.1387\n",
      "Epoch: [972/10000] Training loss: 0.00020426623037904403 / Validation loss: 0.0018581053838155545 / Long term Validation loss: 0.1386\n",
      "Epoch: [973/10000] Training loss: 0.00020397596552805737 / Validation loss: 0.0018563604867553565 / Long term Validation loss: 0.1386\n",
      "Epoch: [974/10000] Training loss: 0.0002036868106779929 / Validation loss: 0.0018546195784734554 / Long term Validation loss: 0.1386\n",
      "Epoch: [975/10000] Training loss: 0.0002033987601324746 / Validation loss: 0.0018528826415766132 / Long term Validation loss: 0.1386\n",
      "Epoch: [976/10000] Training loss: 0.0002031118082060048 / Validation loss: 0.001851149658702423 / Long term Validation loss: 0.1386\n",
      "Epoch: [977/10000] Training loss: 0.00020282594922431076 / Validation loss: 0.0018494206125199092 / Long term Validation loss: 0.1386\n",
      "Epoch: [978/10000] Training loss: 0.00020254117752468518 / Validation loss: 0.0018476954857301062 / Long term Validation loss: 0.1386\n",
      "Epoch: [979/10000] Training loss: 0.00020225748745632022 / Validation loss: 0.0018459742610666414 / Long term Validation loss: 0.1385\n",
      "Epoch: [980/10000] Training loss: 0.00020197487338063588 / Validation loss: 0.0018442569212962757 / Long term Validation loss: 0.1385\n",
      "Epoch: [981/10000] Training loss: 0.0002016933296716012 / Validation loss: 0.0018425434492194647 / Long term Validation loss: 0.1385\n",
      "Epoch: [982/10000] Training loss: 0.00020141285071605082 / Validation loss: 0.0018408338276708765 / Long term Validation loss: 0.1385\n",
      "Epoch: [983/10000] Training loss: 0.0002011334309139936 / Validation loss: 0.0018391280395199223 / Long term Validation loss: 0.1385\n",
      "Epoch: [984/10000] Training loss: 0.00020085506467891663 / Validation loss: 0.0018374260676712493 / Long term Validation loss: 0.1385\n",
      "Epoch: [985/10000] Training loss: 0.00020057774643808207 / Validation loss: 0.00183572789506524 / Long term Validation loss: 0.1385\n",
      "Epoch: [986/10000] Training loss: 0.00020030147063281866 / Validation loss: 0.0018340335046784935 / Long term Validation loss: 0.1384\n",
      "Epoch: [987/10000] Training loss: 0.00020002623171880667 / Validation loss: 0.0018323428795243026 / Long term Validation loss: 0.1384\n",
      "Epoch: [988/10000] Training loss: 0.00019975202416635737 / Validation loss: 0.001830656002653101 / Long term Validation loss: 0.1384\n",
      "Epoch: [989/10000] Training loss: 0.00019947884246068577 / Validation loss: 0.0018289728571529214 / Long term Validation loss: 0.1384\n",
      "Epoch: [990/10000] Training loss: 0.00019920668110217821 / Validation loss: 0.0018272934261498238 / Long term Validation loss: 0.1384\n",
      "Epoch: [991/10000] Training loss: 0.00019893553460665375 / Validation loss: 0.0018256176928083347 / Long term Validation loss: 0.1384\n",
      "Epoch: [992/10000] Training loss: 0.00019866539750561924 / Validation loss: 0.001823945640331852 / Long term Validation loss: 0.1384\n",
      "Epoch: [993/10000] Training loss: 0.0001983962643465194 / Validation loss: 0.0018222772519630728 / Long term Validation loss: 0.1383\n",
      "Epoch: [994/10000] Training loss: 0.00019812812969298018 / Validation loss: 0.0018206125109843636 / Long term Validation loss: 0.1383\n",
      "Epoch: [995/10000] Training loss: 0.0001978609881250474 / Validation loss: 0.0018189514007181716 / Long term Validation loss: 0.1383\n",
      "Epoch: [996/10000] Training loss: 0.00019759483423941856 / Validation loss: 0.001817293904527396 / Long term Validation loss: 0.1383\n",
      "Epoch: [997/10000] Training loss: 0.0001973296626496702 / Validation loss: 0.0018156400058157655 / Long term Validation loss: 0.1383\n",
      "Epoch: [998/10000] Training loss: 0.00019706546798647828 / Validation loss: 0.0018139896880281956 / Long term Validation loss: 0.1383\n",
      "Epoch: [999/10000] Training loss: 0.00019680224489783418 / Validation loss: 0.0018123429346511453 / Long term Validation loss: 0.1383\n",
      "Epoch: [1000/10000] Training loss: 0.00019653998804925455 / Validation loss: 0.0018106997292129615 / Long term Validation loss: 0.1382\n",
      "Epoch: [1001/10000] Training loss: 0.00019627869212398582 / Validation loss: 0.0018090600552842266 / Long term Validation loss: 0.1382\n",
      "Epoch: [1002/10000] Training loss: 0.00019601835182320312 / Validation loss: 0.0018074238964780774 / Long term Validation loss: 0.1382\n",
      "Epoch: [1003/10000] Training loss: 0.0001957589618662042 / Validation loss: 0.0018057912364505431 / Long term Validation loss: 0.1382\n",
      "Epoch: [1004/10000] Training loss: 0.00019550051699059764 / Validation loss: 0.00180416205890085 / Long term Validation loss: 0.1382\n",
      "Epoch: [1005/10000] Training loss: 0.00019524301195248599 / Validation loss: 0.0018025363475717405 / Long term Validation loss: 0.1382\n",
      "Epoch: [1006/10000] Training loss: 0.00019498644152664328 / Validation loss: 0.0018009140862497717 / Long term Validation loss: 0.1381\n",
      "Epoch: [1007/10000] Training loss: 0.00019473080050668825 / Validation loss: 0.0017992952587656133 / Long term Validation loss: 0.1381\n",
      "Epoch: [1008/10000] Training loss: 0.00019447608370525127 / Validation loss: 0.0017976798489943366 / Long term Validation loss: 0.1381\n",
      "Epoch: [1009/10000] Training loss: 0.00019422228595413708 / Validation loss: 0.0017960678408556964 / Long term Validation loss: 0.1381\n",
      "Epoch: [1010/10000] Training loss: 0.0001939694021044824 / Validation loss: 0.0017944592183144151 / Long term Validation loss: 0.1381\n",
      "Epoch: [1011/10000] Training loss: 0.00019371742702690767 / Validation loss: 0.001792853965380452 / Long term Validation loss: 0.1381\n",
      "Epoch: [1012/10000] Training loss: 0.0001934663556116656 / Validation loss: 0.001791252066109266 / Long term Validation loss: 0.1381\n",
      "Epoch: [1013/10000] Training loss: 0.0001932161827687829 / Validation loss: 0.0017896535046020773 / Long term Validation loss: 0.1380\n",
      "Epoch: [1014/10000] Training loss: 0.00019296690342819874 / Validation loss: 0.001788058265006124 / Long term Validation loss: 0.1380\n",
      "Epoch: [1015/10000] Training loss: 0.00019271851253989776 / Validation loss: 0.0017864663315149082 / Long term Validation loss: 0.1380\n",
      "Epoch: [1016/10000] Training loss: 0.0001924710050740383 / Validation loss: 0.001784877688368447 / Long term Validation loss: 0.1380\n",
      "Epoch: [1017/10000] Training loss: 0.00019222437602107667 / Validation loss: 0.0017832923198535019 / Long term Validation loss: 0.1380\n",
      "Epoch: [1018/10000] Training loss: 0.00019197862039188574 / Validation loss: 0.0017817102103038253 / Long term Validation loss: 0.1380\n",
      "Epoch: [1019/10000] Training loss: 0.0001917337332178703 / Validation loss: 0.001780131344100373 / Long term Validation loss: 0.1380\n",
      "Epoch: [1020/10000] Training loss: 0.000191489709551077 / Validation loss: 0.0017785557056715444 / Long term Validation loss: 0.1379\n",
      "Epoch: [1021/10000] Training loss: 0.00019124654446430023 / Validation loss: 0.0017769832794933952 / Long term Validation loss: 0.1379\n",
      "Epoch: [1022/10000] Training loss: 0.0001910042330511837 / Validation loss: 0.0017754140500898512 / Long term Validation loss: 0.1379\n",
      "Epoch: [1023/10000] Training loss: 0.00019076277042631763 / Validation loss: 0.0017738480020329276 / Long term Validation loss: 0.1379\n",
      "Epoch: [1024/10000] Training loss: 0.0001905221517253319 / Validation loss: 0.0017722851199429184 / Long term Validation loss: 0.1379\n",
      "Epoch: [1025/10000] Training loss: 0.00019028237210498455 / Validation loss: 0.0017707253884886175 / Long term Validation loss: 0.1379\n",
      "Epoch: [1026/10000] Training loss: 0.00019004342674324692 / Validation loss: 0.001769168792387509 / Long term Validation loss: 0.1379\n",
      "Epoch: [1027/10000] Training loss: 0.000189805310839384 / Validation loss: 0.0017676153164059595 / Long term Validation loss: 0.1378\n",
      "Epoch: [1028/10000] Training loss: 0.00018956801961403133 / Validation loss: 0.0017660649453594074 / Long term Validation loss: 0.1378\n",
      "Epoch: [1029/10000] Training loss: 0.0001893315483092677 / Validation loss: 0.0017645176641125522 / Long term Validation loss: 0.1378\n",
      "Epoch: [1030/10000] Training loss: 0.00018909589218868357 / Validation loss: 0.001762973457579546 / Long term Validation loss: 0.1378\n",
      "Epoch: [1031/10000] Training loss: 0.00018886104653744715 / Validation loss: 0.0017614323107241457 / Long term Validation loss: 0.1378\n",
      "Epoch: [1032/10000] Training loss: 0.00018862700666236456 / Validation loss: 0.0017598942085599156 / Long term Validation loss: 0.1378\n",
      "Epoch: [1033/10000] Training loss: 0.0001883937678919381 / Validation loss: 0.0017583591361503826 / Long term Validation loss: 0.1378\n",
      "Epoch: [1034/10000] Training loss: 0.00018816132557641989 / Validation loss: 0.001756827078609217 / Long term Validation loss: 0.1377\n",
      "Epoch: [1035/10000] Training loss: 0.0001879296750878623 / Validation loss: 0.0017552980211003855 / Long term Validation loss: 0.1377\n",
      "Epoch: [1036/10000] Training loss: 0.00018769881182016492 / Validation loss: 0.0017537719488383069 / Long term Validation loss: 0.1377\n",
      "Epoch: [1037/10000] Training loss: 0.00018746873118911743 / Validation loss: 0.0017522488470880248 / Long term Validation loss: 0.1377\n",
      "Epoch: [1038/10000] Training loss: 0.0001872394286324402 / Validation loss: 0.0017507287011653586 / Long term Validation loss: 0.1377\n",
      "Epoch: [1039/10000] Training loss: 0.00018701089960982027 / Validation loss: 0.0017492114964370521 / Long term Validation loss: 0.1377\n",
      "Epoch: [1040/10000] Training loss: 0.00018678313960294478 / Validation loss: 0.0017476972183209179 / Long term Validation loss: 0.1377\n",
      "Epoch: [1041/10000] Training loss: 0.00018655614411553116 / Validation loss: 0.0017461858522859793 / Long term Validation loss: 0.1376\n",
      "Epoch: [1042/10000] Training loss: 0.00018632990867335382 / Validation loss: 0.0017446773838526227 / Long term Validation loss: 0.1376\n",
      "Epoch: [1043/10000] Training loss: 0.00018610442882426752 / Validation loss: 0.001743171798592736 / Long term Validation loss: 0.1376\n",
      "Epoch: [1044/10000] Training loss: 0.00018587970013822866 / Validation loss: 0.001741669082129834 / Long term Validation loss: 0.1376\n",
      "Epoch: [1045/10000] Training loss: 0.00018565571820731224 / Validation loss: 0.0017401692201391963 / Long term Validation loss: 0.1376\n",
      "Epoch: [1046/10000] Training loss: 0.00018543247864572657 / Validation loss: 0.001738672198347989 / Long term Validation loss: 0.1376\n",
      "Epoch: [1047/10000] Training loss: 0.00018520997708982532 / Validation loss: 0.0017371780025354077 / Long term Validation loss: 0.1376\n",
      "Epoch: [1048/10000] Training loss: 0.00018498820919811586 / Validation loss: 0.0017356866185327957 / Long term Validation loss: 0.1375\n",
      "Epoch: [1049/10000] Training loss: 0.00018476717065126549 / Validation loss: 0.0017341980322237549 / Long term Validation loss: 0.1375\n",
      "Epoch: [1050/10000] Training loss: 0.00018454685715210478 / Validation loss: 0.0017327122295442711 / Long term Validation loss: 0.1375\n",
      "Epoch: [1051/10000] Training loss: 0.00018432726442562799 / Validation loss: 0.0017312291964828265 / Long term Validation loss: 0.1375\n",
      "Epoch: [1052/10000] Training loss: 0.0001841083882189907 / Validation loss: 0.00172974891908052 / Long term Validation loss: 0.1375\n",
      "Epoch: [1053/10000] Training loss: 0.00018389022430150587 / Validation loss: 0.0017282713834311679 / Long term Validation loss: 0.1375\n",
      "Epoch: [1054/10000] Training loss: 0.00018367276846463558 / Validation loss: 0.001726796575681422 / Long term Validation loss: 0.1375\n",
      "Epoch: [1055/10000] Training loss: 0.00018345601652198213 / Validation loss: 0.0017253244820308572 / Long term Validation loss: 0.1374\n",
      "Epoch: [1056/10000] Training loss: 0.00018323996430927533 / Validation loss: 0.0017238550887320873 / Long term Validation loss: 0.1374\n",
      "Epoch: [1057/10000] Training loss: 0.00018302460768435808 / Validation loss: 0.0017223883820908575 / Long term Validation loss: 0.1374\n",
      "Epoch: [1058/10000] Training loss: 0.00018280994252716935 / Validation loss: 0.001720924348466152 / Long term Validation loss: 0.1374\n",
      "Epoch: [1059/10000] Training loss: 0.00018259596473972492 / Validation loss: 0.001719462974270269 / Long term Validation loss: 0.1374\n",
      "Epoch: [1060/10000] Training loss: 0.00018238267024609607 / Validation loss: 0.001718004245968923 / Long term Validation loss: 0.1374\n",
      "Epoch: [1061/10000] Training loss: 0.0001821700549923854 / Validation loss: 0.0017165481500813363 / Long term Validation loss: 0.1374\n",
      "Epoch: [1062/10000] Training loss: 0.00018195811494670157 / Validation loss: 0.0017150946731803198 / Long term Validation loss: 0.1373\n",
      "Epoch: [1063/10000] Training loss: 0.00018174684609913093 / Validation loss: 0.0017136438018923628 / Long term Validation loss: 0.1373\n",
      "Epoch: [1064/10000] Training loss: 0.00018153624446170757 / Validation loss: 0.0017121955228977068 / Long term Validation loss: 0.1373\n",
      "Epoch: [1065/10000] Training loss: 0.00018132630606838116 / Validation loss: 0.0017107498229304169 / Long term Validation loss: 0.1373\n",
      "Epoch: [1066/10000] Training loss: 0.00018111702697498324 / Validation loss: 0.0017093066887784798 / Long term Validation loss: 0.1373\n",
      "Epoch: [1067/10000] Training loss: 0.00018090840325919104 / Validation loss: 0.0017078661072838653 / Long term Validation loss: 0.1373\n",
      "Epoch: [1068/10000] Training loss: 0.00018070043102048962 / Validation loss: 0.0017064280653425984 / Long term Validation loss: 0.1373\n",
      "Epoch: [1069/10000] Training loss: 0.0001804931063801324 / Validation loss: 0.001704992549904813 / Long term Validation loss: 0.1372\n",
      "Epoch: [1070/10000] Training loss: 0.00018028642548109942 / Validation loss: 0.0017035595479748468 / Long term Validation loss: 0.1372\n",
      "Epoch: [1071/10000] Training loss: 0.00018008038448805443 / Validation loss: 0.0017021290466112846 / Long term Validation loss: 0.1372\n",
      "Epoch: [1072/10000] Training loss: 0.00017987497958729982 / Validation loss: 0.0017007010329270279 / Long term Validation loss: 0.1372\n",
      "Epoch: [1073/10000] Training loss: 0.00017967020698673002 / Validation loss: 0.0016992754940893503 / Long term Validation loss: 0.1372\n",
      "Epoch: [1074/10000] Training loss: 0.0001794660629157833 / Validation loss: 0.001697852417319946 / Long term Validation loss: 0.1372\n",
      "Epoch: [1075/10000] Training loss: 0.0001792625436253921 / Validation loss: 0.001696431789894997 / Long term Validation loss: 0.1372\n",
      "Epoch: [1076/10000] Training loss: 0.00017905964538793157 / Validation loss: 0.0016950135991452273 / Long term Validation loss: 0.1372\n",
      "Epoch: [1077/10000] Training loss: 0.0001788573644971668 / Validation loss: 0.001693597832455942 / Long term Validation loss: 0.1371\n",
      "Epoch: [1078/10000] Training loss: 0.00017865569726819848 / Validation loss: 0.001692184477267064 / Long term Validation loss: 0.1371\n",
      "Epoch: [1079/10000] Training loss: 0.00017845464003740714 / Validation loss: 0.0016907735210731952 / Long term Validation loss: 0.1371\n",
      "Epoch: [1080/10000] Training loss: 0.00017825418916239617 / Validation loss: 0.001689364951423658 / Long term Validation loss: 0.1371\n",
      "Epoch: [1081/10000] Training loss: 0.00017805434102193313 / Validation loss: 0.0016879587559225332 / Long term Validation loss: 0.1371\n",
      "Epoch: [1082/10000] Training loss: 0.00017785509201589044 / Validation loss: 0.0016865549222286856 / Long term Validation loss: 0.1371\n",
      "Epoch: [1083/10000] Training loss: 0.0001776564385651836 / Validation loss: 0.0016851534380558073 / Long term Validation loss: 0.1371\n",
      "Epoch: [1084/10000] Training loss: 0.00017745837711170956 / Validation loss: 0.0016837542911724468 / Long term Validation loss: 0.1371\n",
      "Epoch: [1085/10000] Training loss: 0.000177260904118283 / Validation loss: 0.0016823574694020487 / Long term Validation loss: 0.1370\n",
      "Epoch: [1086/10000] Training loss: 0.00017706401606857147 / Validation loss: 0.001680962960622972 / Long term Validation loss: 0.1370\n",
      "Epoch: [1087/10000] Training loss: 0.00017686770946702987 / Validation loss: 0.0016795707527685131 / Long term Validation loss: 0.1370\n",
      "Epoch: [1088/10000] Training loss: 0.00017667198083883346 / Validation loss: 0.001678180833826932 / Long term Validation loss: 0.1370\n",
      "Epoch: [1089/10000] Training loss: 0.0001764768267298099 / Validation loss: 0.0016767931918414733 / Long term Validation loss: 0.1370\n",
      "Epoch: [1090/10000] Training loss: 0.00017628224370637007 / Validation loss: 0.0016754078149103832 / Long term Validation loss: 0.1370\n",
      "Epoch: [1091/10000] Training loss: 0.00017608822835543845 / Validation loss: 0.0016740246911869294 / Long term Validation loss: 0.1370\n",
      "Epoch: [1092/10000] Training loss: 0.00017589477728438189 / Validation loss: 0.0016726438088794002 / Long term Validation loss: 0.1370\n",
      "Epoch: [1093/10000] Training loss: 0.00017570188712093756 / Validation loss: 0.001671265156251134 / Long term Validation loss: 0.1369\n",
      "Epoch: [1094/10000] Training loss: 0.0001755095545131405 / Validation loss: 0.0016698887216205136 / Long term Validation loss: 0.1369\n",
      "Epoch: [1095/10000] Training loss: 0.00017531777612924977 / Validation loss: 0.0016685144933609789 / Long term Validation loss: 0.1369\n",
      "Epoch: [1096/10000] Training loss: 0.00017512654865767385 / Validation loss: 0.0016671424599010286 / Long term Validation loss: 0.1369\n",
      "Epoch: [1097/10000] Training loss: 0.0001749358688068951 / Validation loss: 0.0016657726097242234 / Long term Validation loss: 0.1369\n",
      "Epoch: [1098/10000] Training loss: 0.00017474573330539427 / Validation loss: 0.0016644049313691814 / Long term Validation loss: 0.1369\n",
      "Epoch: [1099/10000] Training loss: 0.00017455613890157283 / Validation loss: 0.0016630394134295743 / Long term Validation loss: 0.1369\n",
      "Epoch: [1100/10000] Training loss: 0.0001743670823636758 / Validation loss: 0.001661676044554131 / Long term Validation loss: 0.1369\n",
      "Epoch: [1101/10000] Training loss: 0.00017417856047971324 / Validation loss: 0.0016603148134466145 / Long term Validation loss: 0.1369\n",
      "Epoch: [1102/10000] Training loss: 0.00017399057005738132 / Validation loss: 0.0016589557088658273 / Long term Validation loss: 0.1369\n",
      "Epoch: [1103/10000] Training loss: 0.00017380310792398262 / Validation loss: 0.0016575987196255895 / Long term Validation loss: 0.1368\n",
      "Epoch: [1104/10000] Training loss: 0.00017361617092634564 / Validation loss: 0.0016562438345947227 / Long term Validation loss: 0.1368\n",
      "Epoch: [1105/10000] Training loss: 0.00017342975593074433 / Validation loss: 0.0016548910426970353 / Long term Validation loss: 0.1368\n",
      "Epoch: [1106/10000] Training loss: 0.00017324385982281628 / Validation loss: 0.001653540332911308 / Long term Validation loss: 0.1368\n",
      "Epoch: [1107/10000] Training loss: 0.00017305847950748078 / Validation loss: 0.0016521916942712593 / Long term Validation loss: 0.1368\n",
      "Epoch: [1108/10000] Training loss: 0.0001728736119088563 / Validation loss: 0.0016508451158655385 / Long term Validation loss: 0.1368\n",
      "Epoch: [1109/10000] Training loss: 0.00017268925397017756 / Validation loss: 0.0016495005868376804 / Long term Validation loss: 0.1368\n",
      "Epoch: [1110/10000] Training loss: 0.00017250540265371177 / Validation loss: 0.0016481580963860945 / Long term Validation loss: 0.1368\n",
      "Epoch: [1111/10000] Training loss: 0.0001723220549406749 / Validation loss: 0.001646817633764012 / Long term Validation loss: 0.1368\n",
      "Epoch: [1112/10000] Training loss: 0.00017213920783114704 / Validation loss: 0.0016454791882794789 / Long term Validation loss: 0.1368\n",
      "Epoch: [1113/10000] Training loss: 0.00017195685834398784 / Validation loss: 0.0016441427492953032 / Long term Validation loss: 0.1367\n",
      "Epoch: [1114/10000] Training loss: 0.00017177500351675111 / Validation loss: 0.001642808306229017 / Long term Validation loss: 0.1367\n",
      "Epoch: [1115/10000] Training loss: 0.00017159364040559926 / Validation loss: 0.0016414758485528405 / Long term Validation loss: 0.1367\n",
      "Epoch: [1116/10000] Training loss: 0.0001714127660852175 / Validation loss: 0.0016401453657936404 / Long term Validation loss: 0.1367\n",
      "Epoch: [1117/10000] Training loss: 0.00017123237764872748 / Validation loss: 0.0016388168475328856 / Long term Validation loss: 0.1367\n",
      "Epoch: [1118/10000] Training loss: 0.00017105247220760078 / Validation loss: 0.0016374902834065984 / Long term Validation loss: 0.1367\n",
      "Epoch: [1119/10000] Training loss: 0.00017087304689157202 / Validation loss: 0.0016361656631052996 / Long term Validation loss: 0.1367\n",
      "Epoch: [1120/10000] Training loss: 0.00017069409884855197 / Validation loss: 0.0016348429763739655 / Long term Validation loss: 0.1367\n",
      "Epoch: [1121/10000] Training loss: 0.0001705156252445397 / Validation loss: 0.0016335222130119792 / Long term Validation loss: 0.1367\n",
      "Epoch: [1122/10000] Training loss: 0.00017033762326353565 / Validation loss: 0.0016322033628730645 / Long term Validation loss: 0.1367\n",
      "Epoch: [1123/10000] Training loss: 0.0001701600901074533 / Validation loss: 0.001630886415865227 / Long term Validation loss: 0.1367\n",
      "Epoch: [1124/10000] Training loss: 0.0001699830229960314 / Validation loss: 0.0016295713619506908 / Long term Validation loss: 0.1366\n",
      "Epoch: [1125/10000] Training loss: 0.00016980641916674574 / Validation loss: 0.0016282581911458613 / Long term Validation loss: 0.1366\n",
      "Epoch: [1126/10000] Training loss: 0.00016963027587472087 / Validation loss: 0.0016269468935212349 / Long term Validation loss: 0.1366\n",
      "Epoch: [1127/10000] Training loss: 0.00016945459039264168 / Validation loss: 0.0016256374592013379 / Long term Validation loss: 0.1366\n",
      "Epoch: [1128/10000] Training loss: 0.00016927936001066453 / Validation loss: 0.0016243298783646634 / Long term Validation loss: 0.1366\n",
      "Epoch: [1129/10000] Training loss: 0.00016910458203632885 / Validation loss: 0.0016230241412435898 / Long term Validation loss: 0.1366\n",
      "Epoch: [1130/10000] Training loss: 0.000168930253794468 / Validation loss: 0.0016217202381243266 / Long term Validation loss: 0.1366\n",
      "Epoch: [1131/10000] Training loss: 0.00016875637262712083 / Validation loss: 0.0016204181593468246 / Long term Validation loss: 0.1366\n",
      "Epoch: [1132/10000] Training loss: 0.00016858293589344227 / Validation loss: 0.0016191178953046986 / Long term Validation loss: 0.1366\n",
      "Epoch: [1133/10000] Training loss: 0.00016840994096961446 / Validation loss: 0.0016178194364451514 / Long term Validation loss: 0.1366\n",
      "Epoch: [1134/10000] Training loss: 0.0001682373852487577 / Validation loss: 0.001616522773268894 / Long term Validation loss: 0.1365\n",
      "Epoch: [1135/10000] Training loss: 0.0001680652661408416 / Validation loss: 0.0016152278963300656 / Long term Validation loss: 0.1365\n",
      "Epoch: [1136/10000] Training loss: 0.00016789358107259557 / Validation loss: 0.0016139347962361443 / Long term Validation loss: 0.1365\n",
      "Epoch: [1137/10000] Training loss: 0.00016772232748741997 / Validation loss: 0.00161264346364785 / Long term Validation loss: 0.1365\n",
      "Epoch: [1138/10000] Training loss: 0.00016755150284529703 / Validation loss: 0.0016113538892790654 / Long term Validation loss: 0.1365\n",
      "Epoch: [1139/10000] Training loss: 0.00016738110462270178 / Validation loss: 0.0016100660638967504 / Long term Validation loss: 0.1365\n",
      "Epoch: [1140/10000] Training loss: 0.0001672111303125131 / Validation loss: 0.001608779978320841 / Long term Validation loss: 0.1365\n",
      "Epoch: [1141/10000] Training loss: 0.0001670415774239245 / Validation loss: 0.0016074956234241496 / Long term Validation loss: 0.1365\n",
      "Epoch: [1142/10000] Training loss: 0.00016687244348235567 / Validation loss: 0.001606212990132267 / Long term Validation loss: 0.1365\n",
      "Epoch: [1143/10000] Training loss: 0.00016670372602936332 / Validation loss: 0.0016049320694234773 / Long term Validation loss: 0.1365\n",
      "Epoch: [1144/10000] Training loss: 0.00016653542262255245 / Validation loss: 0.00160365285232865 / Long term Validation loss: 0.1365\n",
      "Epoch: [1145/10000] Training loss: 0.0001663675308354879 / Validation loss: 0.0016023753299311294 / Long term Validation loss: 0.1365\n",
      "Epoch: [1146/10000] Training loss: 0.00016620004825760573 / Validation loss: 0.0016010994933666315 / Long term Validation loss: 0.1364\n",
      "Epoch: [1147/10000] Training loss: 0.00016603297249412468 / Validation loss: 0.0015998253338231395 / Long term Validation loss: 0.1364\n",
      "Epoch: [1148/10000] Training loss: 0.00016586630116595825 / Validation loss: 0.0015985528425408089 / Long term Validation loss: 0.1364\n",
      "Epoch: [1149/10000] Training loss: 0.000165700031909626 / Validation loss: 0.0015972820108118343 / Long term Validation loss: 0.1364\n",
      "Epoch: [1150/10000] Training loss: 0.000165534162377166 / Validation loss: 0.0015960128299803466 / Long term Validation loss: 0.1364\n",
      "Epoch: [1151/10000] Training loss: 0.00016536869023604675 / Validation loss: 0.0015947452914422925 / Long term Validation loss: 0.1364\n",
      "Epoch: [1152/10000] Training loss: 0.0001652036131690797 / Validation loss: 0.001593479386645335 / Long term Validation loss: 0.1364\n",
      "Epoch: [1153/10000] Training loss: 0.0001650389288743314 / Validation loss: 0.0015922151070887308 / Long term Validation loss: 0.1364\n",
      "Epoch: [1154/10000] Training loss: 0.00016487463506503641 / Validation loss: 0.001590952444323188 / Long term Validation loss: 0.1364\n",
      "Epoch: [1155/10000] Training loss: 0.00016471072946951012 / Validation loss: 0.0015896913899507664 / Long term Validation loss: 0.1364\n",
      "Epoch: [1156/10000] Training loss: 0.00016454720983106186 / Validation loss: 0.0015884319356247528 / Long term Validation loss: 0.1364\n",
      "Epoch: [1157/10000] Training loss: 0.00016438407390790806 / Validation loss: 0.0015871740730495376 / Long term Validation loss: 0.1364\n",
      "Epoch: [1158/10000] Training loss: 0.00016422131947308558 / Validation loss: 0.0015859177939804789 / Long term Validation loss: 0.1363\n",
      "Epoch: [1159/10000] Training loss: 0.00016405894431436574 / Validation loss: 0.0015846630902237786 / Long term Validation loss: 0.1363\n",
      "Epoch: [1160/10000] Training loss: 0.00016389694623416797 / Validation loss: 0.0015834099536363461 / Long term Validation loss: 0.1363\n",
      "Epoch: [1161/10000] Training loss: 0.00016373532304947406 / Validation loss: 0.0015821583761256891 / Long term Validation loss: 0.1363\n",
      "Epoch: [1162/10000] Training loss: 0.00016357407259174253 / Validation loss: 0.0015809083496497603 / Long term Validation loss: 0.1363\n",
      "Epoch: [1163/10000] Training loss: 0.00016341319270682324 / Validation loss: 0.0015796598662168237 / Long term Validation loss: 0.1363\n",
      "Epoch: [1164/10000] Training loss: 0.00016325268125487238 / Validation loss: 0.0015784129178853222 / Long term Validation loss: 0.1363\n",
      "Epoch: [1165/10000] Training loss: 0.0001630925361102674 / Validation loss: 0.001577167496763741 / Long term Validation loss: 0.1363\n",
      "Epoch: [1166/10000] Training loss: 0.00016293275516152263 / Validation loss: 0.0015759235950104761 / Long term Validation loss: 0.1363\n",
      "Epoch: [1167/10000] Training loss: 0.00016277333631120505 / Validation loss: 0.0015746812048336757 / Long term Validation loss: 0.1363\n",
      "Epoch: [1168/10000] Training loss: 0.0001626142774758498 / Validation loss: 0.0015734403184911037 / Long term Validation loss: 0.1363\n",
      "Epoch: [1169/10000] Training loss: 0.000162455576585877 / Validation loss: 0.00157220092829001 / Long term Validation loss: 0.1363\n",
      "Epoch: [1170/10000] Training loss: 0.0001622972315855076 / Validation loss: 0.0015709630265869695 / Long term Validation loss: 0.1363\n",
      "Epoch: [1171/10000] Training loss: 0.00016213924043268104 / Validation loss: 0.0015697266057877412 / Long term Validation loss: 0.1363\n",
      "Epoch: [1172/10000] Training loss: 0.0001619816010989716 / Validation loss: 0.0015684916583471108 / Long term Validation loss: 0.1362\n",
      "Epoch: [1173/10000] Training loss: 0.00016182431156950612 / Validation loss: 0.0015672581767687613 / Long term Validation loss: 0.1362\n",
      "Epoch: [1174/10000] Training loss: 0.00016166736984288185 / Validation loss: 0.001566026153605102 / Long term Validation loss: 0.1362\n",
      "Epoch: [1175/10000] Training loss: 0.00016151077393108422 / Validation loss: 0.0015647955814571347 / Long term Validation loss: 0.1362\n",
      "Epoch: [1176/10000] Training loss: 0.0001613545218594054 / Validation loss: 0.0015635664529742815 / Long term Validation loss: 0.1362\n",
      "Epoch: [1177/10000] Training loss: 0.00016119861166636267 / Validation loss: 0.0015623387608542367 / Long term Validation loss: 0.1362\n",
      "Epoch: [1178/10000] Training loss: 0.00016104304140361782 / Validation loss: 0.0015611124978428148 / Long term Validation loss: 0.1362\n",
      "Epoch: [1179/10000] Training loss: 0.000160887809135896 / Validation loss: 0.001559887656733797 / Long term Validation loss: 0.1362\n",
      "Epoch: [1180/10000] Training loss: 0.00016073291294090556 / Validation loss: 0.0015586642303687592 / Long term Validation loss: 0.1362\n",
      "Epoch: [1181/10000] Training loss: 0.00016057835090925812 / Validation loss: 0.001557442211636918 / Long term Validation loss: 0.1362\n",
      "Epoch: [1182/10000] Training loss: 0.00016042412114438862 / Validation loss: 0.0015562215934749712 / Long term Validation loss: 0.1362\n",
      "Epoch: [1183/10000] Training loss: 0.00016027022176247634 / Validation loss: 0.0015550023688669409 / Long term Validation loss: 0.1362\n",
      "Epoch: [1184/10000] Training loss: 0.00016011665089236526 / Validation loss: 0.001553784530843997 / Long term Validation loss: 0.1362\n",
      "Epoch: [1185/10000] Training loss: 0.00015996340667548596 / Validation loss: 0.0015525680724842955 / Long term Validation loss: 0.1362\n",
      "Epoch: [1186/10000] Training loss: 0.0001598104872657769 / Validation loss: 0.001551352986912825 / Long term Validation loss: 0.1361\n",
      "Epoch: [1187/10000] Training loss: 0.0001596578908296067 / Validation loss: 0.0015501392673012264 / Long term Validation loss: 0.1361\n",
      "Epoch: [1188/10000] Training loss: 0.0001595056155456963 / Validation loss: 0.0015489269068676365 / Long term Validation loss: 0.1361\n",
      "Epoch: [1189/10000] Training loss: 0.0001593536596050418 / Validation loss: 0.001547715898876503 / Long term Validation loss: 0.1361\n",
      "Epoch: [1190/10000] Training loss: 0.00015920202121083745 / Validation loss: 0.0015465062366384202 / Long term Validation loss: 0.1361\n",
      "Epoch: [1191/10000] Training loss: 0.00015905069857839906 / Validation loss: 0.0015452979135099768 / Long term Validation loss: 0.1361\n",
      "Epoch: [1192/10000] Training loss: 0.0001588996899350879 / Validation loss: 0.0015440909228935597 / Long term Validation loss: 0.1361\n",
      "Epoch: [1193/10000] Training loss: 0.00015874899352023465 / Validation loss: 0.0015428852582371859 / Long term Validation loss: 0.1361\n",
      "Epoch: [1194/10000] Training loss: 0.00015859860758506419 / Validation loss: 0.0015416809130343334 / Long term Validation loss: 0.1361\n",
      "Epoch: [1195/10000] Training loss: 0.00015844853039262 / Validation loss: 0.0015404778808237698 / Long term Validation loss: 0.1361\n",
      "Epoch: [1196/10000] Training loss: 0.0001582987602176902 / Validation loss: 0.0015392761551893694 / Long term Validation loss: 0.1361\n",
      "Epoch: [1197/10000] Training loss: 0.00015814929534673216 / Validation loss: 0.0015380757297599348 / Long term Validation loss: 0.1361\n",
      "Epoch: [1198/10000] Training loss: 0.0001580001340777992 / Validation loss: 0.0015368765982090235 / Long term Validation loss: 0.1361\n",
      "Epoch: [1199/10000] Training loss: 0.00015785127472046662 / Validation loss: 0.001535678754254772 / Long term Validation loss: 0.1360\n",
      "Epoch: [1200/10000] Training loss: 0.0001577027155957587 / Validation loss: 0.001534482191659716 / Long term Validation loss: 0.1360\n",
      "Epoch: [1201/10000] Training loss: 0.00015755445503607524 / Validation loss: 0.0015332869042306036 / Long term Validation loss: 0.1360\n",
      "Epoch: [1202/10000] Training loss: 0.00015740649138511962 / Validation loss: 0.001532092885818214 / Long term Validation loss: 0.1360\n",
      "Epoch: [1203/10000] Training loss: 0.00015725882299782624 / Validation loss: 0.0015309001303171832 / Long term Validation loss: 0.1360\n",
      "Epoch: [1204/10000] Training loss: 0.00015711144824028867 / Validation loss: 0.0015297086316658247 / Long term Validation loss: 0.1360\n",
      "Epoch: [1205/10000] Training loss: 0.00015696436548968875 / Validation loss: 0.0015285183838459383 / Long term Validation loss: 0.1360\n",
      "Epoch: [1206/10000] Training loss: 0.000156817573134225 / Validation loss: 0.0015273293808826139 / Long term Validation loss: 0.1360\n",
      "Epoch: [1207/10000] Training loss: 0.0001566710695730421 / Validation loss: 0.0015261416168440766 / Long term Validation loss: 0.1360\n",
      "Epoch: [1208/10000] Training loss: 0.00015652485321616065 / Validation loss: 0.0015249550858414853 / Long term Validation loss: 0.1360\n",
      "Epoch: [1209/10000] Training loss: 0.00015637892248440734 / Validation loss: 0.0015237697820287446 / Long term Validation loss: 0.1360\n",
      "Epoch: [1210/10000] Training loss: 0.00015623327580934526 / Validation loss: 0.0015225856996023167 / Long term Validation loss: 0.1360\n",
      "Epoch: [1211/10000] Training loss: 0.00015608791163320467 / Validation loss: 0.001521402832801039 / Long term Validation loss: 0.1360\n",
      "Epoch: [1212/10000] Training loss: 0.00015594282840881445 / Validation loss: 0.001520221175905948 / Long term Validation loss: 0.1359\n",
      "Epoch: [1213/10000] Training loss: 0.0001557980245995335 / Validation loss: 0.00151904072324007 / Long term Validation loss: 0.1359\n",
      "Epoch: [1214/10000] Training loss: 0.00015565349867918275 / Validation loss: 0.0015178614691682432 / Long term Validation loss: 0.1359\n",
      "Epoch: [1215/10000] Training loss: 0.00015550924913197765 / Validation loss: 0.0015166834080969228 / Long term Validation loss: 0.1359\n",
      "Epoch: [1216/10000] Training loss: 0.00015536527445246046 / Validation loss: 0.0015155065344740074 / Long term Validation loss: 0.1359\n",
      "Epoch: [1217/10000] Training loss: 0.00015522157314543398 / Validation loss: 0.0015143308427886285 / Long term Validation loss: 0.1359\n",
      "Epoch: [1218/10000] Training loss: 0.0001550781437258946 / Validation loss: 0.001513156327570969 / Long term Validation loss: 0.1359\n",
      "Epoch: [1219/10000] Training loss: 0.0001549349847189661 / Validation loss: 0.0015119829833920683 / Long term Validation loss: 0.1359\n",
      "Epoch: [1220/10000] Training loss: 0.0001547920946598342 / Validation loss: 0.0015108108048636404 / Long term Validation loss: 0.1359\n",
      "Epoch: [1221/10000] Training loss: 0.00015464947209368075 / Validation loss: 0.0015096397866378713 / Long term Validation loss: 0.1359\n",
      "Epoch: [1222/10000] Training loss: 0.00015450711557561921 / Validation loss: 0.0015084699234072328 / Long term Validation loss: 0.1359\n",
      "Epoch: [1223/10000] Training loss: 0.00015436502367062956 / Validation loss: 0.0015073012099042823 / Long term Validation loss: 0.1359\n",
      "Epoch: [1224/10000] Training loss: 0.00015422319495349417 / Validation loss: 0.0015061336409014795 / Long term Validation loss: 0.1358\n",
      "Epoch: [1225/10000] Training loss: 0.00015408162800873397 / Validation loss: 0.0015049672112109882 / Long term Validation loss: 0.1358\n",
      "Epoch: [1226/10000] Training loss: 0.00015394032143054487 / Validation loss: 0.0015038019156844813 / Long term Validation loss: 0.1358\n",
      "Epoch: [1227/10000] Training loss: 0.00015379927382273462 / Validation loss: 0.0015026377492129484 / Long term Validation loss: 0.1358\n",
      "Epoch: [1228/10000] Training loss: 0.00015365848379865986 / Validation loss: 0.0015014747067265048 / Long term Validation loss: 0.1358\n",
      "Epoch: [1229/10000] Training loss: 0.00015351794998116398 / Validation loss: 0.001500312783194188 / Long term Validation loss: 0.1358\n",
      "Epoch: [1230/10000] Training loss: 0.0001533776710025149 / Validation loss: 0.0014991519736237723 / Long term Validation loss: 0.1358\n",
      "Epoch: [1231/10000] Training loss: 0.00015323764550434333 / Validation loss: 0.0014979922730615723 / Long term Validation loss: 0.1358\n",
      "Epoch: [1232/10000] Training loss: 0.0001530978721375817 / Validation loss: 0.0014968336765922405 / Long term Validation loss: 0.1358\n",
      "Epoch: [1233/10000] Training loss: 0.00015295834956240297 / Validation loss: 0.0014956761793385761 / Long term Validation loss: 0.1358\n",
      "Epoch: [1234/10000] Training loss: 0.00015281907644816011 / Validation loss: 0.0014945197764613355 / Long term Validation loss: 0.1358\n",
      "Epoch: [1235/10000] Training loss: 0.00015268005147332604 / Validation loss: 0.0014933644631590342 / Long term Validation loss: 0.1357\n",
      "Epoch: [1236/10000] Training loss: 0.0001525412733254335 / Validation loss: 0.001492210234667739 / Long term Validation loss: 0.1357\n",
      "Epoch: [1237/10000] Training loss: 0.00015240274070101587 / Validation loss: 0.001491057086260885 / Long term Validation loss: 0.1357\n",
      "Epoch: [1238/10000] Training loss: 0.00015226445230554758 / Validation loss: 0.0014899050132490804 / Long term Validation loss: 0.1357\n",
      "Epoch: [1239/10000] Training loss: 0.00015212640685338615 / Validation loss: 0.001488754010979904 / Long term Validation loss: 0.1357\n",
      "Epoch: [1240/10000] Training loss: 0.0001519886030677127 / Validation loss: 0.0014876040748377094 / Long term Validation loss: 0.1357\n",
      "Epoch: [1241/10000] Training loss: 0.00015185103968047494 / Validation loss: 0.001486455200243427 / Long term Validation loss: 0.1357\n",
      "Epoch: [1242/10000] Training loss: 0.00015171371543232868 / Validation loss: 0.001485307382654379 / Long term Validation loss: 0.1357\n",
      "Epoch: [1243/10000] Training loss: 0.00015157662907258105 / Validation loss: 0.0014841606175640758 / Long term Validation loss: 0.1357\n",
      "Epoch: [1244/10000] Training loss: 0.00015143977935913304 / Validation loss: 0.0014830149005020084 / Long term Validation loss: 0.1357\n",
      "Epoch: [1245/10000] Training loss: 0.00015130316505842324 / Validation loss: 0.0014818702270334746 / Long term Validation loss: 0.1357\n",
      "Epoch: [1246/10000] Training loss: 0.00015116678494537145 / Validation loss: 0.0014807265927593658 / Long term Validation loss: 0.1356\n",
      "Epoch: [1247/10000] Training loss: 0.00015103063780332252 / Validation loss: 0.001479583993315981 / Long term Validation loss: 0.1356\n",
      "Epoch: [1248/10000] Training loss: 0.00015089472242399115 / Validation loss: 0.0014784424243748213 / Long term Validation loss: 0.1356\n",
      "Epoch: [1249/10000] Training loss: 0.00015075903760740627 / Validation loss: 0.0014773018816424046 / Long term Validation loss: 0.1356\n",
      "Epoch: [1250/10000] Training loss: 0.00015062358216185636 / Validation loss: 0.0014761623608600587 / Long term Validation loss: 0.1356\n",
      "Epoch: [1251/10000] Training loss: 0.00015048835490383503 / Validation loss: 0.001475023857803735 / Long term Validation loss: 0.1356\n",
      "Epoch: [1252/10000] Training loss: 0.00015035335465798643 / Validation loss: 0.0014738863682838043 / Long term Validation loss: 0.1356\n",
      "Epoch: [1253/10000] Training loss: 0.0001502185802570516 / Validation loss: 0.001472749888144869 / Long term Validation loss: 0.1356\n",
      "Epoch: [1254/10000] Training loss: 0.000150084030541815 / Validation loss: 0.001471614413265563 / Long term Validation loss: 0.1356\n",
      "Epoch: [1255/10000] Training loss: 0.0001499497043610513 / Validation loss: 0.0014704799395583606 / Long term Validation loss: 0.1356\n",
      "Epoch: [1256/10000] Training loss: 0.00014981560057147238 / Validation loss: 0.0014693464629693767 / Long term Validation loss: 0.1356\n",
      "Epoch: [1257/10000] Training loss: 0.00014968171803767495 / Validation loss: 0.0014682139794781714 / Long term Validation loss: 0.1356\n",
      "Epoch: [1258/10000] Training loss: 0.0001495480556320882 / Validation loss: 0.0014670824850975621 / Long term Validation loss: 0.1355\n",
      "Epoch: [1259/10000] Training loss: 0.00014941461223492224 / Validation loss: 0.0014659519758734244 / Long term Validation loss: 0.1355\n",
      "Epoch: [1260/10000] Training loss: 0.00014928138673411633 / Validation loss: 0.0014648224478845007 / Long term Validation loss: 0.1355\n",
      "Epoch: [1261/10000] Training loss: 0.00014914837802528766 / Validation loss: 0.0014636938972422014 / Long term Validation loss: 0.1355\n",
      "Epoch: [1262/10000] Training loss: 0.00014901558501168057 / Validation loss: 0.0014625663200904075 / Long term Validation loss: 0.1355\n",
      "Epoch: [1263/10000] Training loss: 0.00014888300660411622 / Validation loss: 0.0014614397126052944 / Long term Validation loss: 0.1355\n",
      "Epoch: [1264/10000] Training loss: 0.00014875064172094186 / Validation loss: 0.0014603140709951242 / Long term Validation loss: 0.1355\n",
      "Epoch: [1265/10000] Training loss: 0.0001486184892879813 / Validation loss: 0.0014591893915000616 / Long term Validation loss: 0.1355\n",
      "Epoch: [1266/10000] Training loss: 0.0001484865482384854 / Validation loss: 0.0014580656703919663 / Long term Validation loss: 0.1355\n",
      "Epoch: [1267/10000] Training loss: 0.00014835481751308239 / Validation loss: 0.0014569429039742144 / Long term Validation loss: 0.1355\n",
      "Epoch: [1268/10000] Training loss: 0.00014822329605972937 / Validation loss: 0.0014558210885815073 / Long term Validation loss: 0.1355\n",
      "Epoch: [1269/10000] Training loss: 0.00014809198283366343 / Validation loss: 0.0014547002205796806 / Long term Validation loss: 0.1355\n",
      "Epoch: [1270/10000] Training loss: 0.00014796087679735354 / Validation loss: 0.001453580296365498 / Long term Validation loss: 0.1354\n",
      "Epoch: [1271/10000] Training loss: 0.00014782997692045202 / Validation loss: 0.0014524613123664705 / Long term Validation loss: 0.1354\n",
      "Epoch: [1272/10000] Training loss: 0.00014769928217974766 / Validation loss: 0.0014513432650406749 / Long term Validation loss: 0.1354\n",
      "Epoch: [1273/10000] Training loss: 0.0001475687915591177 / Validation loss: 0.0014502261508765534 / Long term Validation loss: 0.1354\n",
      "Epoch: [1274/10000] Training loss: 0.00014743850404948087 / Validation loss: 0.0014491099663927261 / Long term Validation loss: 0.1354\n",
      "Epoch: [1275/10000] Training loss: 0.00014730841864875086 / Validation loss: 0.0014479947081377934 / Long term Validation loss: 0.1354\n",
      "Epoch: [1276/10000] Training loss: 0.0001471785343617895 / Validation loss: 0.0014468803726901632 / Long term Validation loss: 0.1354\n",
      "Epoch: [1277/10000] Training loss: 0.00014704885020036092 / Validation loss: 0.0014457669566578584 / Long term Validation loss: 0.1354\n",
      "Epoch: [1278/10000] Training loss: 0.00014691936518308553 / Validation loss: 0.0014446544566783133 / Long term Validation loss: 0.1354\n",
      "Epoch: [1279/10000] Training loss: 0.0001467900783353942 / Validation loss: 0.0014435428694182112 / Long term Validation loss: 0.1354\n",
      "Epoch: [1280/10000] Training loss: 0.00014666098868948354 / Validation loss: 0.0014424321915732724 / Long term Validation loss: 0.1354\n",
      "Epoch: [1281/10000] Training loss: 0.00014653209528427033 / Validation loss: 0.0014413224198680892 / Long term Validation loss: 0.1354\n",
      "Epoch: [1282/10000] Training loss: 0.00014640339716534724 / Validation loss: 0.0014402135510559258 / Long term Validation loss: 0.1353\n",
      "Epoch: [1283/10000] Training loss: 0.0001462748933849383 / Validation loss: 0.0014391055819185368 / Long term Validation loss: 0.1353\n",
      "Epoch: [1284/10000] Training loss: 0.00014614658300185466 / Validation loss: 0.0014379985092659784 / Long term Validation loss: 0.1353\n",
      "Epoch: [1285/10000] Training loss: 0.00014601846508145095 / Validation loss: 0.001436892329936442 / Long term Validation loss: 0.1353\n",
      "Epoch: [1286/10000] Training loss: 0.00014589053869558171 / Validation loss: 0.0014357870407960397 / Long term Validation loss: 0.1353\n",
      "Epoch: [1287/10000] Training loss: 0.0001457628029225581 / Validation loss: 0.001434682638738641 / Long term Validation loss: 0.1353\n",
      "Epoch: [1288/10000] Training loss: 0.00014563525684710473 / Validation loss: 0.0014335791206856908 / Long term Validation loss: 0.1353\n",
      "Epoch: [1289/10000] Training loss: 0.00014550789956031753 / Validation loss: 0.0014324764835860193 / Long term Validation loss: 0.1353\n",
      "Epoch: [1290/10000] Training loss: 0.0001453807301596206 / Validation loss: 0.0014313747244156635 / Long term Validation loss: 0.1353\n",
      "Epoch: [1291/10000] Training loss: 0.00014525374774872473 / Validation loss: 0.001430273840177677 / Long term Validation loss: 0.1353\n",
      "Epoch: [1292/10000] Training loss: 0.00014512695143758522 / Validation loss: 0.0014291738279019652 / Long term Validation loss: 0.1353\n",
      "Epoch: [1293/10000] Training loss: 0.00014500034034236014 / Validation loss: 0.0014280746846450964 / Long term Validation loss: 0.1353\n",
      "Epoch: [1294/10000] Training loss: 0.00014487391358536944 / Validation loss: 0.0014269764074901238 / Long term Validation loss: 0.1352\n",
      "Epoch: [1295/10000] Training loss: 0.0001447476702950536 / Validation loss: 0.0014258789935463903 / Long term Validation loss: 0.1352\n",
      "Epoch: [1296/10000] Training loss: 0.00014462160960593298 / Validation loss: 0.0014247824399493839 / Long term Validation loss: 0.1352\n",
      "Epoch: [1297/10000] Training loss: 0.00014449573065856712 / Validation loss: 0.0014236867438605302 / Long term Validation loss: 0.1352\n",
      "Epoch: [1298/10000] Training loss: 0.00014437003259951497 / Validation loss: 0.001422591902467029 / Long term Validation loss: 0.1352\n",
      "Epoch: [1299/10000] Training loss: 0.00014424451458129418 / Validation loss: 0.0014214979129816671 / Long term Validation loss: 0.1352\n",
      "Epoch: [1300/10000] Training loss: 0.00014411917576234216 / Validation loss: 0.0014204047726426566 / Long term Validation loss: 0.1352\n",
      "Epoch: [1301/10000] Training loss: 0.00014399401530697608 / Validation loss: 0.0014193124787134513 / Long term Validation loss: 0.1352\n",
      "Epoch: [1302/10000] Training loss: 0.00014386903238535379 / Validation loss: 0.001418221028482566 / Long term Validation loss: 0.1352\n",
      "Epoch: [1303/10000] Training loss: 0.0001437442261734352 / Validation loss: 0.0014171304192634106 / Long term Validation loss: 0.1352\n",
      "Epoch: [1304/10000] Training loss: 0.00014361959585294304 / Validation loss: 0.0014160406483941193 / Long term Validation loss: 0.1352\n",
      "Epoch: [1305/10000] Training loss: 0.00014349514061132493 / Validation loss: 0.0014149517132373743 / Long term Validation loss: 0.1352\n",
      "Epoch: [1306/10000] Training loss: 0.0001433708596417149 / Validation loss: 0.0014138636111802176 / Long term Validation loss: 0.1352\n",
      "Epoch: [1307/10000] Training loss: 0.00014324675214289555 / Validation loss: 0.0014127763396339043 / Long term Validation loss: 0.1351\n",
      "Epoch: [1308/10000] Training loss: 0.00014312281731926045 / Validation loss: 0.001411689896033724 / Long term Validation loss: 0.1351\n",
      "Epoch: [1309/10000] Training loss: 0.0001429990543807765 / Validation loss: 0.0014106042778388196 / Long term Validation loss: 0.1351\n",
      "Epoch: [1310/10000] Training loss: 0.00014287546254294702 / Validation loss: 0.0014095194825320238 / Long term Validation loss: 0.1351\n",
      "Epoch: [1311/10000] Training loss: 0.0001427520410267745 / Validation loss: 0.0014084355076197054 / Long term Validation loss: 0.1351\n",
      "Epoch: [1312/10000] Training loss: 0.00014262878905872413 / Validation loss: 0.0014073523506315721 / Long term Validation loss: 0.1351\n",
      "Epoch: [1313/10000] Training loss: 0.0001425057058706873 / Validation loss: 0.0014062700091205152 / Long term Validation loss: 0.1351\n",
      "Epoch: [1314/10000] Training loss: 0.00014238279069994525 / Validation loss: 0.0014051884806624657 / Long term Validation loss: 0.1351\n",
      "Epoch: [1315/10000] Training loss: 0.0001422600427891331 / Validation loss: 0.001404107762856196 / Long term Validation loss: 0.1351\n",
      "Epoch: [1316/10000] Training loss: 0.00014213746138620436 / Validation loss: 0.0014030278533231609 / Long term Validation loss: 0.1351\n",
      "Epoch: [1317/10000] Training loss: 0.00014201504574439506 / Validation loss: 0.0014019487497073345 / Long term Validation loss: 0.1351\n",
      "Epoch: [1318/10000] Training loss: 0.0001418927951221886 / Validation loss: 0.0014008704496750698 / Long term Validation loss: 0.1351\n",
      "Epoch: [1319/10000] Training loss: 0.00014177070878328087 / Validation loss: 0.0013997929509149036 / Long term Validation loss: 0.1351\n",
      "Epoch: [1320/10000] Training loss: 0.00014164878599654508 / Validation loss: 0.0013987162511373934 / Long term Validation loss: 0.1351\n",
      "Epoch: [1321/10000] Training loss: 0.00014152702603599748 / Validation loss: 0.0013976403480749767 / Long term Validation loss: 0.1351\n",
      "Epoch: [1322/10000] Training loss: 0.0001414054281807628 / Validation loss: 0.0013965652394818072 / Long term Validation loss: 0.1350\n",
      "Epoch: [1323/10000] Training loss: 0.00014128399171504022 / Validation loss: 0.0013954909231335803 / Long term Validation loss: 0.1350\n",
      "Epoch: [1324/10000] Training loss: 0.00014116271592806934 / Validation loss: 0.0013944173968273692 / Long term Validation loss: 0.1350\n",
      "Epoch: [1325/10000] Training loss: 0.00014104160011409653 / Validation loss: 0.0013933446583814875 / Long term Validation loss: 0.1350\n",
      "Epoch: [1326/10000] Training loss: 0.00014092064357234144 / Validation loss: 0.001392272705635327 / Long term Validation loss: 0.1350\n",
      "Epoch: [1327/10000] Training loss: 0.00014079984560696386 / Validation loss: 0.0013912015364491747 / Long term Validation loss: 0.1350\n",
      "Epoch: [1328/10000] Training loss: 0.00014067920552703043 / Validation loss: 0.0013901311487040835 / Long term Validation loss: 0.1350\n",
      "Epoch: [1329/10000] Training loss: 0.00014055872264648187 / Validation loss: 0.0013890615403017068 / Long term Validation loss: 0.1350\n",
      "Epoch: [1330/10000] Training loss: 0.00014043839628410075 / Validation loss: 0.0013879927091641328 / Long term Validation loss: 0.1350\n",
      "Epoch: [1331/10000] Training loss: 0.00014031822576347848 / Validation loss: 0.0013869246532337521 / Long term Validation loss: 0.1350\n",
      "Epoch: [1332/10000] Training loss: 0.00014019821041298344 / Validation loss: 0.001385857370473083 / Long term Validation loss: 0.1350\n",
      "Epoch: [1333/10000] Training loss: 0.00014007834956572917 / Validation loss: 0.0013847908588646262 / Long term Validation loss: 0.1350\n",
      "Epoch: [1334/10000] Training loss: 0.0001399586425595422 / Validation loss: 0.0013837251164107106 / Long term Validation loss: 0.1350\n",
      "Epoch: [1335/10000] Training loss: 0.00013983908873693084 / Validation loss: 0.0013826601411333465 / Long term Validation loss: 0.1350\n",
      "Epoch: [1336/10000] Training loss: 0.00013971968744505347 / Validation loss: 0.0013815959310740733 / Long term Validation loss: 0.1350\n",
      "Epoch: [1337/10000] Training loss: 0.00013960043803568764 / Validation loss: 0.001380532484293799 / Long term Validation loss: 0.1350\n",
      "Epoch: [1338/10000] Training loss: 0.00013948133986519922 / Validation loss: 0.0013794697988726563 / Long term Validation loss: 0.1350\n",
      "Epoch: [1339/10000] Training loss: 0.00013936239229451127 / Validation loss: 0.001378407872909868 / Long term Validation loss: 0.1349\n",
      "Epoch: [1340/10000] Training loss: 0.0001392435946890738 / Validation loss: 0.001377346704523583 / Long term Validation loss: 0.1349\n",
      "Epoch: [1341/10000] Training loss: 0.00013912494641883327 / Validation loss: 0.0013762862918507246 / Long term Validation loss: 0.1349\n",
      "Epoch: [1342/10000] Training loss: 0.0001390064468582025 / Validation loss: 0.0013752266330468515 / Long term Validation loss: 0.1349\n",
      "Epoch: [1343/10000] Training loss: 0.0001388880953860307 / Validation loss: 0.0013741677262860276 / Long term Validation loss: 0.1349\n",
      "Epoch: [1344/10000] Training loss: 0.0001387698913855736 / Validation loss: 0.001373109569760654 / Long term Validation loss: 0.1349\n",
      "Epoch: [1345/10000] Training loss: 0.0001386518342444641 / Validation loss: 0.001372052161681324 / Long term Validation loss: 0.1349\n",
      "Epoch: [1346/10000] Training loss: 0.00013853392335468263 / Validation loss: 0.0013709955002766945 / Long term Validation loss: 0.1349\n",
      "Epoch: [1347/10000] Training loss: 0.00013841615811252826 / Validation loss: 0.0013699395837933467 / Long term Validation loss: 0.1349\n",
      "Epoch: [1348/10000] Training loss: 0.0001382985379185892 / Validation loss: 0.0013688844104956257 / Long term Validation loss: 0.1349\n",
      "Epoch: [1349/10000] Training loss: 0.0001381810621777146 / Validation loss: 0.0013678299786655003 / Long term Validation loss: 0.1349\n",
      "Epoch: [1350/10000] Training loss: 0.0001380637302989855 / Validation loss: 0.0013667762866024422 / Long term Validation loss: 0.1349\n",
      "Epoch: [1351/10000] Training loss: 0.00013794654169568653 / Validation loss: 0.0013657233326232702 / Long term Validation loss: 0.1349\n",
      "Epoch: [1352/10000] Training loss: 0.0001378294957852777 / Validation loss: 0.0013646711150620222 / Long term Validation loss: 0.1349\n",
      "Epoch: [1353/10000] Training loss: 0.00013771259198936615 / Validation loss: 0.001363619632269792 / Long term Validation loss: 0.1349\n",
      "Epoch: [1354/10000] Training loss: 0.0001375958297336784 / Validation loss: 0.0013625688826146248 / Long term Validation loss: 0.1349\n",
      "Epoch: [1355/10000] Training loss: 0.00013747920844803265 / Validation loss: 0.0013615188644813636 / Long term Validation loss: 0.1349\n",
      "Epoch: [1356/10000] Training loss: 0.00013736272756631113 / Validation loss: 0.0013604695762715223 / Long term Validation loss: 0.1349\n",
      "Epoch: [1357/10000] Training loss: 0.00013724638652643284 / Validation loss: 0.001359421016403123 / Long term Validation loss: 0.1349\n",
      "Epoch: [1358/10000] Training loss: 0.0001371301847703263 / Validation loss: 0.0013583731833105956 / Long term Validation loss: 0.1348\n",
      "Epoch: [1359/10000] Training loss: 0.0001370141217439027 / Validation loss: 0.0013573260754446367 / Long term Validation loss: 0.1348\n",
      "Epoch: [1360/10000] Training loss: 0.00013689819689702873 / Validation loss: 0.0013562796912720632 / Long term Validation loss: 0.1348\n",
      "Epoch: [1361/10000] Training loss: 0.0001367824096835003 / Validation loss: 0.001355234029275687 / Long term Validation loss: 0.1348\n",
      "Epoch: [1362/10000] Training loss: 0.00013666675956101586 / Validation loss: 0.0013541890879541789 / Long term Validation loss: 0.1348\n",
      "Epoch: [1363/10000] Training loss: 0.00013655124599115006 / Validation loss: 0.0013531448658219579 / Long term Validation loss: 0.1348\n",
      "Epoch: [1364/10000] Training loss: 0.00013643586843932774 / Validation loss: 0.0013521013614090423 / Long term Validation loss: 0.1348\n",
      "Epoch: [1365/10000] Training loss: 0.0001363206263747977 / Validation loss: 0.001351058573260919 / Long term Validation loss: 0.1348\n",
      "Epoch: [1366/10000] Training loss: 0.00013620551927060717 / Validation loss: 0.0013500164999384206 / Long term Validation loss: 0.1348\n",
      "Epoch: [1367/10000] Training loss: 0.00013609054660357605 / Validation loss: 0.0013489751400176072 / Long term Validation loss: 0.1348\n",
      "Epoch: [1368/10000] Training loss: 0.00013597570785427136 / Validation loss: 0.0013479344920896347 / Long term Validation loss: 0.1348\n",
      "Epoch: [1369/10000] Training loss: 0.00013586100250698201 / Validation loss: 0.0013468945547606072 / Long term Validation loss: 0.1348\n",
      "Epoch: [1370/10000] Training loss: 0.00013574643004969354 / Validation loss: 0.0013458553266514754 / Long term Validation loss: 0.1348\n",
      "Epoch: [1371/10000] Training loss: 0.0001356319899740633 / Validation loss: 0.0013448168063979172 / Long term Validation loss: 0.1348\n",
      "Epoch: [1372/10000] Training loss: 0.00013551768177539535 / Validation loss: 0.0013437789926502 / Long term Validation loss: 0.1348\n",
      "Epoch: [1373/10000] Training loss: 0.0001354035049526161 / Validation loss: 0.0013427418840730456 / Long term Validation loss: 0.1348\n",
      "Epoch: [1374/10000] Training loss: 0.00013528945900824954 / Validation loss: 0.001341705479345537 / Long term Validation loss: 0.1348\n",
      "Epoch: [1375/10000] Training loss: 0.00013517554344839297 / Validation loss: 0.0013406697771609747 / Long term Validation loss: 0.1348\n",
      "Epoch: [1376/10000] Training loss: 0.00013506175778269278 / Validation loss: 0.0013396347762267764 / Long term Validation loss: 0.1348\n",
      "Epoch: [1377/10000] Training loss: 0.00013494810152432047 / Validation loss: 0.0013386004752643256 / Long term Validation loss: 0.1348\n",
      "Epoch: [1378/10000] Training loss: 0.00013483457418994862 / Validation loss: 0.0013375668730088777 / Long term Validation loss: 0.1348\n",
      "Epoch: [1379/10000] Training loss: 0.00013472117529972718 / Validation loss: 0.0013365339682094408 / Long term Validation loss: 0.1347\n",
      "Epoch: [1380/10000] Training loss: 0.0001346079043772601 / Validation loss: 0.0013355017596286488 / Long term Validation loss: 0.1347\n",
      "Epoch: [1381/10000] Training loss: 0.0001344947609495814 / Validation loss: 0.0013344702460426317 / Long term Validation loss: 0.1347\n",
      "Epoch: [1382/10000] Training loss: 0.00013438174454713232 / Validation loss: 0.0013334394262409309 / Long term Validation loss: 0.1347\n",
      "Epoch: [1383/10000] Training loss: 0.00013426885470373812 / Validation loss: 0.0013324092990263697 / Long term Validation loss: 0.1347\n",
      "Epoch: [1384/10000] Training loss: 0.00013415609095658481 / Validation loss: 0.0013313798632149194 / Long term Validation loss: 0.1347\n",
      "Epoch: [1385/10000] Training loss: 0.00013404345284619653 / Validation loss: 0.0013303511176356048 / Long term Validation loss: 0.1347\n",
      "Epoch: [1386/10000] Training loss: 0.00013393093991641286 / Validation loss: 0.0013293230611303977 / Long term Validation loss: 0.1347\n",
      "Epoch: [1387/10000] Training loss: 0.00013381855171436605 / Validation loss: 0.001328295692554082 / Long term Validation loss: 0.1347\n",
      "Epoch: [1388/10000] Training loss: 0.00013370628779045876 / Validation loss: 0.0013272690107741538 / Long term Validation loss: 0.1347\n",
      "Epoch: [1389/10000] Training loss: 0.0001335941476983419 / Validation loss: 0.0013262430146707108 / Long term Validation loss: 0.1347\n",
      "Epoch: [1390/10000] Training loss: 0.00013348213099489208 / Validation loss: 0.001325217703136345 / Long term Validation loss: 0.1347\n",
      "Epoch: [1391/10000] Training loss: 0.0001333702372401902 / Validation loss: 0.001324193075076018 / Long term Validation loss: 0.1347\n",
      "Epoch: [1392/10000] Training loss: 0.00013325846599749908 / Validation loss: 0.0013231691294069649 / Long term Validation loss: 0.1347\n",
      "Epoch: [1393/10000] Training loss: 0.0001331468168332419 / Validation loss: 0.0013221458650585832 / Long term Validation loss: 0.1347\n",
      "Epoch: [1394/10000] Training loss: 0.00013303528931698084 / Validation loss: 0.0013211232809723207 / Long term Validation loss: 0.1347\n",
      "Epoch: [1395/10000] Training loss: 0.0001329238830213953 / Validation loss: 0.0013201013761015724 / Long term Validation loss: 0.1347\n",
      "Epoch: [1396/10000] Training loss: 0.00013281259752226092 / Validation loss: 0.001319080149411579 / Long term Validation loss: 0.1347\n",
      "Epoch: [1397/10000] Training loss: 0.00013270143239842814 / Validation loss: 0.0013180595998793035 / Long term Validation loss: 0.1347\n",
      "Epoch: [1398/10000] Training loss: 0.00013259038723180123 / Validation loss: 0.0013170397264933417 / Long term Validation loss: 0.1347\n",
      "Epoch: [1399/10000] Training loss: 0.00013247946160731746 / Validation loss: 0.0013160205282538215 / Long term Validation loss: 0.1347\n",
      "Epoch: [1400/10000] Training loss: 0.00013236865511292637 / Validation loss: 0.0013150020041722895 / Long term Validation loss: 0.1347\n",
      "Epoch: [1401/10000] Training loss: 0.000132257967339569 / Validation loss: 0.0013139841532716 / Long term Validation loss: 0.1347\n",
      "Epoch: [1402/10000] Training loss: 0.0001321473978811574 / Validation loss: 0.0013129669745858311 / Long term Validation loss: 0.1347\n",
      "Epoch: [1403/10000] Training loss: 0.0001320369463345543 / Validation loss: 0.0013119504671601783 / Long term Validation loss: 0.1347\n",
      "Epoch: [1404/10000] Training loss: 0.00013192661229955295 / Validation loss: 0.0013109346300508482 / Long term Validation loss: 0.1346\n",
      "Epoch: [1405/10000] Training loss: 0.00013181639537885668 / Validation loss: 0.0013099194623249482 / Long term Validation loss: 0.1346\n",
      "Epoch: [1406/10000] Training loss: 0.00013170629517805926 / Validation loss: 0.0013089049630604142 / Long term Validation loss: 0.1346\n",
      "Epoch: [1407/10000] Training loss: 0.0001315963113056249 / Validation loss: 0.001307891131345898 / Long term Validation loss: 0.1346\n",
      "Epoch: [1408/10000] Training loss: 0.00013148644337286836 / Validation loss: 0.0013068779662806507 / Long term Validation loss: 0.1346\n",
      "Epoch: [1409/10000] Training loss: 0.00013137669099393562 / Validation loss: 0.001305865466974453 / Long term Validation loss: 0.1346\n",
      "Epoch: [1410/10000] Training loss: 0.00013126705378578403 / Validation loss: 0.001304853632547515 / Long term Validation loss: 0.1346\n",
      "Epoch: [1411/10000] Training loss: 0.00013115753136816327 / Validation loss: 0.0013038424621303582 / Long term Validation loss: 0.1346\n",
      "Epoch: [1412/10000] Training loss: 0.00013104812336359593 / Validation loss: 0.001302831954863742 / Long term Validation loss: 0.1346\n",
      "Epoch: [1413/10000] Training loss: 0.00013093882939735837 / Validation loss: 0.0013018221098985594 / Long term Validation loss: 0.1346\n",
      "Epoch: [1414/10000] Training loss: 0.00013082964909746187 / Validation loss: 0.0013008129263957459 / Long term Validation loss: 0.1346\n",
      "Epoch: [1415/10000] Training loss: 0.00013072058209463337 / Validation loss: 0.0012998044035261802 / Long term Validation loss: 0.1346\n",
      "Epoch: [1416/10000] Training loss: 0.00013061162802229708 / Validation loss: 0.001298796540470597 / Long term Validation loss: 0.1346\n",
      "Epoch: [1417/10000] Training loss: 0.0001305027865165558 / Validation loss: 0.0012977893364195 / Long term Validation loss: 0.1346\n",
      "Epoch: [1418/10000] Training loss: 0.00013039405721617205 / Validation loss: 0.001296782790573051 / Long term Validation loss: 0.1346\n",
      "Epoch: [1419/10000] Training loss: 0.0001302854397625501 / Validation loss: 0.0012957769021410004 / Long term Validation loss: 0.1346\n",
      "Epoch: [1420/10000] Training loss: 0.00013017693379971723 / Validation loss: 0.001294771670342588 / Long term Validation loss: 0.1346\n",
      "Epoch: [1421/10000] Training loss: 0.000130068538974306 / Validation loss: 0.001293767094406444 / Long term Validation loss: 0.1346\n",
      "Epoch: [1422/10000] Training loss: 0.0001299602549355356 / Validation loss: 0.001292763173570521 / Long term Validation loss: 0.1346\n",
      "Epoch: [1423/10000] Training loss: 0.00012985208133519454 / Validation loss: 0.0012917599070819822 / Long term Validation loss: 0.1346\n",
      "Epoch: [1424/10000] Training loss: 0.00012974401782762227 / Validation loss: 0.0012907572941971356 / Long term Validation loss: 0.1346\n",
      "Epoch: [1425/10000] Training loss: 0.00012963606406969183 / Validation loss: 0.0012897553341813257 / Long term Validation loss: 0.1346\n",
      "Epoch: [1426/10000] Training loss: 0.00012952821972079178 / Validation loss: 0.0012887540263088644 / Long term Validation loss: 0.1346\n",
      "Epoch: [1427/10000] Training loss: 0.00012942048444280932 / Validation loss: 0.0012877533698629324 / Long term Validation loss: 0.1346\n",
      "Epoch: [1428/10000] Training loss: 0.00012931285790011222 / Validation loss: 0.0012867533641355024 / Long term Validation loss: 0.1346\n",
      "Epoch: [1429/10000] Training loss: 0.00012920533975953197 / Validation loss: 0.0012857540084272538 / Long term Validation loss: 0.1346\n",
      "Epoch: [1430/10000] Training loss: 0.0001290979296903465 / Validation loss: 0.0012847553020474748 / Long term Validation loss: 0.1346\n",
      "Epoch: [1431/10000] Training loss: 0.00012899062736426283 / Validation loss: 0.001283757244314005 / Long term Validation loss: 0.1346\n",
      "Epoch: [1432/10000] Training loss: 0.00012888343245540064 / Validation loss: 0.0012827598345531257 / Long term Validation loss: 0.1345\n",
      "Epoch: [1433/10000] Training loss: 0.00012877634464027483 / Validation loss: 0.0012817630720994917 / Long term Validation loss: 0.1345\n",
      "Epoch: [1434/10000] Training loss: 0.00012866936359777917 / Validation loss: 0.0012807669562960517 / Long term Validation loss: 0.1345\n",
      "Epoch: [1435/10000] Training loss: 0.00012856248900916948 / Validation loss: 0.001279771486493958 / Long term Validation loss: 0.1345\n",
      "Epoch: [1436/10000] Training loss: 0.00012845572055804704 / Validation loss: 0.001278776662052487 / Long term Validation loss: 0.1345\n",
      "Epoch: [1437/10000] Training loss: 0.0001283490579303422 / Validation loss: 0.0012777824823389733 / Long term Validation loss: 0.1345\n",
      "Epoch: [1438/10000] Training loss: 0.00012824250081429816 / Validation loss: 0.0012767889467287122 / Long term Validation loss: 0.1345\n",
      "Epoch: [1439/10000] Training loss: 0.00012813604890045445 / Validation loss: 0.001275796054604883 / Long term Validation loss: 0.1345\n",
      "Epoch: [1440/10000] Training loss: 0.0001280297018816311 / Validation loss: 0.0012748038053584877 / Long term Validation loss: 0.1345\n",
      "Epoch: [1441/10000] Training loss: 0.00012792345945291238 / Validation loss: 0.0012738121983882572 / Long term Validation loss: 0.1345\n",
      "Epoch: [1442/10000] Training loss: 0.00012781732131163093 / Validation loss: 0.0012728212331005707 / Long term Validation loss: 0.1345\n",
      "Epoch: [1443/10000] Training loss: 0.00012771128715735204 / Validation loss: 0.0012718309089093948 / Long term Validation loss: 0.1345\n",
      "Epoch: [1444/10000] Training loss: 0.0001276053566918576 / Validation loss: 0.001270841225236207 / Long term Validation loss: 0.1345\n",
      "Epoch: [1445/10000] Training loss: 0.0001274995296191309 / Validation loss: 0.0012698521815098962 / Long term Validation loss: 0.1345\n",
      "Epoch: [1446/10000] Training loss: 0.00012739380564534076 / Validation loss: 0.0012688637771667132 / Long term Validation loss: 0.1345\n",
      "Epoch: [1447/10000] Training loss: 0.00012728818447882613 / Validation loss: 0.0012678760116501917 / Long term Validation loss: 0.1345\n",
      "Epoch: [1448/10000] Training loss: 0.0001271826658300808 / Validation loss: 0.0012668888844110666 / Long term Validation loss: 0.1345\n",
      "Epoch: [1449/10000] Training loss: 0.00012707724941173838 / Validation loss: 0.0012659023949072006 / Long term Validation loss: 0.1345\n",
      "Epoch: [1450/10000] Training loss: 0.00012697193493855656 / Validation loss: 0.0012649165426035199 / Long term Validation loss: 0.1345\n",
      "Epoch: [1451/10000] Training loss: 0.00012686672212740264 / Validation loss: 0.0012639313269719483 / Long term Validation loss: 0.1345\n",
      "Epoch: [1452/10000] Training loss: 0.00012676161069723825 / Validation loss: 0.0012629467474913091 / Long term Validation loss: 0.1345\n",
      "Epoch: [1453/10000] Training loss: 0.00012665660036910456 / Validation loss: 0.0012619628036472706 / Long term Validation loss: 0.1345\n",
      "Epoch: [1454/10000] Training loss: 0.00012655169086610757 / Validation loss: 0.0012609794949322932 / Long term Validation loss: 0.1345\n",
      "Epoch: [1455/10000] Training loss: 0.00012644688191340315 / Validation loss: 0.0012599968208455265 / Long term Validation loss: 0.1345\n",
      "Epoch: [1456/10000] Training loss: 0.0001263421732381829 / Validation loss: 0.001259014780892757 / Long term Validation loss: 0.1345\n",
      "Epoch: [1457/10000] Training loss: 0.0001262375645696593 / Validation loss: 0.001258033374586337 / Long term Validation loss: 0.1345\n",
      "Epoch: [1458/10000] Training loss: 0.00012613305563905126 / Validation loss: 0.0012570526014451218 / Long term Validation loss: 0.1345\n",
      "Epoch: [1459/10000] Training loss: 0.00012602864617957017 / Validation loss: 0.001256072460994399 / Long term Validation loss: 0.1345\n",
      "Epoch: [1460/10000] Training loss: 0.00012592433592640518 / Validation loss: 0.001255092952765796 / Long term Validation loss: 0.1345\n",
      "Epoch: [1461/10000] Training loss: 0.00012582012461670957 / Validation loss: 0.0012541140762972632 / Long term Validation loss: 0.1345\n",
      "Epoch: [1462/10000] Training loss: 0.00012571601198958622 / Validation loss: 0.0012531358311329726 / Long term Validation loss: 0.1345\n",
      "Epoch: [1463/10000] Training loss: 0.00012561199778607398 / Validation loss: 0.0012521582168232488 / Long term Validation loss: 0.1344\n",
      "Epoch: [1464/10000] Training loss: 0.0001255080817491336 / Validation loss: 0.001251181232924521 / Long term Validation loss: 0.1344\n",
      "Epoch: [1465/10000] Training loss: 0.00012540426362363412 / Validation loss: 0.0012502048789992592 / Long term Validation loss: 0.1344\n",
      "Epoch: [1466/10000] Training loss: 0.0001253005431563389 / Validation loss: 0.0012492291546158962 / Long term Validation loss: 0.1344\n",
      "Epoch: [1467/10000] Training loss: 0.00012519692009589227 / Validation loss: 0.00124825405934876 / Long term Validation loss: 0.1344\n",
      "Epoch: [1468/10000] Training loss: 0.00012509339419280587 / Validation loss: 0.0012472795927780343 / Long term Validation loss: 0.1344\n",
      "Epoch: [1469/10000] Training loss: 0.00012498996519944524 / Validation loss: 0.0012463057544896802 / Long term Validation loss: 0.1344\n",
      "Epoch: [1470/10000] Training loss: 0.00012488663287001634 / Validation loss: 0.0012453325440753692 / Long term Validation loss: 0.1344\n",
      "Epoch: [1471/10000] Training loss: 0.00012478339696055248 / Validation loss: 0.001244359961132426 / Long term Validation loss: 0.1344\n",
      "Epoch: [1472/10000] Training loss: 0.00012468025722890092 / Validation loss: 0.0012433880052637786 / Long term Validation loss: 0.1344\n",
      "Epoch: [1473/10000] Training loss: 0.00012457721343470988 / Validation loss: 0.0012424166760778787 / Long term Validation loss: 0.1344\n",
      "Epoch: [1474/10000] Training loss: 0.00012447426533941557 / Validation loss: 0.0012414459731886493 / Long term Validation loss: 0.1344\n",
      "Epoch: [1475/10000] Training loss: 0.00012437141270622897 / Validation loss: 0.0012404758962154304 / Long term Validation loss: 0.1344\n",
      "Epoch: [1476/10000] Training loss: 0.00012426865530012316 / Validation loss: 0.0012395064447829156 / Long term Validation loss: 0.1344\n",
      "Epoch: [1477/10000] Training loss: 0.00012416599288782074 / Validation loss: 0.0012385376185210838 / Long term Validation loss: 0.1344\n",
      "Epoch: [1478/10000] Training loss: 0.00012406342523778056 / Validation loss: 0.001237569417065148 / Long term Validation loss: 0.1344\n",
      "Epoch: [1479/10000] Training loss: 0.0001239609521201857 / Validation loss: 0.001236601840055516 / Long term Validation loss: 0.1344\n",
      "Epoch: [1480/10000] Training loss: 0.00012385857330693049 / Validation loss: 0.001235634887137691 / Long term Validation loss: 0.1344\n",
      "Epoch: [1481/10000] Training loss: 0.00012375628857160835 / Validation loss: 0.001234668557962244 / Long term Validation loss: 0.1344\n",
      "Epoch: [1482/10000] Training loss: 0.00012365409768949907 / Validation loss: 0.0012337028521847608 / Long term Validation loss: 0.1344\n",
      "Epoch: [1483/10000] Training loss: 0.0001235520004375568 / Validation loss: 0.001232737769465772 / Long term Validation loss: 0.1344\n",
      "Epoch: [1484/10000] Training loss: 0.0001234499965943977 / Validation loss: 0.0012317733094706917 / Long term Validation loss: 0.1344\n",
      "Epoch: [1485/10000] Training loss: 0.00012334808594028775 / Validation loss: 0.0012308094718697716 / Long term Validation loss: 0.1344\n",
      "Epoch: [1486/10000] Training loss: 0.00012324626825713068 / Validation loss: 0.0012298462563380662 / Long term Validation loss: 0.1344\n",
      "Epoch: [1487/10000] Training loss: 0.00012314454332845604 / Validation loss: 0.001228883662555334 / Long term Validation loss: 0.1344\n",
      "Epoch: [1488/10000] Training loss: 0.00012304291093940715 / Validation loss: 0.0012279216902060114 / Long term Validation loss: 0.1344\n",
      "Epoch: [1489/10000] Training loss: 0.00012294137087672937 / Validation loss: 0.0012269603389791568 / Long term Validation loss: 0.1344\n",
      "Epoch: [1490/10000] Training loss: 0.00012283992292875823 / Validation loss: 0.0012259996085684116 / Long term Validation loss: 0.1344\n",
      "Epoch: [1491/10000] Training loss: 0.0001227385668854078 / Validation loss: 0.001225039498671899 / Long term Validation loss: 0.1344\n",
      "Epoch: [1492/10000] Training loss: 0.000122637302538159 / Validation loss: 0.0012240800089922189 / Long term Validation loss: 0.1343\n",
      "Epoch: [1493/10000] Training loss: 0.0001225361296800481 / Validation loss: 0.0012231211392363926 / Long term Validation loss: 0.1343\n",
      "Epoch: [1494/10000] Training loss: 0.00012243504810565512 / Validation loss: 0.0012221628891157844 / Long term Validation loss: 0.1343\n",
      "Epoch: [1495/10000] Training loss: 0.00012233405761109248 / Validation loss: 0.001221205258346052 / Long term Validation loss: 0.1343\n",
      "Epoch: [1496/10000] Training loss: 0.00012223315799399383 / Validation loss: 0.0012202482466471347 / Long term Validation loss: 0.1343\n",
      "Epoch: [1497/10000] Training loss: 0.0001221323490535025 / Validation loss: 0.0012192918537431792 / Long term Validation loss: 0.1343\n",
      "Epoch: [1498/10000] Training loss: 0.00012203163059026052 / Validation loss: 0.001218336079362447 / Long term Validation loss: 0.1343\n",
      "Epoch: [1499/10000] Training loss: 0.0001219310024063973 / Validation loss: 0.0012173809232373402 / Long term Validation loss: 0.1343\n",
      "Epoch: [1500/10000] Training loss: 0.0001218304643055188 / Validation loss: 0.001216426385104321 / Long term Validation loss: 0.1343\n",
      "Epoch: [1501/10000] Training loss: 0.0001217300160926964 / Validation loss: 0.0012154724647038386 / Long term Validation loss: 0.1343\n",
      "Epoch: [1502/10000] Training loss: 0.00012162965757445594 / Validation loss: 0.001214519161780296 / Long term Validation loss: 0.1343\n",
      "Epoch: [1503/10000] Training loss: 0.00012152938855876708 / Validation loss: 0.0012135664760820363 / Long term Validation loss: 0.1343\n",
      "Epoch: [1504/10000] Training loss: 0.00012142920885503235 / Validation loss: 0.0012126144073612533 / Long term Validation loss: 0.1343\n",
      "Epoch: [1505/10000] Training loss: 0.00012132911827407633 / Validation loss: 0.0012116629553739341 / Long term Validation loss: 0.1343\n",
      "Epoch: [1506/10000] Training loss: 0.00012122911662813536 / Validation loss: 0.0012107121198798618 / Long term Validation loss: 0.1343\n",
      "Epoch: [1507/10000] Training loss: 0.00012112920373084669 / Validation loss: 0.0012097619006425483 / Long term Validation loss: 0.1343\n",
      "Epoch: [1508/10000] Training loss: 0.00012102937939723812 / Validation loss: 0.0012088122974291547 / Long term Validation loss: 0.1343\n",
      "Epoch: [1509/10000] Training loss: 0.00012092964344371744 / Validation loss: 0.0012078633100104843 / Long term Validation loss: 0.1343\n",
      "Epoch: [1510/10000] Training loss: 0.00012082999568806198 / Validation loss: 0.0012069149381609434 / Long term Validation loss: 0.1343\n",
      "Epoch: [1511/10000] Training loss: 0.0001207304359494087 / Validation loss: 0.00120596718165846 / Long term Validation loss: 0.1343\n",
      "Epoch: [1512/10000] Training loss: 0.00012063096404824349 / Validation loss: 0.0012050200402844511 / Long term Validation loss: 0.1343\n",
      "Epoch: [1513/10000] Training loss: 0.00012053157980639128 / Validation loss: 0.0012040735138238068 / Long term Validation loss: 0.1342\n",
      "Epoch: [1514/10000] Training loss: 0.0001204322830470058 / Validation loss: 0.0012031276020648225 / Long term Validation loss: 0.1342\n",
      "Epoch: [1515/10000] Training loss: 0.00012033307359455959 / Validation loss: 0.0012021823047991423 / Long term Validation loss: 0.1342\n",
      "Epoch: [1516/10000] Training loss: 0.00012023395127483393 / Validation loss: 0.0012012376218217304 / Long term Validation loss: 0.1342\n",
      "Epoch: [1517/10000] Training loss: 0.0001201349159149091 / Validation loss: 0.0012002935529308596 / Long term Validation loss: 0.1342\n",
      "Epoch: [1518/10000] Training loss: 0.00012003596734315438 / Validation loss: 0.0011993500979280147 / Long term Validation loss: 0.1342\n",
      "Epoch: [1519/10000] Training loss: 0.00011993710538921817 / Validation loss: 0.0011984072566178598 / Long term Validation loss: 0.1342\n",
      "Epoch: [1520/10000] Training loss: 0.00011983832988401857 / Validation loss: 0.00119746502880824 / Long term Validation loss: 0.1342\n",
      "Epoch: [1521/10000] Training loss: 0.00011973964065973343 / Validation loss: 0.0011965234143101163 / Long term Validation loss: 0.1342\n",
      "Epoch: [1522/10000] Training loss: 0.00011964103754979081 / Validation loss: 0.0011955824129374817 / Long term Validation loss: 0.1342\n",
      "Epoch: [1523/10000] Training loss: 0.00011954252038885954 / Validation loss: 0.0011946420245073823 / Long term Validation loss: 0.1342\n",
      "Epoch: [1524/10000] Training loss: 0.00011944408901283958 / Validation loss: 0.001193702248839857 / Long term Validation loss: 0.1342\n",
      "Epoch: [1525/10000] Training loss: 0.00011934574325885278 / Validation loss: 0.0011927630857578953 / Long term Validation loss: 0.1342\n",
      "Epoch: [1526/10000] Training loss: 0.00011924748296523327 / Validation loss: 0.0011918245350873676 / Long term Validation loss: 0.1342\n",
      "Epoch: [1527/10000] Training loss: 0.00011914930797151845 / Validation loss: 0.0011908865966570343 / Long term Validation loss: 0.1342\n",
      "Epoch: [1528/10000] Training loss: 0.0001190512181184396 / Validation loss: 0.0011899492702984985 / Long term Validation loss: 0.1342\n",
      "Epoch: [1529/10000] Training loss: 0.00011895321324791252 / Validation loss: 0.0011890125558461323 / Long term Validation loss: 0.1342\n",
      "Epoch: [1530/10000] Training loss: 0.00011885529320302883 / Validation loss: 0.0011880764531370569 / Long term Validation loss: 0.1341\n",
      "Epoch: [1531/10000] Training loss: 0.00011875745782804651 / Validation loss: 0.0011871409620111315 / Long term Validation loss: 0.1341\n",
      "Epoch: [1532/10000] Training loss: 0.00011865970696838104 / Validation loss: 0.0011862060823108864 / Long term Validation loss: 0.1341\n",
      "Epoch: [1533/10000] Training loss: 0.00011856204047059658 / Validation loss: 0.0011852718138814712 / Long term Validation loss: 0.1341\n",
      "Epoch: [1534/10000] Training loss: 0.00011846445818239689 / Validation loss: 0.0011843381565706558 / Long term Validation loss: 0.1341\n",
      "Epoch: [1535/10000] Training loss: 0.00011836695995261651 / Validation loss: 0.0011834051102287799 / Long term Validation loss: 0.1341\n",
      "Epoch: [1536/10000] Training loss: 0.00011826954563121205 / Validation loss: 0.0011824726747087023 / Long term Validation loss: 0.1341\n",
      "Epoch: [1537/10000] Training loss: 0.00011817221506925362 / Validation loss: 0.0011815408498657659 / Long term Validation loss: 0.1341\n",
      "Epoch: [1538/10000] Training loss: 0.00011807496811891573 / Validation loss: 0.0011806096355577883 / Long term Validation loss: 0.1341\n",
      "Epoch: [1539/10000] Training loss: 0.00011797780463346925 / Validation loss: 0.0011796790316450092 / Long term Validation loss: 0.1341\n",
      "Epoch: [1540/10000] Training loss: 0.00011788072446727231 / Validation loss: 0.0011787490379900325 / Long term Validation loss: 0.1341\n",
      "Epoch: [1541/10000] Training loss: 0.00011778372747576223 / Validation loss: 0.0011778196544578292 / Long term Validation loss: 0.1341\n",
      "Epoch: [1542/10000] Training loss: 0.00011768681351544691 / Validation loss: 0.0011768908809156894 / Long term Validation loss: 0.1341\n",
      "Epoch: [1543/10000] Training loss: 0.00011758998244389627 / Validation loss: 0.001175962717233179 / Long term Validation loss: 0.1341\n",
      "Epoch: [1544/10000] Training loss: 0.00011749323411973425 / Validation loss: 0.0011750351632821036 / Long term Validation loss: 0.1341\n",
      "Epoch: [1545/10000] Training loss: 0.00011739656840263041 / Validation loss: 0.0011741082189364875 / Long term Validation loss: 0.1340\n",
      "Epoch: [1546/10000] Training loss: 0.00011729998515329147 / Validation loss: 0.0011731818840725525 / Long term Validation loss: 0.1340\n",
      "Epoch: [1547/10000] Training loss: 0.00011720348423345357 / Validation loss: 0.0011722561585686435 / Long term Validation loss: 0.1340\n",
      "Epoch: [1548/10000] Training loss: 0.00011710706550587374 / Validation loss: 0.0011713310423052132 / Long term Validation loss: 0.1340\n",
      "Epoch: [1549/10000] Training loss: 0.00011701072883432211 / Validation loss: 0.0011704065351648303 / Long term Validation loss: 0.1340\n",
      "Epoch: [1550/10000] Training loss: 0.00011691447408357387 / Validation loss: 0.00116948263703209 / Long term Validation loss: 0.1340\n",
      "Epoch: [1551/10000] Training loss: 0.00011681830111940099 / Validation loss: 0.0011685593477935904 / Long term Validation loss: 0.1340\n",
      "Epoch: [1552/10000] Training loss: 0.00011672220980856487 / Validation loss: 0.0011676366673379316 / Long term Validation loss: 0.1340\n",
      "Epoch: [1553/10000] Training loss: 0.00011662620001880797 / Validation loss: 0.0011667145955556763 / Long term Validation loss: 0.1340\n",
      "Epoch: [1554/10000] Training loss: 0.00011653027161884638 / Validation loss: 0.0011657931323392895 / Long term Validation loss: 0.1340\n",
      "Epoch: [1555/10000] Training loss: 0.00011643442447836178 / Validation loss: 0.0011648722775831164 / Long term Validation loss: 0.1340\n",
      "Epoch: [1556/10000] Training loss: 0.00011633865846799389 / Validation loss: 0.0011639520311833968 / Long term Validation loss: 0.1340\n",
      "Epoch: [1557/10000] Training loss: 0.00011624297345933281 / Validation loss: 0.001163032393038185 / Long term Validation loss: 0.1340\n",
      "Epoch: [1558/10000] Training loss: 0.00011614736932491142 / Validation loss: 0.001162113363047308 / Long term Validation loss: 0.1340\n",
      "Epoch: [1559/10000] Training loss: 0.00011605184593819771 / Validation loss: 0.0011611949411123922 / Long term Validation loss: 0.1340\n",
      "Epoch: [1560/10000] Training loss: 0.00011595640317358745 / Validation loss: 0.0011602771271368001 / Long term Validation loss: 0.1340\n",
      "Epoch: [1561/10000] Training loss: 0.00011586104090639669 / Validation loss: 0.00115935992102559 / Long term Validation loss: 0.1339\n",
      "Epoch: [1562/10000] Training loss: 0.00011576575901285417 / Validation loss: 0.0011584433226855015 / Long term Validation loss: 0.1339\n",
      "Epoch: [1563/10000] Training loss: 0.00011567055737009431 / Validation loss: 0.0011575273320249388 / Long term Validation loss: 0.1339\n",
      "Epoch: [1564/10000] Training loss: 0.00011557543585614957 / Validation loss: 0.001156611948953921 / Long term Validation loss: 0.1339\n",
      "Epoch: [1565/10000] Training loss: 0.00011548039434994339 / Validation loss: 0.0011556971733840493 / Long term Validation loss: 0.1339\n",
      "Epoch: [1566/10000] Training loss: 0.00011538543273128293 / Validation loss: 0.0011547830052285083 / Long term Validation loss: 0.1339\n",
      "Epoch: [1567/10000] Training loss: 0.0001152905508808518 / Validation loss: 0.0011538694444020129 / Long term Validation loss: 0.1339\n",
      "Epoch: [1568/10000] Training loss: 0.0001151957486802032 / Validation loss: 0.0011529564908207828 / Long term Validation loss: 0.1339\n",
      "Epoch: [1569/10000] Training loss: 0.00011510102601175253 / Validation loss: 0.001152044144402522 / Long term Validation loss: 0.1339\n",
      "Epoch: [1570/10000] Training loss: 0.00011500638275877067 / Validation loss: 0.0011511324050663971 / Long term Validation loss: 0.1339\n",
      "Epoch: [1571/10000] Training loss: 0.00011491181880537676 / Validation loss: 0.0011502212727329943 / Long term Validation loss: 0.1339\n",
      "Epoch: [1572/10000] Training loss: 0.00011481733403653138 / Validation loss: 0.0011493107473242962 / Long term Validation loss: 0.1339\n",
      "Epoch: [1573/10000] Training loss: 0.00011472292833802966 / Validation loss: 0.001148400828763676 / Long term Validation loss: 0.1339\n",
      "Epoch: [1574/10000] Training loss: 0.00011462860159649457 / Validation loss: 0.0011474915169758396 / Long term Validation loss: 0.1339\n",
      "Epoch: [1575/10000] Training loss: 0.0001145343536993698 / Validation loss: 0.0011465828118868164 / Long term Validation loss: 0.1338\n",
      "Epoch: [1576/10000] Training loss: 0.00011444018453491339 / Validation loss: 0.0011456747134239427 / Long term Validation loss: 0.1338\n",
      "Epoch: [1577/10000] Training loss: 0.00011434609399219064 / Validation loss: 0.001144767221515814 / Long term Validation loss: 0.1338\n",
      "Epoch: [1578/10000] Training loss: 0.00011425208196106787 / Validation loss: 0.001143860336092269 / Long term Validation loss: 0.1338\n",
      "Epoch: [1579/10000] Training loss: 0.00011415814833220543 / Validation loss: 0.0011429540570843714 / Long term Validation loss: 0.1338\n",
      "Epoch: [1580/10000] Training loss: 0.00011406429299705148 / Validation loss: 0.001142048384424391 / Long term Validation loss: 0.1338\n",
      "Epoch: [1581/10000] Training loss: 0.00011397051584783512 / Validation loss: 0.0011411433180457395 / Long term Validation loss: 0.1338\n",
      "Epoch: [1582/10000] Training loss: 0.00011387681677756013 / Validation loss: 0.0011402388578829938 / Long term Validation loss: 0.1338\n",
      "Epoch: [1583/10000] Training loss: 0.00011378319567999848 / Validation loss: 0.0011393350038718518 / Long term Validation loss: 0.1338\n",
      "Epoch: [1584/10000] Training loss: 0.00011368965244968392 / Validation loss: 0.0011384317559490985 / Long term Validation loss: 0.1338\n",
      "Epoch: [1585/10000] Training loss: 0.00011359618698190559 / Validation loss: 0.0011375291140525948 / Long term Validation loss: 0.1338\n",
      "Epoch: [1586/10000] Training loss: 0.00011350279917270188 / Validation loss: 0.0011366270781212559 / Long term Validation loss: 0.1338\n",
      "Epoch: [1587/10000] Training loss: 0.00011340948891885391 / Validation loss: 0.0011357256480950223 / Long term Validation loss: 0.1338\n",
      "Epoch: [1588/10000] Training loss: 0.00011331625611787947 / Validation loss: 0.0011348248239148205 / Long term Validation loss: 0.1338\n",
      "Epoch: [1589/10000] Training loss: 0.00011322310066802685 / Validation loss: 0.0011339246055225806 / Long term Validation loss: 0.1338\n",
      "Epoch: [1590/10000] Training loss: 0.00011313002246826855 / Validation loss: 0.0011330249928611742 / Long term Validation loss: 0.1337\n",
      "Epoch: [1591/10000] Training loss: 0.00011303702141829534 / Validation loss: 0.0011321259858744026 / Long term Validation loss: 0.1337\n",
      "Epoch: [1592/10000] Training loss: 0.00011294409741851017 / Validation loss: 0.0011312275845069934 / Long term Validation loss: 0.1337\n",
      "Epoch: [1593/10000] Training loss: 0.0001128512503700221 / Validation loss: 0.0011303297887045587 / Long term Validation loss: 0.1337\n",
      "Epoch: [1594/10000] Training loss: 0.00011275848017464022 / Validation loss: 0.0011294325984135603 / Long term Validation loss: 0.1337\n",
      "Epoch: [1595/10000] Training loss: 0.00011266578673486805 / Validation loss: 0.0011285360135813245 / Long term Validation loss: 0.1337\n",
      "Epoch: [1596/10000] Training loss: 0.00011257316995389721 / Validation loss: 0.0011276400341559984 / Long term Validation loss: 0.1337\n",
      "Epoch: [1597/10000] Training loss: 0.00011248062973560197 / Validation loss: 0.001126744660086523 / Long term Validation loss: 0.1337\n",
      "Epoch: [1598/10000] Training loss: 0.00011238816598453309 / Validation loss: 0.0011258498913226242 / Long term Validation loss: 0.1337\n",
      "Epoch: [1599/10000] Training loss: 0.00011229577860591234 / Validation loss: 0.001124955727814785 / Long term Validation loss: 0.1337\n",
      "Epoch: [1600/10000] Training loss: 0.00011220346750562648 / Validation loss: 0.001124062169514235 / Long term Validation loss: 0.1337\n",
      "Epoch: [1601/10000] Training loss: 0.0001121112325902218 / Validation loss: 0.0011231692163729105 / Long term Validation loss: 0.1337\n",
      "Epoch: [1602/10000] Training loss: 0.00011201907376689831 / Validation loss: 0.0011222768683434516 / Long term Validation loss: 0.1337\n",
      "Epoch: [1603/10000] Training loss: 0.00011192699094350419 / Validation loss: 0.001121385125379177 / Long term Validation loss: 0.1337\n",
      "Epoch: [1604/10000] Training loss: 0.00011183498402853007 / Validation loss: 0.0011204939874340503 / Long term Validation loss: 0.1336\n",
      "Epoch: [1605/10000] Training loss: 0.0001117430529311038 / Validation loss: 0.001119603454462691 / Long term Validation loss: 0.1336\n",
      "Epoch: [1606/10000] Training loss: 0.0001116511975609846 / Validation loss: 0.0011187135264203228 / Long term Validation loss: 0.1336\n",
      "Epoch: [1607/10000] Training loss: 0.00011155941782855768 / Validation loss: 0.0011178242032627655 / Long term Validation loss: 0.1336\n",
      "Epoch: [1608/10000] Training loss: 0.00011146771364482904 / Validation loss: 0.0011169354849464286 / Long term Validation loss: 0.1336\n",
      "Epoch: [1609/10000] Training loss: 0.00011137608492141971 / Validation loss: 0.001116047371428262 / Long term Validation loss: 0.1336\n",
      "Epoch: [1610/10000] Training loss: 0.00011128453157056071 / Validation loss: 0.0011151598626657764 / Long term Validation loss: 0.1336\n",
      "Epoch: [1611/10000] Training loss: 0.00011119305350508749 / Validation loss: 0.0011142729586169893 / Long term Validation loss: 0.1336\n",
      "Epoch: [1612/10000] Training loss: 0.0001111016506384347 / Validation loss: 0.0011133866592404195 / Long term Validation loss: 0.1336\n",
      "Epoch: [1613/10000] Training loss: 0.00011101032288463119 / Validation loss: 0.0011125009644950782 / Long term Validation loss: 0.1336\n",
      "Epoch: [1614/10000] Training loss: 0.0001109190701582944 / Validation loss: 0.0011116158743404384 / Long term Validation loss: 0.1336\n",
      "Epoch: [1615/10000] Training loss: 0.00011082789237462542 / Validation loss: 0.0011107313887364207 / Long term Validation loss: 0.1336\n",
      "Epoch: [1616/10000] Training loss: 0.00011073678944940395 / Validation loss: 0.0011098475076433648 / Long term Validation loss: 0.1336\n",
      "Epoch: [1617/10000] Training loss: 0.00011064576129898285 / Validation loss: 0.001108964231022036 / Long term Validation loss: 0.1336\n",
      "Epoch: [1618/10000] Training loss: 0.00011055480784028334 / Validation loss: 0.001108081558833588 / Long term Validation loss: 0.1336\n",
      "Epoch: [1619/10000] Training loss: 0.00011046392899079 / Validation loss: 0.001107199491039548 / Long term Validation loss: 0.1335\n",
      "Epoch: [1620/10000] Training loss: 0.00011037312466854552 / Validation loss: 0.0011063180276017952 / Long term Validation loss: 0.1335\n",
      "Epoch: [1621/10000] Training loss: 0.00011028239479214595 / Validation loss: 0.0011054371684825644 / Long term Validation loss: 0.1335\n",
      "Epoch: [1622/10000] Training loss: 0.00011019173928073571 / Validation loss: 0.0011045569136444147 / Long term Validation loss: 0.1335\n",
      "Epoch: [1623/10000] Training loss: 0.00011010115805400276 / Validation loss: 0.0011036772630501938 / Long term Validation loss: 0.1335\n",
      "Epoch: [1624/10000] Training loss: 0.0001100106510321734 / Validation loss: 0.0011027982166630485 / Long term Validation loss: 0.1335\n",
      "Epoch: [1625/10000] Training loss: 0.00010992021813600804 / Validation loss: 0.0011019197744464176 / Long term Validation loss: 0.1335\n",
      "Epoch: [1626/10000] Training loss: 0.0001098298592867958 / Validation loss: 0.0011010419363639745 / Long term Validation loss: 0.1335\n",
      "Epoch: [1627/10000] Training loss: 0.00010973957440635022 / Validation loss: 0.0011001647023796345 / Long term Validation loss: 0.1335\n",
      "Epoch: [1628/10000] Training loss: 0.00010964936341700411 / Validation loss: 0.0010992880724575518 / Long term Validation loss: 0.1335\n",
      "Epoch: [1629/10000] Training loss: 0.00010955922624160536 / Validation loss: 0.0010984120465620914 / Long term Validation loss: 0.1335\n",
      "Epoch: [1630/10000] Training loss: 0.0001094691628035117 / Validation loss: 0.001097536624657788 / Long term Validation loss: 0.1335\n",
      "Epoch: [1631/10000] Training loss: 0.00010937917302658656 / Validation loss: 0.0010966618067093756 / Long term Validation loss: 0.1335\n",
      "Epoch: [1632/10000] Training loss: 0.00010928925683519422 / Validation loss: 0.001095787592681751 / Long term Validation loss: 0.1335\n",
      "Epoch: [1633/10000] Training loss: 0.00010919941415419524 / Validation loss: 0.001094913982539945 / Long term Validation loss: 0.1335\n",
      "Epoch: [1634/10000] Training loss: 0.00010910964490894213 / Validation loss: 0.0010940409762491238 / Long term Validation loss: 0.1334\n",
      "Epoch: [1635/10000] Training loss: 0.00010901994902527452 / Validation loss: 0.0010931685737745832 / Long term Validation loss: 0.1334\n",
      "Epoch: [1636/10000] Training loss: 0.00010893032642951512 / Validation loss: 0.0010922967750817 / Long term Validation loss: 0.1334\n",
      "Epoch: [1637/10000] Training loss: 0.00010884077704846477 / Validation loss: 0.0010914255801359478 / Long term Validation loss: 0.1334\n",
      "Epoch: [1638/10000] Training loss: 0.00010875130080939846 / Validation loss: 0.0010905549889028835 / Long term Validation loss: 0.1334\n",
      "Epoch: [1639/10000] Training loss: 0.00010866189764006081 / Validation loss: 0.0010896850013480967 / Long term Validation loss: 0.1334\n",
      "Epoch: [1640/10000] Training loss: 0.00010857256746866169 / Validation loss: 0.00108881561743724 / Long term Validation loss: 0.1334\n",
      "Epoch: [1641/10000] Training loss: 0.0001084833102238719 / Validation loss: 0.0010879468371359972 / Long term Validation loss: 0.1334\n",
      "Epoch: [1642/10000] Training loss: 0.00010839412583481894 / Validation loss: 0.0010870786604100496 / Long term Validation loss: 0.1334\n",
      "Epoch: [1643/10000] Training loss: 0.00010830501423108276 / Validation loss: 0.0010862110872250923 / Long term Validation loss: 0.1334\n",
      "Epoch: [1644/10000] Training loss: 0.00010821597534269157 / Validation loss: 0.0010853441175468082 / Long term Validation loss: 0.1334\n",
      "Epoch: [1645/10000] Training loss: 0.00010812700910011738 / Validation loss: 0.001084477751340845 / Long term Validation loss: 0.1334\n",
      "Epoch: [1646/10000] Training loss: 0.00010803811543427226 / Validation loss: 0.0010836119885728212 / Long term Validation loss: 0.1334\n",
      "Epoch: [1647/10000] Training loss: 0.000107949294276504 / Validation loss: 0.0010827468292082967 / Long term Validation loss: 0.1334\n",
      "Epoch: [1648/10000] Training loss: 0.00010786054555859189 / Validation loss: 0.001081882273212763 / Long term Validation loss: 0.1333\n",
      "Epoch: [1649/10000] Training loss: 0.0001077718692127428 / Validation loss: 0.0010810183205516336 / Long term Validation loss: 0.1333\n",
      "Epoch: [1650/10000] Training loss: 0.00010768326517158714 / Validation loss: 0.0010801549711902346 / Long term Validation loss: 0.1333\n",
      "Epoch: [1651/10000] Training loss: 0.0001075947333681748 / Validation loss: 0.0010792922250937736 / Long term Validation loss: 0.1333\n",
      "Epoch: [1652/10000] Training loss: 0.00010750627373597103 / Validation loss: 0.0010784300822273597 / Long term Validation loss: 0.1333\n",
      "Epoch: [1653/10000] Training loss: 0.00010741788620885293 / Validation loss: 0.0010775685425559434 / Long term Validation loss: 0.1333\n",
      "Epoch: [1654/10000] Training loss: 0.00010732957072110486 / Validation loss: 0.0010767076060443585 / Long term Validation loss: 0.1333\n",
      "Epoch: [1655/10000] Training loss: 0.00010724132720741504 / Validation loss: 0.0010758472726572759 / Long term Validation loss: 0.1333\n",
      "Epoch: [1656/10000] Training loss: 0.0001071531556028716 / Validation loss: 0.0010749875423591783 / Long term Validation loss: 0.1333\n",
      "Epoch: [1657/10000] Training loss: 0.00010706505584295852 / Validation loss: 0.001074128415114397 / Long term Validation loss: 0.1333\n",
      "Epoch: [1658/10000] Training loss: 0.00010697702786355203 / Validation loss: 0.001073269890887065 / Long term Validation loss: 0.1333\n",
      "Epoch: [1659/10000] Training loss: 0.00010688907160091675 / Validation loss: 0.0010724119696410841 / Long term Validation loss: 0.1333\n",
      "Epoch: [1660/10000] Training loss: 0.00010680118699170186 / Validation loss: 0.0010715546513401725 / Long term Validation loss: 0.1333\n",
      "Epoch: [1661/10000] Training loss: 0.00010671337397293744 / Validation loss: 0.0010706979359478169 / Long term Validation loss: 0.1333\n",
      "Epoch: [1662/10000] Training loss: 0.00010662563248203084 / Validation loss: 0.0010698418234272413 / Long term Validation loss: 0.1333\n",
      "Epoch: [1663/10000] Training loss: 0.00010653796245676273 / Validation loss: 0.0010689863137414342 / Long term Validation loss: 0.1333\n",
      "Epoch: [1664/10000] Training loss: 0.00010645036383528372 / Validation loss: 0.0010681314068531425 / Long term Validation loss: 0.1332\n",
      "Epoch: [1665/10000] Training loss: 0.00010636283655611072 / Validation loss: 0.0010672771027248039 / Long term Validation loss: 0.1332\n",
      "Epoch: [1666/10000] Training loss: 0.00010627538055812308 / Validation loss: 0.001066423401318584 / Long term Validation loss: 0.1332\n",
      "Epoch: [1667/10000] Training loss: 0.00010618799578055925 / Validation loss: 0.0010655703025963784 / Long term Validation loss: 0.1332\n",
      "Epoch: [1668/10000] Training loss: 0.00010610068216301324 / Validation loss: 0.0010647178065197477 / Long term Validation loss: 0.1332\n",
      "Epoch: [1669/10000] Training loss: 0.00010601343964543096 / Validation loss: 0.00106386591304993 / Long term Validation loss: 0.1332\n",
      "Epoch: [1670/10000] Training loss: 0.00010592626816810671 / Validation loss: 0.001063014622147875 / Long term Validation loss: 0.1332\n",
      "Epoch: [1671/10000] Training loss: 0.00010583916767167994 / Validation loss: 0.0010621639337741616 / Long term Validation loss: 0.1332\n",
      "Epoch: [1672/10000] Training loss: 0.00010575213809713154 / Validation loss: 0.0010613138478890183 / Long term Validation loss: 0.1332\n",
      "Epoch: [1673/10000] Training loss: 0.00010566517938578057 / Validation loss: 0.0010604643644523462 / Long term Validation loss: 0.1332\n",
      "Epoch: [1674/10000] Training loss: 0.00010557829147928075 / Validation loss: 0.0010596154834236486 / Long term Validation loss: 0.1332\n",
      "Epoch: [1675/10000] Training loss: 0.00010549147431961734 / Validation loss: 0.001058767204762051 / Long term Validation loss: 0.1332\n",
      "Epoch: [1676/10000] Training loss: 0.0001054047278491034 / Validation loss: 0.0010579195284263184 / Long term Validation loss: 0.1332\n",
      "Epoch: [1677/10000] Training loss: 0.0001053180520103768 / Validation loss: 0.0010570724543747937 / Long term Validation loss: 0.1332\n",
      "Epoch: [1678/10000] Training loss: 0.00010523144674639679 / Validation loss: 0.0010562259825654008 / Long term Validation loss: 0.1332\n",
      "Epoch: [1679/10000] Training loss: 0.00010514491200044079 / Validation loss: 0.001055380112955678 / Long term Validation loss: 0.1332\n",
      "Epoch: [1680/10000] Training loss: 0.00010505844771610098 / Validation loss: 0.001054534845502726 / Long term Validation loss: 0.1331\n",
      "Epoch: [1681/10000] Training loss: 0.00010497205383728134 / Validation loss: 0.0010536901801631777 / Long term Validation loss: 0.1331\n",
      "Epoch: [1682/10000] Training loss: 0.00010488573030819418 / Validation loss: 0.0010528461168932636 / Long term Validation loss: 0.1331\n",
      "Epoch: [1683/10000] Training loss: 0.00010479947707335713 / Validation loss: 0.0010520026556487443 / Long term Validation loss: 0.1331\n",
      "Epoch: [1684/10000] Training loss: 0.00010471329407759004 / Validation loss: 0.0010511597963848928 / Long term Validation loss: 0.1331\n",
      "Epoch: [1685/10000] Training loss: 0.00010462718126601164 / Validation loss: 0.0010503175390565397 / Long term Validation loss: 0.1331\n",
      "Epoch: [1686/10000] Training loss: 0.00010454113858403654 / Validation loss: 0.0010494758836180266 / Long term Validation loss: 0.1331\n",
      "Epoch: [1687/10000] Training loss: 0.00010445516597737226 / Validation loss: 0.0010486348300231857 / Long term Validation loss: 0.1331\n",
      "Epoch: [1688/10000] Training loss: 0.00010436926339201598 / Validation loss: 0.0010477943782253637 / Long term Validation loss: 0.1331\n",
      "Epoch: [1689/10000] Training loss: 0.0001042834307742516 / Validation loss: 0.0010469545281774093 / Long term Validation loss: 0.1331\n",
      "Epoch: [1690/10000] Training loss: 0.00010419766807064686 / Validation loss: 0.0010461152798316217 / Long term Validation loss: 0.1331\n",
      "Epoch: [1691/10000] Training loss: 0.00010411197522805006 / Validation loss: 0.0010452766331398107 / Long term Validation loss: 0.1331\n",
      "Epoch: [1692/10000] Training loss: 0.00010402635219358727 / Validation loss: 0.0010444385880532248 / Long term Validation loss: 0.1331\n",
      "Epoch: [1693/10000] Training loss: 0.00010394079891465937 / Validation loss: 0.0010436011445225835 / Long term Validation loss: 0.1331\n",
      "Epoch: [1694/10000] Training loss: 0.0001038553153389392 / Validation loss: 0.0010427643024980538 / Long term Validation loss: 0.1331\n",
      "Epoch: [1695/10000] Training loss: 0.0001037699014143684 / Validation loss: 0.0010419280619292433 / Long term Validation loss: 0.1331\n",
      "Epoch: [1696/10000] Training loss: 0.00010368455708915483 / Validation loss: 0.0010410924227651918 / Long term Validation loss: 0.1331\n",
      "Epoch: [1697/10000] Training loss: 0.00010359928231176952 / Validation loss: 0.0010402573849543637 / Long term Validation loss: 0.1331\n",
      "Epoch: [1698/10000] Training loss: 0.00010351407703094396 / Validation loss: 0.0010394229484446535 / Long term Validation loss: 0.1330\n",
      "Epoch: [1699/10000] Training loss: 0.0001034289411956672 / Validation loss: 0.0010385891131833396 / Long term Validation loss: 0.1330\n",
      "Epoch: [1700/10000] Training loss: 0.000103343874755183 / Validation loss: 0.001037755879117128 / Long term Validation loss: 0.1330\n",
      "Epoch: [1701/10000] Training loss: 0.00010325887765898728 / Validation loss: 0.0010369232461921154 / Long term Validation loss: 0.1330\n",
      "Epoch: [1702/10000] Training loss: 0.00010317394985682516 / Validation loss: 0.0010360912143537687 / Long term Validation loss: 0.1330\n",
      "Epoch: [1703/10000] Training loss: 0.00010308909129868824 / Validation loss: 0.0010352597835469456 / Long term Validation loss: 0.1330\n",
      "Epoch: [1704/10000] Training loss: 0.00010300430193481209 / Validation loss: 0.0010344289537158964 / Long term Validation loss: 0.1330\n",
      "Epoch: [1705/10000] Training loss: 0.00010291958171567323 / Validation loss: 0.0010335987248041969 / Long term Validation loss: 0.1330\n",
      "Epoch: [1706/10000] Training loss: 0.00010283493059198684 / Validation loss: 0.001032769096754807 / Long term Validation loss: 0.1330\n",
      "Epoch: [1707/10000] Training loss: 0.00010275034851470384 / Validation loss: 0.001031940069510049 / Long term Validation loss: 0.1330\n",
      "Epoch: [1708/10000] Training loss: 0.00010266583543500834 / Validation loss: 0.0010311116430115539 / Long term Validation loss: 0.1330\n",
      "Epoch: [1709/10000] Training loss: 0.00010258139130431512 / Validation loss: 0.0010302838172003112 / Long term Validation loss: 0.1330\n",
      "Epoch: [1710/10000] Training loss: 0.00010249701607426683 / Validation loss: 0.001029456592016663 / Long term Validation loss: 0.1330\n",
      "Epoch: [1711/10000] Training loss: 0.00010241270969673162 / Validation loss: 0.001028629967400226 / Long term Validation loss: 0.1330\n",
      "Epoch: [1712/10000] Training loss: 0.0001023284721238007 / Validation loss: 0.0010278039432899653 / Long term Validation loss: 0.1330\n",
      "Epoch: [1713/10000] Training loss: 0.00010224430330778544 / Validation loss: 0.0010269785196241766 / Long term Validation loss: 0.1330\n",
      "Epoch: [1714/10000] Training loss: 0.00010216020320121509 / Validation loss: 0.001026153696340408 / Long term Validation loss: 0.1330\n",
      "Epoch: [1715/10000] Training loss: 0.00010207617175683436 / Validation loss: 0.0010253294733755377 / Long term Validation loss: 0.1329\n",
      "Epoch: [1716/10000] Training loss: 0.00010199220892760084 / Validation loss: 0.0010245058506657542 / Long term Validation loss: 0.1329\n",
      "Epoch: [1717/10000] Training loss: 0.00010190831466668258 / Validation loss: 0.0010236828281464908 / Long term Validation loss: 0.1329\n",
      "Epoch: [1718/10000] Training loss: 0.00010182448892745556 / Validation loss: 0.001022860405752464 / Long term Validation loss: 0.1329\n",
      "Epoch: [1719/10000] Training loss: 0.00010174073166350155 / Validation loss: 0.0010220385834177196 / Long term Validation loss: 0.1329\n",
      "Epoch: [1720/10000] Training loss: 0.00010165704282860545 / Validation loss: 0.0010212173610754953 / Long term Validation loss: 0.1329\n",
      "Epoch: [1721/10000] Training loss: 0.0001015734223767529 / Validation loss: 0.0010203967386583248 / Long term Validation loss: 0.1329\n",
      "Epoch: [1722/10000] Training loss: 0.00010148987026212826 / Validation loss: 0.0010195767160980213 / Long term Validation loss: 0.1329\n",
      "Epoch: [1723/10000] Training loss: 0.00010140638643911187 / Validation loss: 0.0010187572933256057 / Long term Validation loss: 0.1329\n",
      "Epoch: [1724/10000] Training loss: 0.00010132297086227803 / Validation loss: 0.0010179384702713504 / Long term Validation loss: 0.1329\n",
      "Epoch: [1725/10000] Training loss: 0.0001012396234863924 / Validation loss: 0.0010171202468648095 / Long term Validation loss: 0.1329\n",
      "Epoch: [1726/10000] Training loss: 0.00010115634426641014 / Validation loss: 0.0010163026230347097 / Long term Validation loss: 0.1329\n",
      "Epoch: [1727/10000] Training loss: 0.00010107313315747325 / Validation loss: 0.0010154855987090316 / Long term Validation loss: 0.1329\n",
      "Epoch: [1728/10000] Training loss: 0.00010098999011490856 / Validation loss: 0.0010146691738150103 / Long term Validation loss: 0.1329\n",
      "Epoch: [1729/10000] Training loss: 0.00010090691509422529 / Validation loss: 0.00101385334827903 / Long term Validation loss: 0.1329\n",
      "Epoch: [1730/10000] Training loss: 0.00010082390805111318 / Validation loss: 0.001013038122026737 / Long term Validation loss: 0.1329\n",
      "Epoch: [1731/10000] Training loss: 0.00010074096894143982 / Validation loss: 0.0010122234949829843 / Long term Validation loss: 0.1329\n",
      "Epoch: [1732/10000] Training loss: 0.00010065809772124903 / Validation loss: 0.0010114094670717934 / Long term Validation loss: 0.1329\n",
      "Epoch: [1733/10000] Training loss: 0.00010057529434675825 / Validation loss: 0.001010596038216396 / Long term Validation loss: 0.1329\n",
      "Epoch: [1734/10000] Training loss: 0.00010049255877435656 / Validation loss: 0.001009783208339247 / Long term Validation loss: 0.1328\n",
      "Epoch: [1735/10000] Training loss: 0.00010040989096060265 / Validation loss: 0.0010089709773619332 / Long term Validation loss: 0.1328\n",
      "Epoch: [1736/10000] Training loss: 0.00010032729086222262 / Validation loss: 0.0010081593452052544 / Long term Validation loss: 0.1328\n",
      "Epoch: [1737/10000] Training loss: 0.00010024475843610772 / Validation loss: 0.0010073483117891966 / Long term Validation loss: 0.1328\n",
      "Epoch: [1738/10000] Training loss: 0.00010016229363931277 / Validation loss: 0.0010065378770328947 / Long term Validation loss: 0.1328\n",
      "Epoch: [1739/10000] Training loss: 0.00010007989642905348 / Validation loss: 0.0010057280408546482 / Long term Validation loss: 0.1328\n",
      "Epoch: [1740/10000] Training loss: 9.999756676270488e-05 / Validation loss: 0.0010049188031719541 / Long term Validation loss: 0.1328\n",
      "Epoch: [1741/10000] Training loss: 9.991530459779918e-05 / Validation loss: 0.0010041101639014338 / Long term Validation loss: 0.1328\n",
      "Epoch: [1742/10000] Training loss: 9.983310989202357e-05 / Validation loss: 0.0010033021229588614 / Long term Validation loss: 0.1328\n",
      "Epoch: [1743/10000] Training loss: 9.975098260321853e-05 / Validation loss: 0.0010024946802591954 / Long term Validation loss: 0.1328\n",
      "Epoch: [1744/10000] Training loss: 9.96689226893756e-05 / Validation loss: 0.001001687835716503 / Long term Validation loss: 0.1328\n",
      "Epoch: [1745/10000] Training loss: 9.958693010863553e-05 / Validation loss: 0.0010008815892440016 / Long term Validation loss: 0.1328\n",
      "Epoch: [1746/10000] Training loss: 9.950500481928645e-05 / Validation loss: 0.0010000759407540597 / Long term Validation loss: 0.1328\n",
      "Epoch: [1747/10000] Training loss: 9.94231467797617e-05 / Validation loss: 0.000999270890158166 / Long term Validation loss: 0.1328\n",
      "Epoch: [1748/10000] Training loss: 9.934135594863809e-05 / Validation loss: 0.0009984664373669297 / Long term Validation loss: 0.1328\n",
      "Epoch: [1749/10000] Training loss: 9.925963228463399e-05 / Validation loss: 0.0009976625822901068 / Long term Validation loss: 0.1328\n",
      "Epoch: [1750/10000] Training loss: 9.917797574660742e-05 / Validation loss: 0.0009968593248365612 / Long term Validation loss: 0.1328\n",
      "Epoch: [1751/10000] Training loss: 9.909638629355419e-05 / Validation loss: 0.0009960566649142594 / Long term Validation loss: 0.1328\n",
      "Epoch: [1752/10000] Training loss: 9.901486388460605e-05 / Validation loss: 0.000995254602430313 / Long term Validation loss: 0.1328\n",
      "Epoch: [1753/10000] Training loss: 9.893340847902882e-05 / Validation loss: 0.000994453137290913 / Long term Validation loss: 0.1328\n",
      "Epoch: [1754/10000] Training loss: 9.885202003622063e-05 / Validation loss: 0.0009936522694013702 / Long term Validation loss: 0.1328\n",
      "Epoch: [1755/10000] Training loss: 9.877069851571019e-05 / Validation loss: 0.0009928519986660991 / Long term Validation loss: 0.1328\n",
      "Epoch: [1756/10000] Training loss: 9.868944387715476e-05 / Validation loss: 0.0009920523249886048 / Long term Validation loss: 0.1328\n",
      "Epoch: [1757/10000] Training loss: 9.860825608033862e-05 / Validation loss: 0.000991253248271485 / Long term Validation loss: 0.1328\n",
      "Epoch: [1758/10000] Training loss: 9.852713508517123e-05 / Validation loss: 0.000990454768416443 / Long term Validation loss: 0.1328\n",
      "Epoch: [1759/10000] Training loss: 9.844608085168539e-05 / Validation loss: 0.000989656885324256 / Long term Validation loss: 0.1328\n",
      "Epoch: [1760/10000] Training loss: 9.836509334003572e-05 / Validation loss: 0.0009888595988947824 / Long term Validation loss: 0.1328\n",
      "Epoch: [1761/10000] Training loss: 9.82841725104966e-05 / Validation loss: 0.0009880629090269837 / Long term Validation loss: 0.1328\n",
      "Epoch: [1762/10000] Training loss: 9.820331832346097e-05 / Validation loss: 0.0009872668156188727 / Long term Validation loss: 0.1327\n",
      "Epoch: [1763/10000] Training loss: 9.812253073943822e-05 / Validation loss: 0.0009864713185675506 / Long term Validation loss: 0.1327\n",
      "Epoch: [1764/10000] Training loss: 9.804180971905262e-05 / Validation loss: 0.0009856764177691984 / Long term Validation loss: 0.1327\n",
      "Epoch: [1765/10000] Training loss: 9.796115522304181e-05 / Validation loss: 0.0009848821131190398 / Long term Validation loss: 0.1327\n",
      "Epoch: [1766/10000] Training loss: 9.788056721225494e-05 / Validation loss: 0.0009840884045113935 / Long term Validation loss: 0.1327\n",
      "Epoch: [1767/10000] Training loss: 9.780004564765118e-05 / Validation loss: 0.0009832952918396238 / Long term Validation loss: 0.1327\n",
      "Epoch: [1768/10000] Training loss: 9.771959049029811e-05 / Validation loss: 0.0009825027749961505 / Long term Validation loss: 0.1327\n",
      "Epoch: [1769/10000] Training loss: 9.763920170137013e-05 / Validation loss: 0.0009817108538724597 / Long term Validation loss: 0.1327\n",
      "Epoch: [1770/10000] Training loss: 9.755887924214666e-05 / Validation loss: 0.0009809195283591008 / Long term Validation loss: 0.1327\n",
      "Epoch: [1771/10000] Training loss: 9.747862307401087e-05 / Validation loss: 0.0009801287983456424 / Long term Validation loss: 0.1327\n",
      "Epoch: [1772/10000] Training loss: 9.739843315844793e-05 / Validation loss: 0.000979338663720737 / Long term Validation loss: 0.1327\n",
      "Epoch: [1773/10000] Training loss: 9.731830945704344e-05 / Validation loss: 0.0009785491243720606 / Long term Validation loss: 0.1327\n",
      "Epoch: [1774/10000] Training loss: 9.723825193148202e-05 / Validation loss: 0.0009777601801863386 / Long term Validation loss: 0.1327\n",
      "Epoch: [1775/10000] Training loss: 9.715826054354577e-05 / Validation loss: 0.0009769718310493401 / Long term Validation loss: 0.1327\n",
      "Epoch: [1776/10000] Training loss: 9.707833525511266e-05 / Validation loss: 0.0009761840768458702 / Long term Validation loss: 0.1327\n",
      "Epoch: [1777/10000] Training loss: 9.699847602815508e-05 / Validation loss: 0.0009753969174597664 / Long term Validation loss: 0.1327\n",
      "Epoch: [1778/10000] Training loss: 9.691868282473837e-05 / Validation loss: 0.0009746103527739069 / Long term Validation loss: 0.1327\n",
      "Epoch: [1779/10000] Training loss: 9.683895560701942e-05 / Validation loss: 0.0009738243826701879 / Long term Validation loss: 0.1327\n",
      "Epoch: [1780/10000] Training loss: 9.675929433724503e-05 / Validation loss: 0.0009730390070295531 / Long term Validation loss: 0.1327\n",
      "Epoch: [1781/10000] Training loss: 9.667969897775067e-05 / Validation loss: 0.0009722542257319526 / Long term Validation loss: 0.1327\n",
      "Epoch: [1782/10000] Training loss: 9.660016949095891e-05 / Validation loss: 0.0009714700386563739 / Long term Validation loss: 0.1327\n",
      "Epoch: [1783/10000] Training loss: 9.652070583937804e-05 / Validation loss: 0.0009706864456808256 / Long term Validation loss: 0.1327\n",
      "Epoch: [1784/10000] Training loss: 9.644130798560073e-05 / Validation loss: 0.0009699034466823235 / Long term Validation loss: 0.1326\n",
      "Epoch: [1785/10000] Training loss: 9.636197589230247e-05 / Validation loss: 0.0009691210415369222 / Long term Validation loss: 0.1326\n",
      "Epoch: [1786/10000] Training loss: 9.628270952224032e-05 / Validation loss: 0.0009683392301196724 / Long term Validation loss: 0.1326\n",
      "Epoch: [1787/10000] Training loss: 9.620350883825159e-05 / Validation loss: 0.0009675580123046503 / Long term Validation loss: 0.1326\n",
      "Epoch: [1788/10000] Training loss: 9.612437380325226e-05 / Validation loss: 0.0009667773879649463 / Long term Validation loss: 0.1326\n",
      "Epoch: [1789/10000] Training loss: 9.604530438023592e-05 / Validation loss: 0.0009659973569726434 / Long term Validation loss: 0.1326\n",
      "Epoch: [1790/10000] Training loss: 9.596630053227221e-05 / Validation loss: 0.0009652179191988505 / Long term Validation loss: 0.1326\n",
      "Epoch: [1791/10000] Training loss: 9.58873622225055e-05 / Validation loss: 0.0009644390745136852 / Long term Validation loss: 0.1326\n",
      "Epoch: [1792/10000] Training loss: 9.580848941415379e-05 / Validation loss: 0.0009636608227862485 / Long term Validation loss: 0.1326\n",
      "Epoch: [1793/10000] Training loss: 9.57296820705072e-05 / Validation loss: 0.0009628831638846631 / Long term Validation loss: 0.1326\n",
      "Epoch: [1794/10000] Training loss: 9.565094015492686e-05 / Validation loss: 0.0009621060976760536 / Long term Validation loss: 0.1326\n",
      "Epoch: [1795/10000] Training loss: 9.557226363084336e-05 / Validation loss: 0.0009613296240265281 / Long term Validation loss: 0.1326\n",
      "Epoch: [1796/10000] Training loss: 9.549365246175578e-05 / Validation loss: 0.0009605537428012082 / Long term Validation loss: 0.1326\n",
      "Epoch: [1797/10000] Training loss: 9.541510661123027e-05 / Validation loss: 0.0009597784538642152 / Long term Validation loss: 0.1326\n",
      "Epoch: [1798/10000] Training loss: 9.533662604289889e-05 / Validation loss: 0.0009590037570786388 / Long term Validation loss: 0.1326\n",
      "Epoch: [1799/10000] Training loss: 9.525821072045827e-05 / Validation loss: 0.0009582296523065983 / Long term Validation loss: 0.1326\n",
      "Epoch: [1800/10000] Training loss: 9.517986060766855e-05 / Validation loss: 0.000957456139409194 / Long term Validation loss: 0.1326\n",
      "Epoch: [1801/10000] Training loss: 9.510157566835201e-05 / Validation loss: 0.0009566832182464795 / Long term Validation loss: 0.1325\n",
      "Epoch: [1802/10000] Training loss: 9.5023355866392e-05 / Validation loss: 0.0009559108886775691 / Long term Validation loss: 0.1325\n",
      "Epoch: [1803/10000] Training loss: 9.494520116573169e-05 / Validation loss: 0.000955139150560505 / Long term Validation loss: 0.1325\n",
      "Epoch: [1804/10000] Training loss: 9.486711153037279e-05 / Validation loss: 0.000954368003752331 / Long term Validation loss: 0.1325\n",
      "Epoch: [1805/10000] Training loss: 9.478908692437464e-05 / Validation loss: 0.0009535974481091121 / Long term Validation loss: 0.1325\n",
      "Epoch: [1806/10000] Training loss: 9.471112731185296e-05 / Validation loss: 0.0009528274834858479 / Long term Validation loss: 0.1325\n",
      "Epoch: [1807/10000] Training loss: 9.463323265697844e-05 / Validation loss: 0.0009520581097365429 / Long term Validation loss: 0.1325\n",
      "Epoch: [1808/10000] Training loss: 9.455540292397606e-05 / Validation loss: 0.000951289326714217 / Long term Validation loss: 0.1325\n",
      "Epoch: [1809/10000] Training loss: 9.44776380771236e-05 / Validation loss: 0.0009505211342708039 / Long term Validation loss: 0.1325\n",
      "Epoch: [1810/10000] Training loss: 9.439993808075064e-05 / Validation loss: 0.0009497535322572641 / Long term Validation loss: 0.1325\n",
      "Epoch: [1811/10000] Training loss: 9.432230289923768e-05 / Validation loss: 0.0009489865205235627 / Long term Validation loss: 0.1325\n",
      "Epoch: [1812/10000] Training loss: 9.424473249701453e-05 / Validation loss: 0.0009482200989185579 / Long term Validation loss: 0.1325\n",
      "Epoch: [1813/10000] Training loss: 9.41672268385599e-05 / Validation loss: 0.0009474542672901684 / Long term Validation loss: 0.1325\n",
      "Epoch: [1814/10000] Training loss: 9.40897858883997e-05 / Validation loss: 0.0009466890254852715 / Long term Validation loss: 0.1325\n",
      "Epoch: [1815/10000] Training loss: 9.401240961110642e-05 / Validation loss: 0.000945924373349669 / Long term Validation loss: 0.1325\n",
      "Epoch: [1816/10000] Training loss: 9.393509797129784e-05 / Validation loss: 0.0009451603107282153 / Long term Validation loss: 0.1324\n",
      "Epoch: [1817/10000] Training loss: 9.385785093363614e-05 / Validation loss: 0.0009443968374646999 / Long term Validation loss: 0.1324\n",
      "Epoch: [1818/10000] Training loss: 9.378066846282676e-05 / Validation loss: 0.0009436339534018637 / Long term Validation loss: 0.1324\n",
      "Epoch: [1819/10000] Training loss: 9.370355052361731e-05 / Validation loss: 0.0009428716583814774 / Long term Validation loss: 0.1324\n",
      "Epoch: [1820/10000] Training loss: 9.362649708079685e-05 / Validation loss: 0.0009421099522442536 / Long term Validation loss: 0.1324\n",
      "Epoch: [1821/10000] Training loss: 9.354950809919452e-05 / Validation loss: 0.0009413488348298489 / Long term Validation loss: 0.1324\n",
      "Epoch: [1822/10000] Training loss: 9.347258354367896e-05 / Validation loss: 0.0009405883059769659 / Long term Validation loss: 0.1324\n",
      "Epoch: [1823/10000] Training loss: 9.339572337915678e-05 / Validation loss: 0.0009398283655232069 / Long term Validation loss: 0.1324\n",
      "Epoch: [1824/10000] Training loss: 9.331892757057225e-05 / Validation loss: 0.0009390690133051641 / Long term Validation loss: 0.1324\n",
      "Epoch: [1825/10000] Training loss: 9.324219608290568e-05 / Validation loss: 0.0009383102491584568 / Long term Validation loss: 0.1324\n",
      "Epoch: [1826/10000] Training loss: 9.316552888117302e-05 / Validation loss: 0.0009375520729175756 / Long term Validation loss: 0.1324\n",
      "Epoch: [1827/10000] Training loss: 9.308892593042451e-05 / Validation loss: 0.0009367944844160572 / Long term Validation loss: 0.1324\n",
      "Epoch: [1828/10000] Training loss: 9.301238719574404e-05 / Validation loss: 0.0009360374834864011 / Long term Validation loss: 0.1324\n",
      "Epoch: [1829/10000] Training loss: 9.293591264224791e-05 / Validation loss: 0.0009352810699600225 / Long term Validation loss: 0.1324\n",
      "Epoch: [1830/10000] Training loss: 9.285950223508429e-05 / Validation loss: 0.0009345252436673694 / Long term Validation loss: 0.1323\n",
      "Epoch: [1831/10000] Training loss: 9.278315593943196e-05 / Validation loss: 0.0009337700044378327 / Long term Validation loss: 0.1323\n",
      "Epoch: [1832/10000] Training loss: 9.270687372049968e-05 / Validation loss: 0.0009330153520997448 / Long term Validation loss: 0.1323\n",
      "Epoch: [1833/10000] Training loss: 9.263065554352509e-05 / Validation loss: 0.0009322612864804683 / Long term Validation loss: 0.1323\n",
      "Epoch: [1834/10000] Training loss: 9.255450137377402e-05 / Validation loss: 0.000931507807406283 / Long term Validation loss: 0.1323\n",
      "Epoch: [1835/10000] Training loss: 9.24784111765394e-05 / Validation loss: 0.0009307549147024403 / Long term Validation loss: 0.1323\n",
      "Epoch: [1836/10000] Training loss: 9.24023849171406e-05 / Validation loss: 0.0009300026081932067 / Long term Validation loss: 0.1323\n",
      "Epoch: [1837/10000] Training loss: 9.232642256092253e-05 / Validation loss: 0.0009292508877017618 / Long term Validation loss: 0.1323\n",
      "Epoch: [1838/10000] Training loss: 9.225052407325465e-05 / Validation loss: 0.000928499753050266 / Long term Validation loss: 0.1323\n",
      "Epoch: [1839/10000] Training loss: 9.21746894195303e-05 / Validation loss: 0.000927749204059893 / Long term Validation loss: 0.1323\n",
      "Epoch: [1840/10000] Training loss: 9.20989185651658e-05 / Validation loss: 0.0009269992405507204 / Long term Validation loss: 0.1323\n",
      "Epoch: [1841/10000] Training loss: 9.20232114755996e-05 / Validation loss: 0.0009262498623418305 / Long term Validation loss: 0.1323\n",
      "Epoch: [1842/10000] Training loss: 9.19475681162916e-05 / Validation loss: 0.0009255010692512881 / Long term Validation loss: 0.1323\n",
      "Epoch: [1843/10000] Training loss: 9.187198845272215e-05 / Validation loss: 0.0009247528610960828 / Long term Validation loss: 0.1322\n",
      "Epoch: [1844/10000] Training loss: 9.179647245039138e-05 / Validation loss: 0.0009240052376922088 / Long term Validation loss: 0.1322\n",
      "Epoch: [1845/10000] Training loss: 9.17210200748184e-05 / Validation loss: 0.0009232581988546233 / Long term Validation loss: 0.1322\n",
      "Epoch: [1846/10000] Training loss: 9.164563129154056e-05 / Validation loss: 0.0009225117443972498 / Long term Validation loss: 0.1322\n",
      "Epoch: [1847/10000] Training loss: 9.157030606611255e-05 / Validation loss: 0.0009217658741329727 / Long term Validation loss: 0.1322\n",
      "Epoch: [1848/10000] Training loss: 9.149504436410567e-05 / Validation loss: 0.0009210205878736718 / Long term Validation loss: 0.1322\n",
      "Epoch: [1849/10000] Training loss: 9.141984615110727e-05 / Validation loss: 0.0009202758854301691 / Long term Validation loss: 0.1322\n",
      "Epoch: [1850/10000] Training loss: 9.13447113927197e-05 / Validation loss: 0.0009195317666122769 / Long term Validation loss: 0.1322\n",
      "Epoch: [1851/10000] Training loss: 9.12696400545598e-05 / Validation loss: 0.0009187882312287841 / Long term Validation loss: 0.1322\n",
      "Epoch: [1852/10000] Training loss: 9.119463210225816e-05 / Validation loss: 0.0009180452790874229 / Long term Validation loss: 0.1322\n",
      "Epoch: [1853/10000] Training loss: 9.11196875014581e-05 / Validation loss: 0.0009173029099949321 / Long term Validation loss: 0.1322\n",
      "Epoch: [1854/10000] Training loss: 9.104480621781539e-05 / Validation loss: 0.000916561123757007 / Long term Validation loss: 0.1321\n",
      "Epoch: [1855/10000] Training loss: 9.096998821699729e-05 / Validation loss: 0.0009158199201783044 / Long term Validation loss: 0.1321\n",
      "Epoch: [1856/10000] Training loss: 9.089523346468182e-05 / Validation loss: 0.0009150792990624982 / Long term Validation loss: 0.1321\n",
      "Epoch: [1857/10000] Training loss: 9.08205419265572e-05 / Validation loss: 0.0009143392602121772 / Long term Validation loss: 0.1321\n",
      "Epoch: [1858/10000] Training loss: 9.07459135683211e-05 / Validation loss: 0.0009135998034289596 / Long term Validation loss: 0.1321\n",
      "Epoch: [1859/10000] Training loss: 9.067134835567993e-05 / Validation loss: 0.0009128609285134178 / Long term Validation loss: 0.1321\n",
      "Epoch: [1860/10000] Training loss: 9.059684625434821e-05 / Validation loss: 0.0009121226352650824 / Long term Validation loss: 0.1321\n",
      "Epoch: [1861/10000] Training loss: 9.052240723004791e-05 / Validation loss: 0.0009113849234825068 / Long term Validation loss: 0.1321\n",
      "Epoch: [1862/10000] Training loss: 9.044803124850788e-05 / Validation loss: 0.0009106477929631878 / Long term Validation loss: 0.1321\n",
      "Epoch: [1863/10000] Training loss: 9.03737182754629e-05 / Validation loss: 0.0009099112435036028 / Long term Validation loss: 0.1321\n",
      "Epoch: [1864/10000] Training loss: 9.029946827665344e-05 / Validation loss: 0.0009091752748992294 / Long term Validation loss: 0.1320\n",
      "Epoch: [1865/10000] Training loss: 9.022528121782474e-05 / Validation loss: 0.00090843988694452 / Long term Validation loss: 0.1320\n",
      "Epoch: [1866/10000] Training loss: 9.015115706472635e-05 / Validation loss: 0.0009077050794328778 / Long term Validation loss: 0.1320\n",
      "Epoch: [1867/10000] Training loss: 9.007709578311129e-05 / Validation loss: 0.0009069708521567505 / Long term Validation loss: 0.1320\n",
      "Epoch: [1868/10000] Training loss: 9.00030973387357e-05 / Validation loss: 0.0009062372049075092 / Long term Validation loss: 0.1320\n",
      "Epoch: [1869/10000] Training loss: 8.99291616973581e-05 / Validation loss: 0.0009055041374755316 / Long term Validation loss: 0.1320\n",
      "Epoch: [1870/10000] Training loss: 8.98552888247388e-05 / Validation loss: 0.0009047716496502173 / Long term Validation loss: 0.1320\n",
      "Epoch: [1871/10000] Training loss: 8.978147868663933e-05 / Validation loss: 0.0009040397412198737 / Long term Validation loss: 0.1320\n",
      "Epoch: [1872/10000] Training loss: 8.970773124882182e-05 / Validation loss: 0.0009033084119718783 / Long term Validation loss: 0.1320\n",
      "Epoch: [1873/10000] Training loss: 8.963404647704842e-05 / Validation loss: 0.0009025776616925513 / Long term Validation loss: 0.1319\n",
      "Epoch: [1874/10000] Training loss: 8.956042433708087e-05 / Validation loss: 0.0009018474901671871 / Long term Validation loss: 0.1319\n",
      "Epoch: [1875/10000] Training loss: 8.948686479467966e-05 / Validation loss: 0.0009011178971801456 / Long term Validation loss: 0.1319\n",
      "Epoch: [1876/10000] Training loss: 8.941336781560369e-05 / Validation loss: 0.0009003888825146845 / Long term Validation loss: 0.1319\n",
      "Epoch: [1877/10000] Training loss: 8.933993336560976e-05 / Validation loss: 0.0008996604459531144 / Long term Validation loss: 0.1319\n",
      "Epoch: [1878/10000] Training loss: 8.926656141045176e-05 / Validation loss: 0.0008989325872767541 / Long term Validation loss: 0.1319\n",
      "Epoch: [1879/10000] Training loss: 8.919325191588038e-05 / Validation loss: 0.0008982053062658328 / Long term Validation loss: 0.1319\n",
      "Epoch: [1880/10000] Training loss: 8.912000484764252e-05 / Validation loss: 0.0008974786026996888 / Long term Validation loss: 0.1319\n",
      "Epoch: [1881/10000] Training loss: 8.90468201714807e-05 / Validation loss: 0.000896752476356583 / Long term Validation loss: 0.1318\n",
      "Epoch: [1882/10000] Training loss: 8.897369785313253e-05 / Validation loss: 0.0008960269270137729 / Long term Validation loss: 0.1318\n",
      "Epoch: [1883/10000] Training loss: 8.89006378583304e-05 / Validation loss: 0.0008953019544475917 / Long term Validation loss: 0.1318\n",
      "Epoch: [1884/10000] Training loss: 8.882764015280065e-05 / Validation loss: 0.0008945775584332774 / Long term Validation loss: 0.1318\n",
      "Epoch: [1885/10000] Training loss: 8.875470470226335e-05 / Validation loss: 0.0008938537387451302 / Long term Validation loss: 0.1318\n",
      "Epoch: [1886/10000] Training loss: 8.868183147243172e-05 / Validation loss: 0.0008931304951564774 / Long term Validation loss: 0.1318\n",
      "Epoch: [1887/10000] Training loss: 8.860902042901143e-05 / Validation loss: 0.000892407827439553 / Long term Validation loss: 0.1318\n",
      "Epoch: [1888/10000] Training loss: 8.85362715377006e-05 / Validation loss: 0.0008916857353657195 / Long term Validation loss: 0.1318\n",
      "Epoch: [1889/10000] Training loss: 8.846358476418885e-05 / Validation loss: 0.0008909642187052836 / Long term Validation loss: 0.1317\n",
      "Epoch: [1890/10000] Training loss: 8.8390960074157e-05 / Validation loss: 0.0008902432772275318 / Long term Validation loss: 0.1317\n",
      "Epoch: [1891/10000] Training loss: 8.831839743327671e-05 / Validation loss: 0.0008895229107008691 / Long term Validation loss: 0.1317\n",
      "Epoch: [1892/10000] Training loss: 8.824589680720987e-05 / Validation loss: 0.0008888031188925932 / Long term Validation loss: 0.1317\n",
      "Epoch: [1893/10000] Training loss: 8.817345816160833e-05 / Validation loss: 0.0008880839015690912 / Long term Validation loss: 0.1317\n",
      "Epoch: [1894/10000] Training loss: 8.810108146211318e-05 / Validation loss: 0.0008873652584957703 / Long term Validation loss: 0.1317\n",
      "Epoch: [1895/10000] Training loss: 8.802876667435462e-05 / Validation loss: 0.0008866471894369945 / Long term Validation loss: 0.1317\n",
      "Epoch: [1896/10000] Training loss: 8.795651376395123e-05 / Validation loss: 0.0008859296941562191 / Long term Validation loss: 0.1316\n",
      "Epoch: [1897/10000] Training loss: 8.78843226965098e-05 / Validation loss: 0.0008852127724158836 / Long term Validation loss: 0.1316\n",
      "Epoch: [1898/10000] Training loss: 8.781219343762485e-05 / Validation loss: 0.0008844964239774455 / Long term Validation loss: 0.1316\n",
      "Epoch: [1899/10000] Training loss: 8.774012595287795e-05 / Validation loss: 0.0008837806486014145 / Long term Validation loss: 0.1316\n",
      "Epoch: [1900/10000] Training loss: 8.76681202078378e-05 / Validation loss: 0.0008830654460473061 / Long term Validation loss: 0.1316\n",
      "Epoch: [1901/10000] Training loss: 8.759617616805925e-05 / Validation loss: 0.000882350816073668 / Long term Validation loss: 0.1316\n",
      "Epoch: [1902/10000] Training loss: 8.752429379908352e-05 / Validation loss: 0.0008816367584380866 / Long term Validation loss: 0.1315\n",
      "Epoch: [1903/10000] Training loss: 8.74524730664372e-05 / Validation loss: 0.0008809232728971749 / Long term Validation loss: 0.1315\n",
      "Epoch: [1904/10000] Training loss: 8.738071393563234e-05 / Validation loss: 0.0008802103592065781 / Long term Validation loss: 0.1315\n",
      "Epoch: [1905/10000] Training loss: 8.730901637216572e-05 / Validation loss: 0.0008794980171209772 / Long term Validation loss: 0.1315\n",
      "Epoch: [1906/10000] Training loss: 8.723738034151884e-05 / Validation loss: 0.0008787862463941043 / Long term Validation loss: 0.1315\n",
      "Epoch: [1907/10000] Training loss: 8.71658058091571e-05 / Validation loss: 0.0008780750467787032 / Long term Validation loss: 0.1315\n",
      "Epoch: [1908/10000] Training loss: 8.709429274052979e-05 / Validation loss: 0.0008773644180265935 / Long term Validation loss: 0.1315\n",
      "Epoch: [1909/10000] Training loss: 8.702284110106965e-05 / Validation loss: 0.0008766543598886118 / Long term Validation loss: 0.1314\n",
      "Epoch: [1910/10000] Training loss: 8.695145085619246e-05 / Validation loss: 0.0008759448721146408 / Long term Validation loss: 0.1314\n",
      "Epoch: [1911/10000] Training loss: 8.688012197129652e-05 / Validation loss: 0.0008752359544536457 / Long term Validation loss: 0.1314\n",
      "Epoch: [1912/10000] Training loss: 8.680885441176282e-05 / Validation loss: 0.0008745276066535878 / Long term Validation loss: 0.1314\n",
      "Epoch: [1913/10000] Training loss: 8.673764814295407e-05 / Validation loss: 0.0008738198284615104 / Long term Validation loss: 0.1314\n",
      "Epoch: [1914/10000] Training loss: 8.666650313021483e-05 / Validation loss: 0.000873112619623539 / Long term Validation loss: 0.1314\n",
      "Epoch: [1915/10000] Training loss: 8.659541933887095e-05 / Validation loss: 0.0008724059798847614 / Long term Validation loss: 0.1313\n",
      "Epoch: [1916/10000] Training loss: 8.65243967342294e-05 / Validation loss: 0.0008716999089894481 / Long term Validation loss: 0.1313\n",
      "Epoch: [1917/10000] Training loss: 8.645343528157769e-05 / Validation loss: 0.0008709944066808224 / Long term Validation loss: 0.1313\n",
      "Epoch: [1918/10000] Training loss: 8.638253494618394e-05 / Validation loss: 0.0008702894727012161 / Long term Validation loss: 0.1313\n",
      "Epoch: [1919/10000] Training loss: 8.631169569329616e-05 / Validation loss: 0.0008695851067920569 / Long term Validation loss: 0.1313\n",
      "Epoch: [1920/10000] Training loss: 8.624091748814233e-05 / Validation loss: 0.000868881308693744 / Long term Validation loss: 0.1312\n",
      "Epoch: [1921/10000] Training loss: 8.617020029592974e-05 / Validation loss: 0.0008681780781458496 / Long term Validation loss: 0.1312\n",
      "Epoch: [1922/10000] Training loss: 8.609954408184504e-05 / Validation loss: 0.0008674754148869622 / Long term Validation loss: 0.1312\n",
      "Epoch: [1923/10000] Training loss: 8.602894881105358e-05 / Validation loss: 0.0008667733186547164 / Long term Validation loss: 0.1312\n",
      "Epoch: [1924/10000] Training loss: 8.595841444869953e-05 / Validation loss: 0.0008660717891859164 / Long term Validation loss: 0.1312\n",
      "Epoch: [1925/10000] Training loss: 8.588794095990536e-05 / Validation loss: 0.0008653708262163199 / Long term Validation loss: 0.1312\n",
      "Epoch: [1926/10000] Training loss: 8.581752830977145e-05 / Validation loss: 0.0008646704294808773 / Long term Validation loss: 0.1311\n",
      "Epoch: [1927/10000] Training loss: 8.574717646337616e-05 / Validation loss: 0.0008639705987135503 / Long term Validation loss: 0.1311\n",
      "Epoch: [1928/10000] Training loss: 8.567688538577523e-05 / Validation loss: 0.0008632713336473908 / Long term Validation loss: 0.1311\n",
      "Epoch: [1929/10000] Training loss: 8.56066550420018e-05 / Validation loss: 0.0008625726340145951 / Long term Validation loss: 0.1311\n",
      "Epoch: [1930/10000] Training loss: 8.553648539706591e-05 / Validation loss: 0.0008618744995463556 / Long term Validation loss: 0.1311\n",
      "Epoch: [1931/10000] Training loss: 8.546637641595441e-05 / Validation loss: 0.0008611769299730335 / Long term Validation loss: 0.1311\n",
      "Epoch: [1932/10000] Training loss: 8.539632806363068e-05 / Validation loss: 0.0008604799250240647 / Long term Validation loss: 0.1310\n",
      "Epoch: [1933/10000] Training loss: 8.532634030503428e-05 / Validation loss: 0.0008597834844279333 / Long term Validation loss: 0.1310\n",
      "Epoch: [1934/10000] Training loss: 8.525641310508097e-05 / Validation loss: 0.0008590876079123082 / Long term Validation loss: 0.1310\n",
      "Epoch: [1935/10000] Training loss: 8.518654642866208e-05 / Validation loss: 0.0008583922952038766 / Long term Validation loss: 0.1310\n",
      "Epoch: [1936/10000] Training loss: 8.511674024064473e-05 / Validation loss: 0.0008576975460284796 / Long term Validation loss: 0.1310\n",
      "Epoch: [1937/10000] Training loss: 8.504699450587121e-05 / Validation loss: 0.0008570033601110531 / Long term Validation loss: 0.1309\n",
      "Epoch: [1938/10000] Training loss: 8.497730918915916e-05 / Validation loss: 0.000856309737175624 / Long term Validation loss: 0.1309\n",
      "Epoch: [1939/10000] Training loss: 8.490768425530078e-05 / Validation loss: 0.0008556166769453555 / Long term Validation loss: 0.1309\n",
      "Epoch: [1940/10000] Training loss: 8.483811966906332e-05 / Validation loss: 0.0008549241791425108 / Long term Validation loss: 0.1309\n",
      "Epoch: [1941/10000] Training loss: 8.476861539518834e-05 / Validation loss: 0.0008542322434884636 / Long term Validation loss: 0.1309\n",
      "Epoch: [1942/10000] Training loss: 8.469917139839168e-05 / Validation loss: 0.0008535408697037274 / Long term Validation loss: 0.1308\n",
      "Epoch: [1943/10000] Training loss: 8.462978764336338e-05 / Validation loss: 0.0008528500575079102 / Long term Validation loss: 0.1308\n",
      "Epoch: [1944/10000] Training loss: 8.456046409476723e-05 / Validation loss: 0.0008521598066197687 / Long term Validation loss: 0.1308\n",
      "Epoch: [1945/10000] Training loss: 8.449120071724092e-05 / Validation loss: 0.0008514701167571705 / Long term Validation loss: 0.1308\n",
      "Epoch: [1946/10000] Training loss: 8.442199747539549e-05 / Validation loss: 0.0008507809876371183 / Long term Validation loss: 0.1308\n",
      "Epoch: [1947/10000] Training loss: 8.435285433381536e-05 / Validation loss: 0.0008500924189757474 / Long term Validation loss: 0.1307\n",
      "Epoch: [1948/10000] Training loss: 8.428377125705817e-05 / Validation loss: 0.0008494044104883241 / Long term Validation loss: 0.1307\n",
      "Epoch: [1949/10000] Training loss: 8.421474820965449e-05 / Validation loss: 0.000848716961889266 / Long term Validation loss: 0.1307\n",
      "Epoch: [1950/10000] Training loss: 8.414578515610777e-05 / Validation loss: 0.0008480300728920938 / Long term Validation loss: 0.1307\n",
      "Epoch: [1951/10000] Training loss: 8.407688206089404e-05 / Validation loss: 0.0008473437432095385 / Long term Validation loss: 0.1307\n",
      "Epoch: [1952/10000] Training loss: 8.400803888846191e-05 / Validation loss: 0.0008466579725533959 / Long term Validation loss: 0.1307\n",
      "Epoch: [1953/10000] Training loss: 8.393925560323223e-05 / Validation loss: 0.0008459727606346728 / Long term Validation loss: 0.1306\n",
      "Epoch: [1954/10000] Training loss: 8.387053216959816e-05 / Validation loss: 0.000845288107163511 / Long term Validation loss: 0.1306\n",
      "Epoch: [1955/10000] Training loss: 8.380186855192476e-05 / Validation loss: 0.0008446040118491829 / Long term Validation loss: 0.1306\n",
      "Epoch: [1956/10000] Training loss: 8.373326471454907e-05 / Validation loss: 0.0008439204744001479 / Long term Validation loss: 0.1306\n",
      "Epoch: [1957/10000] Training loss: 8.366472062177988e-05 / Validation loss: 0.0008432374945240252 / Long term Validation loss: 0.1306\n",
      "Epoch: [1958/10000] Training loss: 8.359623623789755e-05 / Validation loss: 0.0008425550719275544 / Long term Validation loss: 0.1305\n",
      "Epoch: [1959/10000] Training loss: 8.352781152715392e-05 / Validation loss: 0.0008418732063167175 / Long term Validation loss: 0.1305\n",
      "Epoch: [1960/10000] Training loss: 8.345944645377227e-05 / Validation loss: 0.0008411918973965816 / Long term Validation loss: 0.1305\n",
      "Epoch: [1961/10000] Training loss: 8.339114098194694e-05 / Validation loss: 0.0008405111448714479 / Long term Validation loss: 0.1305\n",
      "Epoch: [1962/10000] Training loss: 8.332289507584356e-05 / Validation loss: 0.0008398309484447761 / Long term Validation loss: 0.1305\n",
      "Epoch: [1963/10000] Training loss: 8.325470869959861e-05 / Validation loss: 0.0008391513078191684 / Long term Validation loss: 0.1304\n",
      "Epoch: [1964/10000] Training loss: 8.31865818173194e-05 / Validation loss: 0.0008384722226964773 / Long term Validation loss: 0.1304\n",
      "Epoch: [1965/10000] Training loss: 8.311851439308421e-05 / Validation loss: 0.0008377936927776682 / Long term Validation loss: 0.1304\n",
      "Epoch: [1966/10000] Training loss: 8.305050639094178e-05 / Validation loss: 0.0008371157177629399 / Long term Validation loss: 0.1304\n",
      "Epoch: [1967/10000] Training loss: 8.298255777491142e-05 / Validation loss: 0.000836438297351669 / Long term Validation loss: 0.1304\n",
      "Epoch: [1968/10000] Training loss: 8.291466850898302e-05 / Validation loss: 0.0008357614312424195 / Long term Validation loss: 0.1303\n",
      "Epoch: [1969/10000] Training loss: 8.284683855711666e-05 / Validation loss: 0.0008350851191329481 / Long term Validation loss: 0.1303\n",
      "Epoch: [1970/10000] Training loss: 8.277906788324276e-05 / Validation loss: 0.0008344093607202389 / Long term Validation loss: 0.1303\n",
      "Epoch: [1971/10000] Training loss: 8.27113564512619e-05 / Validation loss: 0.0008337341557004338 / Long term Validation loss: 0.1303\n",
      "Epoch: [1972/10000] Training loss: 8.264370422504474e-05 / Validation loss: 0.000833059503768925 / Long term Validation loss: 0.1302\n",
      "Epoch: [1973/10000] Training loss: 8.257611116843198e-05 / Validation loss: 0.0008323854046202893 / Long term Validation loss: 0.1302\n",
      "Epoch: [1974/10000] Training loss: 8.250857724523423e-05 / Validation loss: 0.0008317118579483113 / Long term Validation loss: 0.1302\n",
      "Epoch: [1975/10000] Training loss: 8.244110241923186e-05 / Validation loss: 0.0008310388634460206 / Long term Validation loss: 0.1302\n",
      "Epoch: [1976/10000] Training loss: 8.237368665417516e-05 / Validation loss: 0.0008303664208056228 / Long term Validation loss: 0.1302\n",
      "Epoch: [1977/10000] Training loss: 8.230632991378407e-05 / Validation loss: 0.0008296945297185902 / Long term Validation loss: 0.1301\n",
      "Epoch: [1978/10000] Training loss: 8.223903216174813e-05 / Validation loss: 0.0008290231898755844 / Long term Validation loss: 0.1301\n",
      "Epoch: [1979/10000] Training loss: 8.217179336172654e-05 / Validation loss: 0.0008283524009665082 / Long term Validation loss: 0.1301\n",
      "Epoch: [1980/10000] Training loss: 8.210461347734801e-05 / Validation loss: 0.0008276821626805178 / Long term Validation loss: 0.1301\n",
      "Epoch: [1981/10000] Training loss: 8.203749247221066e-05 / Validation loss: 0.0008270124747059587 / Long term Validation loss: 0.1301\n",
      "Epoch: [1982/10000] Training loss: 8.197043030988216e-05 / Validation loss: 0.0008263433367304635 / Long term Validation loss: 0.1300\n",
      "Epoch: [1983/10000] Training loss: 8.19034269538994e-05 / Validation loss: 0.0008256747484408713 / Long term Validation loss: 0.1300\n",
      "Epoch: [1984/10000] Training loss: 8.183648236776876e-05 / Validation loss: 0.0008250067095232882 / Long term Validation loss: 0.1300\n",
      "Epoch: [1985/10000] Training loss: 8.176959651496578e-05 / Validation loss: 0.0008243392196630608 / Long term Validation loss: 0.1300\n",
      "Epoch: [1986/10000] Training loss: 8.170276935893528e-05 / Validation loss: 0.0008236722785447761 / Long term Validation loss: 0.1300\n",
      "Epoch: [1987/10000] Training loss: 8.163600086309135e-05 / Validation loss: 0.0008230058858523146 / Long term Validation loss: 0.1299\n",
      "Epoch: [1988/10000] Training loss: 8.156929099081722e-05 / Validation loss: 0.0008223400412687654 / Long term Validation loss: 0.1299\n",
      "Epoch: [1989/10000] Training loss: 8.150263970546534e-05 / Validation loss: 0.0008216747444765172 / Long term Validation loss: 0.1299\n",
      "Epoch: [1990/10000] Training loss: 8.143604697035713e-05 / Validation loss: 0.0008210099951572226 / Long term Validation loss: 0.1299\n",
      "Epoch: [1991/10000] Training loss: 8.13695127487833e-05 / Validation loss: 0.0008203457929917698 / Long term Validation loss: 0.1298\n",
      "Epoch: [1992/10000] Training loss: 8.130303700400358e-05 / Validation loss: 0.000819682137660378 / Long term Validation loss: 0.1298\n",
      "Epoch: [1993/10000] Training loss: 8.123661969924674e-05 / Validation loss: 0.000819019028842478 / Long term Validation loss: 0.1298\n",
      "Epoch: [1994/10000] Training loss: 8.117026079771071e-05 / Validation loss: 0.0008183564662168359 / Long term Validation loss: 0.1298\n",
      "Epoch: [1995/10000] Training loss: 8.110396026256236e-05 / Validation loss: 0.0008176944494614751 / Long term Validation loss: 0.1298\n",
      "Epoch: [1996/10000] Training loss: 8.103771805693768e-05 / Validation loss: 0.0008170329782536996 / Long term Validation loss: 0.1297\n",
      "Epoch: [1997/10000] Training loss: 8.097153414394166e-05 / Validation loss: 0.000816372052270138 / Long term Validation loss: 0.1297\n",
      "Epoch: [1998/10000] Training loss: 8.090540848664837e-05 / Validation loss: 0.0008157116711866641 / Long term Validation loss: 0.1297\n",
      "Epoch: [1999/10000] Training loss: 8.08393410481009e-05 / Validation loss: 0.0008150518346785134 / Long term Validation loss: 0.1297\n",
      "Epoch: [2000/10000] Training loss: 8.077333179131143e-05 / Validation loss: 0.0008143925424201493 / Long term Validation loss: 0.1297\n",
      "Epoch: [2001/10000] Training loss: 8.07073806792611e-05 / Validation loss: 0.0008137337940854184 / Long term Validation loss: 0.1296\n",
      "Epoch: [2002/10000] Training loss: 8.064148767490026e-05 / Validation loss: 0.0008130755893474085 / Long term Validation loss: 0.1296\n",
      "Epoch: [2003/10000] Training loss: 8.057565274114821e-05 / Validation loss: 0.0008124179278785715 / Long term Validation loss: 0.1296\n",
      "Epoch: [2004/10000] Training loss: 8.050987584089346e-05 / Validation loss: 0.0008117608093506485 / Long term Validation loss: 0.1296\n",
      "Epoch: [2005/10000] Training loss: 8.044415693699358e-05 / Validation loss: 0.0008111042334347083 / Long term Validation loss: 0.1295\n",
      "Epoch: [2006/10000] Training loss: 8.037849599227538e-05 / Validation loss: 0.0008104481998011548 / Long term Validation loss: 0.1295\n",
      "Epoch: [2007/10000] Training loss: 8.031289296953462e-05 / Validation loss: 0.0008097927081196978 / Long term Validation loss: 0.1295\n",
      "Epoch: [2008/10000] Training loss: 8.024734783153654e-05 / Validation loss: 0.0008091377580594113 / Long term Validation loss: 0.1295\n",
      "Epoch: [2009/10000] Training loss: 8.01818605410154e-05 / Validation loss: 0.0008084833492886747 / Long term Validation loss: 0.1295\n",
      "Epoch: [2010/10000] Training loss: 8.011643106067482e-05 / Validation loss: 0.0008078294814752218 / Long term Validation loss: 0.1294\n",
      "Epoch: [2011/10000] Training loss: 8.005105935318773e-05 / Validation loss: 0.0008071761542861442 / Long term Validation loss: 0.1294\n",
      "Epoch: [2012/10000] Training loss: 7.998574538119633e-05 / Validation loss: 0.0008065233673878386 / Long term Validation loss: 0.1294\n",
      "Epoch: [2013/10000] Training loss: 7.99204891073123e-05 / Validation loss: 0.000805871120446123 / Long term Validation loss: 0.1294\n",
      "Epoch: [2014/10000] Training loss: 7.985529049411674e-05 / Validation loss: 0.0008052194131260786 / Long term Validation loss: 0.1293\n",
      "Epoch: [2015/10000] Training loss: 7.979014950416014e-05 / Validation loss: 0.0008045682450922586 / Long term Validation loss: 0.1293\n",
      "Epoch: [2016/10000] Training loss: 7.97250660999626e-05 / Validation loss: 0.0008039176160084563 / Long term Validation loss: 0.1293\n",
      "Epoch: [2017/10000] Training loss: 7.966004024401375e-05 / Validation loss: 0.000803267525537952 / Long term Validation loss: 0.1293\n",
      "Epoch: [2018/10000] Training loss: 7.959507189877292e-05 / Validation loss: 0.0008026179733433028 / Long term Validation loss: 0.1293\n",
      "Epoch: [2019/10000] Training loss: 7.95301610266691e-05 / Validation loss: 0.0008019689590865029 / Long term Validation loss: 0.1292\n",
      "Epoch: [2020/10000] Training loss: 7.946530759010107e-05 / Validation loss: 0.0008013204824288998 / Long term Validation loss: 0.1292\n",
      "Epoch: [2021/10000] Training loss: 7.940051155143732e-05 / Validation loss: 0.0008006725430312228 / Long term Validation loss: 0.1292\n",
      "Epoch: [2022/10000] Training loss: 7.933577287301649e-05 / Validation loss: 0.000800025140553615 / Long term Validation loss: 0.1292\n",
      "Epoch: [2023/10000] Training loss: 7.927109151714691e-05 / Validation loss: 0.0007993782746555605 / Long term Validation loss: 0.1291\n",
      "Epoch: [2024/10000] Training loss: 7.920646744610702e-05 / Validation loss: 0.0007987319449960188 / Long term Validation loss: 0.1291\n",
      "Epoch: [2025/10000] Training loss: 7.914190062214547e-05 / Validation loss: 0.00079808615123325 / Long term Validation loss: 0.1291\n",
      "Epoch: [2026/10000] Training loss: 7.907739100748095e-05 / Validation loss: 0.0007974408930250262 / Long term Validation loss: 0.1291\n",
      "Epoch: [2027/10000] Training loss: 7.901293856430253e-05 / Validation loss: 0.0007967961700284297 / Long term Validation loss: 0.1291\n",
      "Epoch: [2028/10000] Training loss: 7.894854325476956e-05 / Validation loss: 0.0007961519819000361 / Long term Validation loss: 0.1290\n",
      "Epoch: [2029/10000] Training loss: 7.888420504101176e-05 / Validation loss: 0.000795508328295781 / Long term Validation loss: 0.1290\n",
      "Epoch: [2030/10000] Training loss: 7.881992388512954e-05 / Validation loss: 0.0007948652088710522 / Long term Validation loss: 0.1290\n",
      "Epoch: [2031/10000] Training loss: 7.875569974919372e-05 / Validation loss: 0.0007942226232806633 / Long term Validation loss: 0.1290\n",
      "Epoch: [2032/10000] Training loss: 7.869153259524603e-05 / Validation loss: 0.0007935805711788301 / Long term Validation loss: 0.1290\n",
      "Epoch: [2033/10000] Training loss: 7.862742238529873e-05 / Validation loss: 0.0007929390522192509 / Long term Validation loss: 0.1289\n",
      "Epoch: [2034/10000] Training loss: 7.856336908133517e-05 / Validation loss: 0.0007922980660550018 / Long term Validation loss: 0.1289\n",
      "Epoch: [2035/10000] Training loss: 7.849937264530965e-05 / Validation loss: 0.0007916576123386703 / Long term Validation loss: 0.1289\n",
      "Epoch: [2036/10000] Training loss: 7.843543303914749e-05 / Validation loss: 0.0007910176907222179 / Long term Validation loss: 0.1289\n",
      "Epoch: [2037/10000] Training loss: 7.83715502247453e-05 / Validation loss: 0.0007903783008571365 / Long term Validation loss: 0.1288\n",
      "Epoch: [2038/10000] Training loss: 7.830772416397086e-05 / Validation loss: 0.0007897394423942956 / Long term Validation loss: 0.1288\n",
      "Epoch: [2039/10000] Training loss: 7.82439548186635e-05 / Validation loss: 0.0007891011149840972 / Long term Validation loss: 0.1288\n",
      "Epoch: [2040/10000] Training loss: 7.818024215063398e-05 / Validation loss: 0.0007884633182763507 / Long term Validation loss: 0.1288\n",
      "Epoch: [2041/10000] Training loss: 7.811658612166464e-05 / Validation loss: 0.0007878260519203753 / Long term Validation loss: 0.1288\n",
      "Epoch: [2042/10000] Training loss: 7.805298669350972e-05 / Validation loss: 0.000787189315564936 / Long term Validation loss: 0.1287\n",
      "Epoch: [2043/10000] Training loss: 7.79894438278952e-05 / Validation loss: 0.0007865531088582991 / Long term Validation loss: 0.1287\n",
      "Epoch: [2044/10000] Training loss: 7.792595748651912e-05 / Validation loss: 0.000785917431448198 / Long term Validation loss: 0.1287\n",
      "Epoch: [2045/10000] Training loss: 7.786252763105156e-05 / Validation loss: 0.0007852822829818552 / Long term Validation loss: 0.1287\n",
      "Epoch: [2046/10000] Training loss: 7.779915422313484e-05 / Validation loss: 0.0007846476631060081 / Long term Validation loss: 0.1287\n",
      "Epoch: [2047/10000] Training loss: 7.773583722438369e-05 / Validation loss: 0.0007840135714668335 / Long term Validation loss: 0.1286\n",
      "Epoch: [2048/10000] Training loss: 7.767257659638527e-05 / Validation loss: 0.0007833800077101125 / Long term Validation loss: 0.1286\n",
      "Epoch: [2049/10000] Training loss: 7.760937230069939e-05 / Validation loss: 0.0007827469714809751 / Long term Validation loss: 0.1286\n",
      "Epoch: [2050/10000] Training loss: 7.75462242988586e-05 / Validation loss: 0.0007821144624242682 / Long term Validation loss: 0.1286\n",
      "Epoch: [2051/10000] Training loss: 7.748313255236836e-05 / Validation loss: 0.0007814824801841204 / Long term Validation loss: 0.1286\n",
      "Epoch: [2052/10000] Training loss: 7.742009702270708e-05 / Validation loss: 0.0007808510244044077 / Long term Validation loss: 0.1285\n",
      "Epoch: [2053/10000] Training loss: 7.735711767132646e-05 / Validation loss: 0.0007802200947283369 / Long term Validation loss: 0.1285\n",
      "Epoch: [2054/10000] Training loss: 7.729419445965137e-05 / Validation loss: 0.0007795896907987918 / Long term Validation loss: 0.1285\n",
      "Epoch: [2055/10000] Training loss: 7.72313273490802e-05 / Validation loss: 0.0007789598122580891 / Long term Validation loss: 0.1285\n",
      "Epoch: [2056/10000] Training loss: 7.71685163009849e-05 / Validation loss: 0.0007783304587481425 / Long term Validation loss: 0.1285\n",
      "Epoch: [2057/10000] Training loss: 7.710576127671127e-05 / Validation loss: 0.0007777016299103889 / Long term Validation loss: 0.1284\n",
      "Epoch: [2058/10000] Training loss: 7.704306223757885e-05 / Validation loss: 0.0007770733253858026 / Long term Validation loss: 0.1284\n",
      "Epoch: [2059/10000] Training loss: 7.698041914488137e-05 / Validation loss: 0.0007764455448149415 / Long term Validation loss: 0.1284\n",
      "Epoch: [2060/10000] Training loss: 7.69178319598866e-05 / Validation loss: 0.0007758182878378805 / Long term Validation loss: 0.1284\n",
      "Epoch: [2061/10000] Training loss: 7.685530064383682e-05 / Validation loss: 0.000775191554094305 / Long term Validation loss: 0.1284\n",
      "Epoch: [2062/10000] Training loss: 7.679282515794876e-05 / Validation loss: 0.0007745653432234104 / Long term Validation loss: 0.1283\n",
      "Epoch: [2063/10000] Training loss: 7.673040546341381e-05 / Validation loss: 0.0007739396548640381 / Long term Validation loss: 0.1283\n",
      "Epoch: [2064/10000] Training loss: 7.666804152139824e-05 / Validation loss: 0.0007733144886545114 / Long term Validation loss: 0.1283\n",
      "Epoch: [2065/10000] Training loss: 7.660573329304327e-05 / Validation loss: 0.0007726898442328584 / Long term Validation loss: 0.1283\n",
      "Epoch: [2066/10000] Training loss: 7.654348073946538e-05 / Validation loss: 0.0007720657212365454 / Long term Validation loss: 0.1283\n",
      "Epoch: [2067/10000] Training loss: 7.648128382175629e-05 / Validation loss: 0.0007714421193027881 / Long term Validation loss: 0.1282\n",
      "Epoch: [2068/10000] Training loss: 7.641914250098328e-05 / Validation loss: 0.0007708190380682691 / Long term Validation loss: 0.1282\n",
      "Epoch: [2069/10000] Training loss: 7.635705673818926e-05 / Validation loss: 0.0007701964771693391 / Long term Validation loss: 0.1282\n",
      "Epoch: [2070/10000] Training loss: 7.629502649439308e-05 / Validation loss: 0.0007695744362419802 / Long term Validation loss: 0.1282\n",
      "Epoch: [2071/10000] Training loss: 7.623305173058958e-05 / Validation loss: 0.0007689529149216639 / Long term Validation loss: 0.1282\n",
      "Epoch: [2072/10000] Training loss: 7.617113240774978e-05 / Validation loss: 0.0007683319128436941 / Long term Validation loss: 0.1281\n",
      "Epoch: [2073/10000] Training loss: 7.61092684868211e-05 / Validation loss: 0.0007677114296427011 / Long term Validation loss: 0.1281\n",
      "Epoch: [2074/10000] Training loss: 7.604745992872759e-05 / Validation loss: 0.0007670914649533044 / Long term Validation loss: 0.1281\n",
      "Epoch: [2075/10000] Training loss: 7.598570669436996e-05 / Validation loss: 0.0007664720184093412 / Long term Validation loss: 0.1281\n",
      "Epoch: [2076/10000] Training loss: 7.592400874462594e-05 / Validation loss: 0.0007658530896447458 / Long term Validation loss: 0.1281\n",
      "Epoch: [2077/10000] Training loss: 7.586236604035032e-05 / Validation loss: 0.0007652346782925961 / Long term Validation loss: 0.1281\n",
      "Epoch: [2078/10000] Training loss: 7.580077854237525e-05 / Validation loss: 0.0007646167839861575 / Long term Validation loss: 0.1280\n",
      "Epoch: [2079/10000] Training loss: 7.573924621151037e-05 / Validation loss: 0.0007639994063577616 / Long term Validation loss: 0.1280\n",
      "Epoch: [2080/10000] Training loss: 7.567776900854308e-05 / Validation loss: 0.0007633825450400414 / Long term Validation loss: 0.1280\n",
      "Epoch: [2081/10000] Training loss: 7.561634689423854e-05 / Validation loss: 0.0007627661996645942 / Long term Validation loss: 0.1280\n",
      "Epoch: [2082/10000] Training loss: 7.555497982934009e-05 / Validation loss: 0.0007621503698634278 / Long term Validation loss: 0.1280\n",
      "Epoch: [2083/10000] Training loss: 7.549366777456938e-05 / Validation loss: 0.0007615350552674632 / Long term Validation loss: 0.1279\n",
      "Epoch: [2084/10000] Training loss: 7.543241069062648e-05 / Validation loss: 0.0007609202555080626 / Long term Validation loss: 0.1279\n",
      "Epoch: [2085/10000] Training loss: 7.537120853819021e-05 / Validation loss: 0.00076030597021551 / Long term Validation loss: 0.1279\n",
      "Epoch: [2086/10000] Training loss: 7.531006127791821e-05 / Validation loss: 0.0007596921990205564 / Long term Validation loss: 0.1279\n",
      "Epoch: [2087/10000] Training loss: 7.524896887044733e-05 / Validation loss: 0.0007590789415528425 / Long term Validation loss: 0.1279\n",
      "Epoch: [2088/10000] Training loss: 7.518793127639359e-05 / Validation loss: 0.0007584661974425599 / Long term Validation loss: 0.1279\n",
      "Epoch: [2089/10000] Training loss: 7.512694845635253e-05 / Validation loss: 0.0007578539663186937 / Long term Validation loss: 0.1278\n",
      "Epoch: [2090/10000] Training loss: 7.506602037089963e-05 / Validation loss: 0.0007572422478109326 / Long term Validation loss: 0.1278\n",
      "Epoch: [2091/10000] Training loss: 7.500514698059e-05 / Validation loss: 0.0007566310415475989 / Long term Validation loss: 0.1278\n",
      "Epoch: [2092/10000] Training loss: 7.494432824595908e-05 / Validation loss: 0.0007560203471579246 / Long term Validation loss: 0.1278\n",
      "Epoch: [2093/10000] Training loss: 7.488356412752266e-05 / Validation loss: 0.0007554101642695589 / Long term Validation loss: 0.1278\n",
      "Epoch: [2094/10000] Training loss: 7.482285458577698e-05 / Validation loss: 0.0007548004925113461 / Long term Validation loss: 0.1278\n",
      "Epoch: [2095/10000] Training loss: 7.476219958119925e-05 / Validation loss: 0.0007541913315102208 / Long term Validation loss: 0.1278\n",
      "Epoch: [2096/10000] Training loss: 7.470159907424761e-05 / Validation loss: 0.00075358268089474 / Long term Validation loss: 0.1277\n",
      "Epoch: [2097/10000] Training loss: 7.464105302536137e-05 / Validation loss: 0.0007529745402910474 / Long term Validation loss: 0.1277\n",
      "Epoch: [2098/10000] Training loss: 7.458056139496145e-05 / Validation loss: 0.0007523669093275671 / Long term Validation loss: 0.1277\n",
      "Epoch: [2099/10000] Training loss: 7.45201241434503e-05 / Validation loss: 0.0007517597876294984 / Long term Validation loss: 0.1277\n",
      "Epoch: [2100/10000] Training loss: 7.445974123121241e-05 / Validation loss: 0.000751153174825385 / Long term Validation loss: 0.1277\n",
      "Epoch: [2101/10000] Training loss: 7.439941261861433e-05 / Validation loss: 0.0007505470705392021 / Long term Validation loss: 0.1277\n",
      "Epoch: [2102/10000] Training loss: 7.433913826600494e-05 / Validation loss: 0.0007499414744000194 / Long term Validation loss: 0.1276\n",
      "Epoch: [2103/10000] Training loss: 7.427891813371579e-05 / Validation loss: 0.0007493363860301422 / Long term Validation loss: 0.1276\n",
      "Epoch: [2104/10000] Training loss: 7.421875218206129e-05 / Validation loss: 0.000748731805059764 / Long term Validation loss: 0.1276\n",
      "Epoch: [2105/10000] Training loss: 7.415864037133879e-05 / Validation loss: 0.0007481277311088065 / Long term Validation loss: 0.1276\n",
      "Epoch: [2106/10000] Training loss: 7.409858266182903e-05 / Validation loss: 0.000747524163809582 / Long term Validation loss: 0.1276\n",
      "Epoch: [2107/10000] Training loss: 7.403857901379626e-05 / Validation loss: 0.0007469211027783593 / Long term Validation loss: 0.1276\n",
      "Epoch: [2108/10000] Training loss: 7.397862938748847e-05 / Validation loss: 0.0007463185476512994 / Long term Validation loss: 0.1276\n",
      "Epoch: [2109/10000] Training loss: 7.391873374313769e-05 / Validation loss: 0.0007457164980387923 / Long term Validation loss: 0.1275\n",
      "Epoch: [2110/10000] Training loss: 7.38588920409602e-05 / Validation loss: 0.0007451149535838479 / Long term Validation loss: 0.1275\n",
      "Epoch: [2111/10000] Training loss: 7.379910424115677e-05 / Validation loss: 0.0007445139138870349 / Long term Validation loss: 0.1275\n",
      "Epoch: [2112/10000] Training loss: 7.373937030391282e-05 / Validation loss: 0.0007439133786035491 / Long term Validation loss: 0.1275\n",
      "Epoch: [2113/10000] Training loss: 7.367969018939897e-05 / Validation loss: 0.0007433133473170061 / Long term Validation loss: 0.1275\n",
      "Epoch: [2114/10000] Training loss: 7.362006385777083e-05 / Validation loss: 0.0007427138197044774 / Long term Validation loss: 0.1275\n",
      "Epoch: [2115/10000] Training loss: 7.35604912691696e-05 / Validation loss: 0.0007421147953195486 / Long term Validation loss: 0.1274\n",
      "Epoch: [2116/10000] Training loss: 7.350097238372216e-05 / Validation loss: 0.0007415162738790217 / Long term Validation loss: 0.1274\n",
      "Epoch: [2117/10000] Training loss: 7.344150716154138e-05 / Validation loss: 0.0007409182548820967 / Long term Validation loss: 0.1274\n",
      "Epoch: [2118/10000] Training loss: 7.33820955627264e-05 / Validation loss: 0.0007403207381187954 / Long term Validation loss: 0.1274\n",
      "Epoch: [2119/10000] Training loss: 7.332273754736277e-05 / Validation loss: 0.0007397237229878425 / Long term Validation loss: 0.1274\n",
      "Epoch: [2120/10000] Training loss: 7.326343307552278e-05 / Validation loss: 0.0007391272094162896 / Long term Validation loss: 0.1274\n",
      "Epoch: [2121/10000] Training loss: 7.320418210726578e-05 / Validation loss: 0.0007385311966138178 / Long term Validation loss: 0.1274\n",
      "Epoch: [2122/10000] Training loss: 7.314498460263818e-05 / Validation loss: 0.0007379356847680939 / Long term Validation loss: 0.1273\n",
      "Epoch: [2123/10000] Training loss: 7.308584052167419e-05 / Validation loss: 0.0007373406727267411 / Long term Validation loss: 0.1273\n",
      "Epoch: [2124/10000] Training loss: 7.302674982439539e-05 / Validation loss: 0.0007367461611813505 / Long term Validation loss: 0.1273\n",
      "Epoch: [2125/10000] Training loss: 7.296771247081185e-05 / Validation loss: 0.000736152148274143 / Long term Validation loss: 0.1273\n",
      "Epoch: [2126/10000] Training loss: 7.290872842092137e-05 / Validation loss: 0.0007355586356871405 / Long term Validation loss: 0.1273\n",
      "Epoch: [2127/10000] Training loss: 7.284979763471101e-05 / Validation loss: 0.0007349656201652987 / Long term Validation loss: 0.1273\n",
      "Epoch: [2128/10000] Training loss: 7.279092007215562e-05 / Validation loss: 0.0007343731053688882 / Long term Validation loss: 0.1272\n",
      "Epoch: [2129/10000] Training loss: 7.273209569322051e-05 / Validation loss: 0.0007337810852299846 / Long term Validation loss: 0.1272\n",
      "Epoch: [2130/10000] Training loss: 7.267332445785852e-05 / Validation loss: 0.0007331895674236087 / Long term Validation loss: 0.1272\n",
      "Epoch: [2131/10000] Training loss: 7.261460632601458e-05 / Validation loss: 0.000732598540128424 / Long term Validation loss: 0.1272\n",
      "Epoch: [2132/10000] Training loss: 7.255594125761993e-05 / Validation loss: 0.000732008019296011 / Long term Validation loss: 0.1272\n",
      "Epoch: [2133/10000] Training loss: 7.249732921260112e-05 / Validation loss: 0.0007314179811521569 / Long term Validation loss: 0.1272\n",
      "Epoch: [2134/10000] Training loss: 7.243877015086771e-05 / Validation loss: 0.0007308284589763382 / Long term Validation loss: 0.1271\n",
      "Epoch: [2135/10000] Training loss: 7.238026403233057e-05 / Validation loss: 0.0007302394037783175 / Long term Validation loss: 0.1271\n",
      "Epoch: [2136/10000] Training loss: 7.232181081687631e-05 / Validation loss: 0.0007296508856708208 / Long term Validation loss: 0.1271\n",
      "Epoch: [2137/10000] Training loss: 7.226341046440492e-05 / Validation loss: 0.0007290628016591741 / Long term Validation loss: 0.1271\n",
      "Epoch: [2138/10000] Training loss: 7.220506293477587e-05 / Validation loss: 0.0007284753013307788 / Long term Validation loss: 0.1271\n",
      "Epoch: [2139/10000] Training loss: 7.21467681878875e-05 / Validation loss: 0.0007278881643020116 / Long term Validation loss: 0.1271\n",
      "Epoch: [2140/10000] Training loss: 7.20885261835624e-05 / Validation loss: 0.0007273017141854211 / Long term Validation loss: 0.1270\n",
      "Epoch: [2141/10000] Training loss: 7.203033688171712e-05 / Validation loss: 0.0007267154716740441 / Long term Validation loss: 0.1270\n",
      "Epoch: [2142/10000] Training loss: 7.19722002421158e-05 / Validation loss: 0.000726130147008456 / Long term Validation loss: 0.1270\n",
      "Epoch: [2143/10000] Training loss: 7.191411622474098e-05 / Validation loss: 0.0007255446814976028 / Long term Validation loss: 0.1270\n",
      "Epoch: [2144/10000] Training loss: 7.18560847892583e-05 / Validation loss: 0.0007249606567069584 / Long term Validation loss: 0.1270\n",
      "Epoch: [2145/10000] Training loss: 7.179810589584589e-05 / Validation loss: 0.0007243756989555243 / Long term Validation loss: 0.1270\n",
      "Epoch: [2146/10000] Training loss: 7.174017950403185e-05 / Validation loss: 0.0007237933813226726 / Long term Validation loss: 0.1269\n",
      "Epoch: [2147/10000] Training loss: 7.168230557461346e-05 / Validation loss: 0.0007232083035529072 / Long term Validation loss: 0.1269\n",
      "Epoch: [2148/10000] Training loss: 7.162448406718696e-05 / Validation loss: 0.000722628654210579 / Long term Validation loss: 0.1269\n",
      "Epoch: [2149/10000] Training loss: 7.156671494486709e-05 / Validation loss: 0.0007220419703645989 / Long term Validation loss: 0.1269\n",
      "Epoch: [2150/10000] Training loss: 7.150899816950465e-05 / Validation loss: 0.0007214672848369151 / Long term Validation loss: 0.1269\n",
      "Epoch: [2151/10000] Training loss: 7.145133371466084e-05 / Validation loss: 0.0007208754278285607 / Long term Validation loss: 0.1269\n",
      "Epoch: [2152/10000] Training loss: 7.139372155963206e-05 / Validation loss: 0.0007203112578403502 / Long term Validation loss: 0.1268\n",
      "Epoch: [2153/10000] Training loss: 7.133616173231805e-05 / Validation loss: 0.0007197055506640912 / Long term Validation loss: 0.1268\n",
      "Epoch: [2154/10000] Training loss: 7.127865432614029e-05 / Validation loss: 0.0007191654948472204 / Long term Validation loss: 0.1268\n",
      "Epoch: [2155/10000] Training loss: 7.122119967850126e-05 / Validation loss: 0.000718524555882564 / Long term Validation loss: 0.1268\n",
      "Epoch: [2156/10000] Training loss: 7.116379860329125e-05 / Validation loss: 0.0007180423505056381 / Long term Validation loss: 0.1268\n",
      "Epoch: [2157/10000] Training loss: 7.110645330249026e-05 / Validation loss: 0.0007173128225609512 / Long term Validation loss: 0.1267\n",
      "Epoch: [2158/10000] Training loss: 7.104916915029562e-05 / Validation loss: 0.0007169732387742085 / Long term Validation loss: 0.1268\n",
      "Epoch: [2159/10000] Training loss: 7.099196002174469e-05 / Validation loss: 0.0007160203294660937 / Long term Validation loss: 0.1267\n",
      "Epoch: [2160/10000] Training loss: 7.09348606293574e-05 / Validation loss: 0.000716039158374686 / Long term Validation loss: 0.1268\n",
      "Epoch: [2161/10000] Training loss: 7.087796018201955e-05 / Validation loss: 0.000714518382433115 / Long term Validation loss: 0.1266\n",
      "Epoch: [2162/10000] Training loss: 7.082148621494326e-05 / Validation loss: 0.0007154524889288337 / Long term Validation loss: 0.1268\n",
      "Epoch: [2163/10000] Training loss: 7.076602734293917e-05 / Validation loss: 0.0007124745211101038 / Long term Validation loss: 0.1264\n",
      "Epoch: [2164/10000] Training loss: 7.071310814944154e-05 / Validation loss: 0.0007157835617439088 / Long term Validation loss: 0.1270\n",
      "Epoch: [2165/10000] Training loss: 7.066671201474955e-05 / Validation loss: 0.0007090380885069124 / Long term Validation loss: 0.1259\n",
      "Epoch: [2166/10000] Training loss: 7.063729557972265e-05 / Validation loss: 0.0007186285849630436 / Long term Validation loss: 0.1275\n",
      "Epoch: [2167/10000] Training loss: 7.065246173019691e-05 / Validation loss: 0.0007021407955044178 / Long term Validation loss: 0.1248\n",
      "Epoch: [2168/10000] Training loss: 7.07855028572078e-05 / Validation loss: 0.0007288197306912216 / Long term Validation loss: 0.1292\n",
      "Epoch: [2169/10000] Training loss: 7.12310538976036e-05 / Validation loss: 0.0006877168499996675 / Long term Validation loss: 0.1221\n",
      "Epoch: [2170/10000] Training loss: 7.250675860219701e-05 / Validation loss: 0.0007629868457669042 / Long term Validation loss: 0.1300\n",
      "Epoch: [2171/10000] Training loss: 7.595499097544272e-05 / Validation loss: 0.000665628855290073 / Long term Validation loss: 0.1164\n",
      "Epoch: [2172/10000] Training loss: 8.48994419815134e-05 / Validation loss: 0.0008806891367524542 / Long term Validation loss: 0.1311\n",
      "Epoch: [2173/10000] Training loss: 0.00010602953280590932 / Validation loss: 0.0006829241760278886 / Long term Validation loss: 0.1213\n",
      "Epoch: [2174/10000] Training loss: 0.0001461225485853133 / Validation loss: 0.0011047057374050323 / Long term Validation loss: 0.1466\n",
      "Epoch: [2175/10000] Training loss: 0.00018427509648717428 / Validation loss: 0.0006908763019235946 / Long term Validation loss: 0.1224\n",
      "Epoch: [2176/10000] Training loss: 0.000158166687324585 / Validation loss: 0.000798473721342276 / Long term Validation loss: 0.1259\n",
      "Epoch: [2177/10000] Training loss: 8.309273018875318e-05 / Validation loss: 0.0008006806143383993 / Long term Validation loss: 0.1256\n",
      "Epoch: [2178/10000] Training loss: 8.356644998910962e-05 / Validation loss: 0.0006718061245866064 / Long term Validation loss: 0.1182\n",
      "Epoch: [2179/10000] Training loss: 0.0001330798444772152 / Validation loss: 0.0008939538323874388 / Long term Validation loss: 0.1324\n",
      "Epoch: [2180/10000] Training loss: 0.00010950034160688155 / Validation loss: 0.0007051069006465297 / Long term Validation loss: 0.1263\n",
      "Epoch: [2181/10000] Training loss: 6.987673539717063e-05 / Validation loss: 0.0006565569217563777 / Long term Validation loss: 0.1141\n",
      "Epoch: [2182/10000] Training loss: 0.00010061744638328712 / Validation loss: 0.0008843343148099506 / Long term Validation loss: 0.1315\n",
      "Epoch: [2183/10000] Training loss: 0.00010688927460021937 / Validation loss: 0.0006820030429230083 / Long term Validation loss: 0.1212\n",
      "Epoch: [2184/10000] Training loss: 7.17559864634421e-05 / Validation loss: 0.0006574613936453025 / Long term Validation loss: 0.1139\n",
      "Epoch: [2185/10000] Training loss: 8.600315702196287e-05 / Validation loss: 0.0008538385090486733 / Long term Validation loss: 0.1278\n",
      "Epoch: [2186/10000] Training loss: 9.798053063022889e-05 / Validation loss: 0.0006785866061207972 / Long term Validation loss: 0.1206\n",
      "Epoch: [2187/10000] Training loss: 7.225801091520549e-05 / Validation loss: 0.0006619406004690387 / Long term Validation loss: 0.1154\n",
      "Epoch: [2188/10000] Training loss: 8.000628830031993e-05 / Validation loss: 0.0008253867146607415 / Long term Validation loss: 0.1250\n",
      "Epoch: [2189/10000] Training loss: 9.009058959896431e-05 / Validation loss: 0.0006816396506902276 / Long term Validation loss: 0.1213\n",
      "Epoch: [2190/10000] Training loss: 7.141495592308485e-05 / Validation loss: 0.0006649836883683049 / Long term Validation loss: 0.1167\n",
      "Epoch: [2191/10000] Training loss: 7.732663704833917e-05 / Validation loss: 0.0008007130318036427 / Long term Validation loss: 0.1244\n",
      "Epoch: [2192/10000] Training loss: 8.402164387262166e-05 / Validation loss: 0.0006862821804126221 / Long term Validation loss: 0.1226\n",
      "Epoch: [2193/10000] Training loss: 7.036394278598459e-05 / Validation loss: 0.0006659239888163647 / Long term Validation loss: 0.1176\n",
      "Epoch: [2194/10000] Training loss: 7.58629211721882e-05 / Validation loss: 0.0007790149213587703 / Long term Validation loss: 0.1267\n",
      "Epoch: [2195/10000] Training loss: 7.94356484963823e-05 / Validation loss: 0.000691116970796726 / Long term Validation loss: 0.1238\n",
      "Epoch: [2196/10000] Training loss: 6.960159650558932e-05 / Validation loss: 0.0006661999779784275 / Long term Validation loss: 0.1182\n",
      "Epoch: [2197/10000] Training loss: 7.480630438721238e-05 / Validation loss: 0.0007606388124444847 / Long term Validation loss: 0.1288\n",
      "Epoch: [2198/10000] Training loss: 7.599901717502134e-05 / Validation loss: 0.0006961386223401378 / Long term Validation loss: 0.1253\n",
      "Epoch: [2199/10000] Training loss: 6.919764018129623e-05 / Validation loss: 0.0006671294689119875 / Long term Validation loss: 0.1189\n",
      "Epoch: [2200/10000] Training loss: 7.385601481927397e-05 / Validation loss: 0.00074574412966927 / Long term Validation loss: 0.1296\n",
      "Epoch: [2201/10000] Training loss: 7.347274738327607e-05 / Validation loss: 0.0007008701108816546 / Long term Validation loss: 0.1264\n",
      "Epoch: [2202/10000] Training loss: 6.904922291361749e-05 / Validation loss: 0.0006686875314154547 / Long term Validation loss: 0.1195\n",
      "Epoch: [2203/10000] Training loss: 7.293026903938834e-05 / Validation loss: 0.000733528766150683 / Long term Validation loss: 0.1298\n",
      "Epoch: [2204/10000] Training loss: 7.167168029388055e-05 / Validation loss: 0.0007042964699835118 / Long term Validation loss: 0.1272\n",
      "Epoch: [2205/10000] Training loss: 6.902757585257799e-05 / Validation loss: 0.0006701967934294819 / Long term Validation loss: 0.1201\n",
      "Epoch: [2206/10000] Training loss: 7.203774095072618e-05 / Validation loss: 0.0007231281902739787 / Long term Validation loss: 0.1295\n",
      "Epoch: [2207/10000] Training loss: 7.043011775956774e-05 / Validation loss: 0.00070596401771758 / Long term Validation loss: 0.1276\n",
      "Epoch: [2208/10000] Training loss: 6.903223709612135e-05 / Validation loss: 0.000671486681931244 / Long term Validation loss: 0.1206\n",
      "Epoch: [2209/10000] Training loss: 7.121230939972605e-05 / Validation loss: 0.0007144115915173528 / Long term Validation loss: 0.1287\n",
      "Epoch: [2210/10000] Training loss: 6.959923053650579e-05 / Validation loss: 0.0007063359788004551 / Long term Validation loss: 0.1278\n",
      "Epoch: [2211/10000] Training loss: 6.900833094605168e-05 / Validation loss: 0.0006729205586012685 / Long term Validation loss: 0.1210\n",
      "Epoch: [2212/10000] Training loss: 7.048422923419727e-05 / Validation loss: 0.0007075578013901675 / Long term Validation loss: 0.1281\n",
      "Epoch: [2213/10000] Training loss: 6.905251701787573e-05 / Validation loss: 0.0007060133514958669 / Long term Validation loss: 0.1279\n",
      "Epoch: [2214/10000] Training loss: 6.893658536492405e-05 / Validation loss: 0.0006746251936571197 / Long term Validation loss: 0.1214\n",
      "Epoch: [2215/10000] Training loss: 6.986676123862388e-05 / Validation loss: 0.0007023195525001335 / Long term Validation loss: 0.1273\n",
      "Epoch: [2216/10000] Training loss: 6.869050990139875e-05 / Validation loss: 0.0007051021189107404 / Long term Validation loss: 0.1279\n",
      "Epoch: [2217/10000] Training loss: 6.882007129030731e-05 / Validation loss: 0.0006762351082076512 / Long term Validation loss: 0.1220\n",
      "Epoch: [2218/10000] Training loss: 6.93582637416718e-05 / Validation loss: 0.0006980841988583045 / Long term Validation loss: 0.1266\n",
      "Epoch: [2219/10000] Training loss: 6.844179004786803e-05 / Validation loss: 0.0007034798150533036 / Long term Validation loss: 0.1278\n",
      "Epoch: [2220/10000] Training loss: 6.867121570872465e-05 / Validation loss: 0.0006773956552596709 / Long term Validation loss: 0.1224\n",
      "Epoch: [2221/10000] Training loss: 6.894624254197942e-05 / Validation loss: 0.0006944552887562753 / Long term Validation loss: 0.1261\n",
      "Epoch: [2222/10000] Training loss: 6.825797006185008e-05 / Validation loss: 0.0007013052993768387 / Long term Validation loss: 0.1276\n",
      "Epoch: [2223/10000] Training loss: 6.85037313456925e-05 / Validation loss: 0.0006781546175701611 / Long term Validation loss: 0.1228\n",
      "Epoch: [2224/10000] Training loss: 6.86135766187405e-05 / Validation loss: 0.00069143331407022 / Long term Validation loss: 0.1257\n",
      "Epoch: [2225/10000] Training loss: 6.810890791231081e-05 / Validation loss: 0.000698971940965534 / Long term Validation loss: 0.1273\n",
      "Epoch: [2226/10000] Training loss: 6.83291289769373e-05 / Validation loss: 0.0006787310633224998 / Long term Validation loss: 0.1230\n",
      "Epoch: [2227/10000] Training loss: 6.83424741538766e-05 / Validation loss: 0.000689050227088082 / Long term Validation loss: 0.1254\n",
      "Epoch: [2228/10000] Training loss: 6.797677625306693e-05 / Validation loss: 0.000696697992790347 / Long term Validation loss: 0.1270\n",
      "Epoch: [2229/10000] Training loss: 6.815509984316512e-05 / Validation loss: 0.0006791384366162367 / Long term Validation loss: 0.1232\n",
      "Epoch: [2230/10000] Training loss: 6.811698638127344e-05 / Validation loss: 0.0006871028874049222 / Long term Validation loss: 0.1252\n",
      "Epoch: [2231/10000] Training loss: 6.785199362977271e-05 / Validation loss: 0.0006944181703120382 / Long term Validation loss: 0.1267\n",
      "Epoch: [2232/10000] Training loss: 6.798615571023232e-05 / Validation loss: 0.000679221563929085 / Long term Validation loss: 0.1235\n",
      "Epoch: [2233/10000] Training loss: 6.79242225628625e-05 / Validation loss: 0.000685328579640127 / Long term Validation loss: 0.1250\n",
      "Epoch: [2234/10000] Training loss: 6.772982071028173e-05 / Validation loss: 0.0006920584435864208 / Long term Validation loss: 0.1264\n",
      "Epoch: [2235/10000] Training loss: 6.782420917730129e-05 / Validation loss: 0.0006789456925930864 / Long term Validation loss: 0.1237\n",
      "Epoch: [2236/10000] Training loss: 6.77542765661905e-05 / Validation loss: 0.0006836598635843585 / Long term Validation loss: 0.1249\n",
      "Epoch: [2237/10000] Training loss: 6.760821230459737e-05 / Validation loss: 0.0006897191585583266 / Long term Validation loss: 0.1261\n",
      "Epoch: [2238/10000] Training loss: 6.766965112150252e-05 / Validation loss: 0.0006784576559617104 / Long term Validation loss: 0.1238\n",
      "Epoch: [2239/10000] Training loss: 6.760000965358821e-05 / Validation loss: 0.0006821705511473548 / Long term Validation loss: 0.1248\n",
      "Epoch: [2240/10000] Training loss: 6.748652138841599e-05 / Validation loss: 0.0006875365026280442 / Long term Validation loss: 0.1258\n",
      "Epoch: [2241/10000] Training loss: 6.752196379879886e-05 / Validation loss: 0.0006778800714072769 / Long term Validation loss: 0.1239\n",
      "Epoch: [2242/10000] Training loss: 6.745636002660219e-05 / Validation loss: 0.0006808589450377457 / Long term Validation loss: 0.1247\n",
      "Epoch: [2243/10000] Training loss: 6.736465817121214e-05 / Validation loss: 0.0006855067575436136 / Long term Validation loss: 0.1256\n",
      "Epoch: [2244/10000] Training loss: 6.738022592331998e-05 / Validation loss: 0.0006771934360033804 / Long term Validation loss: 0.1240\n",
      "Epoch: [2245/10000] Training loss: 6.731988824726461e-05 / Validation loss: 0.0006796133850763535 / Long term Validation loss: 0.1247\n",
      "Epoch: [2246/10000] Training loss: 6.724276887534935e-05 / Validation loss: 0.0006835402744327501 / Long term Validation loss: 0.1255\n",
      "Epoch: [2247/10000] Training loss: 6.724341687489527e-05 / Validation loss: 0.0006763491768057366 / Long term Validation loss: 0.1241\n",
      "Epoch: [2248/10000] Training loss: 6.718826316701548e-05 / Validation loss: 0.0006783591541883419 / Long term Validation loss: 0.1246\n",
      "Epoch: [2249/10000] Training loss: 6.712099202225579e-05 / Validation loss: 0.0006816145481641678 / Long term Validation loss: 0.1253\n",
      "Epoch: [2250/10000] Training loss: 6.71105539896557e-05 / Validation loss: 0.0006753900897441584 / Long term Validation loss: 0.1241\n",
      "Epoch: [2251/10000] Training loss: 6.70599323412362e-05 / Validation loss: 0.0006771254086624322 / Long term Validation loss: 0.1246\n",
      "Epoch: [2252/10000] Training loss: 6.699944419776889e-05 / Validation loss: 0.0006797841030287604 / Long term Validation loss: 0.1251\n",
      "Epoch: [2253/10000] Training loss: 6.698080354228723e-05 / Validation loss: 0.000674401073275887 / Long term Validation loss: 0.1241\n",
      "Epoch: [2254/10000] Training loss: 6.69338694215084e-05 / Validation loss: 0.0006759527462649687 / Long term Validation loss: 0.1245\n",
      "Epoch: [2255/10000] Training loss: 6.68781822840919e-05 / Validation loss: 0.0006780722483658791 / Long term Validation loss: 0.1250\n",
      "Epoch: [2256/10000] Training loss: 6.685347338535508e-05 / Validation loss: 0.000673405001750649 / Long term Validation loss: 0.1241\n",
      "Epoch: [2257/10000] Training loss: 6.680939334465549e-05 / Validation loss: 0.0006748142019535627 / Long term Validation loss: 0.1245\n",
      "Epoch: [2258/10000] Training loss: 6.675722913396374e-05 / Validation loss: 0.0006764340930665016 / Long term Validation loss: 0.1249\n",
      "Epoch: [2259/10000] Training loss: 6.672802847580926e-05 / Validation loss: 0.0006723692886775666 / Long term Validation loss: 0.1241\n",
      "Epoch: [2260/10000] Training loss: 6.668606343068737e-05 / Validation loss: 0.0006736588966104592 / Long term Validation loss: 0.1245\n",
      "Epoch: [2261/10000] Training loss: 6.66365822113369e-05 / Validation loss: 0.0006748270433913304 / Long term Validation loss: 0.1248\n",
      "Epoch: [2262/10000] Training loss: 6.660405344926425e-05 / Validation loss: 0.0006712851332713617 / Long term Validation loss: 0.1241\n",
      "Epoch: [2263/10000] Training loss: 6.656358589852495e-05 / Validation loss: 0.0006724814856591892 / Long term Validation loss: 0.1245\n",
      "Epoch: [2264/10000] Training loss: 6.65162230979581e-05 / Validation loss: 0.0006732591613596027 / Long term Validation loss: 0.1247\n",
      "Epoch: [2265/10000] Training loss: 6.648123668241372e-05 / Validation loss: 0.0006701862935708962 / Long term Validation loss: 0.1241\n",
      "Epoch: [2266/10000] Training loss: 6.644177132430488e-05 / Validation loss: 0.0006713093186037693 / Long term Validation loss: 0.1244\n",
      "Epoch: [2267/10000] Training loss: 6.639612928562523e-05 / Validation loss: 0.0006717521535565512 / Long term Validation loss: 0.1246\n",
      "Epoch: [2268/10000] Training loss: 6.635934259963304e-05 / Validation loss: 0.0006690976991636645 / Long term Validation loss: 0.1241\n",
      "Epoch: [2269/10000] Training loss: 6.63204918647124e-05 / Validation loss: 0.0006701479169373818 / Long term Validation loss: 0.1244\n",
      "Epoch: [2270/10000] Training loss: 6.627627395648556e-05 / Validation loss: 0.0006702963320501655 / Long term Validation loss: 0.1245\n",
      "Epoch: [2271/10000] Training loss: 6.623819300687704e-05 / Validation loss: 0.0006680083330006966 / Long term Validation loss: 0.1241\n",
      "Epoch: [2272/10000] Training loss: 6.619966420496045e-05 / Validation loss: 0.000668974685133214 / Long term Validation loss: 0.1244\n",
      "Epoch: [2273/10000] Training loss: 6.615663441914223e-05 / Validation loss: 0.000668864953201866 / Long term Validation loss: 0.1244\n",
      "Epoch: [2274/10000] Training loss: 6.611765312824594e-05 / Validation loss: 0.0006669013859841148 / Long term Validation loss: 0.1241\n",
      "Epoch: [2275/10000] Training loss: 6.607923237992527e-05 / Validation loss: 0.0006677761874184291 / Long term Validation loss: 0.1243\n",
      "Epoch: [2276/10000] Training loss: 6.603719055868596e-05 / Validation loss: 0.00066745065436291 / Long term Validation loss: 0.1243\n",
      "Epoch: [2277/10000] Training loss: 6.599761861645662e-05 / Validation loss: 0.0006657826954584409 / Long term Validation loss: 0.1241\n",
      "Epoch: [2278/10000] Training loss: 6.595915941919274e-05 / Validation loss: 0.0006665629240264841 / Long term Validation loss: 0.1243\n",
      "Epoch: [2279/10000] Training loss: 6.5917928835719e-05 / Validation loss: 0.0006660652609026436 / Long term Validation loss: 0.1243\n",
      "Epoch: [2280/10000] Training loss: 6.587800945826977e-05 / Validation loss: 0.0006646674908551608 / Long term Validation loss: 0.1240\n",
      "Epoch: [2281/10000] Training loss: 6.583942081210622e-05 / Validation loss: 0.000665346640816444 / Long term Validation loss: 0.1242\n",
      "Epoch: [2282/10000] Training loss: 6.579884100081896e-05 / Validation loss: 0.0006647134615953274 / Long term Validation loss: 0.1242\n",
      "Epoch: [2283/10000] Training loss: 6.575876227046733e-05 / Validation loss: 0.0006635565944900771 / Long term Validation loss: 0.1240\n",
      "Epoch: [2284/10000] Training loss: 6.57199992279192e-05 / Validation loss: 0.0006641232296324548 / Long term Validation loss: 0.1242\n",
      "Epoch: [2285/10000] Training loss: 6.567992465714577e-05 / Validation loss: 0.0006633852443524215 / Long term Validation loss: 0.1241\n",
      "Epoch: [2286/10000] Training loss: 6.563982746074761e-05 / Validation loss: 0.000662439306411194 / Long term Validation loss: 0.1240\n",
      "Epoch: [2287/10000] Training loss: 6.560088172861053e-05 / Validation loss: 0.0006628839315777283 / Long term Validation loss: 0.1242\n",
      "Epoch: [2288/10000] Training loss: 6.556118216500894e-05 / Validation loss: 0.0006620722386512926 / Long term Validation loss: 0.1241\n",
      "Epoch: [2289/10000] Training loss: 6.552116549781527e-05 / Validation loss: 0.0006613113420130339 / Long term Validation loss: 0.1240\n",
      "Epoch: [2290/10000] Training loss: 6.5482056470349e-05 / Validation loss: 0.0006616306453809401 / Long term Validation loss: 0.1241\n",
      "Epoch: [2291/10000] Training loss: 6.544261946227714e-05 / Validation loss: 0.0006607775285359309 / Long term Validation loss: 0.1240\n",
      "Epoch: [2292/10000] Training loss: 6.540274574671888e-05 / Validation loss: 0.0006601780601937305 / Long term Validation loss: 0.1240\n",
      "Epoch: [2293/10000] Training loss: 6.536351175106399e-05 / Validation loss: 0.0006603722408349208 / Long term Validation loss: 0.1241\n",
      "Epoch: [2294/10000] Training loss: 6.532424499730523e-05 / Validation loss: 0.0006595065335561786 / Long term Validation loss: 0.1240\n",
      "Epoch: [2295/10000] Training loss: 6.528454518713809e-05 / Validation loss: 0.0006590426739918981 / Long term Validation loss: 0.1239\n",
      "Epoch: [2296/10000] Training loss: 6.524523477846117e-05 / Validation loss: 0.0006591124115956559 / Long term Validation loss: 0.1240\n",
      "Epoch: [2297/10000] Training loss: 6.520606804775454e-05 / Validation loss: 0.0006582570696610515 / Long term Validation loss: 0.1239\n",
      "Epoch: [2298/10000] Training loss: 6.516654784499076e-05 / Validation loss: 0.0006579006221918984 / Long term Validation loss: 0.1239\n",
      "Epoch: [2299/10000] Training loss: 6.51272119662884e-05 / Validation loss: 0.0006578486808479772 / Long term Validation loss: 0.1240\n",
      "Epoch: [2300/10000] Training loss: 6.508809759864158e-05 / Validation loss: 0.0006570228281547186 / Long term Validation loss: 0.1239\n",
      "Epoch: [2301/10000] Training loss: 6.50487441895545e-05 / Validation loss: 0.0006567466065505725 / Long term Validation loss: 0.1239\n",
      "Epoch: [2302/10000] Training loss: 6.500942927880766e-05 / Validation loss: 0.0006565808163935571 / Long term Validation loss: 0.1239\n",
      "Epoch: [2303/10000] Training loss: 6.497034084910112e-05 / Validation loss: 0.0006558014888307898 / Long term Validation loss: 0.1238\n",
      "Epoch: [2304/10000] Training loss: 6.493113019661595e-05 / Validation loss: 0.0006555809516705654 / Long term Validation loss: 0.1239\n",
      "Epoch: [2305/10000] Training loss: 6.489187323999527e-05 / Validation loss: 0.0006553139309142062 / Long term Validation loss: 0.1239\n",
      "Epoch: [2306/10000] Training loss: 6.485280243827209e-05 / Validation loss: 0.0006545944969531452 / Long term Validation loss: 0.1238\n",
      "Epoch: [2307/10000] Training loss: 6.481370627714027e-05 / Validation loss: 0.0006544065140920414 / Long term Validation loss: 0.1238\n",
      "Epoch: [2308/10000] Training loss: 6.477453195817795e-05 / Validation loss: 0.00065405308335938 / Long term Validation loss: 0.1238\n",
      "Epoch: [2309/10000] Training loss: 6.473548386910298e-05 / Validation loss: 0.0006534010092797869 / Long term Validation loss: 0.1238\n",
      "Epoch: [2310/10000] Training loss: 6.46964757176282e-05 / Validation loss: 0.000653223261169033 / Long term Validation loss: 0.1238\n",
      "Epoch: [2311/10000] Training loss: 6.465739606061134e-05 / Validation loss: 0.0006527992527393877 / Long term Validation loss: 0.1238\n",
      "Epoch: [2312/10000] Training loss: 6.461838369456651e-05 / Validation loss: 0.000652216338142781 / Long term Validation loss: 0.1237\n",
      "Epoch: [2313/10000] Training loss: 6.457944316221518e-05 / Validation loss: 0.0006520291390370449 / Long term Validation loss: 0.1238\n",
      "Epoch: [2314/10000] Training loss: 6.454045931730922e-05 / Validation loss: 0.0006515520155531249 / Long term Validation loss: 0.1237\n",
      "Epoch: [2315/10000] Training loss: 6.450149815555595e-05 / Validation loss: 0.000651036117695882 / Long term Validation loss: 0.1237\n",
      "Epoch: [2316/10000] Training loss: 6.446261305433961e-05 / Validation loss: 0.0006508244624289581 / Long term Validation loss: 0.1237\n",
      "Epoch: [2317/10000] Training loss: 6.442371868746885e-05 / Validation loss: 0.00065031312891132 / Long term Validation loss: 0.1237\n",
      "Epoch: [2318/10000] Training loss: 6.43848222656141e-05 / Validation loss: 0.0006498587952273572 / Long term Validation loss: 0.1237\n",
      "Epoch: [2319/10000] Training loss: 6.434598853413287e-05 / Validation loss: 0.0006496125132707306 / Long term Validation loss: 0.1237\n",
      "Epoch: [2320/10000] Training loss: 6.43071738782318e-05 / Validation loss: 0.000649085327885199 / Long term Validation loss: 0.1236\n",
      "Epoch: [2321/10000] Training loss: 6.426835102081145e-05 / Validation loss: 0.0006486833772534757 / Long term Validation loss: 0.1236\n",
      "Epoch: [2322/10000] Training loss: 6.422957082335146e-05 / Validation loss: 0.0006483963403891228 / Long term Validation loss: 0.1236\n",
      "Epoch: [2323/10000] Training loss: 6.41908263977288e-05 / Validation loss: 0.0006478691443840687 / Long term Validation loss: 0.1236\n",
      "Epoch: [2324/10000] Training loss: 6.415208042778908e-05 / Validation loss: 0.0006475071768022501 / Long term Validation loss: 0.1236\n",
      "Epoch: [2325/10000] Training loss: 6.411335926998418e-05 / Validation loss: 0.0006471773228029098 / Long term Validation loss: 0.1236\n",
      "Epoch: [2326/10000] Training loss: 6.407467845127858e-05 / Validation loss: 0.0006466628826327564 / Long term Validation loss: 0.1236\n",
      "Epoch: [2327/10000] Training loss: 6.403600812094154e-05 / Validation loss: 0.00064632717522803 / Long term Validation loss: 0.1236\n",
      "Epoch: [2328/10000] Training loss: 6.399735190132425e-05 / Validation loss: 0.0006459570055726137 / Long term Validation loss: 0.1236\n",
      "Epoch: [2329/10000] Training loss: 6.395873190226606e-05 / Validation loss: 0.0006454649935699597 / Long term Validation loss: 0.1235\n",
      "Epoch: [2330/10000] Training loss: 6.392013340190789e-05 / Validation loss: 0.0006451422918073958 / Long term Validation loss: 0.1235\n",
      "Epoch: [2331/10000] Training loss: 6.388154626850047e-05 / Validation loss: 0.0006447383626975754 / Long term Validation loss: 0.1235\n",
      "Epoch: [2332/10000] Training loss: 6.384298762825812e-05 / Validation loss: 0.0006442747419764275 / Long term Validation loss: 0.1235\n",
      "Epoch: [2333/10000] Training loss: 6.380445680306589e-05 / Validation loss: 0.0006439530818927436 / Long term Validation loss: 0.1235\n",
      "Epoch: [2334/10000] Training loss: 6.376594026763275e-05 / Validation loss: 0.0006435243986391036 / Long term Validation loss: 0.1235\n",
      "Epoch: [2335/10000] Training loss: 6.372744537321137e-05 / Validation loss: 0.0006430907372985993 / Long term Validation loss: 0.1235\n",
      "Epoch: [2336/10000] Training loss: 6.368897935960257e-05 / Validation loss: 0.0006427600527430375 / Long term Validation loss: 0.1235\n",
      "Epoch: [2337/10000] Training loss: 6.365053265333908e-05 / Validation loss: 0.0006423165558938139 / Long term Validation loss: 0.1234\n",
      "Epoch: [2338/10000] Training loss: 6.361210406801671e-05 / Validation loss: 0.0006419103487546186 / Long term Validation loss: 0.1234\n",
      "Epoch: [2339/10000] Training loss: 6.357370191822386e-05 / Validation loss: 0.0006415635012465998 / Long term Validation loss: 0.1234\n",
      "Epoch: [2340/10000] Training loss: 6.353532312094646e-05 / Validation loss: 0.0006411151298358541 / Long term Validation loss: 0.1234\n",
      "Epoch: [2341/10000] Training loss: 6.34969623945698e-05 / Validation loss: 0.0006407309986523139 / Long term Validation loss: 0.1234\n",
      "Epoch: [2342/10000] Training loss: 6.345862471689681e-05 / Validation loss: 0.0006403646162427288 / Long term Validation loss: 0.1234\n",
      "Epoch: [2343/10000] Training loss: 6.342031199682684e-05 / Validation loss: 0.0006399203554426332 / Long term Validation loss: 0.1234\n",
      "Epoch: [2344/10000] Training loss: 6.338201931307253e-05 / Validation loss: 0.0006395512322706859 / Long term Validation loss: 0.1234\n",
      "Epoch: [2345/10000] Training loss: 6.334374735836818e-05 / Validation loss: 0.0006391656019880067 / Long term Validation loss: 0.1234\n",
      "Epoch: [2346/10000] Training loss: 6.330549975896318e-05 / Validation loss: 0.0006387322533936124 / Long term Validation loss: 0.1233\n",
      "Epoch: [2347/10000] Training loss: 6.326727433055376e-05 / Validation loss: 0.0006383703190778427 / Long term Validation loss: 0.1233\n",
      "Epoch: [2348/10000] Training loss: 6.322906909147579e-05 / Validation loss: 0.0006379685643159257 / Long term Validation loss: 0.1233\n",
      "Epoch: [2349/10000] Training loss: 6.319088663355555e-05 / Validation loss: 0.0006375498548338943 / Long term Validation loss: 0.1233\n",
      "Epoch: [2350/10000] Training loss: 6.315272744034285e-05 / Validation loss: 0.0006371876446690017 / Long term Validation loss: 0.1233\n",
      "Epoch: [2351/10000] Training loss: 6.31145891948973e-05 / Validation loss: 0.0006367748210133434 / Long term Validation loss: 0.1233\n",
      "Epoch: [2352/10000] Training loss: 6.30764724485965e-05 / Validation loss: 0.0006363713541050818 / Long term Validation loss: 0.1233\n",
      "Epoch: [2353/10000] Training loss: 6.303837884264858e-05 / Validation loss: 0.0006360030261411513 / Long term Validation loss: 0.1233\n",
      "Epoch: [2354/10000] Training loss: 6.300030723960801e-05 / Validation loss: 0.0006355852173689911 / Long term Validation loss: 0.1233\n",
      "Epoch: [2355/10000] Training loss: 6.29622567506558e-05 / Validation loss: 0.0006351950170233981 / Long term Validation loss: 0.1232\n",
      "Epoch: [2356/10000] Training loss: 6.292422864450626e-05 / Validation loss: 0.000634817216480147 / Long term Validation loss: 0.1232\n",
      "Epoch: [2357/10000] Training loss: 6.288622311724612e-05 / Validation loss: 0.0006344003993871848 / Long term Validation loss: 0.1232\n",
      "Epoch: [2358/10000] Training loss: 6.284823905388706e-05 / Validation loss: 0.000634019626549339 / Long term Validation loss: 0.1232\n",
      "Epoch: [2359/10000] Training loss: 6.281027671499013e-05 / Validation loss: 0.0006336316195638476 / Long term Validation loss: 0.1232\n",
      "Epoch: [2360/10000] Training loss: 6.277233687876799e-05 / Validation loss: 0.0006332204699020703 / Long term Validation loss: 0.1232\n",
      "Epoch: [2361/10000] Training loss: 6.27344190374933e-05 / Validation loss: 0.000632844291282628 / Long term Validation loss: 0.1232\n",
      "Epoch: [2362/10000] Training loss: 6.26965227379884e-05 / Validation loss: 0.0006324475853575623 / Long term Validation loss: 0.1232\n",
      "Epoch: [2363/10000] Training loss: 6.265864853115418e-05 / Validation loss: 0.0006320447114329184 / Long term Validation loss: 0.1232\n",
      "Epoch: [2364/10000] Training loss: 6.262079658224267e-05 / Validation loss: 0.0006316683954954554 / Long term Validation loss: 0.1232\n",
      "Epoch: [2365/10000] Training loss: 6.258296637502322e-05 / Validation loss: 0.0006312661162553729 / Long term Validation loss: 0.1231\n",
      "Epoch: [2366/10000] Training loss: 6.254515793493407e-05 / Validation loss: 0.00063087192670283 / Long term Validation loss: 0.1231\n",
      "Epoch: [2367/10000] Training loss: 6.250737166565278e-05 / Validation loss: 0.0006304919051271821 / Long term Validation loss: 0.1231\n",
      "Epoch: [2368/10000] Training loss: 6.246960739612998e-05 / Validation loss: 0.0006300879510883309 / Long term Validation loss: 0.1231\n",
      "Epoch: [2369/10000] Training loss: 6.243186484246994e-05 / Validation loss: 0.0006297009831944747 / Long term Validation loss: 0.1231\n",
      "Epoch: [2370/10000] Training loss: 6.239414422845608e-05 / Validation loss: 0.0006293154769480733 / Long term Validation loss: 0.1231\n",
      "Epoch: [2371/10000] Training loss: 6.235644569523821e-05 / Validation loss: 0.000628913503043848 / Long term Validation loss: 0.1231\n",
      "Epoch: [2372/10000] Training loss: 6.231876900862541e-05 / Validation loss: 0.0006285310278698124 / Long term Validation loss: 0.1231\n",
      "Epoch: [2373/10000] Training loss: 6.228111411828258e-05 / Validation loss: 0.0006281401094610428 / Long term Validation loss: 0.1231\n",
      "Epoch: [2374/10000] Training loss: 6.224348121161394e-05 / Validation loss: 0.0006277426358552384 / Long term Validation loss: 0.1230\n",
      "Epoch: [2375/10000] Training loss: 6.220587026743107e-05 / Validation loss: 0.0006273614816615048 / Long term Validation loss: 0.1230\n",
      "Epoch: [2376/10000] Training loss: 6.216828113032418e-05 / Validation loss: 0.0006269667151299431 / Long term Validation loss: 0.1230\n",
      "Epoch: [2377/10000] Training loss: 6.213071384669823e-05 / Validation loss: 0.0006265746792454071 / Long term Validation loss: 0.1230\n",
      "Epoch: [2378/10000] Training loss: 6.209316852360783e-05 / Validation loss: 0.0006261921344930923 / Long term Validation loss: 0.1230\n",
      "Epoch: [2379/10000] Training loss: 6.205564508233784e-05 / Validation loss: 0.0006257959468765493 / Long term Validation loss: 0.1230\n",
      "Epoch: [2380/10000] Training loss: 6.201814344572347e-05 / Validation loss: 0.0006254087667083781 / Long term Validation loss: 0.1230\n",
      "Epoch: [2381/10000] Training loss: 6.198066369150865e-05 / Validation loss: 0.000625023269635384 / Long term Validation loss: 0.1230\n",
      "Epoch: [2382/10000] Training loss: 6.194320584903911e-05 / Validation loss: 0.0006246281647943016 / Long term Validation loss: 0.1230\n",
      "Epoch: [2383/10000] Training loss: 6.190576983950979e-05 / Validation loss: 0.0006242441771354861 / Long term Validation loss: 0.1230\n",
      "Epoch: [2384/10000] Training loss: 6.186835565122189e-05 / Validation loss: 0.0006238555545440864 / Long term Validation loss: 0.1230\n",
      "Epoch: [2385/10000] Training loss: 6.183096333771435e-05 / Validation loss: 0.0006234633612489649 / Long term Validation loss: 0.1229\n",
      "Epoch: [2386/10000] Training loss: 6.179359288935211e-05 / Validation loss: 0.0006230804724947828 / Long term Validation loss: 0.1229\n",
      "Epoch: [2387/10000] Training loss: 6.17562442563231e-05 / Validation loss: 0.0006226897192477123 / Long term Validation loss: 0.1229\n",
      "Epoch: [2388/10000] Training loss: 6.171891744598365e-05 / Validation loss: 0.0006223011341327125 / Long term Validation loss: 0.1229\n",
      "Epoch: [2389/10000] Training loss: 6.16816124911588e-05 / Validation loss: 0.0006219175256006959 / Long term Validation loss: 0.1229\n",
      "Epoch: [2390/10000] Training loss: 6.164432936900075e-05 / Validation loss: 0.0006215262828716707 / Long term Validation loss: 0.1229\n",
      "Epoch: [2391/10000] Training loss: 6.160706804832944e-05 / Validation loss: 0.0006211408591020737 / Long term Validation loss: 0.1229\n",
      "Epoch: [2392/10000] Training loss: 6.156982854769789e-05 / Validation loss: 0.0006207555392589618 / Long term Validation loss: 0.1229\n",
      "Epoch: [2393/10000] Training loss: 6.15326108781603e-05 / Validation loss: 0.0006203654605192779 / Long term Validation loss: 0.1229\n",
      "Epoch: [2394/10000] Training loss: 6.149541501532897e-05 / Validation loss: 0.0006199819811359925 / Long term Validation loss: 0.1229\n",
      "Epoch: [2395/10000] Training loss: 6.14582409477213e-05 / Validation loss: 0.0006195949846444134 / Long term Validation loss: 0.1229\n",
      "Epoch: [2396/10000] Training loss: 6.142108868691787e-05 / Validation loss: 0.0006192071737424952 / Long term Validation loss: 0.1228\n",
      "Epoch: [2397/10000] Training loss: 6.138395823415528e-05 / Validation loss: 0.0006188242143027862 / Long term Validation loss: 0.1228\n",
      "Epoch: [2398/10000] Training loss: 6.134684957189807e-05 / Validation loss: 0.0006184363947177504 / Long term Validation loss: 0.1228\n",
      "Epoch: [2399/10000] Training loss: 6.130976269301956e-05 / Validation loss: 0.0006180510988861513 / Long term Validation loss: 0.1228\n",
      "Epoch: [2400/10000] Training loss: 6.127269760704454e-05 / Validation loss: 0.0006176675706290771 / Long term Validation loss: 0.1228\n",
      "Epoch: [2401/10000] Training loss: 6.123565430967815e-05 / Validation loss: 0.0006172801231273471 / Long term Validation loss: 0.1228\n",
      "Epoch: [2402/10000] Training loss: 6.119863278646296e-05 / Validation loss: 0.0006168967916777242 / Long term Validation loss: 0.1228\n",
      "Epoch: [2403/10000] Training loss: 6.116163303673954e-05 / Validation loss: 0.0006165122917560462 / Long term Validation loss: 0.1228\n",
      "Epoch: [2404/10000] Training loss: 6.112465506358272e-05 / Validation loss: 0.0006161262326768306 / Long term Validation loss: 0.1228\n",
      "Epoch: [2405/10000] Training loss: 6.108769886101384e-05 / Validation loss: 0.0006157438890727632 / Long term Validation loss: 0.1228\n",
      "Epoch: [2406/10000] Training loss: 6.10507644205069e-05 / Validation loss: 0.000615358742312029 / Long term Validation loss: 0.1228\n",
      "Epoch: [2407/10000] Training loss: 6.101385173981722e-05 / Validation loss: 0.0006149745432728305 / Long term Validation loss: 0.1228\n",
      "Epoch: [2408/10000] Training loss: 6.097696082052709e-05 / Validation loss: 0.000614592270970079 / Long term Validation loss: 0.1228\n",
      "Epoch: [2409/10000] Training loss: 6.094009165697469e-05 / Validation loss: 0.0006142072620556074 / Long term Validation loss: 0.1227\n",
      "Epoch: [2410/10000] Training loss: 6.090324424146687e-05 / Validation loss: 0.0006138247543737047 / Long term Validation loss: 0.1227\n",
      "Epoch: [2411/10000] Training loss: 6.086641857367495e-05 / Validation loss: 0.000613442071600947 / Long term Validation loss: 0.1227\n",
      "Epoch: [2412/10000] Training loss: 6.08296146523606e-05 / Validation loss: 0.0006130580132920723 / Long term Validation loss: 0.1227\n",
      "Epoch: [2413/10000] Training loss: 6.079283247196147e-05 / Validation loss: 0.000612676576944274 / Long term Validation loss: 0.1227\n",
      "Epoch: [2414/10000] Training loss: 6.07560720278918e-05 / Validation loss: 0.0006122935620955086 / Long term Validation loss: 0.1227\n",
      "Epoch: [2415/10000] Training loss: 6.071933331783609e-05 / Validation loss: 0.0006119109216989483 / Long term Validation loss: 0.1227\n",
      "Epoch: [2416/10000] Training loss: 6.068261634031326e-05 / Validation loss: 0.0006115298540197261 / Long term Validation loss: 0.1227\n",
      "Epoch: [2417/10000] Training loss: 6.064592109087684e-05 / Validation loss: 0.0006111470047599842 / Long term Validation loss: 0.1227\n",
      "Epoch: [2418/10000] Training loss: 6.060924756458835e-05 / Validation loss: 0.0006107657567521216 / Long term Validation loss: 0.1227\n",
      "Epoch: [2419/10000] Training loss: 6.057259576001852e-05 / Validation loss: 0.0006103846268608817 / Long term Validation loss: 0.1227\n",
      "Epoch: [2420/10000] Training loss: 6.0535965674662314e-05 / Validation loss: 0.0006100025453717277 / Long term Validation loss: 0.1227\n",
      "Epoch: [2421/10000] Training loss: 6.0499357304201056e-05 / Validation loss: 0.0006096222828570897 / Long term Validation loss: 0.1227\n",
      "Epoch: [2422/10000] Training loss: 6.046277064530451e-05 / Validation loss: 0.0006092410928942774 / Long term Validation loss: 0.1227\n",
      "Epoch: [2423/10000] Training loss: 6.042620569522888e-05 / Validation loss: 0.0006088601645548037 / Long term Validation loss: 0.1226\n",
      "Epoch: [2424/10000] Training loss: 6.038966245165967e-05 / Validation loss: 0.0006084803770871563 / Long term Validation loss: 0.1226\n",
      "Epoch: [2425/10000] Training loss: 6.035314091108958e-05 / Validation loss: 0.0006080994736290921 / Long term Validation loss: 0.1226\n",
      "Epoch: [2426/10000] Training loss: 6.031664106966429e-05 / Validation loss: 0.0006077197065956888 / Long term Validation loss: 0.1226\n",
      "Epoch: [2427/10000] Training loss: 6.028016292527857e-05 / Validation loss: 0.0006073400663655268 / Long term Validation loss: 0.1226\n",
      "Epoch: [2428/10000] Training loss: 6.0243706475196004e-05 / Validation loss: 0.0006069598877988583 / Long term Validation loss: 0.1226\n",
      "Epoch: [2429/10000] Training loss: 6.020727171595421e-05 / Validation loss: 0.0006065809835744883 / Long term Validation loss: 0.1226\n",
      "Epoch: [2430/10000] Training loss: 6.017085864462562e-05 / Validation loss: 0.0006062014902315575 / Long term Validation loss: 0.1226\n",
      "Epoch: [2431/10000] Training loss: 6.0134467258315194e-05 / Validation loss: 0.0006058223096239874 / Long term Validation loss: 0.1226\n",
      "Epoch: [2432/10000] Training loss: 6.00980975545619e-05 / Validation loss: 0.0006054438935385747 / Long term Validation loss: 0.1226\n",
      "Epoch: [2433/10000] Training loss: 6.006174953036831e-05 / Validation loss: 0.0006050648121692346 / Long term Validation loss: 0.1226\n",
      "Epoch: [2434/10000] Training loss: 6.0025423182425824e-05 / Validation loss: 0.0006046866155055787 / Long term Validation loss: 0.1226\n",
      "Epoch: [2435/10000] Training loss: 5.998911850834976e-05 / Validation loss: 0.0006043084696123163 / Long term Validation loss: 0.1226\n",
      "Epoch: [2436/10000] Training loss: 5.9952835505420435e-05 / Validation loss: 0.0006039301221177805 / Long term Validation loss: 0.1226\n",
      "Epoch: [2437/10000] Training loss: 5.991657417069344e-05 / Validation loss: 0.0006035526728892682 / Long term Validation loss: 0.1226\n",
      "Epoch: [2438/10000] Training loss: 5.9880334501431846e-05 / Validation loss: 0.0006031748343200806 / Long term Validation loss: 0.1225\n",
      "Epoch: [2439/10000] Training loss: 5.98441164947731e-05 / Validation loss: 0.0006027973914082248 / Long term Validation loss: 0.1225\n",
      "Epoch: [2440/10000] Training loss: 5.980792014827233e-05 / Validation loss: 0.0006024204201999276 / Long term Validation loss: 0.1225\n",
      "Epoch: [2441/10000] Training loss: 5.97717454592027e-05 / Validation loss: 0.0006020431071320438 / Long term Validation loss: 0.1225\n",
      "Epoch: [2442/10000] Training loss: 5.973559242462286e-05 / Validation loss: 0.0006016665122750085 / Long term Validation loss: 0.1225\n",
      "Epoch: [2443/10000] Training loss: 5.969946104207098e-05 / Validation loss: 0.0006012898965095276 / Long term Validation loss: 0.1225\n",
      "Epoch: [2444/10000] Training loss: 5.966335130888862e-05 / Validation loss: 0.0006009133328676757 / Long term Validation loss: 0.1225\n",
      "Epoch: [2445/10000] Training loss: 5.96272632224462e-05 / Validation loss: 0.0006005373867786352 / Long term Validation loss: 0.1225\n",
      "Epoch: [2446/10000] Training loss: 5.9591196780145366e-05 / Validation loss: 0.000600161203197536 / Long term Validation loss: 0.1225\n",
      "Epoch: [2447/10000] Training loss: 5.9555151979243394e-05 / Validation loss: 0.000599785469106162 / Long term Validation loss: 0.1225\n",
      "Epoch: [2448/10000] Training loss: 5.951912881733553e-05 / Validation loss: 0.000599409993532677 / Long term Validation loss: 0.1225\n",
      "Epoch: [2449/10000] Training loss: 5.948312729184624e-05 / Validation loss: 0.0005990344253690517 / Long term Validation loss: 0.1225\n",
      "Epoch: [2450/10000] Training loss: 5.9447147400113506e-05 / Validation loss: 0.0005986594309867012 / Long term Validation loss: 0.1225\n",
      "Epoch: [2451/10000] Training loss: 5.941118913969016e-05 / Validation loss: 0.0005982843885733351 / Long term Validation loss: 0.1225\n",
      "Epoch: [2452/10000] Training loss: 5.9375252507993125e-05 / Validation loss: 0.0005979095745875438 / Long term Validation loss: 0.1225\n",
      "Epoch: [2453/10000] Training loss: 5.933933750258027e-05 / Validation loss: 0.000597535159250691 / Long term Validation loss: 0.1225\n",
      "Epoch: [2454/10000] Training loss: 5.9303444120964036e-05 / Validation loss: 0.0005971606522907488 / Long term Validation loss: 0.1225\n",
      "Epoch: [2455/10000] Training loss: 5.926757236055886e-05 / Validation loss: 0.0005967865956871511 / Long term Validation loss: 0.1225\n",
      "Epoch: [2456/10000] Training loss: 5.923172221899952e-05 / Validation loss: 0.000596412662012159 / Long term Validation loss: 0.1225\n",
      "Epoch: [2457/10000] Training loss: 5.9195893693804085e-05 / Validation loss: 0.0005960388287045035 / Long term Validation loss: 0.1224\n",
      "Epoch: [2458/10000] Training loss: 5.916008678251636e-05 / Validation loss: 0.0005956654237296124 / Long term Validation loss: 0.1224\n",
      "Epoch: [2459/10000] Training loss: 5.912430148274102e-05 / Validation loss: 0.000595291999646568 / Long term Validation loss: 0.1224\n",
      "Epoch: [2460/10000] Training loss: 5.9088537791989e-05 / Validation loss: 0.0005949189010317766 / Long term Validation loss: 0.1224\n",
      "Epoch: [2461/10000] Training loss: 5.9052795707923484e-05 / Validation loss: 0.0005945460382784951 / Long term Validation loss: 0.1224\n",
      "Epoch: [2462/10000] Training loss: 5.901707522813989e-05 / Validation loss: 0.0005941732295185424 / Long term Validation loss: 0.1224\n",
      "Epoch: [2463/10000] Training loss: 5.898137635021008e-05 / Validation loss: 0.0005938008172937597 / Long term Validation loss: 0.1224\n",
      "Epoch: [2464/10000] Training loss: 5.8945699071809635e-05 / Validation loss: 0.0005934284726990522 / Long term Validation loss: 0.1224\n",
      "Epoch: [2465/10000] Training loss: 5.891004339053164e-05 / Validation loss: 0.0005930563613059512 / Long term Validation loss: 0.1224\n",
      "Epoch: [2466/10000] Training loss: 5.8874409304053716e-05 / Validation loss: 0.0005926845408113621 / Long term Validation loss: 0.1224\n",
      "Epoch: [2467/10000] Training loss: 5.8838796810037996e-05 / Validation loss: 0.0005923127786017074 / Long term Validation loss: 0.1224\n",
      "Epoch: [2468/10000] Training loss: 5.8803205906105106e-05 / Validation loss: 0.000591941360052193 / Long term Validation loss: 0.1224\n",
      "Epoch: [2469/10000] Training loss: 5.876763658997674e-05 / Validation loss: 0.0005915700809859401 / Long term Validation loss: 0.1224\n",
      "Epoch: [2470/10000] Training loss: 5.8732088859311334e-05 / Validation loss: 0.0005911989812645782 / Long term Validation loss: 0.1224\n",
      "Epoch: [2471/10000] Training loss: 5.8696562711806325e-05 / Validation loss: 0.0005908281873803685 / Long term Validation loss: 0.1224\n",
      "Epoch: [2472/10000] Training loss: 5.866105814517878e-05 / Validation loss: 0.0005904574792115667 / Long term Validation loss: 0.1224\n",
      "Epoch: [2473/10000] Training loss: 5.862557515710047e-05 / Validation loss: 0.0005900870647372402 / Long term Validation loss: 0.1224\n",
      "Epoch: [2474/10000] Training loss: 5.8590113745323476e-05 / Validation loss: 0.0005897168384075809 / Long term Validation loss: 0.1224\n",
      "Epoch: [2475/10000] Training loss: 5.855467390755907e-05 / Validation loss: 0.0005893467677022663 / Long term Validation loss: 0.1224\n",
      "Epoch: [2476/10000] Training loss: 5.851925564153086e-05 / Validation loss: 0.0005889769958695427 / Long term Validation loss: 0.1224\n",
      "Epoch: [2477/10000] Training loss: 5.848385894499781e-05 / Validation loss: 0.0005886073417608615 / Long term Validation loss: 0.1223\n",
      "Epoch: [2478/10000] Training loss: 5.8448483815677585e-05 / Validation loss: 0.0005882379433313546 / Long term Validation loss: 0.1223\n",
      "Epoch: [2479/10000] Training loss: 5.8413130251345304e-05 / Validation loss: 0.0005878687606055593 / Long term Validation loss: 0.1223\n",
      "Epoch: [2480/10000] Training loss: 5.837779824975438e-05 / Validation loss: 0.0005874997285023365 / Long term Validation loss: 0.1223\n",
      "Epoch: [2481/10000] Training loss: 5.834248780865663e-05 / Validation loss: 0.0005871309797313834 / Long term Validation loss: 0.1223\n",
      "Epoch: [2482/10000] Training loss: 5.830719892584206e-05 / Validation loss: 0.0005867623764648172 / Long term Validation loss: 0.1223\n",
      "Epoch: [2483/10000] Training loss: 5.8271931599066144e-05 / Validation loss: 0.0005863940040847483 / Long term Validation loss: 0.1223\n",
      "Epoch: [2484/10000] Training loss: 5.823668582612364e-05 / Validation loss: 0.0005860258601936089 / Long term Validation loss: 0.1223\n",
      "Epoch: [2485/10000] Training loss: 5.82014616048007e-05 / Validation loss: 0.0005856578716843827 / Long term Validation loss: 0.1223\n",
      "Epoch: [2486/10000] Training loss: 5.8166258932875384e-05 / Validation loss: 0.0005852901501845349 / Long term Validation loss: 0.1223\n",
      "Epoch: [2487/10000] Training loss: 5.813107780816013e-05 / Validation loss: 0.0005849225950189166 / Long term Validation loss: 0.1223\n",
      "Epoch: [2488/10000] Training loss: 5.809591822844027e-05 / Validation loss: 0.0005845552565712646 / Long term Validation loss: 0.1223\n",
      "Epoch: [2489/10000] Training loss: 5.80607801915275e-05 / Validation loss: 0.0005841881503283414 / Long term Validation loss: 0.1223\n",
      "Epoch: [2490/10000] Training loss: 5.8025663695232095e-05 / Validation loss: 0.0005838212078258044 / Long term Validation loss: 0.1223\n",
      "Epoch: [2491/10000] Training loss: 5.7990568737354e-05 / Validation loss: 0.0005834545183873907 / Long term Validation loss: 0.1223\n",
      "Epoch: [2492/10000] Training loss: 5.7955495315721716e-05 / Validation loss: 0.0005830880093650647 / Long term Validation loss: 0.1223\n",
      "Epoch: [2493/10000] Training loss: 5.7920443428142845e-05 / Validation loss: 0.0005827217101494369 / Long term Validation loss: 0.1223\n",
      "Epoch: [2494/10000] Training loss: 5.78854130724425e-05 / Validation loss: 0.0005823556420971666 / Long term Validation loss: 0.1223\n",
      "Epoch: [2495/10000] Training loss: 5.785040424644762e-05 / Validation loss: 0.0005819897466384469 / Long term Validation loss: 0.1223\n",
      "Epoch: [2496/10000] Training loss: 5.7815416947974805e-05 / Validation loss: 0.0005816240936754955 / Long term Validation loss: 0.1223\n",
      "Epoch: [2497/10000] Training loss: 5.778045117486297e-05 / Validation loss: 0.0005812586301083253 / Long term Validation loss: 0.1222\n",
      "Epoch: [2498/10000] Training loss: 5.774550692493489e-05 / Validation loss: 0.0005808933740097861 / Long term Validation loss: 0.1222\n",
      "Epoch: [2499/10000] Training loss: 5.771058419602511e-05 / Validation loss: 0.0005805283460089345 / Long term Validation loss: 0.1222\n",
      "Epoch: [2500/10000] Training loss: 5.767568298597046e-05 / Validation loss: 0.0005801634985787888 / Long term Validation loss: 0.1222\n",
      "Epoch: [2501/10000] Training loss: 5.764080329259847e-05 / Validation loss: 0.0005797988860683917 / Long term Validation loss: 0.1222\n",
      "Epoch: [2502/10000] Training loss: 5.7605945113753475e-05 / Validation loss: 0.0005794344683423005 / Long term Validation loss: 0.1222\n",
      "Epoch: [2503/10000] Training loss: 5.7571108447266685e-05 / Validation loss: 0.0005790702582141175 / Long term Validation loss: 0.1222\n",
      "Epoch: [2504/10000] Training loss: 5.7536293290977584e-05 / Validation loss: 0.0005787062724453857 / Long term Validation loss: 0.1222\n",
      "Epoch: [2505/10000] Training loss: 5.750149964272684e-05 / Validation loss: 0.000578342473935859 / Long term Validation loss: 0.1222\n",
      "Epoch: [2506/10000] Training loss: 5.74667275003471e-05 / Validation loss: 0.0005779789050316756 / Long term Validation loss: 0.1222\n",
      "Epoch: [2507/10000] Training loss: 5.743197686168305e-05 / Validation loss: 0.0005776155340779588 / Long term Validation loss: 0.1222\n",
      "Epoch: [2508/10000] Training loss: 5.739724772456846e-05 / Validation loss: 0.0005772523721171446 / Long term Validation loss: 0.1222\n",
      "Epoch: [2509/10000] Training loss: 5.7362540086842454e-05 / Validation loss: 0.0005768894308746618 / Long term Validation loss: 0.1222\n",
      "Epoch: [2510/10000] Training loss: 5.732785394634411e-05 / Validation loss: 0.0005765266824757687 / Long term Validation loss: 0.1222\n",
      "Epoch: [2511/10000] Training loss: 5.729318930090538e-05 / Validation loss: 0.0005761641599358596 / Long term Validation loss: 0.1222\n",
      "Epoch: [2512/10000] Training loss: 5.725854614836648e-05 / Validation loss: 0.000575801837274773 / Long term Validation loss: 0.1222\n",
      "Epoch: [2513/10000] Training loss: 5.7223924486557645e-05 / Validation loss: 0.0005754397254754436 / Long term Validation loss: 0.1222\n",
      "Epoch: [2514/10000] Training loss: 5.7189324313312876e-05 / Validation loss: 0.000575077831075889 / Long term Validation loss: 0.1222\n",
      "Epoch: [2515/10000] Training loss: 5.7154745626464057e-05 / Validation loss: 0.0005747161341963643 / Long term Validation loss: 0.1222\n",
      "Epoch: [2516/10000] Training loss: 5.7120188423837055e-05 / Validation loss: 0.0005743546603296281 / Long term Validation loss: 0.1222\n",
      "Epoch: [2517/10000] Training loss: 5.708565270326238e-05 / Validation loss: 0.0005739933876657287 / Long term Validation loss: 0.1222\n",
      "Epoch: [2518/10000] Training loss: 5.7051138462561444e-05 / Validation loss: 0.0005736323277381127 / Long term Validation loss: 0.1221\n",
      "Epoch: [2519/10000] Training loss: 5.701664569955776e-05 / Validation loss: 0.0005732714823278404 / Long term Validation loss: 0.1221\n",
      "Epoch: [2520/10000] Training loss: 5.69821744120709e-05 / Validation loss: 0.0005729108384883305 / Long term Validation loss: 0.1221\n",
      "Epoch: [2521/10000] Training loss: 5.694772459791518e-05 / Validation loss: 0.0005725504153165954 / Long term Validation loss: 0.1221\n",
      "Epoch: [2522/10000] Training loss: 5.6913296254906466e-05 / Validation loss: 0.0005721901945572729 / Long term Validation loss: 0.1221\n",
      "Epoch: [2523/10000] Training loss: 5.6878889380851924e-05 / Validation loss: 0.0005718301881572691 / Long term Validation loss: 0.1221\n",
      "Epoch: [2524/10000] Training loss: 5.6844503973559445e-05 / Validation loss: 0.0005714703938895488 / Long term Validation loss: 0.1221\n",
      "Epoch: [2525/10000] Training loss: 5.681014003083116e-05 / Validation loss: 0.0005711108047699829 / Long term Validation loss: 0.1221\n",
      "Epoch: [2526/10000] Training loss: 5.6775797550464545e-05 / Validation loss: 0.000570751434169826 / Long term Validation loss: 0.1221\n",
      "Epoch: [2527/10000] Training loss: 5.6741476530255784e-05 / Validation loss: 0.0005703922673281726 / Long term Validation loss: 0.1221\n",
      "Epoch: [2528/10000] Training loss: 5.6707176967992666e-05 / Validation loss: 0.000570033315992022 / Long term Validation loss: 0.1221\n",
      "Epoch: [2529/10000] Training loss: 5.66728988614622e-05 / Validation loss: 0.0005696745749447903 / Long term Validation loss: 0.1221\n",
      "Epoch: [2530/10000] Training loss: 5.66386422084443e-05 / Validation loss: 0.0005693160421932498 / Long term Validation loss: 0.1221\n",
      "Epoch: [2531/10000] Training loss: 5.660440700671415e-05 / Validation loss: 0.0005689577258901484 / Long term Validation loss: 0.1221\n",
      "Epoch: [2532/10000] Training loss: 5.657019325404335e-05 / Validation loss: 0.0005685996149926266 / Long term Validation loss: 0.1221\n",
      "Epoch: [2533/10000] Training loss: 5.6536000948195246e-05 / Validation loss: 0.0005682417201634004 / Long term Validation loss: 0.1221\n",
      "Epoch: [2534/10000] Training loss: 5.650183008693086e-05 / Validation loss: 0.0005678840344375454 / Long term Validation loss: 0.1221\n",
      "Epoch: [2535/10000] Training loss: 5.64676806680028e-05 / Validation loss: 0.0005675265596873861 / Long term Validation loss: 0.1221\n",
      "Epoch: [2536/10000] Training loss: 5.643355268915898e-05 / Validation loss: 0.000567169299418542 / Long term Validation loss: 0.1221\n",
      "Epoch: [2537/10000] Training loss: 5.6399446148141346e-05 / Validation loss: 0.0005668122465157865 / Long term Validation loss: 0.1221\n",
      "Epoch: [2538/10000] Training loss: 5.636536104268398e-05 / Validation loss: 0.0005664554095963033 / Long term Validation loss: 0.1221\n",
      "Epoch: [2539/10000] Training loss: 5.633129737051657e-05 / Validation loss: 0.0005660987813315949 / Long term Validation loss: 0.1221\n",
      "Epoch: [2540/10000] Training loss: 5.629725512935982e-05 / Validation loss: 0.0005657423661089764 / Long term Validation loss: 0.1221\n",
      "Epoch: [2541/10000] Training loss: 5.6263234316929026e-05 / Validation loss: 0.0005653861636368307 / Long term Validation loss: 0.1221\n",
      "Epoch: [2542/10000] Training loss: 5.6229234930931825e-05 / Validation loss: 0.0005650301706776274 / Long term Validation loss: 0.1221\n",
      "Epoch: [2543/10000] Training loss: 5.619525696906775e-05 / Validation loss: 0.0005646743930273534 / Long term Validation loss: 0.1221\n",
      "Epoch: [2544/10000] Training loss: 5.616130042903042e-05 / Validation loss: 0.0005643188243688174 / Long term Validation loss: 0.1221\n",
      "Epoch: [2545/10000] Training loss: 5.612736530850364e-05 / Validation loss: 0.0005639634700704856 / Long term Validation loss: 0.1220\n",
      "Epoch: [2546/10000] Training loss: 5.60934516051651e-05 / Validation loss: 0.0005636083272650926 / Long term Validation loss: 0.1220\n",
      "Epoch: [2547/10000] Training loss: 5.60595593166831e-05 / Validation loss: 0.0005632533960672688 / Long term Validation loss: 0.1220\n",
      "Epoch: [2548/10000] Training loss: 5.602568844071803e-05 / Validation loss: 0.0005628986791173066 / Long term Validation loss: 0.1220\n",
      "Epoch: [2549/10000] Training loss: 5.599183897492205e-05 / Validation loss: 0.0005625441722008635 / Long term Validation loss: 0.1220\n",
      "Epoch: [2550/10000] Training loss: 5.595801091693762e-05 / Validation loss: 0.0005621898801352136 / Long term Validation loss: 0.1220\n",
      "Epoch: [2551/10000] Training loss: 5.592420426439947e-05 / Validation loss: 0.0005618357989951025 / Long term Validation loss: 0.1220\n",
      "Epoch: [2552/10000] Training loss: 5.589041901493212e-05 / Validation loss: 0.000561481931175251 / Long term Validation loss: 0.1220\n",
      "Epoch: [2553/10000] Training loss: 5.5856655166151456e-05 / Validation loss: 0.0005611282764826025 / Long term Validation loss: 0.1220\n",
      "Epoch: [2554/10000] Training loss: 5.5822912715663575e-05 / Validation loss: 0.000560774833316713 / Long term Validation loss: 0.1220\n",
      "Epoch: [2555/10000] Training loss: 5.578919166106466e-05 / Validation loss: 0.0005604216047530993 / Long term Validation loss: 0.1220\n",
      "Epoch: [2556/10000] Training loss: 5.575549199994155e-05 / Validation loss: 0.0005600685873498709 / Long term Validation loss: 0.1220\n",
      "Epoch: [2557/10000] Training loss: 5.572181372987012e-05 / Validation loss: 0.0005597157843156507 / Long term Validation loss: 0.1220\n",
      "Epoch: [2558/10000] Training loss: 5.568815684841689e-05 / Validation loss: 0.0005593631936311096 / Long term Validation loss: 0.1220\n",
      "Epoch: [2559/10000] Training loss: 5.565452135313705e-05 / Validation loss: 0.000559010816020461 / Long term Validation loss: 0.1220\n",
      "Epoch: [2560/10000] Training loss: 5.5620907241575586e-05 / Validation loss: 0.0005586586523284535 / Long term Validation loss: 0.1220\n",
      "Epoch: [2561/10000] Training loss: 5.558731451126655e-05 / Validation loss: 0.0005583067007204272 / Long term Validation loss: 0.1220\n",
      "Epoch: [2562/10000] Training loss: 5.555374315973254e-05 / Validation loss: 0.0005579549637550142 / Long term Validation loss: 0.1220\n",
      "Epoch: [2563/10000] Training loss: 5.552019318448555e-05 / Validation loss: 0.0005576034390136075 / Long term Validation loss: 0.1220\n",
      "Epoch: [2564/10000] Training loss: 5.548666458302531e-05 / Validation loss: 0.0005572521285019158 / Long term Validation loss: 0.1220\n",
      "Epoch: [2565/10000] Training loss: 5.5453157352840555e-05 / Validation loss: 0.0005569010312480019 / Long term Validation loss: 0.1220\n",
      "Epoch: [2566/10000] Training loss: 5.541967149140774e-05 / Validation loss: 0.0005565501473210391 / Long term Validation loss: 0.1220\n",
      "Epoch: [2567/10000] Training loss: 5.538620699619141e-05 / Validation loss: 0.0005561994777144579 / Long term Validation loss: 0.1220\n",
      "Epoch: [2568/10000] Training loss: 5.535276386464409e-05 / Validation loss: 0.0005558490209144787 / Long term Validation loss: 0.1220\n",
      "Epoch: [2569/10000] Training loss: 5.531934209420537e-05 / Validation loss: 0.000555498778831048 / Long term Validation loss: 0.1220\n",
      "Epoch: [2570/10000] Training loss: 5.5285941682302756e-05 / Validation loss: 0.0005551487498130325 / Long term Validation loss: 0.1220\n",
      "Epoch: [2571/10000] Training loss: 5.525256262635044e-05 / Validation loss: 0.0005547989351849009 / Long term Validation loss: 0.1220\n",
      "Epoch: [2572/10000] Training loss: 5.521920492375e-05 / Validation loss: 0.000554449334407967 / Long term Validation loss: 0.1220\n",
      "Epoch: [2573/10000] Training loss: 5.518586857188964e-05 / Validation loss: 0.0005540999474405268 / Long term Validation loss: 0.1220\n",
      "Epoch: [2574/10000] Training loss: 5.5152553568144045e-05 / Validation loss: 0.0005537507750712152 / Long term Validation loss: 0.1220\n",
      "Epoch: [2575/10000] Training loss: 5.5119259909874546e-05 / Validation loss: 0.0005534018162165153 / Long term Validation loss: 0.1220\n",
      "Epoch: [2576/10000] Training loss: 5.5085987594428503e-05 / Validation loss: 0.0005530530722558576 / Long term Validation loss: 0.1220\n",
      "Epoch: [2577/10000] Training loss: 5.5052736619139445e-05 / Validation loss: 0.0005527045420246338 / Long term Validation loss: 0.1220\n",
      "Epoch: [2578/10000] Training loss: 5.5019506981326586e-05 / Validation loss: 0.0005523562265140436 / Long term Validation loss: 0.1220\n",
      "Epoch: [2579/10000] Training loss: 5.498629867829492e-05 / Validation loss: 0.000552008125294665 / Long term Validation loss: 0.1220\n",
      "Epoch: [2580/10000] Training loss: 5.495311170733478e-05 / Validation loss: 0.0005516602384436905 / Long term Validation loss: 0.1220\n",
      "Epoch: [2581/10000] Training loss: 5.491994606572181e-05 / Validation loss: 0.0005513125664441171 / Long term Validation loss: 0.1220\n",
      "Epoch: [2582/10000] Training loss: 5.4886801750716716e-05 / Validation loss: 0.0005509651086186119 / Long term Validation loss: 0.1220\n",
      "Epoch: [2583/10000] Training loss: 5.4853678759565045e-05 / Validation loss: 0.0005506178659351428 / Long term Validation loss: 0.1220\n",
      "Epoch: [2584/10000] Training loss: 5.4820577089497086e-05 / Validation loss: 0.0005502708375509604 / Long term Validation loss: 0.1220\n",
      "Epoch: [2585/10000] Training loss: 5.4787496737727465e-05 / Validation loss: 0.0005499240242869386 / Long term Validation loss: 0.1220\n",
      "Epoch: [2586/10000] Training loss: 5.475443770145533e-05 / Validation loss: 0.0005495774256998137 / Long term Validation loss: 0.1220\n",
      "Epoch: [2587/10000] Training loss: 5.4721399977863764e-05 / Validation loss: 0.0005492310420493316 / Long term Validation loss: 0.1220\n",
      "Epoch: [2588/10000] Training loss: 5.468838356411984e-05 / Validation loss: 0.0005488848735074369 / Long term Validation loss: 0.1220\n",
      "Epoch: [2589/10000] Training loss: 5.465538845737433e-05 / Validation loss: 0.0005485389197647044 / Long term Validation loss: 0.1220\n",
      "Epoch: [2590/10000] Training loss: 5.462241465476146e-05 / Validation loss: 0.0005481931814338657 / Long term Validation loss: 0.1220\n",
      "Epoch: [2591/10000] Training loss: 5.4589462153398974e-05 / Validation loss: 0.0005478476579436248 / Long term Validation loss: 0.1220\n",
      "Epoch: [2592/10000] Training loss: 5.455653095038757e-05 / Validation loss: 0.0005475023499705563 / Long term Validation loss: 0.1220\n",
      "Epoch: [2593/10000] Training loss: 5.452362104281106e-05 / Validation loss: 0.0005471572570633923 / Long term Validation loss: 0.1220\n",
      "Epoch: [2594/10000] Training loss: 5.449073242773589e-05 / Validation loss: 0.000546812379631793 / Long term Validation loss: 0.1220\n",
      "Epoch: [2595/10000] Training loss: 5.4457865102211144e-05 / Validation loss: 0.0005464677175832358 / Long term Validation loss: 0.1220\n",
      "Epoch: [2596/10000] Training loss: 5.44250190632683e-05 / Validation loss: 0.0005461232709352248 / Long term Validation loss: 0.1220\n",
      "Epoch: [2597/10000] Training loss: 5.4392194307921e-05 / Validation loss: 0.0005457790399630594 / Long term Validation loss: 0.1220\n",
      "Epoch: [2598/10000] Training loss: 5.4359390833164917e-05 / Validation loss: 0.0005454350243851343 / Long term Validation loss: 0.1220\n",
      "Epoch: [2599/10000] Training loss: 5.4326608635977434e-05 / Validation loss: 0.000545091224675194 / Long term Validation loss: 0.1220\n",
      "Epoch: [2600/10000] Training loss: 5.4293847713317684e-05 / Validation loss: 0.0005447476404661197 / Long term Validation loss: 0.1220\n",
      "Epoch: [2601/10000] Training loss: 5.426110806212617e-05 / Validation loss: 0.0005444042722056163 / Long term Validation loss: 0.1220\n",
      "Epoch: [2602/10000] Training loss: 5.4228389679324565e-05 / Validation loss: 0.0005440611196469058 / Long term Validation loss: 0.1220\n",
      "Epoch: [2603/10000] Training loss: 5.419569256181565e-05 / Validation loss: 0.0005437181830473643 / Long term Validation loss: 0.1220\n",
      "Epoch: [2604/10000] Training loss: 5.416301670648311e-05 / Validation loss: 0.0005433754623890988 / Long term Validation loss: 0.1220\n",
      "Epoch: [2605/10000] Training loss: 5.413036211019118e-05 / Validation loss: 0.0005430329576915962 / Long term Validation loss: 0.1220\n",
      "Epoch: [2606/10000] Training loss: 5.409772876978466e-05 / Validation loss: 0.0005426906691551555 / Long term Validation loss: 0.1220\n",
      "Epoch: [2607/10000] Training loss: 5.4065116682088564e-05 / Validation loss: 0.0005423485966210013 / Long term Validation loss: 0.1220\n",
      "Epoch: [2608/10000] Training loss: 5.403252584390809e-05 / Validation loss: 0.0005420067404123384 / Long term Validation loss: 0.1220\n",
      "Epoch: [2609/10000] Training loss: 5.399995625202817e-05 / Validation loss: 0.0005416651003076676 / Long term Validation loss: 0.1220\n",
      "Epoch: [2610/10000] Training loss: 5.39674079032136e-05 / Validation loss: 0.0005413236766323602 / Long term Validation loss: 0.1220\n",
      "Epoch: [2611/10000] Training loss: 5.3934880794208635e-05 / Validation loss: 0.0005409824692148884 / Long term Validation loss: 0.1220\n",
      "Epoch: [2612/10000] Training loss: 5.390237492173684e-05 / Validation loss: 0.0005406414782884216 / Long term Validation loss: 0.1220\n",
      "Epoch: [2613/10000] Training loss: 5.3869890282501016e-05 / Validation loss: 0.0005403007038008077 / Long term Validation loss: 0.1220\n",
      "Epoch: [2614/10000] Training loss: 5.383742687318275e-05 / Validation loss: 0.0005399601458516784 / Long term Validation loss: 0.1220\n",
      "Epoch: [2615/10000] Training loss: 5.380498469044255e-05 / Validation loss: 0.0005396198045217849 / Long term Validation loss: 0.1220\n",
      "Epoch: [2616/10000] Training loss: 5.3772563730919366e-05 / Validation loss: 0.0005392796797886353 / Long term Validation loss: 0.1220\n",
      "Epoch: [2617/10000] Training loss: 5.374016399123059e-05 / Validation loss: 0.0005389397718342276 / Long term Validation loss: 0.1220\n",
      "Epoch: [2618/10000] Training loss: 5.3707785467971846e-05 / Validation loss: 0.0005386000805599839 / Long term Validation loss: 0.1220\n",
      "Epoch: [2619/10000] Training loss: 5.367542815771664e-05 / Validation loss: 0.0005382606061947479 / Long term Validation loss: 0.1220\n",
      "Epoch: [2620/10000] Training loss: 5.3643092057016396e-05 / Validation loss: 0.0005379213486207811 / Long term Validation loss: 0.1220\n",
      "Epoch: [2621/10000] Training loss: 5.36107771624001e-05 / Validation loss: 0.0005375823080592229 / Long term Validation loss: 0.1220\n",
      "Epoch: [2622/10000] Training loss: 5.3578483470374176e-05 / Validation loss: 0.0005372434844214417 / Long term Validation loss: 0.1220\n",
      "Epoch: [2623/10000] Training loss: 5.3546210977422285e-05 / Validation loss: 0.0005369048778815072 / Long term Validation loss: 0.1220\n",
      "Epoch: [2624/10000] Training loss: 5.35139596800052e-05 / Validation loss: 0.000536566488408917 / Long term Validation loss: 0.1220\n",
      "Epoch: [2625/10000] Training loss: 5.34817295745605e-05 / Validation loss: 0.000536228316112375 / Long term Validation loss: 0.1220\n",
      "Epoch: [2626/10000] Training loss: 5.344952065750244e-05 / Validation loss: 0.0005358903610275954 / Long term Validation loss: 0.1220\n",
      "Epoch: [2627/10000] Training loss: 5.3417332925221774e-05 / Validation loss: 0.0005355526231988461 / Long term Validation loss: 0.1220\n",
      "Epoch: [2628/10000] Training loss: 5.338516637408562e-05 / Validation loss: 0.0005352151027197233 / Long term Validation loss: 0.1220\n",
      "Epoch: [2629/10000] Training loss: 5.335302100043706e-05 / Validation loss: 0.0005348777995839354 / Long term Validation loss: 0.1220\n",
      "Epoch: [2630/10000] Training loss: 5.3320896800595215e-05 / Validation loss: 0.0005345407139253193 / Long term Validation loss: 0.1220\n",
      "Epoch: [2631/10000] Training loss: 5.328879377085494e-05 / Validation loss: 0.0005342038457066561 / Long term Validation loss: 0.1220\n",
      "Epoch: [2632/10000] Training loss: 5.3256711907486574e-05 / Validation loss: 0.0005338671950818562 / Long term Validation loss: 0.1220\n",
      "Epoch: [2633/10000] Training loss: 5.322465120673586e-05 / Validation loss: 0.0005335307620021833 / Long term Validation loss: 0.1220\n",
      "Epoch: [2634/10000] Training loss: 5.31926116648237e-05 / Validation loss: 0.0005331945466238571 / Long term Validation loss: 0.1220\n",
      "Epoch: [2635/10000] Training loss: 5.3160593277945975e-05 / Validation loss: 0.0005328585489020953 / Long term Validation loss: 0.1220\n",
      "Epoch: [2636/10000] Training loss: 5.3128596042273355e-05 / Validation loss: 0.0005325227689825767 / Long term Validation loss: 0.1220\n",
      "Epoch: [2637/10000] Training loss: 5.309661995395123e-05 / Validation loss: 0.0005321872068345661 / Long term Validation loss: 0.1220\n",
      "Epoch: [2638/10000] Training loss: 5.3064665009099185e-05 / Validation loss: 0.0005318518625857817 / Long term Validation loss: 0.1220\n",
      "Epoch: [2639/10000] Training loss: 5.30327312038113e-05 / Validation loss: 0.0005315167362244702 / Long term Validation loss: 0.1220\n",
      "Epoch: [2640/10000] Training loss: 5.300081853415547e-05 / Validation loss: 0.0005311818278576091 / Long term Validation loss: 0.1220\n",
      "Epoch: [2641/10000] Training loss: 5.29689269961737e-05 / Validation loss: 0.0005308471374933705 / Long term Validation loss: 0.1220\n",
      "Epoch: [2642/10000] Training loss: 5.293705658588151e-05 / Validation loss: 0.0005305126652184492 / Long term Validation loss: 0.1220\n",
      "Epoch: [2643/10000] Training loss: 5.2905207299267916e-05 / Validation loss: 0.0005301784110594036 / Long term Validation loss: 0.1220\n",
      "Epoch: [2644/10000] Training loss: 5.2873379132295295e-05 / Validation loss: 0.0005298443750848895 / Long term Validation loss: 0.1220\n",
      "Epoch: [2645/10000] Training loss: 5.284157208089919e-05 / Validation loss: 0.0005295105573371354 / Long term Validation loss: 0.1220\n",
      "Epoch: [2646/10000] Training loss: 5.2809786140987955e-05 / Validation loss: 0.0005291769578696573 / Long term Validation loss: 0.1220\n",
      "Epoch: [2647/10000] Training loss: 5.277802130844283e-05 / Validation loss: 0.0005288435767374516 / Long term Validation loss: 0.1220\n",
      "Epoch: [2648/10000] Training loss: 5.2746277579117575e-05 / Validation loss: 0.0005285104139815967 / Long term Validation loss: 0.1220\n",
      "Epoch: [2649/10000] Training loss: 5.2714554948838324e-05 / Validation loss: 0.0005281774696674525 / Long term Validation loss: 0.1220\n",
      "Epoch: [2650/10000] Training loss: 5.2682853413403444e-05 / Validation loss: 0.0005278447438256561 / Long term Validation loss: 0.1220\n",
      "Epoch: [2651/10000] Training loss: 5.26511729685833e-05 / Validation loss: 0.0005275122365303672 / Long term Validation loss: 0.1220\n",
      "Epoch: [2652/10000] Training loss: 5.261951361012014e-05 / Validation loss: 0.0005271799478028205 / Long term Validation loss: 0.1220\n",
      "Epoch: [2653/10000] Training loss: 5.258787533372777e-05 / Validation loss: 0.000526847877725465 / Long term Validation loss: 0.1220\n",
      "Epoch: [2654/10000] Training loss: 5.255625813509162e-05 / Validation loss: 0.0005265160263100371 / Long term Validation loss: 0.1220\n",
      "Epoch: [2655/10000] Training loss: 5.2524662009868264e-05 / Validation loss: 0.0005261843936479586 / Long term Validation loss: 0.1220\n",
      "Epoch: [2656/10000] Training loss: 5.24930869536855e-05 / Validation loss: 0.0005258529797400964 / Long term Validation loss: 0.1220\n",
      "Epoch: [2657/10000] Training loss: 5.2461532962141974e-05 / Validation loss: 0.0005255217846889293 / Long term Validation loss: 0.1220\n",
      "Epoch: [2658/10000] Training loss: 5.2430000030807115e-05 / Validation loss: 0.0005251908084815153 / Long term Validation loss: 0.1220\n",
      "Epoch: [2659/10000] Training loss: 5.239848815522094e-05 / Validation loss: 0.0005248600512352718 / Long term Validation loss: 0.1220\n",
      "Epoch: [2660/10000] Training loss: 5.236699733089386e-05 / Validation loss: 0.0005245295129184296 / Long term Validation loss: 0.1220\n",
      "Epoch: [2661/10000] Training loss: 5.233552755330638e-05 / Validation loss: 0.000524199193669665 / Long term Validation loss: 0.1220\n",
      "Epoch: [2662/10000] Training loss: 5.230407881790922e-05 / Validation loss: 0.0005238690934304447 / Long term Validation loss: 0.1220\n",
      "Epoch: [2663/10000] Training loss: 5.227265112012279e-05 / Validation loss: 0.0005235392123706082 / Long term Validation loss: 0.1220\n",
      "Epoch: [2664/10000] Training loss: 5.224124445533731e-05 / Validation loss: 0.000523209550392443 / Long term Validation loss: 0.1220\n",
      "Epoch: [2665/10000] Training loss: 5.220985881891232e-05 / Validation loss: 0.0005228801077124561 / Long term Validation loss: 0.1220\n",
      "Epoch: [2666/10000] Training loss: 5.217849420617686e-05 / Validation loss: 0.0005225508841742967 / Long term Validation loss: 0.1220\n",
      "Epoch: [2667/10000] Training loss: 5.2147150612428935e-05 / Validation loss: 0.0005222218800655674 / Long term Validation loss: 0.1220\n",
      "Epoch: [2668/10000] Training loss: 5.2115828032935714e-05 / Validation loss: 0.0005218930951404562 / Long term Validation loss: 0.1220\n",
      "Epoch: [2669/10000] Training loss: 5.208452646293297e-05 / Validation loss: 0.00052156452979663 / Long term Validation loss: 0.1220\n",
      "Epoch: [2670/10000] Training loss: 5.205324589762517e-05 / Validation loss: 0.0005212361836492943 / Long term Validation loss: 0.1220\n",
      "Epoch: [2671/10000] Training loss: 5.20219863321852e-05 / Validation loss: 0.0005209080572693028 / Long term Validation loss: 0.1220\n",
      "Epoch: [2672/10000] Training loss: 5.199074776175421e-05 / Validation loss: 0.0005205801500520737 / Long term Validation loss: 0.1220\n",
      "Epoch: [2673/10000] Training loss: 5.19595301814414e-05 / Validation loss: 0.0005202524628454201 / Long term Validation loss: 0.1220\n",
      "Epoch: [2674/10000] Training loss: 5.192833358632402e-05 / Validation loss: 0.0005199249946911163 / Long term Validation loss: 0.1220\n",
      "Epoch: [2675/10000] Training loss: 5.189715797144682e-05 / Validation loss: 0.0005195977468872367 / Long term Validation loss: 0.1220\n",
      "Epoch: [2676/10000] Training loss: 5.186600333182235e-05 / Validation loss: 0.0005192707178965901 / Long term Validation loss: 0.1220\n",
      "Epoch: [2677/10000] Training loss: 5.1834869662430305e-05 / Validation loss: 0.0005189439097615334 / Long term Validation loss: 0.1220\n",
      "Epoch: [2678/10000] Training loss: 5.180375695821794e-05 / Validation loss: 0.0005186173199807105 / Long term Validation loss: 0.1220\n",
      "Epoch: [2679/10000] Training loss: 5.1772665214099155e-05 / Validation loss: 0.0005182909518472741 / Long term Validation loss: 0.1220\n",
      "Epoch: [2680/10000] Training loss: 5.174159442495523e-05 / Validation loss: 0.0005179648012270744 / Long term Validation loss: 0.1220\n",
      "Epoch: [2681/10000] Training loss: 5.171054458563342e-05 / Validation loss: 0.0005176388735499621 / Long term Validation loss: 0.1220\n",
      "Epoch: [2682/10000] Training loss: 5.1679515690948566e-05 / Validation loss: 0.0005173131618707448 / Long term Validation loss: 0.1220\n",
      "Epoch: [2683/10000] Training loss: 5.164850773568048e-05 / Validation loss: 0.0005169876753288274 / Long term Validation loss: 0.1220\n",
      "Epoch: [2684/10000] Training loss: 5.161752071457698e-05 / Validation loss: 0.0005166624020604974 / Long term Validation loss: 0.1220\n",
      "Epoch: [2685/10000] Training loss: 5.15865546223496e-05 / Validation loss: 0.0005163373577488795 / Long term Validation loss: 0.1220\n",
      "Epoch: [2686/10000] Training loss: 5.155560945367879e-05 / Validation loss: 0.0005160125217863049 / Long term Validation loss: 0.1220\n",
      "Epoch: [2687/10000] Training loss: 5.152468520320665e-05 / Validation loss: 0.0005156879215817322 / Long term Validation loss: 0.1220\n",
      "Epoch: [2688/10000] Training loss: 5.149378186554612e-05 / Validation loss: 0.000515363520738176 / Long term Validation loss: 0.1220\n",
      "Epoch: [2689/10000] Training loss: 5.146289943526858e-05 / Validation loss: 0.0005150393680032901 / Long term Validation loss: 0.1220\n",
      "Epoch: [2690/10000] Training loss: 5.1432037906919904e-05 / Validation loss: 0.0005147153980278974 / Long term Validation loss: 0.1220\n",
      "Epoch: [2691/10000] Training loss: 5.140119727499847e-05 / Validation loss: 0.0005143916989860491 / Long term Validation loss: 0.1220\n",
      "Epoch: [2692/10000] Training loss: 5.137037753398456e-05 / Validation loss: 0.0005140681516336922 / Long term Validation loss: 0.1220\n",
      "Epoch: [2693/10000] Training loss: 5.133957867830042e-05 / Validation loss: 0.0005137449180870856 / Long term Validation loss: 0.1220\n",
      "Epoch: [2694/10000] Training loss: 5.13088007023639e-05 / Validation loss: 0.0005134217772781878 / Long term Validation loss: 0.1220\n",
      "Epoch: [2695/10000] Training loss: 5.1278043600515626e-05 / Validation loss: 0.000513099032050018 / Long term Validation loss: 0.1220\n",
      "Epoch: [2696/10000] Training loss: 5.1247307367118566e-05 / Validation loss: 0.0005127762661337583 / Long term Validation loss: 0.1220\n",
      "Epoch: [2697/10000] Training loss: 5.121659199642263e-05 / Validation loss: 0.0005124540541012301 / Long term Validation loss: 0.1220\n",
      "Epoch: [2698/10000] Training loss: 5.1185897482752455e-05 / Validation loss: 0.0005121316000730183 / Long term Validation loss: 0.1220\n",
      "Epoch: [2699/10000] Training loss: 5.115522382025331e-05 / Validation loss: 0.0005118100108134391 / Long term Validation loss: 0.1220\n",
      "Epoch: [2700/10000] Training loss: 5.112457100325183e-05 / Validation loss: 0.0005114877417206378 / Long term Validation loss: 0.1220\n",
      "Epoch: [2701/10000] Training loss: 5.109393902577454e-05 / Validation loss: 0.0005111669565677531 / Long term Validation loss: 0.1220\n",
      "Epoch: [2702/10000] Training loss: 5.1063327882259845e-05 / Validation loss: 0.0005108446133687512 / Long term Validation loss: 0.1220\n",
      "Epoch: [2703/10000] Training loss: 5.103273756664518e-05 / Validation loss: 0.0005105250043866516 / Long term Validation loss: 0.1220\n",
      "Epoch: [2704/10000] Training loss: 5.100216807382334e-05 / Validation loss: 0.0005102020517642234 / Long term Validation loss: 0.1220\n",
      "Epoch: [2705/10000] Training loss: 5.0971619397954534e-05 / Validation loss: 0.0005098843924413313 / Long term Validation loss: 0.1220\n",
      "Epoch: [2706/10000] Training loss: 5.0941091535620005e-05 / Validation loss: 0.0005095597100279188 / Long term Validation loss: 0.1220\n",
      "Epoch: [2707/10000] Training loss: 5.091058448296729e-05 / Validation loss: 0.0005092456291831334 / Long term Validation loss: 0.1220\n",
      "Epoch: [2708/10000] Training loss: 5.088009824322758e-05 / Validation loss: 0.0005089168424377923 / Long term Validation loss: 0.1220\n",
      "Epoch: [2709/10000] Training loss: 5.084963282336267e-05 / Validation loss: 0.0005086098137277105 / Long term Validation loss: 0.1220\n",
      "Epoch: [2710/10000] Training loss: 5.081918825480156e-05 / Validation loss: 0.000508271826827141 / Long term Validation loss: 0.1220\n",
      "Epoch: [2711/10000] Training loss: 5.078876459813174e-05 / Validation loss: 0.0005079793513443608 / Long term Validation loss: 0.1220\n",
      "Epoch: [2712/10000] Training loss: 5.0758362011653724e-05 / Validation loss: 0.0005076210929610112 / Long term Validation loss: 0.1220\n",
      "Epoch: [2713/10000] Training loss: 5.072798081737694e-05 / Validation loss: 0.0005073595701129039 / Long term Validation loss: 0.1220\n",
      "Epoch: [2714/10000] Training loss: 5.0697621770085836e-05 / Validation loss: 0.0005069566924941783 / Long term Validation loss: 0.1220\n",
      "Epoch: [2715/10000] Training loss: 5.066728647566033e-05 / Validation loss: 0.0005067624183269089 / Long term Validation loss: 0.1220\n",
      "Epoch: [2716/10000] Training loss: 5.06369785904723e-05 / Validation loss: 0.0005062607347672029 / Long term Validation loss: 0.1220\n",
      "Epoch: [2717/10000] Training loss: 5.060670613260034e-05 / Validation loss: 0.0005062150327792743 / Long term Validation loss: 0.1220\n",
      "Epoch: [2718/10000] Training loss: 5.057648728412294e-05 / Validation loss: 0.000505492545634927 / Long term Validation loss: 0.1219\n",
      "Epoch: [2719/10000] Training loss: 5.054636275794463e-05 / Validation loss: 0.0005057799071626886 / Long term Validation loss: 0.1221\n",
      "Epoch: [2720/10000] Training loss: 5.0516425363054033e-05 / Validation loss: 0.0005045589234869835 / Long term Validation loss: 0.1218\n",
      "Epoch: [2721/10000] Training loss: 5.04868862682365e-05 / Validation loss: 0.0005056033799990214 / Long term Validation loss: 0.1222\n",
      "Epoch: [2722/10000] Training loss: 5.045823117040637e-05 / Validation loss: 0.0005032456778286995 / Long term Validation loss: 0.1216\n",
      "Epoch: [2723/10000] Training loss: 5.043158037383044e-05 / Validation loss: 0.0005060361416793574 / Long term Validation loss: 0.1225\n",
      "Epoch: [2724/10000] Training loss: 5.0409537152364226e-05 / Validation loss: 0.0005010650434179808 / Long term Validation loss: 0.1210\n",
      "Epoch: [2725/10000] Training loss: 5.0398181642958366e-05 / Validation loss: 0.0005079504098798424 / Long term Validation loss: 0.1233\n",
      "Epoch: [2726/10000] Training loss: 5.041178677694929e-05 / Validation loss: 0.0004969514753928706 / Long term Validation loss: 0.1195\n",
      "Epoch: [2727/10000] Training loss: 5.048406762477008e-05 / Validation loss: 0.0005136599957284078 / Long term Validation loss: 0.1255\n",
      "Epoch: [2728/10000] Training loss: 5.0694776110279186e-05 / Validation loss: 0.0004888981960757484 / Long term Validation loss: 0.1163\n",
      "Epoch: [2729/10000] Training loss: 5.1233574249152674e-05 / Validation loss: 0.0005299728225658393 / Long term Validation loss: 0.1303\n",
      "Epoch: [2730/10000] Training loss: 5.2546612753812874e-05 / Validation loss: 0.0004752740330898611 / Long term Validation loss: 0.1092\n",
      "Epoch: [2731/10000] Training loss: 5.567396016481528e-05 / Validation loss: 0.0005790815481863034 / Long term Validation loss: 0.1294\n",
      "Epoch: [2732/10000] Training loss: 6.287595483848725e-05 / Validation loss: 0.0004683954348900301 / Long term Validation loss: 0.1003\n",
      "Epoch: [2733/10000] Training loss: 7.848914199667836e-05 / Validation loss: 0.0007180291281517466 / Long term Validation loss: 0.1406\n",
      "Epoch: [2734/10000] Training loss: 0.00010716199094463436 / Validation loss: 0.0005129043370765205 / Long term Validation loss: 0.1043\n",
      "Epoch: [2735/10000] Training loss: 0.00014292787243917575 / Validation loss: 0.0008263087998019108 / Long term Validation loss: 0.1400\n",
      "Epoch: [2736/10000] Training loss: 0.00014797893412980744 / Validation loss: 0.00047509223438265563 / Long term Validation loss: 0.0997\n",
      "Epoch: [2737/10000] Training loss: 9.618720992024046e-05 / Validation loss: 0.000516265512559958 / Long term Validation loss: 0.1282\n",
      "Epoch: [2738/10000] Training loss: 5.080019070799776e-05 / Validation loss: 0.0006189631711967977 / Long term Validation loss: 0.1333\n",
      "Epoch: [2739/10000] Training loss: 7.379643851399594e-05 / Validation loss: 0.0004795563056010525 / Long term Validation loss: 0.0995\n",
      "Epoch: [2740/10000] Training loss: 0.00010574470260218072 / Validation loss: 0.0006361311375865204 / Long term Validation loss: 0.1353\n",
      "Epoch: [2741/10000] Training loss: 7.907394151276339e-05 / Validation loss: 0.0004981551872445838 / Long term Validation loss: 0.1226\n",
      "Epoch: [2742/10000] Training loss: 4.995199068224091e-05 / Validation loss: 0.00046326727706887393 / Long term Validation loss: 0.0993\n",
      "Epoch: [2743/10000] Training loss: 7.226362521458736e-05 / Validation loss: 0.0006512873951407709 / Long term Validation loss: 0.1368\n",
      "Epoch: [2744/10000] Training loss: 8.397052920598373e-05 / Validation loss: 0.00046875867374250303 / Long term Validation loss: 0.1064\n",
      "Epoch: [2745/10000] Training loss: 5.712492258748195e-05 / Validation loss: 0.00047290824042858705 / Long term Validation loss: 0.1100\n",
      "Epoch: [2746/10000] Training loss: 5.436869106542745e-05 / Validation loss: 0.0006175681195383555 / Long term Validation loss: 0.1331\n",
      "Epoch: [2747/10000] Training loss: 7.351794761979736e-05 / Validation loss: 0.00046375600884117834 / Long term Validation loss: 0.1016\n",
      "Epoch: [2748/10000] Training loss: 6.299947602836588e-05 / Validation loss: 0.0004938577284212776 / Long term Validation loss: 0.1218\n",
      "Epoch: [2749/10000] Training loss: 4.99079077343522e-05 / Validation loss: 0.000579123537142401 / Long term Validation loss: 0.1283\n",
      "Epoch: [2750/10000] Training loss: 6.288985108622083e-05 / Validation loss: 0.0004633133097634084 / Long term Validation loss: 0.1013\n",
      "Epoch: [2751/10000] Training loss: 6.353045100465232e-05 / Validation loss: 0.000511935156802024 / Long term Validation loss: 0.1284\n",
      "Epoch: [2752/10000] Training loss: 5.036183339636264e-05 / Validation loss: 0.0005482453530234563 / Long term Validation loss: 0.1292\n",
      "Epoch: [2753/10000] Training loss: 5.5891696314743716e-05 / Validation loss: 0.0004638878530460643 / Long term Validation loss: 0.1026\n",
      "Epoch: [2754/10000] Training loss: 6.123147210692086e-05 / Validation loss: 0.0005231124172745132 / Long term Validation loss: 0.1303\n",
      "Epoch: [2755/10000] Training loss: 5.164597683985748e-05 / Validation loss: 0.0005258221127191995 / Long term Validation loss: 0.1304\n",
      "Epoch: [2756/10000] Training loss: 5.205673235793425e-05 / Validation loss: 0.00046548571535444757 / Long term Validation loss: 0.1050\n",
      "Epoch: [2757/10000] Training loss: 5.817537148017541e-05 / Validation loss: 0.0005284463613372734 / Long term Validation loss: 0.1305\n",
      "Epoch: [2758/10000] Training loss: 5.250809354444356e-05 / Validation loss: 0.0005104085211297479 / Long term Validation loss: 0.1279\n",
      "Epoch: [2759/10000] Training loss: 5.0246733621207436e-05 / Validation loss: 0.0004684291901341168 / Long term Validation loss: 0.1081\n",
      "Epoch: [2760/10000] Training loss: 5.535153893646593e-05 / Validation loss: 0.0005298285810706413 / Long term Validation loss: 0.1306\n",
      "Epoch: [2761/10000] Training loss: 5.277258602897022e-05 / Validation loss: 0.0005001133457998264 / Long term Validation loss: 0.1240\n",
      "Epoch: [2762/10000] Training loss: 4.956804805537349e-05 / Validation loss: 0.0004723248933672449 / Long term Validation loss: 0.1113\n",
      "Epoch: [2763/10000] Training loss: 5.3101757032552075e-05 / Validation loss: 0.0005284033105008197 / Long term Validation loss: 0.1305\n",
      "Epoch: [2764/10000] Training loss: 5.257394307596152e-05 / Validation loss: 0.0004930512897510978 / Long term Validation loss: 0.1217\n",
      "Epoch: [2765/10000] Training loss: 4.944984436715896e-05 / Validation loss: 0.00047641678647571806 / Long term Validation loss: 0.1139\n",
      "Epoch: [2766/10000] Training loss: 5.146639233868954e-05 / Validation loss: 0.0005249243923813891 / Long term Validation loss: 0.1303\n",
      "Epoch: [2767/10000] Training loss: 5.209091283616509e-05 / Validation loss: 0.00048801130515839286 / Long term Validation loss: 0.1196\n",
      "Epoch: [2768/10000] Training loss: 4.955556361756878e-05 / Validation loss: 0.00048037482793147847 / Long term Validation loss: 0.1160\n",
      "Epoch: [2769/10000] Training loss: 5.0366751387148775e-05 / Validation loss: 0.0005203016742262 / Long term Validation loss: 0.1299\n",
      "Epoch: [2770/10000] Training loss: 5.1474765123244034e-05 / Validation loss: 0.0004845848068820212 / Long term Validation loss: 0.1183\n",
      "Epoch: [2771/10000] Training loss: 4.9699586828964606e-05 / Validation loss: 0.00048425673991433804 / Long term Validation loss: 0.1182\n",
      "Epoch: [2772/10000] Training loss: 4.968804342922153e-05 / Validation loss: 0.0005153518268759486 / Long term Validation loss: 0.1292\n",
      "Epoch: [2773/10000] Training loss: 5.0836644334508585e-05 / Validation loss: 0.00048253947151957033 / Long term Validation loss: 0.1175\n",
      "Epoch: [2774/10000] Training loss: 4.9789138525581706e-05 / Validation loss: 0.0004879271571729222 / Long term Validation loss: 0.1200\n",
      "Epoch: [2775/10000] Training loss: 4.931304130509799e-05 / Validation loss: 0.0005103982125508649 / Long term Validation loss: 0.1283\n",
      "Epoch: [2776/10000] Training loss: 5.025115055955428e-05 / Validation loss: 0.0004814232626947099 / Long term Validation loss: 0.1171\n",
      "Epoch: [2777/10000] Training loss: 4.979013683057025e-05 / Validation loss: 0.0004909889022199461 / Long term Validation loss: 0.1216\n",
      "Epoch: [2778/10000] Training loss: 4.913696637626713e-05 / Validation loss: 0.0005054745868314952 / Long term Validation loss: 0.1268\n",
      "Epoch: [2779/10000] Training loss: 4.976137845298646e-05 / Validation loss: 0.0004808432554148845 / Long term Validation loss: 0.1172\n",
      "Epoch: [2780/10000] Training loss: 4.9703874681670255e-05 / Validation loss: 0.0004932120805858675 / Long term Validation loss: 0.1225\n",
      "Epoch: [2781/10000] Training loss: 4.907340784531515e-05 / Validation loss: 0.0005007513579299673 / Long term Validation loss: 0.1251\n",
      "Epoch: [2782/10000] Training loss: 4.9384269874384125e-05 / Validation loss: 0.0004807325343147923 / Long term Validation loss: 0.1175\n",
      "Epoch: [2783/10000] Training loss: 4.95513245695386e-05 / Validation loss: 0.000494670733703181 / Long term Validation loss: 0.1231\n",
      "Epoch: [2784/10000] Training loss: 4.905729165434477e-05 / Validation loss: 0.0004965167146818909 / Long term Validation loss: 0.1239\n",
      "Epoch: [2785/10000] Training loss: 4.911674091813087e-05 / Validation loss: 0.0004811263660691847 / Long term Validation loss: 0.1180\n",
      "Epoch: [2786/10000] Training loss: 4.9361449021847106e-05 / Validation loss: 0.0004954535869766932 / Long term Validation loss: 0.1237\n",
      "Epoch: [2787/10000] Training loss: 4.904461190548709e-05 / Validation loss: 0.000492884897501746 / Long term Validation loss: 0.1228\n",
      "Epoch: [2788/10000] Training loss: 4.8941631986306174e-05 / Validation loss: 0.000481883555755297 / Long term Validation loss: 0.1187\n",
      "Epoch: [2789/10000] Training loss: 4.916298555463408e-05 / Validation loss: 0.0004955108868811117 / Long term Validation loss: 0.1240\n",
      "Epoch: [2790/10000] Training loss: 4.90109339363996e-05 / Validation loss: 0.0004897780927852253 / Long term Validation loss: 0.1222\n",
      "Epoch: [2791/10000] Training loss: 4.883411171880402e-05 / Validation loss: 0.0004827733214288059 / Long term Validation loss: 0.1194\n",
      "Epoch: [2792/10000] Training loss: 4.8978870518494194e-05 / Validation loss: 0.000494829146853811 / Long term Validation loss: 0.1240\n",
      "Epoch: [2793/10000] Training loss: 4.89480026132059e-05 / Validation loss: 0.0004871513318097083 / Long term Validation loss: 0.1215\n",
      "Epoch: [2794/10000] Training loss: 4.876742630998615e-05 / Validation loss: 0.00048367575228587035 / Long term Validation loss: 0.1201\n",
      "Epoch: [2795/10000] Training loss: 4.882314478179528e-05 / Validation loss: 0.0004935638491579652 / Long term Validation loss: 0.1238\n",
      "Epoch: [2796/10000] Training loss: 4.8859309451741095e-05 / Validation loss: 0.00048505875538782663 / Long term Validation loss: 0.1209\n",
      "Epoch: [2797/10000] Training loss: 4.8717919819241364e-05 / Validation loss: 0.00048455391080037396 / Long term Validation loss: 0.1208\n",
      "Epoch: [2798/10000] Training loss: 4.8700408827493057e-05 / Validation loss: 0.000491927971178009 / Long term Validation loss: 0.1234\n",
      "Epoch: [2799/10000] Training loss: 4.875500278845121e-05 / Validation loss: 0.000483516688037597 / Long term Validation loss: 0.1206\n",
      "Epoch: [2800/10000] Training loss: 4.8668296621621256e-05 / Validation loss: 0.0004853155331540031 / Long term Validation loss: 0.1214\n",
      "Epoch: [2801/10000] Training loss: 4.8607193373349584e-05 / Validation loss: 0.000490061411033185 / Long term Validation loss: 0.1229\n",
      "Epoch: [2802/10000] Training loss: 4.8647017562097376e-05 / Validation loss: 0.0004824390528088814 / Long term Validation loss: 0.1204\n",
      "Epoch: [2803/10000] Training loss: 4.8609100107567996e-05 / Validation loss: 0.0004858162971817843 / Long term Validation loss: 0.1218\n",
      "Epoch: [2804/10000] Training loss: 4.853487338363615e-05 / Validation loss: 0.0004880621778213081 / Long term Validation loss: 0.1225\n",
      "Epoch: [2805/10000] Training loss: 4.854518978848423e-05 / Validation loss: 0.000481721130764273 / Long term Validation loss: 0.1204\n",
      "Epoch: [2806/10000] Training loss: 4.853818980009896e-05 / Validation loss: 0.0004859659434062817 / Long term Validation loss: 0.1221\n",
      "Epoch: [2807/10000] Training loss: 4.847311553696193e-05 / Validation loss: 0.0004860616730438092 / Long term Validation loss: 0.1221\n",
      "Epoch: [2808/10000] Training loss: 4.845493834749468e-05 / Validation loss: 0.00048130125292897876 / Long term Validation loss: 0.1206\n",
      "Epoch: [2809/10000] Training loss: 4.845867837027963e-05 / Validation loss: 0.0004857605974897112 / Long term Validation loss: 0.1222\n",
      "Epoch: [2810/10000] Training loss: 4.841304656654712e-05 / Validation loss: 0.00048420373526319776 / Long term Validation loss: 0.1218\n",
      "Epoch: [2811/10000] Training loss: 4.837684877819032e-05 / Validation loss: 0.00048112120680145366 / Long term Validation loss: 0.1208\n",
      "Epoch: [2812/10000] Training loss: 4.8376059572872526e-05 / Validation loss: 0.0004852287287101002 / Long term Validation loss: 0.1222\n",
      "Epoch: [2813/10000] Training loss: 4.834924071561157e-05 / Validation loss: 0.00048257868488477915 / Long term Validation loss: 0.1215\n",
      "Epoch: [2814/10000] Training loss: 4.8307823488336674e-05 / Validation loss: 0.00048108137040066545 / Long term Validation loss: 0.1211\n",
      "Epoch: [2815/10000] Training loss: 4.8295456555822944e-05 / Validation loss: 0.00048439951648875985 / Long term Validation loss: 0.1222\n",
      "Epoch: [2816/10000] Training loss: 4.828020065970516e-05 / Validation loss: 0.0004812140446134082 / Long term Validation loss: 0.1213\n",
      "Epoch: [2817/10000] Training loss: 4.824317639879894e-05 / Validation loss: 0.00048106225789219077 / Long term Validation loss: 0.1213\n",
      "Epoch: [2818/10000] Training loss: 4.821980951881507e-05 / Validation loss: 0.00048332532418641713 / Long term Validation loss: 0.1221\n",
      "Epoch: [2819/10000] Training loss: 4.8207435254927465e-05 / Validation loss: 0.00048011207023621597 / Long term Validation loss: 0.1212\n",
      "Epoch: [2820/10000] Training loss: 4.8178726289573654e-05 / Validation loss: 0.00048096956116802197 / Long term Validation loss: 0.1215\n",
      "Epoch: [2821/10000] Training loss: 4.814937857658938e-05 / Validation loss: 0.00048209815655020653 / Long term Validation loss: 0.1219\n",
      "Epoch: [2822/10000] Training loss: 4.813375364804911e-05 / Validation loss: 0.0004792673249943063 / Long term Validation loss: 0.1211\n",
      "Epoch: [2823/10000] Training loss: 4.81121537931123e-05 / Validation loss: 0.00048074525015776064 / Long term Validation loss: 0.1217\n",
      "Epoch: [2824/10000] Training loss: 4.808245533042376e-05 / Validation loss: 0.0004808251059192834 / Long term Validation loss: 0.1218\n",
      "Epoch: [2825/10000] Training loss: 4.8061584511034917e-05 / Validation loss: 0.00047865088864110266 / Long term Validation loss: 0.1212\n",
      "Epoch: [2826/10000] Training loss: 4.8043241837883575e-05 / Validation loss: 0.0004803520970508194 / Long term Validation loss: 0.1218\n",
      "Epoch: [2827/10000] Training loss: 4.801668529531798e-05 / Validation loss: 0.00047959292165531537 / Long term Validation loss: 0.1216\n",
      "Epoch: [2828/10000] Training loss: 4.799197786199796e-05 / Validation loss: 0.0004781992784555239 / Long term Validation loss: 0.1213\n",
      "Epoch: [2829/10000] Training loss: 4.797317716150567e-05 / Validation loss: 0.00047976923197417653 / Long term Validation loss: 0.1218\n",
      "Epoch: [2830/10000] Training loss: 4.795029999555717e-05 / Validation loss: 0.000478458003935796 / Long term Validation loss: 0.1215\n",
      "Epoch: [2831/10000] Training loss: 4.792454643525576e-05 / Validation loss: 0.0004778309532926801 / Long term Validation loss: 0.1214\n",
      "Epoch: [2832/10000] Training loss: 4.790344586357024e-05 / Validation loss: 0.00047900595123650294 / Long term Validation loss: 0.1218\n",
      "Epoch: [2833/10000] Training loss: 4.788271123720489e-05 / Validation loss: 0.00047745696140447666 / Long term Validation loss: 0.1214\n",
      "Epoch: [2834/10000] Training loss: 4.785811788087194e-05 / Validation loss: 0.00047747229167785076 / Long term Validation loss: 0.1215\n",
      "Epoch: [2835/10000] Training loss: 4.78349443725147e-05 / Validation loss: 0.00047810800120664734 / Long term Validation loss: 0.1218\n",
      "Epoch: [2836/10000] Training loss: 4.7814354756182235e-05 / Validation loss: 0.0004766101190501727 / Long term Validation loss: 0.1214\n",
      "Epoch: [2837/10000] Training loss: 4.77915899419863e-05 / Validation loss: 0.00047706712074099675 / Long term Validation loss: 0.1216\n",
      "Epoch: [2838/10000] Training loss: 4.7767685674618346e-05 / Validation loss: 0.0004771403283555079 / Long term Validation loss: 0.1217\n",
      "Epoch: [2839/10000] Training loss: 4.7746058951671296e-05 / Validation loss: 0.00047591116888455915 / Long term Validation loss: 0.1214\n",
      "Epoch: [2840/10000] Training loss: 4.7724482127347444e-05 / Validation loss: 0.00047657254782325527 / Long term Validation loss: 0.1217\n",
      "Epoch: [2841/10000] Training loss: 4.7701086182344934e-05 / Validation loss: 0.00047616301343257736 / Long term Validation loss: 0.1216\n",
      "Epoch: [2842/10000] Training loss: 4.7678410101581e-05 / Validation loss: 0.0004753234340552756 / Long term Validation loss: 0.1214\n",
      "Epoch: [2843/10000] Training loss: 4.7656967840013474e-05 / Validation loss: 0.0004759618230123624 / Long term Validation loss: 0.1217\n",
      "Epoch: [2844/10000] Training loss: 4.763450748589418e-05 / Validation loss: 0.0004752226131639657 / Long term Validation loss: 0.1216\n",
      "Epoch: [2845/10000] Training loss: 4.761147222395437e-05 / Validation loss: 0.00047479492885541344 / Long term Validation loss: 0.1215\n",
      "Epoch: [2846/10000] Training loss: 4.7589513208336066e-05 / Validation loss: 0.0004752359225262679 / Long term Validation loss: 0.1217\n",
      "Epoch: [2847/10000] Training loss: 4.7567656690391336e-05 / Validation loss: 0.00047435444417391197 / Long term Validation loss: 0.1215\n",
      "Epoch: [2848/10000] Training loss: 4.7544925128658444e-05 / Validation loss: 0.0004742768338716532 / Long term Validation loss: 0.1216\n",
      "Epoch: [2849/10000] Training loss: 4.7522458505442086e-05 / Validation loss: 0.0004744245753967374 / Long term Validation loss: 0.1217\n",
      "Epoch: [2850/10000] Training loss: 4.750063816819484e-05 / Validation loss: 0.000473578323547408 / Long term Validation loss: 0.1215\n",
      "Epoch: [2851/10000] Training loss: 4.747840804946552e-05 / Validation loss: 0.00047372928447332314 / Long term Validation loss: 0.1216\n",
      "Epoch: [2852/10000] Training loss: 4.7455826359259265e-05 / Validation loss: 0.00047357055348116946 / Long term Validation loss: 0.1216\n",
      "Epoch: [2853/10000] Training loss: 4.7433725431558375e-05 / Validation loss: 0.00047288964906228145 / Long term Validation loss: 0.1215\n",
      "Epoch: [2854/10000] Training loss: 4.741177688691005e-05 / Validation loss: 0.0004731217415444759 / Long term Validation loss: 0.1216\n",
      "Epoch: [2855/10000] Training loss: 4.738941709780811e-05 / Validation loss: 0.0004727138824089072 / Long term Validation loss: 0.1216\n",
      "Epoch: [2856/10000] Training loss: 4.736709524100123e-05 / Validation loss: 0.00047226168574869364 / Long term Validation loss: 0.1215\n",
      "Epoch: [2857/10000] Training loss: 4.7345121408693444e-05 / Validation loss: 0.0004724398449943985 / Long term Validation loss: 0.1217\n",
      "Epoch: [2858/10000] Training loss: 4.732303396232338e-05 / Validation loss: 0.0004718870307043477 / Long term Validation loss: 0.1216\n",
      "Epoch: [2859/10000] Training loss: 4.7300726223750465e-05 / Validation loss: 0.00047165962403603577 / Long term Validation loss: 0.1216\n",
      "Epoch: [2860/10000] Training loss: 4.727860360502029e-05 / Validation loss: 0.0004716920752591689 / Long term Validation loss: 0.1217\n",
      "Epoch: [2861/10000] Training loss: 4.725662934397953e-05 / Validation loss: 0.0004711127578488094 / Long term Validation loss: 0.1216\n",
      "Epoch: [2862/10000] Training loss: 4.7234486692147326e-05 / Validation loss: 0.00047105192493826 / Long term Validation loss: 0.1216\n",
      "Epoch: [2863/10000] Training loss: 4.72122977152326e-05 / Validation loss: 0.0004709043585443646 / Long term Validation loss: 0.1216\n",
      "Epoch: [2864/10000] Training loss: 4.719028283766488e-05 / Validation loss: 0.00047039780313070636 / Long term Validation loss: 0.1216\n",
      "Epoch: [2865/10000] Training loss: 4.7168281190541236e-05 / Validation loss: 0.00047041396702064735 / Long term Validation loss: 0.1216\n",
      "Epoch: [2866/10000] Training loss: 4.7146159183311605e-05 / Validation loss: 0.00047010688348794846 / Long term Validation loss: 0.1216\n",
      "Epoch: [2867/10000] Training loss: 4.712408276371128e-05 / Validation loss: 0.00046973010672701803 / Long term Validation loss: 0.1216\n",
      "Epoch: [2868/10000] Training loss: 4.7102115485203476e-05 / Validation loss: 0.00046973151602662034 / Long term Validation loss: 0.1217\n",
      "Epoch: [2869/10000] Training loss: 4.708010659820049e-05 / Validation loss: 0.0004693251079608872 / Long term Validation loss: 0.1216\n",
      "Epoch: [2870/10000] Training loss: 4.705804325876931e-05 / Validation loss: 0.00046908607196151044 / Long term Validation loss: 0.1216\n",
      "Epoch: [2871/10000] Training loss: 4.7036050369015736e-05 / Validation loss: 0.0004690056928164836 / Long term Validation loss: 0.1217\n",
      "Epoch: [2872/10000] Training loss: 4.701410742169737e-05 / Validation loss: 0.0004685763849127906 / Long term Validation loss: 0.1216\n",
      "Epoch: [2873/10000] Training loss: 4.699211842240798e-05 / Validation loss: 0.0004684414959588629 / Long term Validation loss: 0.1216\n",
      "Epoch: [2874/10000] Training loss: 4.697012251905884e-05 / Validation loss: 0.0004682521839810675 / Long term Validation loss: 0.1217\n",
      "Epoch: [2875/10000] Training loss: 4.694818804106482e-05 / Validation loss: 0.00046786683060980717 / Long term Validation loss: 0.1216\n",
      "Epoch: [2876/10000] Training loss: 4.6926267954874236e-05 / Validation loss: 0.0004677781516164983 / Long term Validation loss: 0.1217\n",
      "Epoch: [2877/10000] Training loss: 4.690431740834739e-05 / Validation loss: 0.0004674927934836202 / Long term Validation loss: 0.1217\n",
      "Epoch: [2878/10000] Training loss: 4.688238388408469e-05 / Validation loss: 0.0004671892787478073 / Long term Validation loss: 0.1216\n",
      "Epoch: [2879/10000] Training loss: 4.686049362794928e-05 / Validation loss: 0.0004670866413963729 / Long term Validation loss: 0.1217\n",
      "Epoch: [2880/10000] Training loss: 4.683860288991699e-05 / Validation loss: 0.0004667462966602945 / Long term Validation loss: 0.1217\n",
      "Epoch: [2881/10000] Training loss: 4.681669911770822e-05 / Validation loss: 0.0004665269973773336 / Long term Validation loss: 0.1217\n",
      "Epoch: [2882/10000] Training loss: 4.679482005537485e-05 / Validation loss: 0.00046636832581112877 / Long term Validation loss: 0.1217\n",
      "Epoch: [2883/10000] Training loss: 4.6772968643095684e-05 / Validation loss: 0.0004660237076024181 / Long term Validation loss: 0.1217\n",
      "Epoch: [2884/10000] Training loss: 4.675111384499045e-05 / Validation loss: 0.0004658619409671041 / Long term Validation loss: 0.1217\n",
      "Epoch: [2885/10000] Training loss: 4.672925880223311e-05 / Validation loss: 0.00046563447138931797 / Long term Validation loss: 0.1217\n",
      "Epoch: [2886/10000] Training loss: 4.670742794358408e-05 / Validation loss: 0.0004653268157952304 / Long term Validation loss: 0.1217\n",
      "Epoch: [2887/10000] Training loss: 4.668561464279861e-05 / Validation loss: 0.0004651816954121941 / Long term Validation loss: 0.1217\n",
      "Epoch: [2888/10000] Training loss: 4.666380023704956e-05 / Validation loss: 0.00046490070335333534 / Long term Validation loss: 0.1217\n",
      "Epoch: [2889/10000] Training loss: 4.6641992936110825e-05 / Validation loss: 0.00046464857164340916 / Long term Validation loss: 0.1217\n",
      "Epoch: [2890/10000] Training loss: 4.6620206563794604e-05 / Validation loss: 0.00046448227640408967 / Long term Validation loss: 0.1217\n",
      "Epoch: [2891/10000] Training loss: 4.659843244144197e-05 / Validation loss: 0.00046417975146526605 / Long term Validation loss: 0.1217\n",
      "Epoch: [2892/10000] Training loss: 4.6576660601233095e-05 / Validation loss: 0.0004639763704850447 / Long term Validation loss: 0.1217\n",
      "Epoch: [2893/10000] Training loss: 4.655489939463258e-05 / Validation loss: 0.00046376779431730536 / Long term Validation loss: 0.1217\n",
      "Epoch: [2894/10000] Training loss: 4.6533155734094914e-05 / Validation loss: 0.0004634769742222454 / Long term Validation loss: 0.1217\n",
      "Epoch: [2895/10000] Training loss: 4.6511422013787924e-05 / Validation loss: 0.0004632979456848525 / Long term Validation loss: 0.1217\n",
      "Epoch: [2896/10000] Training loss: 4.648969355246818e-05 / Validation loss: 0.00046304790158762017 / Long term Validation loss: 0.1217\n",
      "Epoch: [2897/10000] Training loss: 4.6467976973244255e-05 / Validation loss: 0.00046278996255373574 / Long term Validation loss: 0.1217\n",
      "Epoch: [2898/10000] Training loss: 4.644627534078539e-05 / Validation loss: 0.00046260665938798395 / Long term Validation loss: 0.1218\n",
      "Epoch: [2899/10000] Training loss: 4.6424582977298185e-05 / Validation loss: 0.00046233328371627296 / Long term Validation loss: 0.1217\n",
      "Epoch: [2900/10000] Training loss: 4.6402897932396705e-05 / Validation loss: 0.0004621109659460679 / Long term Validation loss: 0.1218\n",
      "Epoch: [2901/10000] Training loss: 4.6381224967523155e-05 / Validation loss: 0.00046190327576813093 / Long term Validation loss: 0.1218\n",
      "Epoch: [2902/10000] Training loss: 4.6359565215668256e-05 / Validation loss: 0.0004616307987119819 / Long term Validation loss: 0.1218\n",
      "Epoch: [2903/10000] Training loss: 4.633791473334479e-05 / Validation loss: 0.0004614307829863968 / Long term Validation loss: 0.1218\n",
      "Epoch: [2904/10000] Training loss: 4.6316272898193655e-05 / Validation loss: 0.0004611940640388876 / Long term Validation loss: 0.1218\n",
      "Epoch: [2905/10000] Training loss: 4.629464290204236e-05 / Validation loss: 0.0004609408722331977 / Long term Validation loss: 0.1218\n",
      "Epoch: [2906/10000] Training loss: 4.6273025041908884e-05 / Validation loss: 0.0004607428815166205 / Long term Validation loss: 0.1218\n",
      "Epoch: [2907/10000] Training loss: 4.6251416712375694e-05 / Validation loss: 0.0004604869792936689 / Long term Validation loss: 0.1218\n",
      "Epoch: [2908/10000] Training loss: 4.622981778749512e-05 / Validation loss: 0.00046025844865765726 / Long term Validation loss: 0.1218\n",
      "Epoch: [2909/10000] Training loss: 4.62082303749499e-05 / Validation loss: 0.00046004606021777416 / Long term Validation loss: 0.1218\n",
      "Epoch: [2910/10000] Training loss: 4.6186654466582946e-05 / Validation loss: 0.00045978785834856434 / Long term Validation loss: 0.1218\n",
      "Epoch: [2911/10000] Training loss: 4.6165088351066625e-05 / Validation loss: 0.00045957648877605013 / Long term Validation loss: 0.1218\n",
      "Epoch: [2912/10000] Training loss: 4.6143532082940305e-05 / Validation loss: 0.00045934425086147405 / Long term Validation loss: 0.1218\n",
      "Epoch: [2913/10000] Training loss: 4.612198701649515e-05 / Validation loss: 0.00045909808413711927 / Long term Validation loss: 0.1218\n",
      "Epoch: [2914/10000] Training loss: 4.610045307817323e-05 / Validation loss: 0.00045888975810485334 / Long term Validation loss: 0.1218\n",
      "Epoch: [2915/10000] Training loss: 4.607892916569241e-05 / Validation loss: 0.00045864363660016186 / Long term Validation loss: 0.1218\n",
      "Epoch: [2916/10000] Training loss: 4.6057415327612264e-05 / Validation loss: 0.00045841457762623697 / Long term Validation loss: 0.1218\n",
      "Epoch: [2917/10000] Training loss: 4.603591245060203e-05 / Validation loss: 0.0004581970137866669 / Long term Validation loss: 0.1218\n",
      "Epoch: [2918/10000] Training loss: 4.6014420481105104e-05 / Validation loss: 0.0004579489694931722 / Long term Validation loss: 0.1218\n",
      "Epoch: [2919/10000] Training loss: 4.599293869268503e-05 / Validation loss: 0.0004577320652523172 / Long term Validation loss: 0.1219\n",
      "Epoch: [2920/10000] Training loss: 4.597146711241128e-05 / Validation loss: 0.0004575008902036219 / Long term Validation loss: 0.1219\n",
      "Epoch: [2921/10000] Training loss: 4.5950006307114043e-05 / Validation loss: 0.00045726132282730974 / Long term Validation loss: 0.1219\n",
      "Epoch: [2922/10000] Training loss: 4.592855626477281e-05 / Validation loss: 0.00045704633822862253 / Long term Validation loss: 0.1219\n",
      "Epoch: [2923/10000] Training loss: 4.590711651739569e-05 / Validation loss: 0.0004568058814439264 / Long term Validation loss: 0.1219\n",
      "Epoch: [2924/10000] Training loss: 4.588568704117035e-05 / Validation loss: 0.0004565782786774333 / Long term Validation loss: 0.1219\n",
      "Epoch: [2925/10000] Training loss: 4.586426820827457e-05 / Validation loss: 0.0004563564882389362 / Long term Validation loss: 0.1219\n",
      "Epoch: [2926/10000] Training loss: 4.5842860041135056e-05 / Validation loss: 0.0004561155692214819 / Long term Validation loss: 0.1219\n",
      "Epoch: [2927/10000] Training loss: 4.58214622311068e-05 / Validation loss: 0.00045589594477924203 / Long term Validation loss: 0.1219\n",
      "Epoch: [2928/10000] Training loss: 4.580007473461014e-05 / Validation loss: 0.00045566480576526874 / Long term Validation loss: 0.1219\n",
      "Epoch: [2929/10000] Training loss: 4.577869778145161e-05 / Validation loss: 0.0004554306589743994 / Long term Validation loss: 0.1219\n",
      "Epoch: [2930/10000] Training loss: 4.575733141752157e-05 / Validation loss: 0.0004552114057576517 / Long term Validation loss: 0.1219\n",
      "Epoch: [2931/10000] Training loss: 4.573597545046265e-05 / Validation loss: 0.0004549747621896372 / Long term Validation loss: 0.1219\n",
      "Epoch: [2932/10000] Training loss: 4.571462981405595e-05 / Validation loss: 0.0004547490209530402 / Long term Validation loss: 0.1219\n",
      "Epoch: [2933/10000] Training loss: 4.569329465176306e-05 / Validation loss: 0.0004545242742438137 / Long term Validation loss: 0.1219\n",
      "Epoch: [2934/10000] Training loss: 4.567197001703311e-05 / Validation loss: 0.00045428869113752895 / Long term Validation loss: 0.1219\n",
      "Epoch: [2935/10000] Training loss: 4.565065579090743e-05 / Validation loss: 0.0004540675678129926 / Long term Validation loss: 0.1219\n",
      "Epoch: [2936/10000] Training loss: 4.5629351909560126e-05 / Validation loss: 0.0004538365350347588 / Long term Validation loss: 0.1219\n",
      "Epoch: [2937/10000] Training loss: 4.56080584479649e-05 / Validation loss: 0.0004536066012705677 / Long term Validation loss: 0.1220\n",
      "Epoch: [2938/10000] Training loss: 4.558677546007603e-05 / Validation loss: 0.0004533844494047026 / Long term Validation loss: 0.1220\n",
      "Epoch: [2939/10000] Training loss: 4.556550288130857e-05 / Validation loss: 0.00045315085970248486 / Long term Validation loss: 0.1220\n",
      "Epoch: [2940/10000] Training loss: 4.5544240648762876e-05 / Validation loss: 0.0004529265991190898 / Long term Validation loss: 0.1220\n",
      "Epoch: [2941/10000] Training loss: 4.552298879898717e-05 / Validation loss: 0.0004527000066120709 / Long term Validation loss: 0.1220\n",
      "Epoch: [2942/10000] Training loss: 4.5501747375339815e-05 / Validation loss: 0.0004524686597437626 / Long term Validation loss: 0.1220\n",
      "Epoch: [2943/10000] Training loss: 4.548051634681397e-05 / Validation loss: 0.0004522464598579606 / Long term Validation loss: 0.1220\n",
      "Epoch: [2944/10000] Training loss: 4.5459295664721486e-05 / Validation loss: 0.0004520160897729521 / Long term Validation loss: 0.1220\n",
      "Epoch: [2945/10000] Training loss: 4.5438085334461456e-05 / Validation loss: 0.0004517893358610075 / Long term Validation loss: 0.1220\n",
      "Epoch: [2946/10000] Training loss: 4.541688538918883e-05 / Validation loss: 0.0004515652284728136 / Long term Validation loss: 0.1220\n",
      "Epoch: [2947/10000] Training loss: 4.53956958193176e-05 / Validation loss: 0.00045133450726200547 / Long term Validation loss: 0.1220\n",
      "Epoch: [2948/10000] Training loss: 4.537451658699449e-05 / Validation loss: 0.00045111111634451467 / Long term Validation loss: 0.1220\n",
      "Epoch: [2949/10000] Training loss: 4.5353347685032645e-05 / Validation loss: 0.0004508837019480913 / Long term Validation loss: 0.1220\n",
      "Epoch: [2950/10000] Training loss: 4.5332189131079564e-05 / Validation loss: 0.00045065576731810814 / Long term Validation loss: 0.1220\n",
      "Epoch: [2951/10000] Training loss: 4.5311040926725124e-05 / Validation loss: 0.0004504325869630507 / Long term Validation loss: 0.1220\n",
      "Epoch: [2952/10000] Training loss: 4.5289903048453107e-05 / Validation loss: 0.00045020351629308406 / Long term Validation loss: 0.1220\n",
      "Epoch: [2953/10000] Training loss: 4.52687754798642e-05 / Validation loss: 0.00044997893707762656 / Long term Validation loss: 0.1220\n",
      "Epoch: [2954/10000] Training loss: 4.5247658228770815e-05 / Validation loss: 0.00044975366093317543 / Long term Validation loss: 0.1221\n",
      "Epoch: [2955/10000] Training loss: 4.522655129939695e-05 / Validation loss: 0.00044952572615551516 / Long term Validation loss: 0.1221\n",
      "Epoch: [2956/10000] Training loss: 4.520545467795086e-05 / Validation loss: 0.0004493026107576703 / Long term Validation loss: 0.1221\n",
      "Epoch: [2957/10000] Training loss: 4.518436834908176e-05 / Validation loss: 0.0004490753829423949 / Long term Validation loss: 0.1221\n",
      "Epoch: [2958/10000] Training loss: 4.516329230986143e-05 / Validation loss: 0.00044885009739553325 / Long term Validation loss: 0.1221\n",
      "Epoch: [2959/10000] Training loss: 4.514222656420286e-05 / Validation loss: 0.0004486261186069617 / Long term Validation loss: 0.1221\n",
      "Epoch: [2960/10000] Training loss: 4.5121171105545325e-05 / Validation loss: 0.00044839890221147645 / Long term Validation loss: 0.1221\n",
      "Epoch: [2961/10000] Training loss: 4.510012592001409e-05 / Validation loss: 0.0004481755643372086 / Long term Validation loss: 0.1221\n",
      "Epoch: [2962/10000] Training loss: 4.507909100097404e-05 / Validation loss: 0.00044794994611352157 / Long term Validation loss: 0.1221\n",
      "Epoch: [2963/10000] Training loss: 4.505806634766986e-05 / Validation loss: 0.00044772452572721457 / Long term Validation loss: 0.1221\n",
      "Epoch: [2964/10000] Training loss: 4.503705195720266e-05 / Validation loss: 0.0004475012743534953 / Long term Validation loss: 0.1221\n",
      "Epoch: [2965/10000] Training loss: 4.501604782011383e-05 / Validation loss: 0.00044727510391935453 / Long term Validation loss: 0.1221\n",
      "Epoch: [2966/10000] Training loss: 4.499505392669489e-05 / Validation loss: 0.00044705161659223117 / Long term Validation loss: 0.1221\n",
      "Epoch: [2967/10000] Training loss: 4.4974070273625535e-05 / Validation loss: 0.0004468272620215298 / Long term Validation loss: 0.1221\n",
      "Epoch: [2968/10000] Training loss: 4.4953096857967184e-05 / Validation loss: 0.0004466021531085773 / Long term Validation loss: 0.1221\n",
      "Epoch: [2969/10000] Training loss: 4.493213367316365e-05 / Validation loss: 0.00044637932552939083 / Long term Validation loss: 0.1221\n",
      "Epoch: [2970/10000] Training loss: 4.491118071073713e-05 / Validation loss: 0.00044615424756934115 / Long term Validation loss: 0.1221\n",
      "Epoch: [2971/10000] Training loss: 4.48902379639007e-05 / Validation loss: 0.0004459308042505987 / Long term Validation loss: 0.1222\n",
      "Epoch: [2972/10000] Training loss: 4.486930542921105e-05 / Validation loss: 0.00044570739607331314 / Long term Validation loss: 0.1222\n",
      "Epoch: [2973/10000] Training loss: 4.484838310162238e-05 / Validation loss: 0.0004454828653669156 / Long term Validation loss: 0.1222\n",
      "Epoch: [2974/10000] Training loss: 4.4827470973671165e-05 / Validation loss: 0.0004452603545195213 / Long term Validation loss: 0.1222\n",
      "Epoch: [2975/10000] Training loss: 4.480656903834912e-05 / Validation loss: 0.0004450362886966477 / Long term Validation loss: 0.1222\n",
      "Epoch: [2976/10000] Training loss: 4.478567728995342e-05 / Validation loss: 0.0004448130986969349 / Long term Validation loss: 0.1222\n",
      "Epoch: [2977/10000] Training loss: 4.476479572398391e-05 / Validation loss: 0.00044459042399170375 / Long term Validation loss: 0.1222\n",
      "Epoch: [2978/10000] Training loss: 4.474392433436586e-05 / Validation loss: 0.00044436660558711276 / Long term Validation loss: 0.1222\n",
      "Epoch: [2979/10000] Training loss: 4.47230631138543e-05 / Validation loss: 0.00044414442196570293 / Long term Validation loss: 0.1222\n",
      "Epoch: [2980/10000] Training loss: 4.470221205629034e-05 / Validation loss: 0.0004439212534225224 / Long term Validation loss: 0.1222\n",
      "Epoch: [2981/10000] Training loss: 4.468137115607098e-05 / Validation loss: 0.00044369848037269146 / Long term Validation loss: 0.1222\n",
      "Epoch: [2982/10000] Training loss: 4.466054040781977e-05 / Validation loss: 0.0004434764138381863 / Long term Validation loss: 0.1222\n",
      "Epoch: [2983/10000] Training loss: 4.463971980515967e-05 / Validation loss: 0.00044325334636459326 / Long term Validation loss: 0.1222\n",
      "Epoch: [2984/10000] Training loss: 4.461890934123389e-05 / Validation loss: 0.0004430315415859381 / Long term Validation loss: 0.1222\n",
      "Epoch: [2985/10000] Training loss: 4.4598109010159585e-05 / Validation loss: 0.0004428091615177405 / Long term Validation loss: 0.1222\n",
      "Epoch: [2986/10000] Training loss: 4.457731880613147e-05 / Validation loss: 0.0004425869123178058 / Long term Validation loss: 0.1222\n",
      "Epoch: [2987/10000] Training loss: 4.455653872326184e-05 / Validation loss: 0.0004423653948396734 / Long term Validation loss: 0.1223\n",
      "Epoch: [2988/10000] Training loss: 4.453576875515317e-05 / Validation loss: 0.0004421430724178947 / Long term Validation loss: 0.1223\n",
      "Epoch: [2989/10000] Training loss: 4.451500889520572e-05 / Validation loss: 0.00044192171039922606 / Long term Validation loss: 0.1223\n",
      "Epoch: [2990/10000] Training loss: 4.4494259137517926e-05 / Validation loss: 0.00044170004091049743 / Long term Validation loss: 0.1223\n",
      "Epoch: [2991/10000] Training loss: 4.447351947606726e-05 / Validation loss: 0.00044147838129249706 / Long term Validation loss: 0.1223\n",
      "Epoch: [2992/10000] Training loss: 4.4452789904705366e-05 / Validation loss: 0.00044125739632243084 / Long term Validation loss: 0.1223\n",
      "Epoch: [2993/10000] Training loss: 4.4432070417062485e-05 / Validation loss: 0.0004410357961326911 / Long term Validation loss: 0.1223\n",
      "Epoch: [2994/10000] Training loss: 4.441136100665153e-05 / Validation loss: 0.000440814931753234 / Long term Validation loss: 0.1223\n",
      "Epoch: [2995/10000] Training loss: 4.439066166744961e-05 / Validation loss: 0.00044059392016225436 / Long term Validation loss: 0.1223\n",
      "Epoch: [2996/10000] Training loss: 4.436997239326767e-05 / Validation loss: 0.00044037288249040694 / Long term Validation loss: 0.1223\n",
      "Epoch: [2997/10000] Training loss: 4.434929317782071e-05 / Validation loss: 0.0004401524310633328 / Long term Validation loss: 0.1223\n",
      "Epoch: [2998/10000] Training loss: 4.432862401473994e-05 / Validation loss: 0.00043993152558888845 / Long term Validation loss: 0.1223\n",
      "Epoch: [2999/10000] Training loss: 4.430796489755906e-05 / Validation loss: 0.0004397111990370162 / Long term Validation loss: 0.1223\n",
      "Epoch: [3000/10000] Training loss: 4.4287315820116984e-05 / Validation loss: 0.0004394908139919839 / Long term Validation loss: 0.1223\n",
      "Epoch: [3001/10000] Training loss: 4.426667677610008e-05 / Validation loss: 0.00043927041338292063 / Long term Validation loss: 0.1223\n",
      "Epoch: [3002/10000] Training loss: 4.42460477591356e-05 / Validation loss: 0.0004390505066185175 / Long term Validation loss: 0.1223\n",
      "Epoch: [3003/10000] Training loss: 4.42254287628213e-05 / Validation loss: 0.0004388302747674476 / Long term Validation loss: 0.1224\n",
      "Epoch: [3004/10000] Training loss: 4.420481978066353e-05 / Validation loss: 0.0004386105153342354 / Long term Validation loss: 0.1224\n",
      "Epoch: [3005/10000] Training loss: 4.418422080637275e-05 / Validation loss: 0.0004383907412359512 / Long term Validation loss: 0.1224\n",
      "Epoch: [3006/10000] Training loss: 4.416363183353393e-05 / Validation loss: 0.0004381709830446219 / Long term Validation loss: 0.1224\n",
      "Epoch: [3007/10000] Training loss: 4.41430528557015e-05 / Validation loss: 0.0004379516328023859 / Long term Validation loss: 0.1224\n",
      "Epoch: [3008/10000] Training loss: 4.412248386641539e-05 / Validation loss: 0.0004377320579010078 / Long term Validation loss: 0.1224\n",
      "Epoch: [3009/10000] Training loss: 4.410192485913164e-05 / Validation loss: 0.0004375128827762948 / Long term Validation loss: 0.1224\n",
      "Epoch: [3010/10000] Training loss: 4.408137582744523e-05 / Validation loss: 0.00043729371141200515 / Long term Validation loss: 0.1224\n",
      "Epoch: [3011/10000] Training loss: 4.406083676485002e-05 / Validation loss: 0.0004370745949596381 / Long term Validation loss: 0.1224\n",
      "Epoch: [3012/10000] Training loss: 4.404030766482901e-05 / Validation loss: 0.00043685581136024005 / Long term Validation loss: 0.1224\n",
      "Epoch: [3013/10000] Training loss: 4.4019788520850204e-05 / Validation loss: 0.0004366368826949979 / Long term Validation loss: 0.1224\n",
      "Epoch: [3014/10000] Training loss: 4.399927932630818e-05 / Validation loss: 0.00043641830270280216 / Long term Validation loss: 0.1224\n",
      "Epoch: [3015/10000] Training loss: 4.397878007469242e-05 / Validation loss: 0.0004361997326553279 / Long term Validation loss: 0.1224\n",
      "Epoch: [3016/10000] Training loss: 4.3958290759409464e-05 / Validation loss: 0.0004359812564044055 / Long term Validation loss: 0.1224\n",
      "Epoch: [3017/10000] Training loss: 4.393781137386845e-05 / Validation loss: 0.0004357630482799126 / Long term Validation loss: 0.1224\n",
      "Epoch: [3018/10000] Training loss: 4.391734191145792e-05 / Validation loss: 0.00043554475970032386 / Long term Validation loss: 0.1224\n",
      "Epoch: [3019/10000] Training loss: 4.38968823655055e-05 / Validation loss: 0.00043532678012089826 / Long term Validation loss: 0.1225\n",
      "Epoch: [3020/10000] Training loss: 4.387643272940174e-05 / Validation loss: 0.00043510881222601233 / Long term Validation loss: 0.1225\n",
      "Epoch: [3021/10000] Training loss: 4.385599299646738e-05 / Validation loss: 0.0004348909725268425 / Long term Validation loss: 0.1225\n",
      "Epoch: [3022/10000] Training loss: 4.3835563160034376e-05 / Validation loss: 0.00043467334569083534 / Long term Validation loss: 0.1225\n",
      "Epoch: [3023/10000] Training loss: 4.381514321340758e-05 / Validation loss: 0.0004344556932808671 / Long term Validation loss: 0.1225\n",
      "Epoch: [3024/10000] Training loss: 4.379473314984451e-05 / Validation loss: 0.0004342383157378599 / Long term Validation loss: 0.1225\n",
      "Epoch: [3025/10000] Training loss: 4.377433296264088e-05 / Validation loss: 0.00043402095298285964 / Long term Validation loss: 0.1225\n",
      "Epoch: [3026/10000] Training loss: 4.375394264503254e-05 / Validation loss: 0.00043380374624987246 / Long term Validation loss: 0.1225\n",
      "Epoch: [3027/10000] Training loss: 4.3733562190270936e-05 / Validation loss: 0.0004335867060926443 / Long term Validation loss: 0.1225\n",
      "Epoch: [3028/10000] Training loss: 4.371319159157525e-05 / Validation loss: 0.00043336968847014674 / Long term Validation loss: 0.1225\n",
      "Epoch: [3029/10000] Training loss: 4.369283084213073e-05 / Validation loss: 0.00043315291303621463 / Long term Validation loss: 0.1225\n",
      "Epoch: [3030/10000] Training loss: 4.367247993514067e-05 / Validation loss: 0.00043293615973338963 / Long term Validation loss: 0.1225\n",
      "Epoch: [3031/10000] Training loss: 4.365213886375752e-05 / Validation loss: 0.00043271958159719833 / Long term Validation loss: 0.1225\n",
      "Epoch: [3032/10000] Training loss: 4.363180762114945e-05 / Validation loss: 0.00043250313255091804 / Long term Validation loss: 0.1225\n",
      "Epoch: [3033/10000] Training loss: 4.3611486200449035e-05 / Validation loss: 0.0004322867483761668 / Long term Validation loss: 0.1225\n",
      "Epoch: [3034/10000] Training loss: 4.359117459476709e-05 / Validation loss: 0.0004320705735709853 / Long term Validation loss: 0.1226\n",
      "Epoch: [3035/10000] Training loss: 4.35708727972164e-05 / Validation loss: 0.0004318544342749485 / Long term Validation loss: 0.1226\n",
      "Epoch: [3036/10000] Training loss: 4.355058080086747e-05 / Validation loss: 0.00043163847965390696 / Long term Validation loss: 0.1226\n",
      "Epoch: [3037/10000] Training loss: 4.353029859880311e-05 / Validation loss: 0.0004314226265364268 / Long term Validation loss: 0.1226\n",
      "Epoch: [3038/10000] Training loss: 4.351002618406929e-05 / Validation loss: 0.0004312068745269443 / Long term Validation loss: 0.1226\n",
      "Epoch: [3039/10000] Training loss: 4.34897635497002e-05 / Validation loss: 0.0004309912993071093 / Long term Validation loss: 0.1226\n",
      "Epoch: [3040/10000] Training loss: 4.346951068871949e-05 / Validation loss: 0.0004307757789748915 / Long term Validation loss: 0.1226\n",
      "Epoch: [3041/10000] Training loss: 4.344926759411738e-05 / Validation loss: 0.00043056044257356555 / Long term Validation loss: 0.1226\n",
      "Epoch: [3042/10000] Training loss: 4.34290342588899e-05 / Validation loss: 0.0004303451909072609 / Long term Validation loss: 0.1226\n",
      "Epoch: [3043/10000] Training loss: 4.3408810675997626e-05 / Validation loss: 0.000430130068608546 / Long term Validation loss: 0.1226\n",
      "Epoch: [3044/10000] Training loss: 4.3388596838395545e-05 / Validation loss: 0.0004299150925938829 / Long term Validation loss: 0.1226\n",
      "Epoch: [3045/10000] Training loss: 4.3368392739019415e-05 / Validation loss: 0.0004297001951144508 / Long term Validation loss: 0.1226\n",
      "Epoch: [3046/10000] Training loss: 4.334819837078035e-05 / Validation loss: 0.0004294854713549795 / Long term Validation loss: 0.1226\n",
      "Epoch: [3047/10000] Training loss: 4.3328013726587225e-05 / Validation loss: 0.0004292708270995484 / Long term Validation loss: 0.1226\n",
      "Epoch: [3048/10000] Training loss: 4.330783879931676e-05 / Validation loss: 0.00042905633053993324 / Long term Validation loss: 0.1226\n",
      "Epoch: [3049/10000] Training loss: 4.3287673581842654e-05 / Validation loss: 0.00042884195505973877 / Long term Validation loss: 0.1227\n",
      "Epoch: [3050/10000] Training loss: 4.326751806701397e-05 / Validation loss: 0.00042862768282637666 / Long term Validation loss: 0.1227\n",
      "Epoch: [3051/10000] Training loss: 4.32473722476631e-05 / Validation loss: 0.00042841356712223393 / Long term Validation loss: 0.1227\n",
      "Epoch: [3052/10000] Training loss: 4.322723611661207e-05 / Validation loss: 0.00042819953646787087 / Long term Validation loss: 0.1227\n",
      "Epoch: [3053/10000] Training loss: 4.320710966665593e-05 / Validation loss: 0.00042798566046018375 / Long term Validation loss: 0.1227\n",
      "Epoch: [3054/10000] Training loss: 4.318699289058522e-05 / Validation loss: 0.0004277718888393563 / Long term Validation loss: 0.1227\n",
      "Epoch: [3055/10000] Training loss: 4.316688578116415e-05 / Validation loss: 0.0004275582417929018 / Long term Validation loss: 0.1227\n",
      "Epoch: [3056/10000] Training loss: 4.314678833114585e-05 / Validation loss: 0.00042734473126101514 / Long term Validation loss: 0.1227\n",
      "Epoch: [3057/10000] Training loss: 4.312670053326639e-05 / Validation loss: 0.0004271313192722767 / Long term Validation loss: 0.1227\n",
      "Epoch: [3058/10000] Training loss: 4.310662238024102e-05 / Validation loss: 0.00042691805798305784 / Long term Validation loss: 0.1227\n",
      "Epoch: [3059/10000] Training loss: 4.3086553864776166e-05 / Validation loss: 0.0004267048950640282 / Long term Validation loss: 0.1227\n",
      "Epoch: [3060/10000] Training loss: 4.306649497955348e-05 / Validation loss: 0.00042649187045173525 / Long term Validation loss: 0.1227\n",
      "Epoch: [3061/10000] Training loss: 4.304644571724573e-05 / Validation loss: 0.00042627896485575523 / Long term Validation loss: 0.1227\n",
      "Epoch: [3062/10000] Training loss: 4.302640607050461e-05 / Validation loss: 0.00042606617440672497 / Long term Validation loss: 0.1227\n",
      "Epoch: [3063/10000] Training loss: 4.3006376031966505e-05 / Validation loss: 0.0004258535229384759 / Long term Validation loss: 0.1227\n",
      "Epoch: [3064/10000] Training loss: 4.298635559425396e-05 / Validation loss: 0.0004256409740872947 / Long term Validation loss: 0.1228\n",
      "Epoch: [3065/10000] Training loss: 4.296634474996831e-05 / Validation loss: 0.0004254285671912403 / Long term Validation loss: 0.1228\n",
      "Epoch: [3066/10000] Training loss: 4.2946343491700996e-05 / Validation loss: 0.0004252162690791437 / Long term Validation loss: 0.1228\n",
      "Epoch: [3067/10000] Training loss: 4.292635181202148e-05 / Validation loss: 0.00042500409991228785 / Long term Validation loss: 0.1228\n",
      "Epoch: [3068/10000] Training loss: 4.290636970348743e-05 / Validation loss: 0.0004247920556774516 / Long term Validation loss: 0.1228\n",
      "Epoch: [3069/10000] Training loss: 4.288639715863847e-05 / Validation loss: 0.0004245801249094812 / Long term Validation loss: 0.1228\n",
      "Epoch: [3070/10000] Training loss: 4.286643416999756e-05 / Validation loss: 0.0004243683305056825 / Long term Validation loss: 0.1228\n",
      "Epoch: [3071/10000] Training loss: 4.284648073007453e-05 / Validation loss: 0.0004241566441241716 / Long term Validation loss: 0.1228\n",
      "Epoch: [3072/10000] Training loss: 4.282653683135908e-05 / Validation loss: 0.0004239450929715931 / Long term Validation loss: 0.1228\n",
      "Epoch: [3073/10000] Training loss: 4.280660246632965e-05 / Validation loss: 0.0004237336565228906 / Long term Validation loss: 0.1228\n",
      "Epoch: [3074/10000] Training loss: 4.278667762744502e-05 / Validation loss: 0.00042352234498370704 / Long term Validation loss: 0.1228\n",
      "Epoch: [3075/10000] Training loss: 4.276676230715057e-05 / Validation loss: 0.00042331115949609087 / Long term Validation loss: 0.1228\n",
      "Epoch: [3076/10000] Training loss: 4.274685649787521e-05 / Validation loss: 0.00042310008886728564 / Long term Validation loss: 0.1228\n",
      "Epoch: [3077/10000] Training loss: 4.272696019203126e-05 / Validation loss: 0.00042288915105610786 / Long term Validation loss: 0.1229\n",
      "Epoch: [3078/10000] Training loss: 4.270707338201764e-05 / Validation loss: 0.0004226783254959535 / Long term Validation loss: 0.1229\n",
      "Epoch: [3079/10000] Training loss: 4.2687196060214775e-05 / Validation loss: 0.0004224676310209773 / Long term Validation loss: 0.1229\n",
      "Epoch: [3080/10000] Training loss: 4.266732821899054e-05 / Validation loss: 0.00042225705393034114 / Long term Validation loss: 0.1229\n",
      "Epoch: [3081/10000] Training loss: 4.264746985069492e-05 / Validation loss: 0.0004220466006154089 / Long term Validation loss: 0.1229\n",
      "Epoch: [3082/10000] Training loss: 4.2627620947663924e-05 / Validation loss: 0.0004218362724115953 / Long term Validation loss: 0.1229\n",
      "Epoch: [3083/10000] Training loss: 4.2607781502217907e-05 / Validation loss: 0.00042162606119760507 / Long term Validation loss: 0.1229\n",
      "Epoch: [3084/10000] Training loss: 4.258795150666111e-05 / Validation loss: 0.0004214159796525016 / Long term Validation loss: 0.1229\n",
      "Epoch: [3085/10000] Training loss: 4.256813095328414e-05 / Validation loss: 0.0004212060132187697 / Long term Validation loss: 0.1229\n",
      "Epoch: [3086/10000] Training loss: 4.25483198343604e-05 / Validation loss: 0.0004209961754701561 / Long term Validation loss: 0.1229\n",
      "Epoch: [3087/10000] Training loss: 4.25285181421502e-05 / Validation loss: 0.00042078645604455946 / Long term Validation loss: 0.1229\n",
      "Epoch: [3088/10000] Training loss: 4.250872586889702e-05 / Validation loss: 0.0004205768605247567 / Long term Validation loss: 0.1229\n",
      "Epoch: [3089/10000] Training loss: 4.248894300683052e-05 / Validation loss: 0.00042036738851634935 / Long term Validation loss: 0.1229\n",
      "Epoch: [3090/10000] Training loss: 4.246916954816499e-05 / Validation loss: 0.0004201580355847399 / Long term Validation loss: 0.1230\n",
      "Epoch: [3091/10000] Training loss: 4.244940548509938e-05 / Validation loss: 0.00041994880968954307 / Long term Validation loss: 0.1230\n",
      "Epoch: [3092/10000] Training loss: 4.242965080981896e-05 / Validation loss: 0.0004197397009166267 / Long term Validation loss: 0.1230\n",
      "Epoch: [3093/10000] Training loss: 4.2409905514492646e-05 / Validation loss: 0.00041953071924051593 / Long term Validation loss: 0.1230\n",
      "Epoch: [3094/10000] Training loss: 4.239016959127636e-05 / Validation loss: 0.0004193218561391079 / Long term Validation loss: 0.1230\n",
      "Epoch: [3095/10000] Training loss: 4.2370443032309856e-05 / Validation loss: 0.00041911311738837326 / Long term Validation loss: 0.1230\n",
      "Epoch: [3096/10000] Training loss: 4.2350725829719475e-05 / Validation loss: 0.00041890450049377506 / Long term Validation loss: 0.1230\n",
      "Epoch: [3097/10000] Training loss: 4.233101797561628e-05 / Validation loss: 0.0004186960045155575 / Long term Validation loss: 0.1230\n",
      "Epoch: [3098/10000] Training loss: 4.231131946209718e-05 / Validation loss: 0.00041848763325238526 / Long term Validation loss: 0.1230\n",
      "Epoch: [3099/10000] Training loss: 4.229163028124494e-05 / Validation loss: 0.00041827938079323855 / Long term Validation loss: 0.1230\n",
      "Epoch: [3100/10000] Training loss: 4.2271950425127315e-05 / Validation loss: 0.0004180712539895985 / Long term Validation loss: 0.1230\n",
      "Epoch: [3101/10000] Training loss: 4.225227988579878e-05 / Validation loss: 0.0004178632460221815 / Long term Validation loss: 0.1230\n",
      "Epoch: [3102/10000] Training loss: 4.223261865529862e-05 / Validation loss: 0.0004176553626116766 / Long term Validation loss: 0.1231\n",
      "Epoch: [3103/10000] Training loss: 4.221296672565291e-05 / Validation loss: 0.00041744759971389886 / Long term Validation loss: 0.1231\n",
      "Epoch: [3104/10000] Training loss: 4.219332408887303e-05 / Validation loss: 0.0004172399591994682 / Long term Validation loss: 0.1231\n",
      "Epoch: [3105/10000] Training loss: 4.2173690736956745e-05 / Validation loss: 0.0004170324412971199 / Long term Validation loss: 0.1231\n",
      "Epoch: [3106/10000] Training loss: 4.215406666188781e-05 / Validation loss: 0.00041682504380320776 / Long term Validation loss: 0.1231\n",
      "Epoch: [3107/10000] Training loss: 4.213445185563606e-05 / Validation loss: 0.00041661777030359623 / Long term Validation loss: 0.1231\n",
      "Epoch: [3108/10000] Training loss: 4.211484631015792e-05 / Validation loss: 0.00041641061630693277 / Long term Validation loss: 0.1231\n",
      "Epoch: [3109/10000] Training loss: 4.209525001739553e-05 / Validation loss: 0.0004162035864452145 / Long term Validation loss: 0.1231\n",
      "Epoch: [3110/10000] Training loss: 4.207566296927818e-05 / Validation loss: 0.0004159966764076608 / Long term Validation loss: 0.1231\n",
      "Epoch: [3111/10000] Training loss: 4.2056085157720866e-05 / Validation loss: 0.0004157898895788113 / Long term Validation loss: 0.1231\n",
      "Epoch: [3112/10000] Training loss: 4.203651657462579e-05 / Validation loss: 0.0004155832236844327 / Long term Validation loss: 0.1231\n",
      "Epoch: [3113/10000] Training loss: 4.2016957211881196e-05 / Validation loss: 0.00041537667961175746 / Long term Validation loss: 0.1231\n",
      "Epoch: [3114/10000] Training loss: 4.1997407061362405e-05 / Validation loss: 0.0004151702576978589 / Long term Validation loss: 0.1232\n",
      "Epoch: [3115/10000] Training loss: 4.1977866114931424e-05 / Validation loss: 0.0004149639564111838 / Long term Validation loss: 0.1232\n",
      "Epoch: [3116/10000] Training loss: 4.1958334364436875e-05 / Validation loss: 0.0004147577780658195 / Long term Validation loss: 0.1232\n",
      "Epoch: [3117/10000] Training loss: 4.193881180171466e-05 / Validation loss: 0.0004145517197557221 / Long term Validation loss: 0.1232\n",
      "Epoch: [3118/10000] Training loss: 4.1919298418587275e-05 / Validation loss: 0.00041434578448939764 / Long term Validation loss: 0.1232\n",
      "Epoch: [3119/10000] Training loss: 4.189979420686468e-05 / Validation loss: 0.00041413996933586386 / Long term Validation loss: 0.1232\n",
      "Epoch: [3120/10000] Training loss: 4.188029915834364e-05 / Validation loss: 0.00041393427673378424 / Long term Validation loss: 0.1232\n",
      "Epoch: [3121/10000] Training loss: 4.1860813264808405e-05 / Validation loss: 0.00041372870478631187 / Long term Validation loss: 0.1232\n",
      "Epoch: [3122/10000] Training loss: 4.1841336518030335e-05 / Validation loss: 0.0004135232545869702 / Long term Validation loss: 0.1232\n",
      "Epoch: [3123/10000] Training loss: 4.18218689097683e-05 / Validation loss: 0.00041331792572693154 / Long term Validation loss: 0.1232\n",
      "Epoch: [3124/10000] Training loss: 4.180241043176855e-05 / Validation loss: 0.0004131127178205689 / Long term Validation loss: 0.1232\n",
      "Epoch: [3125/10000] Training loss: 4.1782961075764905e-05 / Validation loss: 0.00041290763179404314 / Long term Validation loss: 0.1233\n",
      "Epoch: [3126/10000] Training loss: 4.1763520833478904e-05 / Validation loss: 0.00041270266616746086 / Long term Validation loss: 0.1233\n",
      "Epoch: [3127/10000] Training loss: 4.174408969661954e-05 / Validation loss: 0.0004124978226539618 / Long term Validation loss: 0.1233\n",
      "Epoch: [3128/10000] Training loss: 4.172466765688394e-05 / Validation loss: 0.00041229309931866575 / Long term Validation loss: 0.1233\n",
      "Epoch: [3129/10000] Training loss: 4.1705254705956726e-05 / Validation loss: 0.0004120884979999988 / Long term Validation loss: 0.1233\n",
      "Epoch: [3130/10000] Training loss: 4.168585083551077e-05 / Validation loss: 0.00041188401693350994 / Long term Validation loss: 0.1233\n",
      "Epoch: [3131/10000] Training loss: 4.166645603720676e-05 / Validation loss: 0.0004116796575395951 / Long term Validation loss: 0.1233\n",
      "Epoch: [3132/10000] Training loss: 4.164707030269366e-05 / Validation loss: 0.00041147541865458454 / Long term Validation loss: 0.1233\n",
      "Epoch: [3133/10000] Training loss: 4.1627693623608474e-05 / Validation loss: 0.0004112713009793993 / Long term Validation loss: 0.1233\n",
      "Epoch: [3134/10000] Training loss: 4.1608325991576634e-05 / Validation loss: 0.00041106730412080264 / Long term Validation loss: 0.1233\n",
      "Epoch: [3135/10000] Training loss: 4.15889673982118e-05 / Validation loss: 0.0004108634280140007 / Long term Validation loss: 0.1233\n",
      "Epoch: [3136/10000] Training loss: 4.156961783511616e-05 / Validation loss: 0.00041065967297527513 / Long term Validation loss: 0.1234\n",
      "Epoch: [3137/10000] Training loss: 4.155027729388041e-05 / Validation loss: 0.00041045603832068133 / Long term Validation loss: 0.1234\n",
      "Epoch: [3138/10000] Training loss: 4.153094576608381e-05 / Validation loss: 0.0004102525248677676 / Long term Validation loss: 0.1234\n",
      "Epoch: [3139/10000] Training loss: 4.151162324329445e-05 / Validation loss: 0.000410049131559642 / Long term Validation loss: 0.1234\n",
      "Epoch: [3140/10000] Training loss: 4.149230971706903e-05 / Validation loss: 0.00040984585945318336 / Long term Validation loss: 0.1234\n",
      "Epoch: [3141/10000] Training loss: 4.147300517895328e-05 / Validation loss: 0.00040964270737754716 / Long term Validation loss: 0.1234\n",
      "Epoch: [3142/10000] Training loss: 4.145370962048176e-05 / Validation loss: 0.0004094396763880681 / Long term Validation loss: 0.1234\n",
      "Epoch: [3143/10000] Training loss: 4.1434423033178146e-05 / Validation loss: 0.00040923676541197 / Long term Validation loss: 0.1234\n",
      "Epoch: [3144/10000] Training loss: 4.141514540855519e-05 / Validation loss: 0.0004090339753267494 / Long term Validation loss: 0.1234\n",
      "Epoch: [3145/10000] Training loss: 4.139587673811495e-05 / Validation loss: 0.0004088313052951095 / Long term Validation loss: 0.1234\n",
      "Epoch: [3146/10000] Training loss: 4.1376617013348595e-05 / Validation loss: 0.000408628755918191 / Long term Validation loss: 0.1235\n",
      "Epoch: [3147/10000] Training loss: 4.1357366225736935e-05 / Validation loss: 0.00040842632665604323 / Long term Validation loss: 0.1235\n",
      "Epoch: [3148/10000] Training loss: 4.1338124366750035e-05 / Validation loss: 0.00040822401780417323 / Long term Validation loss: 0.1235\n",
      "Epoch: [3149/10000] Training loss: 4.131889142784765e-05 / Validation loss: 0.0004080218291217015 / Long term Validation loss: 0.1235\n",
      "Epoch: [3150/10000] Training loss: 4.1299667400479126e-05 / Validation loss: 0.0004078197606187969 / Long term Validation loss: 0.1235\n",
      "Epoch: [3151/10000] Training loss: 4.128045227608356e-05 / Validation loss: 0.0004076178123170572 / Long term Validation loss: 0.1235\n",
      "Epoch: [3152/10000] Training loss: 4.12612460460899e-05 / Validation loss: 0.00040741598398904904 / Long term Validation loss: 0.1235\n",
      "Epoch: [3153/10000] Training loss: 4.124204870191701e-05 / Validation loss: 0.0004072142758648736 / Long term Validation loss: 0.1235\n",
      "Epoch: [3154/10000] Training loss: 4.122286023497369e-05 / Validation loss: 0.00040701268753581186 / Long term Validation loss: 0.1235\n",
      "Epoch: [3155/10000] Training loss: 4.120368063665888e-05 / Validation loss: 0.00040681121938521733 / Long term Validation loss: 0.1235\n",
      "Epoch: [3156/10000] Training loss: 4.1184509898361724e-05 / Validation loss: 0.00040660987087475496 / Long term Validation loss: 0.1236\n",
      "Epoch: [3157/10000] Training loss: 4.116534801146161e-05 / Validation loss: 0.0004064086424948064 / Long term Validation loss: 0.1236\n",
      "Epoch: [3158/10000] Training loss: 4.114619496732829e-05 / Validation loss: 0.0004062075336169078 / Long term Validation loss: 0.1236\n",
      "Epoch: [3159/10000] Training loss: 4.112705075732195e-05 / Validation loss: 0.0004060065448064075 / Long term Validation loss: 0.1236\n",
      "Epoch: [3160/10000] Training loss: 4.110791537279338e-05 / Validation loss: 0.000405805675368957 / Long term Validation loss: 0.1236\n",
      "Epoch: [3161/10000] Training loss: 4.10887888050839e-05 / Validation loss: 0.00040560492592854185 / Long term Validation loss: 0.1236\n",
      "Epoch: [3162/10000] Training loss: 4.1069671045525634e-05 / Validation loss: 0.00040540429573349487 / Long term Validation loss: 0.1236\n",
      "Epoch: [3163/10000] Training loss: 4.105056208544145e-05 / Validation loss: 0.00040520378546555173 / Long term Validation loss: 0.1236\n",
      "Epoch: [3164/10000] Training loss: 4.103146191614523e-05 / Validation loss: 0.00040500339430922625 / Long term Validation loss: 0.1236\n",
      "Epoch: [3165/10000] Training loss: 4.1012370528941696e-05 / Validation loss: 0.0004048031230179364 / Long term Validation loss: 0.1236\n",
      "Epoch: [3166/10000] Training loss: 4.0993287915126784e-05 / Validation loss: 0.00040460297069106035 / Long term Validation loss: 0.1237\n",
      "Epoch: [3167/10000] Training loss: 4.097421406598748e-05 / Validation loss: 0.00040440293818269223 / Long term Validation loss: 0.1237\n",
      "Epoch: [3168/10000] Training loss: 4.095514897280222e-05 / Validation loss: 0.00040420302446992755 / Long term Validation loss: 0.1237\n",
      "Epoch: [3169/10000] Training loss: 4.093609262684056e-05 / Validation loss: 0.0004040032305536511 / Long term Validation loss: 0.1237\n",
      "Epoch: [3170/10000] Training loss: 4.091704501936375e-05 / Validation loss: 0.0004038035552323566 / Long term Validation loss: 0.1237\n",
      "Epoch: [3171/10000] Training loss: 4.089800614162436e-05 / Validation loss: 0.00040360399972190204 / Long term Validation loss: 0.1237\n",
      "Epoch: [3172/10000] Training loss: 4.087897598486678e-05 / Validation loss: 0.000403404562559896 / Long term Validation loss: 0.1237\n",
      "Epoch: [3173/10000] Training loss: 4.0859954540326965e-05 / Validation loss: 0.0004032052452765377 / Long term Validation loss: 0.1237\n",
      "Epoch: [3174/10000] Training loss: 4.0840941799232855e-05 / Validation loss: 0.00040300604602836875 / Long term Validation loss: 0.1237\n",
      "Epoch: [3175/10000] Training loss: 4.082193775280406e-05 / Validation loss: 0.00040280696680587225 / Long term Validation loss: 0.1237\n",
      "Epoch: [3176/10000] Training loss: 4.0802942392252505e-05 / Validation loss: 0.00040260800520669135 / Long term Validation loss: 0.1238\n",
      "Epoch: [3177/10000] Training loss: 4.078395570878187e-05 / Validation loss: 0.00040240916389932494 / Long term Validation loss: 0.1238\n",
      "Epoch: [3178/10000] Training loss: 4.076497769358841e-05 / Validation loss: 0.0004022104396548073 / Long term Validation loss: 0.1238\n",
      "Epoch: [3179/10000] Training loss: 4.074600833786021e-05 / Validation loss: 0.00040201183615032446 / Long term Validation loss: 0.1238\n",
      "Epoch: [3180/10000] Training loss: 4.072704763277815e-05 / Validation loss: 0.00040181334892008336 / Long term Validation loss: 0.1238\n",
      "Epoch: [3181/10000] Training loss: 4.0708095569515156e-05 / Validation loss: 0.0004016149831610343 / Long term Validation loss: 0.1238\n",
      "Epoch: [3182/10000] Training loss: 4.068915213923728e-05 / Validation loss: 0.000401416732531179 / Long term Validation loss: 0.1238\n",
      "Epoch: [3183/10000] Training loss: 4.0670217333102444e-05 / Validation loss: 0.00040121860455040584 / Long term Validation loss: 0.1238\n",
      "Epoch: [3184/10000] Training loss: 4.065129114226227e-05 / Validation loss: 0.00040102058998756676 / Long term Validation loss: 0.1238\n",
      "Epoch: [3185/10000] Training loss: 4.063237355786008e-05 / Validation loss: 0.0004008226999680886 / Long term Validation loss: 0.1239\n",
      "Epoch: [3186/10000] Training loss: 4.0613464571033524e-05 / Validation loss: 0.0004006249207414155 / Long term Validation loss: 0.1239\n",
      "Epoch: [3187/10000] Training loss: 4.059456417291156e-05 / Validation loss: 0.0004004272691186481 / Long term Validation loss: 0.1239\n",
      "Epoch: [3188/10000] Training loss: 4.0575672354618355e-05 / Validation loss: 0.00040022972416569284 / Long term Validation loss: 0.1239\n",
      "Epoch: [3189/10000] Training loss: 4.0556789107268674e-05 / Validation loss: 0.000400032311804292 / Long term Validation loss: 0.1239\n",
      "Epoch: [3190/10000] Training loss: 4.0537914421973907e-05 / Validation loss: 0.0003998349994973625 / Long term Validation loss: 0.1239\n",
      "Epoch: [3191/10000] Training loss: 4.0519048289834665e-05 / Validation loss: 0.0003996378280010788 / Long term Validation loss: 0.1239\n",
      "Epoch: [3192/10000] Training loss: 4.050019070195026e-05 / Validation loss: 0.0003994407457351099 / Long term Validation loss: 0.1239\n",
      "Epoch: [3193/10000] Training loss: 4.04813416494071e-05 / Validation loss: 0.00039924381799683804 / Long term Validation loss: 0.1239\n",
      "Epoch: [3194/10000] Training loss: 4.046250112329352e-05 / Validation loss: 0.0003990469614530526 / Long term Validation loss: 0.1239\n",
      "Epoch: [3195/10000] Training loss: 4.0443669114681263e-05 / Validation loss: 0.0003988502826437768 / Long term Validation loss: 0.1240\n",
      "Epoch: [3196/10000] Training loss: 4.042484561464914e-05 / Validation loss: 0.0003986536444574329 / Long term Validation loss: 0.1240\n",
      "Epoch: [3197/10000] Training loss: 4.040603061425322e-05 / Validation loss: 0.00039845722382682996 / Long term Validation loss: 0.1240\n",
      "Epoch: [3198/10000] Training loss: 4.0387224104565544e-05 / Validation loss: 0.0003982607911461676 / Long term Validation loss: 0.1240\n",
      "Epoch: [3199/10000] Training loss: 4.0368426076624405e-05 / Validation loss: 0.0003980646453425017 / Long term Validation loss: 0.1240\n",
      "Epoch: [3200/10000] Training loss: 4.034963652149969e-05 / Validation loss: 0.0003978683952998016 / Long term Validation loss: 0.1240\n",
      "Epoch: [3201/10000] Training loss: 4.0330855430208705e-05 / Validation loss: 0.0003976725545676295 / Long term Validation loss: 0.1240\n",
      "Epoch: [3202/10000] Training loss: 4.0312082793827956e-05 / Validation loss: 0.000397476445771851 / Long term Validation loss: 0.1240\n",
      "Epoch: [3203/10000] Training loss: 4.029331860334957e-05 / Validation loss: 0.00039728096566603977 / Long term Validation loss: 0.1240\n",
      "Epoch: [3204/10000] Training loss: 4.0274562849875636e-05 / Validation loss: 0.0003970849220243 / Long term Validation loss: 0.1240\n",
      "Epoch: [3205/10000] Training loss: 4.02558155243709e-05 / Validation loss: 0.0003968899058232841 / Long term Validation loss: 0.1241\n",
      "Epoch: [3206/10000] Training loss: 4.023707661800959e-05 / Validation loss: 0.00039669378539640995 / Long term Validation loss: 0.1241\n",
      "Epoch: [3207/10000] Training loss: 4.021834612174701e-05 / Validation loss: 0.00039649942751149785 / Long term Validation loss: 0.1241\n",
      "Epoch: [3208/10000] Training loss: 4.019962402695806e-05 / Validation loss: 0.0003963029618295218 / Long term Validation loss: 0.1241\n",
      "Epoch: [3209/10000] Training loss: 4.018091032470314e-05 / Validation loss: 0.00039610963289842205 / Long term Validation loss: 0.1241\n",
      "Epoch: [3210/10000] Training loss: 4.016220500695787e-05 / Validation loss: 0.0003959123072908392 / Long term Validation loss: 0.1241\n",
      "Epoch: [3211/10000] Training loss: 4.0143508065411375e-05 / Validation loss: 0.00039572072298899925 / Long term Validation loss: 0.1241\n",
      "Epoch: [3212/10000] Training loss: 4.012481949400573e-05 / Validation loss: 0.00039552153777450643 / Long term Validation loss: 0.1241\n",
      "Epoch: [3213/10000] Training loss: 4.010613928725039e-05 / Validation loss: 0.00039533309770281404 / Long term Validation loss: 0.1241\n",
      "Epoch: [3214/10000] Training loss: 4.008746744604895e-05 / Validation loss: 0.0003951300859808415 / Long term Validation loss: 0.1242\n",
      "Epoch: [3215/10000] Training loss: 4.006880397669227e-05 / Validation loss: 0.00039494756203132256 / Long term Validation loss: 0.1242\n",
      "Epoch: [3216/10000] Training loss: 4.005014890626445e-05 / Validation loss: 0.00039473680459024135 / Long term Validation loss: 0.1242\n",
      "Epoch: [3217/10000] Training loss: 4.0031502289557654e-05 / Validation loss: 0.0003945657556237544 / Long term Validation loss: 0.1242\n",
      "Epoch: [3218/10000] Training loss: 4.001286425670652e-05 / Validation loss: 0.00039433934522332747 / Long term Validation loss: 0.1242\n",
      "Epoch: [3219/10000] Training loss: 3.999423506352601e-05 / Validation loss: 0.00039419105843116074 / Long term Validation loss: 0.1242\n",
      "Epoch: [3220/10000] Training loss: 3.997561526060773e-05 / Validation loss: 0.00039393284500014125 / Long term Validation loss: 0.1242\n",
      "Epoch: [3221/10000] Training loss: 3.995700595038925e-05 / Validation loss: 0.0003938305231107001 / Long term Validation loss: 0.1242\n",
      "Epoch: [3222/10000] Training loss: 3.993840945172027e-05 / Validation loss: 0.0003935071219099561 / Long term Validation loss: 0.1242\n",
      "Epoch: [3223/10000] Training loss: 3.991983050546764e-05 / Validation loss: 0.00039349905215269705 / Long term Validation loss: 0.1243\n",
      "Epoch: [3224/10000] Training loss: 3.990127907159559e-05 / Validation loss: 0.00039304063795731175 / Long term Validation loss: 0.1242\n",
      "Epoch: [3225/10000] Training loss: 3.988277587690983e-05 / Validation loss: 0.0003932285638941894 / Long term Validation loss: 0.1244\n",
      "Epoch: [3226/10000] Training loss: 3.986436474962523e-05 / Validation loss: 0.00039248744531657074 / Long term Validation loss: 0.1242\n",
      "Epoch: [3227/10000] Training loss: 3.984613827861291e-05 / Validation loss: 0.00039308849821883553 / Long term Validation loss: 0.1245\n",
      "Epoch: [3228/10000] Training loss: 3.982829410918838e-05 / Validation loss: 0.000391749041130403 / Long term Validation loss: 0.1240\n",
      "Epoch: [3229/10000] Training loss: 3.981125542508636e-05 / Validation loss: 0.00039323299965195673 / Long term Validation loss: 0.1247\n",
      "Epoch: [3230/10000] Training loss: 3.9795935159892585e-05 / Validation loss: 0.00039061491900469625 / Long term Validation loss: 0.1237\n",
      "Epoch: [3231/10000] Training loss: 3.978431273961173e-05 / Validation loss: 0.00039401449781366967 / Long term Validation loss: 0.1252\n",
      "Epoch: [3232/10000] Training loss: 3.978070431769318e-05 / Validation loss: 0.00038864521291879074 / Long term Validation loss: 0.1231\n",
      "Epoch: [3233/10000] Training loss: 3.979458069701783e-05 / Validation loss: 0.0003962781339321172 / Long term Validation loss: 0.1265\n",
      "Epoch: [3234/10000] Training loss: 3.984679643587087e-05 / Validation loss: 0.0003849876828616993 / Long term Validation loss: 0.1211\n",
      "Epoch: [3235/10000] Training loss: 3.998359926778696e-05 / Validation loss: 0.0004022141978501836 / Long term Validation loss: 0.1298\n",
      "Epoch: [3236/10000] Training loss: 4.030741488959327e-05 / Validation loss: 0.000378389121654071 / Long term Validation loss: 0.1159\n",
      "Epoch: [3237/10000] Training loss: 4.1046331920108165e-05 / Validation loss: 0.00041815019274652924 / Long term Validation loss: 0.1351\n",
      "Epoch: [3238/10000] Training loss: 4.2699448443854505e-05 / Validation loss: 0.0003693726732396889 / Long term Validation loss: 0.1058\n",
      "Epoch: [3239/10000] Training loss: 4.634559707876605e-05 / Validation loss: 0.0004637969281867181 / Long term Validation loss: 0.1344\n",
      "Epoch: [3240/10000] Training loss: 5.411695846914645e-05 / Validation loss: 0.0003728874029244367 / Long term Validation loss: 0.0956\n",
      "Epoch: [3241/10000] Training loss: 6.965807586206988e-05 / Validation loss: 0.0005832280060508268 / Long term Validation loss: 0.1364\n",
      "Epoch: [3242/10000] Training loss: 9.588671150976969e-05 / Validation loss: 0.00042156045210835436 / Long term Validation loss: 0.0972\n",
      "Epoch: [3243/10000] Training loss: 0.00012620677400578846 / Validation loss: 0.0006691520806727599 / Long term Validation loss: 0.1344\n",
      "Epoch: [3244/10000] Training loss: 0.00013025084939528053 / Validation loss: 0.00038393699506415063 / Long term Validation loss: 0.0948\n",
      "Epoch: [3245/10000] Training loss: 8.73259256219278e-05 / Validation loss: 0.0004181732541301809 / Long term Validation loss: 0.1347\n",
      "Epoch: [3246/10000] Training loss: 4.2688221076572015e-05 / Validation loss: 0.0004595706169560857 / Long term Validation loss: 0.1333\n",
      "Epoch: [3247/10000] Training loss: 5.272421252315433e-05 / Validation loss: 0.00038187728789275085 / Long term Validation loss: 0.0944\n",
      "Epoch: [3248/10000] Training loss: 8.592357705260712e-05 / Validation loss: 0.0005368408996777538 / Long term Validation loss: 0.1374\n",
      "Epoch: [3249/10000] Training loss: 7.797977066727417e-05 / Validation loss: 0.00036926356356776796 / Long term Validation loss: 0.1089\n",
      "Epoch: [3250/10000] Training loss: 4.443218786457374e-05 / Validation loss: 0.00036711570670488574 / Long term Validation loss: 0.1052\n",
      "Epoch: [3251/10000] Training loss: 4.6549497503724304e-05 / Validation loss: 0.0005122333172633725 / Long term Validation loss: 0.1371\n",
      "Epoch: [3252/10000] Training loss: 6.928369913905043e-05 / Validation loss: 0.0003658013182213294 / Long term Validation loss: 0.0960\n",
      "Epoch: [3253/10000] Training loss: 6.068518034853277e-05 / Validation loss: 0.00040175931742817734 / Long term Validation loss: 0.1328\n",
      "Epoch: [3254/10000] Training loss: 4.0200854665616146e-05 / Validation loss: 0.0004457783865437073 / Long term Validation loss: 0.1316\n",
      "Epoch: [3255/10000] Training loss: 4.891396147705293e-05 / Validation loss: 0.00036543983255330777 / Long term Validation loss: 0.0961\n",
      "Epoch: [3256/10000] Training loss: 6.031923651880371e-05 / Validation loss: 0.00043902387860919725 / Long term Validation loss: 0.1314\n",
      "Epoch: [3257/10000] Training loss: 4.7202503716799886e-05 / Validation loss: 0.00039981248330029875 / Long term Validation loss: 0.1322\n",
      "Epoch: [3258/10000] Training loss: 3.99652085368836e-05 / Validation loss: 0.00036466183259708726 / Long term Validation loss: 0.1002\n",
      "Epoch: [3259/10000] Training loss: 5.101744964983413e-05 / Validation loss: 0.00045287957925934627 / Long term Validation loss: 0.1322\n",
      "Epoch: [3260/10000] Training loss: 5.0859115454530195e-05 / Validation loss: 0.0003780289356492507 / Long term Validation loss: 0.1193\n",
      "Epoch: [3261/10000] Training loss: 4.025198540647171e-05 / Validation loss: 0.00037000012910038695 / Long term Validation loss: 0.1111\n",
      "Epoch: [3262/10000] Training loss: 4.308372968879003e-05 / Validation loss: 0.00044572461594842767 / Long term Validation loss: 0.1320\n",
      "Epoch: [3263/10000] Training loss: 4.9106049187860376e-05 / Validation loss: 0.0003697507298359314 / Long term Validation loss: 0.1111\n",
      "Epoch: [3264/10000] Training loss: 4.301073811383128e-05 / Validation loss: 0.00038118027656098994 / Long term Validation loss: 0.1225\n",
      "Epoch: [3265/10000] Training loss: 3.961320091817116e-05 / Validation loss: 0.000428502707158381 / Long term Validation loss: 0.1326\n",
      "Epoch: [3266/10000] Training loss: 4.502698129353101e-05 / Validation loss: 0.00036743804125223317 / Long term Validation loss: 0.1080\n",
      "Epoch: [3267/10000] Training loss: 4.4458346728217305e-05 / Validation loss: 0.00039415032518908634 / Long term Validation loss: 0.1293\n",
      "Epoch: [3268/10000] Training loss: 3.947723731092143e-05 / Validation loss: 0.0004102128181864981 / Long term Validation loss: 0.1347\n",
      "Epoch: [3269/10000] Training loss: 4.1440494856636635e-05 / Validation loss: 0.00036796506213988893 / Long term Validation loss: 0.1089\n",
      "Epoch: [3270/10000] Training loss: 4.39051726210262e-05 / Validation loss: 0.0004044459195163432 / Long term Validation loss: 0.1332\n",
      "Epoch: [3271/10000] Training loss: 4.056305262871428e-05 / Validation loss: 0.00039528623053983425 / Long term Validation loss: 0.1296\n",
      "Epoch: [3272/10000] Training loss: 3.9533548307851815e-05 / Validation loss: 0.0003702981628375956 / Long term Validation loss: 0.1122\n",
      "Epoch: [3273/10000] Training loss: 4.22063499463382e-05 / Validation loss: 0.0004093231770265939 / Long term Validation loss: 0.1347\n",
      "Epoch: [3274/10000] Training loss: 4.135448526556473e-05 / Validation loss: 0.000384792267058211 / Long term Validation loss: 0.1243\n",
      "Epoch: [3275/10000] Training loss: 3.915782982862369e-05 / Validation loss: 0.00037423716289349055 / Long term Validation loss: 0.1161\n",
      "Epoch: [3276/10000] Training loss: 4.046975232773455e-05 / Validation loss: 0.0004086381821738124 / Long term Validation loss: 0.1346\n",
      "Epoch: [3277/10000] Training loss: 4.1309557986452704e-05 / Validation loss: 0.0003784070126352563 / Long term Validation loss: 0.1201\n",
      "Epoch: [3278/10000] Training loss: 3.955192857716236e-05 / Validation loss: 0.00037961654365456537 / Long term Validation loss: 0.1212\n",
      "Epoch: [3279/10000] Training loss: 3.937716411822938e-05 / Validation loss: 0.0004040587452152939 / Long term Validation loss: 0.1333\n",
      "Epoch: [3280/10000] Training loss: 4.0622037766750846e-05 / Validation loss: 0.0003753258242902434 / Long term Validation loss: 0.1174\n",
      "Epoch: [3281/10000] Training loss: 3.997135468575205e-05 / Validation loss: 0.00038551985310671853 / Long term Validation loss: 0.1248\n",
      "Epoch: [3282/10000] Training loss: 3.903260944935164e-05 / Validation loss: 0.0003976018460550393 / Long term Validation loss: 0.1307\n",
      "Epoch: [3283/10000] Training loss: 3.977717563930448e-05 / Validation loss: 0.0003745383970409168 / Long term Validation loss: 0.1170\n",
      "Epoch: [3284/10000] Training loss: 4.003084764931656e-05 / Validation loss: 0.00039044061570458425 / Long term Validation loss: 0.1274\n",
      "Epoch: [3285/10000] Training loss: 3.914860738046773e-05 / Validation loss: 0.000390887052940689 / Long term Validation loss: 0.1277\n",
      "Epoch: [3286/10000] Training loss: 3.9170985587206127e-05 / Validation loss: 0.0003752843334932727 / Long term Validation loss: 0.1180\n",
      "Epoch: [3287/10000] Training loss: 3.973563590183611e-05 / Validation loss: 0.0003932327996578531 / Long term Validation loss: 0.1292\n",
      "Epoch: [3288/10000] Training loss: 3.9346032965937305e-05 / Validation loss: 0.00038510875495116614 / Long term Validation loss: 0.1251\n",
      "Epoch: [3289/10000] Training loss: 3.893165677365696e-05 / Validation loss: 0.0003772332382158316 / Long term Validation loss: 0.1204\n",
      "Epoch: [3290/10000] Training loss: 3.930914991378802e-05 / Validation loss: 0.00039362783400353286 / Long term Validation loss: 0.1297\n",
      "Epoch: [3291/10000] Training loss: 3.938463025160648e-05 / Validation loss: 0.0003809620982346285 / Long term Validation loss: 0.1233\n",
      "Epoch: [3292/10000] Training loss: 3.895066472970245e-05 / Validation loss: 0.0003800379159534211 / Long term Validation loss: 0.1228\n",
      "Epoch: [3293/10000] Training loss: 3.897587495670116e-05 / Validation loss: 0.0003919946388124537 / Long term Validation loss: 0.1291\n",
      "Epoch: [3294/10000] Training loss: 3.923352071802435e-05 / Validation loss: 0.00037852555582937043 / Long term Validation loss: 0.1220\n",
      "Epoch: [3295/10000] Training loss: 3.903121920104975e-05 / Validation loss: 0.00038299838021535053 / Long term Validation loss: 0.1247\n",
      "Epoch: [3296/10000] Training loss: 3.88277827327459e-05 / Validation loss: 0.0003890055718292218 / Long term Validation loss: 0.1278\n",
      "Epoch: [3297/10000] Training loss: 3.9001612021151086e-05 / Validation loss: 0.00037751403239433864 / Long term Validation loss: 0.1216\n",
      "Epoch: [3298/10000] Training loss: 3.9033335400212056e-05 / Validation loss: 0.000385316877181108 / Long term Validation loss: 0.1260\n",
      "Epoch: [3299/10000] Training loss: 3.88174089662655e-05 / Validation loss: 0.00038549960115501154 / Long term Validation loss: 0.1261\n",
      "Epoch: [3300/10000] Training loss: 3.8813875670783513e-05 / Validation loss: 0.00037763995365107156 / Long term Validation loss: 0.1221\n",
      "Epoch: [3301/10000] Training loss: 3.8933678193058124e-05 / Validation loss: 0.0003864802323611228 / Long term Validation loss: 0.1268\n",
      "Epoch: [3302/10000] Training loss: 3.883803285407459e-05 / Validation loss: 0.00038228170476439836 / Long term Validation loss: 0.1249\n",
      "Epoch: [3303/10000] Training loss: 3.872385442525409e-05 / Validation loss: 0.00037862640759623955 / Long term Validation loss: 0.1231\n",
      "Epoch: [3304/10000] Training loss: 3.879279043010291e-05 / Validation loss: 0.0003863490237042597 / Long term Validation loss: 0.1270\n",
      "Epoch: [3305/10000] Training loss: 3.881294250043675e-05 / Validation loss: 0.0003798542353444551 / Long term Validation loss: 0.1240\n",
      "Epoch: [3306/10000] Training loss: 3.870369047557418e-05 / Validation loss: 0.00038005185522679337 / Long term Validation loss: 0.1242\n",
      "Epoch: [3307/10000] Training loss: 3.86804906206165e-05 / Validation loss: 0.0003850913942537547 / Long term Validation loss: 0.1265\n",
      "Epoch: [3308/10000] Training loss: 3.8733775984876226e-05 / Validation loss: 0.00037836617773674117 / Long term Validation loss: 0.1234\n",
      "Epoch: [3309/10000] Training loss: 3.869260777642015e-05 / Validation loss: 0.00038139242662738786 / Long term Validation loss: 0.1250\n",
      "Epoch: [3310/10000] Training loss: 3.862216050394291e-05 / Validation loss: 0.00038312350703837556 / Long term Validation loss: 0.1258\n",
      "Epoch: [3311/10000] Training loss: 3.86388198527407e-05 / Validation loss: 0.0003777626966608592 / Long term Validation loss: 0.1234\n",
      "Epoch: [3312/10000] Training loss: 3.865172225532708e-05 / Validation loss: 0.00038220884865336285 / Long term Validation loss: 0.1255\n",
      "Epoch: [3313/10000] Training loss: 3.8596070265392945e-05 / Validation loss: 0.00038098183163562925 / Long term Validation loss: 0.1250\n",
      "Epoch: [3314/10000] Training loss: 3.8565795834810515e-05 / Validation loss: 0.0003778772642841587 / Long term Validation loss: 0.1237\n",
      "Epoch: [3315/10000] Training loss: 3.858376505418804e-05 / Validation loss: 0.0003822687726829761 / Long term Validation loss: 0.1257\n",
      "Epoch: [3316/10000] Training loss: 3.856700896917643e-05 / Validation loss: 0.00037912547061015465 / Long term Validation loss: 0.1244\n",
      "Epoch: [3317/10000] Training loss: 3.852216188439081e-05 / Validation loss: 0.00037842946554050453 / Long term Validation loss: 0.1242\n",
      "Epoch: [3318/10000] Training loss: 3.85141957713319e-05 / Validation loss: 0.0003815764596197189 / Long term Validation loss: 0.1256\n",
      "Epoch: [3319/10000] Training loss: 3.851865679168302e-05 / Validation loss: 0.0003778160373378382 / Long term Validation loss: 0.1241\n",
      "Epoch: [3320/10000] Training loss: 3.8490084246904645e-05 / Validation loss: 0.0003790629889049352 / Long term Validation loss: 0.1247\n",
      "Epoch: [3321/10000] Training loss: 3.846107172986487e-05 / Validation loss: 0.00038034679402183135 / Long term Validation loss: 0.1253\n",
      "Epoch: [3322/10000] Training loss: 3.845922729044324e-05 / Validation loss: 0.0003771201195665831 / Long term Validation loss: 0.1240\n",
      "Epoch: [3323/10000] Training loss: 3.845064421650718e-05 / Validation loss: 0.000379446808914621 / Long term Validation loss: 0.1250\n",
      "Epoch: [3324/10000] Training loss: 3.8422108719272306e-05 / Validation loss: 0.00037891308406707886 / Long term Validation loss: 0.1249\n",
      "Epoch: [3325/10000] Training loss: 3.840446450423926e-05 / Validation loss: 0.00037694956797478966 / Long term Validation loss: 0.1241\n",
      "Epoch: [3326/10000] Training loss: 3.8400378216724546e-05 / Validation loss: 0.00037937310805049744 / Long term Validation loss: 0.1252\n",
      "Epoch: [3327/10000] Training loss: 3.838425028643506e-05 / Validation loss: 0.00037759741316334156 / Long term Validation loss: 0.1246\n",
      "Epoch: [3328/10000] Training loss: 3.836027952644656e-05 / Validation loss: 0.00037710245315516127 / Long term Validation loss: 0.1244\n",
      "Epoch: [3329/10000] Training loss: 3.834809527916212e-05 / Validation loss: 0.0003788125286642588 / Long term Validation loss: 0.1252\n",
      "Epoch: [3330/10000] Training loss: 3.8339608853393965e-05 / Validation loss: 0.00037661695769632364 / Long term Validation loss: 0.1244\n",
      "Epoch: [3331/10000] Training loss: 3.832085031963115e-05 / Validation loss: 0.0003773244116779483 / Long term Validation loss: 0.1247\n",
      "Epoch: [3332/10000] Training loss: 3.830153316375814e-05 / Validation loss: 0.0003779036489514529 / Long term Validation loss: 0.1250\n",
      "Epoch: [3333/10000] Training loss: 3.8290753958052364e-05 / Validation loss: 0.00037604815468174455 / Long term Validation loss: 0.1243\n",
      "Epoch: [3334/10000] Training loss: 3.8278673019088046e-05 / Validation loss: 0.00037738313583059863 / Long term Validation loss: 0.1250\n",
      "Epoch: [3335/10000] Training loss: 3.826004046714898e-05 / Validation loss: 0.0003768728908428979 / Long term Validation loss: 0.1248\n",
      "Epoch: [3336/10000] Training loss: 3.824388713106541e-05 / Validation loss: 0.00037582870830200085 / Long term Validation loss: 0.1245\n",
      "Epoch: [3337/10000] Training loss: 3.8232606608596954e-05 / Validation loss: 0.0003771353735192137 / Long term Validation loss: 0.1251\n",
      "Epoch: [3338/10000] Training loss: 3.8218428646362084e-05 / Validation loss: 0.00037593496544344375 / Long term Validation loss: 0.1247\n",
      "Epoch: [3339/10000] Training loss: 3.820091780330103e-05 / Validation loss: 0.0003757972380926751 / Long term Validation loss: 0.1247\n",
      "Epoch: [3340/10000] Training loss: 3.818642291472718e-05 / Validation loss: 0.0003765707410179599 / Long term Validation loss: 0.1251\n",
      "Epoch: [3341/10000] Training loss: 3.817412878029031e-05 / Validation loss: 0.00037522924065406274 / Long term Validation loss: 0.1246\n",
      "Epoch: [3342/10000] Training loss: 3.8159052371982265e-05 / Validation loss: 0.0003757645483411147 / Long term Validation loss: 0.1249\n",
      "Epoch: [3343/10000] Training loss: 3.8142709320951884e-05 / Validation loss: 0.00037580302690883976 / Long term Validation loss: 0.1249\n",
      "Epoch: [3344/10000] Training loss: 3.812886368323706e-05 / Validation loss: 0.0003747935758259469 / Long term Validation loss: 0.1246\n",
      "Epoch: [3345/10000] Training loss: 3.8115677709674156e-05 / Validation loss: 0.00037558087503682884 / Long term Validation loss: 0.1250\n",
      "Epoch: [3346/10000] Training loss: 3.8100400531107675e-05 / Validation loss: 0.00037500047573973977 / Long term Validation loss: 0.1248\n",
      "Epoch: [3347/10000] Training loss: 3.808493131879816e-05 / Validation loss: 0.0003745625564551794 / Long term Validation loss: 0.1247\n",
      "Epoch: [3348/10000] Training loss: 3.807120872883077e-05 / Validation loss: 0.00037517715757556347 / Long term Validation loss: 0.1250\n",
      "Epoch: [3349/10000] Training loss: 3.8057423514697546e-05 / Validation loss: 0.0003743033560494722 / Long term Validation loss: 0.1247\n",
      "Epoch: [3350/10000] Training loss: 3.804225677443393e-05 / Validation loss: 0.0003744025249495242 / Long term Validation loss: 0.1249\n",
      "Epoch: [3351/10000] Training loss: 3.802734019699259e-05 / Validation loss: 0.00037458077179686715 / Long term Validation loss: 0.1250\n",
      "Epoch: [3352/10000] Training loss: 3.801353059865055e-05 / Validation loss: 0.0003737801735488342 / Long term Validation loss: 0.1247\n",
      "Epoch: [3353/10000] Training loss: 3.799940677629338e-05 / Validation loss: 0.00037417883991607985 / Long term Validation loss: 0.1250\n",
      "Epoch: [3354/10000] Training loss: 3.798444082064327e-05 / Validation loss: 0.0003738958141684788 / Long term Validation loss: 0.1249\n",
      "Epoch: [3355/10000] Training loss: 3.796983639435561e-05 / Validation loss: 0.00037342146432493514 / Long term Validation loss: 0.1248\n",
      "Epoch: [3356/10000] Training loss: 3.7955892101301696e-05 / Validation loss: 0.0003738131440932693 / Long term Validation loss: 0.1250\n",
      "Epoch: [3357/10000] Training loss: 3.794160634203146e-05 / Validation loss: 0.0003732445561361945 / Long term Validation loss: 0.1249\n",
      "Epoch: [3358/10000] Training loss: 3.7926832087243165e-05 / Validation loss: 0.00037315298815799615 / Long term Validation loss: 0.1249\n",
      "Epoch: [3359/10000] Training loss: 3.791238986182836e-05 / Validation loss: 0.0003733017712820988 / Long term Validation loss: 0.1250\n",
      "Epoch: [3360/10000] Training loss: 3.789832872371466e-05 / Validation loss: 0.00037270547296427075 / Long term Validation loss: 0.1248\n",
      "Epoch: [3361/10000] Training loss: 3.7883982474966755e-05 / Validation loss: 0.00037287191026138795 / Long term Validation loss: 0.1250\n",
      "Epoch: [3362/10000] Training loss: 3.786935995154113e-05 / Validation loss: 0.0003727026794344714 / Long term Validation loss: 0.1250\n",
      "Epoch: [3363/10000] Training loss: 3.78549972779517e-05 / Validation loss: 0.00037228860388219953 / Long term Validation loss: 0.1249\n",
      "Epoch: [3364/10000] Training loss: 3.784085347987808e-05 / Validation loss: 0.00037250026512164344 / Long term Validation loss: 0.1250\n",
      "Epoch: [3365/10000] Training loss: 3.7826498189033606e-05 / Validation loss: 0.0003721028355994928 / Long term Validation loss: 0.1250\n",
      "Epoch: [3366/10000] Training loss: 3.781198633877966e-05 / Validation loss: 0.0003719487760194033 / Long term Validation loss: 0.1250\n",
      "Epoch: [3367/10000] Training loss: 3.77976622472257e-05 / Validation loss: 0.0003720209713523662 / Long term Validation loss: 0.1251\n",
      "Epoch: [3368/10000] Training loss: 3.778346715218389e-05 / Validation loss: 0.00037157374761898965 / Long term Validation loss: 0.1250\n",
      "Epoch: [3369/10000] Training loss: 3.7769125262617e-05 / Validation loss: 0.00037161516446973725 / Long term Validation loss: 0.1250\n",
      "Epoch: [3370/10000] Training loss: 3.775469215816733e-05 / Validation loss: 0.0003714730854889009 / Long term Validation loss: 0.1251\n",
      "Epoch: [3371/10000] Training loss: 3.774038853322448e-05 / Validation loss: 0.00037113675349100244 / Long term Validation loss: 0.1250\n",
      "Epoch: [3372/10000] Training loss: 3.77261651819653e-05 / Validation loss: 0.0003712263442419154 / Long term Validation loss: 0.1251\n",
      "Epoch: [3373/10000] Training loss: 3.7711844715959885e-05 / Validation loss: 0.0003709198939852549 / Long term Validation loss: 0.1250\n",
      "Epoch: [3374/10000] Training loss: 3.769746809965806e-05 / Validation loss: 0.000370761189468971 / Long term Validation loss: 0.1250\n",
      "Epoch: [3375/10000] Training loss: 3.768317815952278e-05 / Validation loss: 0.0003707608531169876 / Long term Validation loss: 0.1251\n",
      "Epoch: [3376/10000] Training loss: 3.7668942053591506e-05 / Validation loss: 0.00037041426772263053 / Long term Validation loss: 0.1250\n",
      "Epoch: [3377/10000] Training loss: 3.765464427362049e-05 / Validation loss: 0.000370392067572147 / Long term Validation loss: 0.1251\n",
      "Epoch: [3378/10000] Training loss: 3.76403096988131e-05 / Validation loss: 0.00037024402687593235 / Long term Validation loss: 0.1251\n",
      "Epoch: [3379/10000] Training loss: 3.7626031742448224e-05 / Validation loss: 0.0003699746460364033 / Long term Validation loss: 0.1251\n",
      "Epoch: [3380/10000] Training loss: 3.7611792591638036e-05 / Validation loss: 0.00036998391078761845 / Long term Validation loss: 0.1252\n",
      "Epoch: [3381/10000] Training loss: 3.7597516354230674e-05 / Validation loss: 0.0003697250766868299 / Long term Validation loss: 0.1251\n",
      "Epoch: [3382/10000] Training loss: 3.75832145757725e-05 / Validation loss: 0.00036958014534862197 / Long term Validation loss: 0.1251\n",
      "Epoch: [3383/10000] Training loss: 3.756894888176284e-05 / Validation loss: 0.00036952247745925296 / Long term Validation loss: 0.1252\n",
      "Epoch: [3384/10000] Training loss: 3.755471274278605e-05 / Validation loss: 0.0003692437720063073 / Long term Validation loss: 0.1251\n",
      "Epoch: [3385/10000] Training loss: 3.754045610152224e-05 / Validation loss: 0.00036918852324862813 / Long term Validation loss: 0.1252\n",
      "Epoch: [3386/10000] Training loss: 3.752618131321567e-05 / Validation loss: 0.00036902718540628176 / Long term Validation loss: 0.1252\n",
      "Epoch: [3387/10000] Training loss: 3.751192876525654e-05 / Validation loss: 0.00036881006735547625 / Long term Validation loss: 0.1252\n",
      "Epoch: [3388/10000] Training loss: 3.7497699332694134e-05 / Validation loss: 0.0003687653915022125 / Long term Validation loss: 0.1252\n",
      "Epoch: [3389/10000] Training loss: 3.7483460499267564e-05 / Validation loss: 0.0003685344327169784 / Long term Validation loss: 0.1252\n",
      "Epoch: [3390/10000] Training loss: 3.746920888713154e-05 / Validation loss: 0.00036840463789637685 / Long term Validation loss: 0.1252\n",
      "Epoch: [3391/10000] Training loss: 3.7454970405773604e-05 / Validation loss: 0.0003683037728670994 / Long term Validation loss: 0.1253\n",
      "Epoch: [3392/10000] Training loss: 3.744075008406141e-05 / Validation loss: 0.0003680722757766301 / Long term Validation loss: 0.1252\n",
      "Epoch: [3393/10000] Training loss: 3.742652751383076e-05 / Validation loss: 0.00036799558853933736 / Long term Validation loss: 0.1253\n",
      "Epoch: [3394/10000] Training loss: 3.741229653789543e-05 / Validation loss: 0.00036782272314635927 / Long term Validation loss: 0.1253\n",
      "Epoch: [3395/10000] Training loss: 3.739807286688171e-05 / Validation loss: 0.0003676439606864228 / Long term Validation loss: 0.1253\n",
      "Epoch: [3396/10000] Training loss: 3.738386325503724e-05 / Validation loss: 0.00036756035263993826 / Long term Validation loss: 0.1253\n",
      "Epoch: [3397/10000] Training loss: 3.736965582782936e-05 / Validation loss: 0.0003673500205633485 / Long term Validation loss: 0.1253\n",
      "Epoch: [3398/10000] Training loss: 3.7355443618216405e-05 / Validation loss: 0.00036723053037010625 / Long term Validation loss: 0.1253\n",
      "Epoch: [3399/10000] Training loss: 3.7341235278296087e-05 / Validation loss: 0.00036709877584844777 / Long term Validation loss: 0.1253\n",
      "Epoch: [3400/10000] Training loss: 3.732703756030001e-05 / Validation loss: 0.0003669024753324527 / Long term Validation loss: 0.1253\n",
      "Epoch: [3401/10000] Training loss: 3.731284445224918e-05 / Validation loss: 0.00036680765632597553 / Long term Validation loss: 0.1254\n",
      "Epoch: [3402/10000] Training loss: 3.729864961340873e-05 / Validation loss: 0.00036662903968354333 / Long term Validation loss: 0.1254\n",
      "Epoch: [3403/10000] Training loss: 3.728445686764685e-05 / Validation loss: 0.00036647665060068126 / Long term Validation loss: 0.1254\n",
      "Epoch: [3404/10000] Training loss: 3.727027193634143e-05 / Validation loss: 0.0003663631075671081 / Long term Validation loss: 0.1254\n",
      "Epoch: [3405/10000] Training loss: 3.725609266298575e-05 / Validation loss: 0.00036617144906605446 / Long term Validation loss: 0.1254\n",
      "Epoch: [3406/10000] Training loss: 3.724191405246734e-05 / Validation loss: 0.00036605497859831966 / Long term Validation loss: 0.1254\n",
      "Epoch: [3407/10000] Training loss: 3.72277369530919e-05 / Validation loss: 0.000365902986499419 / Long term Validation loss: 0.1254\n",
      "Epoch: [3408/10000] Training loss: 3.7213565508565564e-05 / Validation loss: 0.00036573360326077985 / Long term Validation loss: 0.1254\n",
      "Epoch: [3409/10000] Training loss: 3.719939981829921e-05 / Validation loss: 0.0003656210241384879 / Long term Validation loss: 0.1254\n",
      "Epoch: [3410/10000] Training loss: 3.7185236552131857e-05 / Validation loss: 0.00036544379886319295 / Long term Validation loss: 0.1254\n",
      "Epoch: [3411/10000] Training loss: 3.7171074937514514e-05 / Validation loss: 0.0003653072971270963 / Long term Validation loss: 0.1255\n",
      "Epoch: [3412/10000] Training loss: 3.715691749641627e-05 / Validation loss: 0.0003651713433461814 / Long term Validation loss: 0.1255\n",
      "Epoch: [3413/10000] Training loss: 3.714276536253107e-05 / Validation loss: 0.00036499807197537803 / Long term Validation loss: 0.1255\n",
      "Epoch: [3414/10000] Training loss: 3.7128616745956485e-05 / Validation loss: 0.0003648776477947955 / Long term Validation loss: 0.1255\n",
      "Epoch: [3415/10000] Training loss: 3.7114470326105994e-05 / Validation loss: 0.0003647155509774007 / Long term Validation loss: 0.1255\n",
      "Epoch: [3416/10000] Training loss: 3.7100327215670995e-05 / Validation loss: 0.0003645655511980197 / Long term Validation loss: 0.1255\n",
      "Epoch: [3417/10000] Training loss: 3.708618874012655e-05 / Validation loss: 0.0003644359478186831 / Long term Validation loss: 0.1255\n",
      "Epoch: [3418/10000] Training loss: 3.7072054308692865e-05 / Validation loss: 0.0003642663763733275 / Long term Validation loss: 0.1255\n",
      "Epoch: [3419/10000] Training loss: 3.705792269559243e-05 / Validation loss: 0.0003641360105898104 / Long term Validation loss: 0.1255\n",
      "Epoch: [3420/10000] Training loss: 3.704379406158232e-05 / Validation loss: 0.0003639854987059779 / Long term Validation loss: 0.1256\n",
      "Epoch: [3421/10000] Training loss: 3.702966941418356e-05 / Validation loss: 0.00036382889716039834 / Long term Validation loss: 0.1256\n",
      "Epoch: [3422/10000] Training loss: 3.7015548889062034e-05 / Validation loss: 0.0003636994661643179 / Long term Validation loss: 0.1256\n",
      "Epoch: [3423/10000] Training loss: 3.7001431701697854e-05 / Validation loss: 0.0003635365046396563 / Long term Validation loss: 0.1256\n",
      "Epoch: [3424/10000] Training loss: 3.6987317509938483e-05 / Validation loss: 0.0003633977695970725 / Long term Validation loss: 0.1256\n",
      "Epoch: [3425/10000] Training loss: 3.697320684543017e-05 / Validation loss: 0.0003632543595764621 / Long term Validation loss: 0.1256\n",
      "Epoch: [3426/10000] Training loss: 3.695910012102828e-05 / Validation loss: 0.00036309609078452446 / Long term Validation loss: 0.1256\n",
      "Epoch: [3427/10000] Training loss: 3.694499702536187e-05 / Validation loss: 0.0003629638082444185 / Long term Validation loss: 0.1256\n",
      "Epoch: [3428/10000] Training loss: 3.693089712478242e-05 / Validation loss: 0.0003628072987868727 / Long term Validation loss: 0.1256\n",
      "Epoch: [3429/10000] Training loss: 3.691680052104731e-05 / Validation loss: 0.00036266286854832043 / Long term Validation loss: 0.1256\n",
      "Epoch: [3430/10000] Training loss: 3.6902707592160876e-05 / Validation loss: 0.00036252286130123215 / Long term Validation loss: 0.1257\n",
      "Epoch: [3431/10000] Training loss: 3.6888618364300414e-05 / Validation loss: 0.00036236573434709785 / Long term Validation loss: 0.1257\n",
      "Epoch: [3432/10000] Training loss: 3.687453253391691e-05 / Validation loss: 0.0003622299111498647 / Long term Validation loss: 0.1257\n",
      "Epoch: [3433/10000] Training loss: 3.686044997331508e-05 / Validation loss: 0.0003620785383160705 / Long term Validation loss: 0.1257\n",
      "Epoch: [3434/10000] Training loss: 3.684637086941152e-05 / Validation loss: 0.00036193102599600083 / Long term Validation loss: 0.1257\n",
      "Epoch: [3435/10000] Training loss: 3.6832295385594906e-05 / Validation loss: 0.0003617920844901722 / Long term Validation loss: 0.1257\n",
      "Epoch: [3436/10000] Training loss: 3.6818223418488156e-05 / Validation loss: 0.00036163726928120806 / Long term Validation loss: 0.1257\n",
      "Epoch: [3437/10000] Training loss: 3.6804154790593404e-05 / Validation loss: 0.00036149833505674003 / Long term Validation loss: 0.1257\n",
      "Epoch: [3438/10000] Training loss: 3.679008951565848e-05 / Validation loss: 0.0003613504661467391 / Long term Validation loss: 0.1257\n",
      "Epoch: [3439/10000] Training loss: 3.6776027729065634e-05 / Validation loss: 0.0003612016913815412 / Long term Validation loss: 0.1258\n",
      "Epoch: [3440/10000] Training loss: 3.676196946586689e-05 / Validation loss: 0.0003610625112208431 / Long term Validation loss: 0.1258\n",
      "Epoch: [3441/10000] Training loss: 3.674791462201592e-05 / Validation loss: 0.00036091013219754245 / Long term Validation loss: 0.1258\n",
      "Epoch: [3442/10000] Training loss: 3.673386311944396e-05 / Validation loss: 0.00036076891639081796 / Long term Validation loss: 0.1258\n",
      "Epoch: [3443/10000] Training loss: 3.671981500634722e-05 / Validation loss: 0.0003606232218683997 / Long term Validation loss: 0.1258\n",
      "Epoch: [3444/10000] Training loss: 3.670577035387098e-05 / Validation loss: 0.0003604743618236992 / Long term Validation loss: 0.1258\n",
      "Epoch: [3445/10000] Training loss: 3.669172914669815e-05 / Validation loss: 0.00036033448434537053 / Long term Validation loss: 0.1258\n",
      "Epoch: [3446/10000] Training loss: 3.6677691313915375e-05 / Validation loss: 0.0003601842712146111 / Long term Validation loss: 0.1258\n",
      "Epoch: [3447/10000] Training loss: 3.666365682879679e-05 / Validation loss: 0.000360041634205921 / Long term Validation loss: 0.1258\n",
      "Epoch: [3448/10000] Training loss: 3.664962573226345e-05 / Validation loss: 0.0003598971789108356 / Long term Validation loss: 0.1259\n",
      "Epoch: [3449/10000] Training loss: 3.663559805382908e-05 / Validation loss: 0.0003597488290121327 / Long term Validation loss: 0.1259\n",
      "Epoch: [3450/10000] Training loss: 3.662157376757414e-05 / Validation loss: 0.0003596081698889651 / Long term Validation loss: 0.1259\n",
      "Epoch: [3451/10000] Training loss: 3.6607552831610626e-05 / Validation loss: 0.00035945965991348206 / Long term Validation loss: 0.1259\n",
      "Epoch: [3452/10000] Training loss: 3.6593535239055666e-05 / Validation loss: 0.0003593162487285939 / Long term Validation loss: 0.1259\n",
      "Epoch: [3453/10000] Training loss: 3.657952101560173e-05 / Validation loss: 0.00035917243512261654 / Long term Validation loss: 0.1259\n",
      "Epoch: [3454/10000] Training loss: 3.6565510170064e-05 / Validation loss: 0.0003590248501993038 / Long term Validation loss: 0.1259\n",
      "Epoch: [3455/10000] Training loss: 3.655150267979684e-05 / Validation loss: 0.00035888355058215674 / Long term Validation loss: 0.1259\n",
      "Epoch: [3456/10000] Training loss: 3.653749852075524e-05 / Validation loss: 0.0003587363516247953 / Long term Validation loss: 0.1259\n",
      "Epoch: [3457/10000] Training loss: 3.652349769127411e-05 / Validation loss: 0.000358592656053459 / Long term Validation loss: 0.1260\n",
      "Epoch: [3458/10000] Training loss: 3.650950020493142e-05 / Validation loss: 0.00035844917546849737 / Long term Validation loss: 0.1260\n",
      "Epoch: [3459/10000] Training loss: 3.649550606195696e-05 / Validation loss: 0.0003583024279753101 / Long term Validation loss: 0.1260\n",
      "Epoch: [3460/10000] Training loss: 3.648151524543988e-05 / Validation loss: 0.0003581606778708611 / Long term Validation loss: 0.1260\n",
      "Epoch: [3461/10000] Training loss: 3.6467527740729746e-05 / Validation loss: 0.00035801446428740456 / Long term Validation loss: 0.1260\n",
      "Epoch: [3462/10000] Training loss: 3.6453543546196453e-05 / Validation loss: 0.000357870779011246 / Long term Validation loss: 0.1260\n",
      "Epoch: [3463/10000] Training loss: 3.643956266778061e-05 / Validation loss: 0.0003577274649322985 / Long term Validation loss: 0.1260\n",
      "Epoch: [3464/10000] Training loss: 3.6425585102703624e-05 / Validation loss: 0.000357581521446241 / Long term Validation loss: 0.1260\n",
      "Epoch: [3465/10000] Training loss: 3.641161083874541e-05 / Validation loss: 0.00035743949613004534 / Long term Validation loss: 0.1261\n",
      "Epoch: [3466/10000] Training loss: 3.6397639865873886e-05 / Validation loss: 0.00035729404352204607 / Long term Validation loss: 0.1261\n",
      "Epoch: [3467/10000] Training loss: 3.638367218128756e-05 / Validation loss: 0.000357150554070347 / Long term Validation loss: 0.1261\n",
      "Epoch: [3468/10000] Training loss: 3.6369707786519285e-05 / Validation loss: 0.00035700735250612716 / Long term Validation loss: 0.1261\n",
      "Epoch: [3469/10000] Training loss: 3.635574667794166e-05 / Validation loss: 0.0003568621607783627 / Long term Validation loss: 0.1261\n",
      "Epoch: [3470/10000] Training loss: 3.634178884653222e-05 / Validation loss: 0.0003567200064593255 / Long term Validation loss: 0.1261\n",
      "Epoch: [3471/10000] Training loss: 3.6327834284540947e-05 / Validation loss: 0.00035657516982977674 / Long term Validation loss: 0.1261\n",
      "Epoch: [3472/10000] Training loss: 3.631388298809438e-05 / Validation loss: 0.0003564319729053016 / Long term Validation loss: 0.1261\n",
      "Epoch: [3473/10000] Training loss: 3.62999349562246e-05 / Validation loss: 0.00035628887211485176 / Long term Validation loss: 0.1261\n",
      "Epoch: [3474/10000] Training loss: 3.628599018513448e-05 / Validation loss: 0.00035614436675506157 / Long term Validation loss: 0.1262\n",
      "Epoch: [3475/10000] Training loss: 3.627204866785339e-05 / Validation loss: 0.00035600218232422664 / Long term Validation loss: 0.1262\n",
      "Epoch: [3476/10000] Training loss: 3.6258110397859193e-05 / Validation loss: 0.00035585787166020664 / Long term Validation loss: 0.1262\n",
      "Epoch: [3477/10000] Training loss: 3.624417537058495e-05 / Validation loss: 0.00035571500937546594 / Long term Validation loss: 0.1262\n",
      "Epoch: [3478/10000] Training loss: 3.6230243583604645e-05 / Validation loss: 0.00035557202461077974 / Long term Validation loss: 0.1262\n",
      "Epoch: [3479/10000] Training loss: 3.6216315033081314e-05 / Validation loss: 0.0003554281500381899 / Long term Validation loss: 0.1262\n",
      "Epoch: [3480/10000] Training loss: 3.620238971333837e-05 / Validation loss: 0.00035528600364805507 / Long term Validation loss: 0.1262\n",
      "Epoch: [3481/10000] Training loss: 3.618846761861942e-05 / Validation loss: 0.00035514217268881255 / Long term Validation loss: 0.1262\n",
      "Epoch: [3482/10000] Training loss: 3.617454874401631e-05 / Validation loss: 0.0003549996546756535 / Long term Validation loss: 0.1262\n",
      "Epoch: [3483/10000] Training loss: 3.6160633086218065e-05 / Validation loss: 0.0003548568143168639 / Long term Validation loss: 0.1263\n",
      "Epoch: [3484/10000] Training loss: 3.614672064131427e-05 / Validation loss: 0.00035471352321061886 / Long term Validation loss: 0.1263\n",
      "Epoch: [3485/10000] Training loss: 3.613281140442992e-05 / Validation loss: 0.0003545714583529238 / Long term Validation loss: 0.1263\n",
      "Epoch: [3486/10000] Training loss: 3.611890537035625e-05 / Validation loss: 0.0003544280876548773 / Long term Validation loss: 0.1263\n",
      "Epoch: [3487/10000] Training loss: 3.6105002534109276e-05 / Validation loss: 0.00035428590176322374 / Long term Validation loss: 0.1263\n",
      "Epoch: [3488/10000] Training loss: 3.609110289183476e-05 / Validation loss: 0.00035414324056640584 / Long term Validation loss: 0.1263\n",
      "Epoch: [3489/10000] Training loss: 3.6077206439497055e-05 / Validation loss: 0.00035400048970102017 / Long term Validation loss: 0.1263\n",
      "Epoch: [3490/10000] Training loss: 3.6063313172689425e-05 / Validation loss: 0.0003538585338622894 / Long term Validation loss: 0.1263\n",
      "Epoch: [3491/10000] Training loss: 3.604942308661678e-05 / Validation loss: 0.00035371561889619633 / Long term Validation loss: 0.1264\n",
      "Epoch: [3492/10000] Training loss: 3.6035536176398334e-05 / Validation loss: 0.00035357373974881427 / Long term Validation loss: 0.1264\n",
      "Epoch: [3493/10000] Training loss: 3.602165243786912e-05 / Validation loss: 0.0003534312983592098 / Long term Validation loss: 0.1264\n",
      "Epoch: [3494/10000] Training loss: 3.600777186685029e-05 / Validation loss: 0.0003532890471081548 / Long term Validation loss: 0.1264\n",
      "Epoch: [3495/10000] Training loss: 3.5993894459170765e-05 / Validation loss: 0.0003531472246065937 / Long term Validation loss: 0.1264\n",
      "Epoch: [3496/10000] Training loss: 3.598002021033441e-05 / Validation loss: 0.00035300477048899576 / Long term Validation loss: 0.1264\n",
      "Epoch: [3497/10000] Training loss: 3.596614911566418e-05 / Validation loss: 0.00035286316916722734 / Long term Validation loss: 0.1264\n",
      "Epoch: [3498/10000] Training loss: 3.5952281170858634e-05 / Validation loss: 0.0003527209939202762 / Long term Validation loss: 0.1264\n",
      "Epoch: [3499/10000] Training loss: 3.593841637162565e-05 / Validation loss: 0.0003525791990670658 / Long term Validation loss: 0.1264\n",
      "Epoch: [3500/10000] Training loss: 3.592455471387525e-05 / Validation loss: 0.0003524375349204109 / Long term Validation loss: 0.1265\n",
      "Epoch: [3501/10000] Training loss: 3.591069619329987e-05 / Validation loss: 0.00035229554321012193 / Long term Validation loss: 0.1265\n",
      "Epoch: [3502/10000] Training loss: 3.589684080545311e-05 / Validation loss: 0.00035215418877949657 / Long term Validation loss: 0.1265\n",
      "Epoch: [3503/10000] Training loss: 3.588298854602295e-05 / Validation loss: 0.00035201232575699437 / Long term Validation loss: 0.1265\n",
      "Epoch: [3504/10000] Training loss: 3.5869139410671793e-05 / Validation loss: 0.00035187093883824353 / Long term Validation loss: 0.1265\n",
      "Epoch: [3505/10000] Training loss: 3.585529339530828e-05 / Validation loss: 0.00035172946589882815 / Long term Validation loss: 0.1265\n",
      "Epoch: [3506/10000] Training loss: 3.584145049571849e-05 / Validation loss: 0.0003515879314394568 / Long term Validation loss: 0.1265\n",
      "Epoch: [3507/10000] Training loss: 3.582761070765373e-05 / Validation loss: 0.00035144680268601027 / Long term Validation loss: 0.1265\n",
      "Epoch: [3508/10000] Training loss: 3.581377402686425e-05 / Validation loss: 0.000351305297234465 / Long term Validation loss: 0.1265\n",
      "Epoch: [3509/10000] Training loss: 3.579994044904708e-05 / Validation loss: 0.00035116426847728435 / Long term Validation loss: 0.1266\n",
      "Epoch: [3510/10000] Training loss: 3.578610997009146e-05 / Validation loss: 0.0003510230293022068 / Long term Validation loss: 0.1266\n",
      "Epoch: [3511/10000] Training loss: 3.577228258581914e-05 / Validation loss: 0.0003508819325837765 / Long term Validation loss: 0.1266\n",
      "Epoch: [3512/10000] Training loss: 3.575845829211328e-05 / Validation loss: 0.00035074102047148297 / Long term Validation loss: 0.1266\n",
      "Epoch: [3513/10000] Training loss: 3.5744637084805905e-05 / Validation loss: 0.0003505999056698957 / Long term Validation loss: 0.1266\n",
      "Epoch: [3514/10000] Training loss: 3.5730818959688075e-05 / Validation loss: 0.0003504591856303837 / Long term Validation loss: 0.1266\n",
      "Epoch: [3515/10000] Training loss: 3.5717003912652514e-05 / Validation loss: 0.0003503182282931476 / Long term Validation loss: 0.1266\n",
      "Epoch: [3516/10000] Training loss: 3.570319193954563e-05 / Validation loss: 0.0003501775348020635 / Long term Validation loss: 0.1266\n",
      "Epoch: [3517/10000] Training loss: 3.568938303631911e-05 / Validation loss: 0.0003500368497798793 / Long term Validation loss: 0.1267\n",
      "Epoch: [3518/10000] Training loss: 3.5675577198871164e-05 / Validation loss: 0.00034989614259242597 / Long term Validation loss: 0.1267\n",
      "Epoch: [3519/10000] Training loss: 3.5661774423103684e-05 / Validation loss: 0.00034975569409032204 / Long term Validation loss: 0.1267\n",
      "Epoch: [3520/10000] Training loss: 3.564797470494463e-05 / Validation loss: 0.00034961506726860026 / Long term Validation loss: 0.1267\n",
      "Epoch: [3521/10000] Training loss: 3.563417804028869e-05 / Validation loss: 0.00034947473242946973 / Long term Validation loss: 0.1267\n",
      "Epoch: [3522/10000] Training loss: 3.5620384425122324e-05 / Validation loss: 0.0003493343035358016 / Long term Validation loss: 0.1267\n",
      "Epoch: [3523/10000] Training loss: 3.5606593855389984e-05 / Validation loss: 0.0003491939973923937 / Long term Validation loss: 0.1267\n",
      "Epoch: [3524/10000] Training loss: 3.559280632708415e-05 / Validation loss: 0.00034905380152221567 / Long term Validation loss: 0.1267\n",
      "Epoch: [3525/10000] Training loss: 3.5579021836182825e-05 / Validation loss: 0.0003489135411669778 / Long term Validation loss: 0.1268\n",
      "Epoch: [3526/10000] Training loss: 3.556524037865579e-05 / Validation loss: 0.00034877351898191715 / Long term Validation loss: 0.1268\n",
      "Epoch: [3527/10000] Training loss: 3.555146195052134e-05 / Validation loss: 0.0003486333872951693 / Long term Validation loss: 0.1268\n",
      "Epoch: [3528/10000] Training loss: 3.553768654776881e-05 / Validation loss: 0.00034849345444611503 / Long term Validation loss: 0.1268\n",
      "Epoch: [3529/10000] Training loss: 3.552391416645014e-05 / Validation loss: 0.0003483535177697257 / Long term Validation loss: 0.1268\n",
      "Epoch: [3530/10000] Training loss: 3.551014480259129e-05 / Validation loss: 0.000348213638881149 / Long term Validation loss: 0.1268\n",
      "Epoch: [3531/10000] Training loss: 3.549637845224222e-05 / Validation loss: 0.00034807389701883856 / Long term Validation loss: 0.1268\n",
      "Epoch: [3532/10000] Training loss: 3.54826151114615e-05 / Validation loss: 0.0003479341033125182 / Long term Validation loss: 0.1268\n",
      "Epoch: [3533/10000] Training loss: 3.546885477629815e-05 / Validation loss: 0.00034779450405967097 / Long term Validation loss: 0.1268\n",
      "Epoch: [3534/10000] Training loss: 3.545509744284581e-05 / Validation loss: 0.00034765485438532483 / Long term Validation loss: 0.1269\n",
      "Epoch: [3535/10000] Training loss: 3.544134310717681e-05 / Validation loss: 0.00034751534533892413 / Long term Validation loss: 0.1269\n",
      "Epoch: [3536/10000] Training loss: 3.5427591765405454e-05 / Validation loss: 0.00034737587485376856 / Long term Validation loss: 0.1269\n",
      "Epoch: [3537/10000] Training loss: 3.541384341363473e-05 / Validation loss: 0.0003472364433134512 / Long term Validation loss: 0.1269\n",
      "Epoch: [3538/10000] Training loss: 3.5400098047982155e-05 / Validation loss: 0.0003470971420864056 / Long term Validation loss: 0.1269\n",
      "Epoch: [3539/10000] Training loss: 3.538635566458142e-05 / Validation loss: 0.00034695781578492275 / Long term Validation loss: 0.1269\n",
      "Epoch: [3540/10000] Training loss: 3.537261625955891e-05 / Validation loss: 0.0003468186459533725 / Long term Validation loss: 0.1269\n",
      "Epoch: [3541/10000] Training loss: 3.535887982907678e-05 / Validation loss: 0.0003466794638379821 / Long term Validation loss: 0.1269\n",
      "Epoch: [3542/10000] Training loss: 3.534514636928376e-05 / Validation loss: 0.0003465403931041873 / Long term Validation loss: 0.1270\n",
      "Epoch: [3543/10000] Training loss: 3.533141587635913e-05 / Validation loss: 0.00034640137492857374 / Long term Validation loss: 0.1270\n",
      "Epoch: [3544/10000] Training loss: 3.531768834647895e-05 / Validation loss: 0.0003462623981768718 / Long term Validation loss: 0.1270\n",
      "Epoch: [3545/10000] Training loss: 3.530396377583117e-05 / Validation loss: 0.00034612353486348694 / Long term Validation loss: 0.1270\n",
      "Epoch: [3546/10000] Training loss: 3.529024216061965e-05 / Validation loss: 0.000345984671642881 / Long term Validation loss: 0.1270\n",
      "Epoch: [3547/10000] Training loss: 3.527652349704498e-05 / Validation loss: 0.0003458459378667529 / Long term Validation loss: 0.1270\n",
      "Epoch: [3548/10000] Training loss: 3.526280778133567e-05 / Validation loss: 0.0003457072136006164 / Long term Validation loss: 0.1270\n",
      "Epoch: [3549/10000] Training loss: 3.52490950097129e-05 / Validation loss: 0.00034556858833803235 / Long term Validation loss: 0.1270\n",
      "Epoch: [3550/10000] Training loss: 3.523538517842244e-05 / Validation loss: 0.000345430016114581 / Long term Validation loss: 0.1271\n",
      "Epoch: [3551/10000] Training loss: 3.522167828371014e-05 / Validation loss: 0.0003452914953998671 / Long term Validation loss: 0.1271\n",
      "Epoch: [3552/10000] Training loss: 3.520797432183406e-05 / Validation loss: 0.00034515307022478964 / Long term Validation loss: 0.1271\n",
      "Epoch: [3553/10000] Training loss: 3.51942732890652e-05 / Validation loss: 0.00034501466580580284 / Long term Validation loss: 0.1271\n",
      "Epoch: [3554/10000] Training loss: 3.5180575181675806e-05 / Validation loss: 0.00034487637179508705 / Long term Validation loss: 0.1271\n",
      "Epoch: [3555/10000] Training loss: 3.516687999595977e-05 / Validation loss: 0.0003447381002092385 / Long term Validation loss: 0.1271\n",
      "Epoch: [3556/10000] Training loss: 3.5153187728208243e-05 / Validation loss: 0.00034459992280362096 / Long term Validation loss: 0.1271\n",
      "Epoch: [3557/10000] Training loss: 3.5139498374733304e-05 / Validation loss: 0.0003444617941815632 / Long term Validation loss: 0.1271\n",
      "Epoch: [3558/10000] Training loss: 3.512581193184843e-05 / Validation loss: 0.0003443237285437924 / Long term Validation loss: 0.1272\n",
      "Epoch: [3559/10000] Training loss: 3.5112128395880366e-05 / Validation loss: 0.00034418574201728377 / Long term Validation loss: 0.1272\n",
      "Epoch: [3560/10000] Training loss: 3.509844776316577e-05 / Validation loss: 0.00034404779358853646 / Long term Validation loss: 0.1272\n",
      "Epoch: [3561/10000] Training loss: 3.508477003004646e-05 / Validation loss: 0.00034390994027168196 / Long term Validation loss: 0.1272\n",
      "Epoch: [3562/10000] Training loss: 3.507109519288082e-05 / Validation loss: 0.0003437721192621317 / Long term Validation loss: 0.1272\n",
      "Epoch: [3563/10000] Training loss: 3.505742324802836e-05 / Validation loss: 0.00034363438905323727 / Long term Validation loss: 0.1272\n",
      "Epoch: [3564/10000] Training loss: 3.504375419186659e-05 / Validation loss: 0.00034349670364121115 / Long term Validation loss: 0.1272\n",
      "Epoch: [3565/10000] Training loss: 3.5030088020775166e-05 / Validation loss: 0.0003433590909797368 / Long term Validation loss: 0.1272\n",
      "Epoch: [3566/10000] Training loss: 3.5016424731148186e-05 / Validation loss: 0.00034322154333982317 / Long term Validation loss: 0.1273\n",
      "Epoch: [3567/10000] Training loss: 3.500276431938663e-05 / Validation loss: 0.00034308404904696393 / Long term Validation loss: 0.1273\n",
      "Epoch: [3568/10000] Training loss: 3.498910678190042e-05 / Validation loss: 0.00034294663557391913 / Long term Validation loss: 0.1273\n",
      "Epoch: [3569/10000] Training loss: 3.4975452115111614e-05 / Validation loss: 0.0003428092648783458 / Long term Validation loss: 0.1273\n",
      "Epoch: [3570/10000] Training loss: 3.4961800315446983e-05 / Validation loss: 0.000342671979331473 / Long term Validation loss: 0.1273\n",
      "Epoch: [3571/10000] Training loss: 3.4948151379348237e-05 / Validation loss: 0.0003425347381702944 / Long term Validation loss: 0.1273\n",
      "Epoch: [3572/10000] Training loss: 3.493450530326061e-05 / Validation loss: 0.0003423975753222798 / Long term Validation loss: 0.1273\n",
      "Epoch: [3573/10000] Training loss: 3.492086208364376e-05 / Validation loss: 0.0003422604672640254 / Long term Validation loss: 0.1273\n",
      "Epoch: [3574/10000] Training loss: 3.4907221716962637e-05 / Validation loss: 0.0003421234251037078 / Long term Validation loss: 0.1274\n",
      "Epoch: [3575/10000] Training loss: 3.489358419969365e-05 / Validation loss: 0.0003419864502130235 / Long term Validation loss: 0.1274\n",
      "Epoch: [3576/10000] Training loss: 3.487994952832191e-05 / Validation loss: 0.0003418495300518057 / Long term Validation loss: 0.1274\n",
      "Epoch: [3577/10000] Training loss: 3.4866317699340584e-05 / Validation loss: 0.0003417126856692017 / Long term Validation loss: 0.1274\n",
      "Epoch: [3578/10000] Training loss: 3.4852688709254405e-05 / Validation loss: 0.00034157589070470866 / Long term Validation loss: 0.1274\n",
      "Epoch: [3579/10000] Training loss: 3.483906255457395e-05 / Validation loss: 0.0003414391732275309 / Long term Validation loss: 0.1274\n",
      "Epoch: [3580/10000] Training loss: 3.482543923182282e-05 / Validation loss: 0.00034130250666670645 / Long term Validation loss: 0.1274\n",
      "Epoch: [3581/10000] Training loss: 3.481181873752983e-05 / Validation loss: 0.00034116591325222183 / Long term Validation loss: 0.1274\n",
      "Epoch: [3582/10000] Training loss: 3.4798201068236376e-05 / Validation loss: 0.00034102937694818357 / Long term Validation loss: 0.1275\n",
      "Epoch: [3583/10000] Training loss: 3.478458622049028e-05 / Validation loss: 0.0003408929064354203 / Long term Validation loss: 0.1275\n",
      "Epoch: [3584/10000] Training loss: 3.4770974190850115e-05 / Validation loss: 0.00034075650045935164 / Long term Validation loss: 0.1275\n",
      "Epoch: [3585/10000] Training loss: 3.475736497588292e-05 / Validation loss: 0.00034062015336167983 / Long term Validation loss: 0.1275\n",
      "Epoch: [3586/10000] Training loss: 3.474375857216443e-05 / Validation loss: 0.00034048387639538594 / Long term Validation loss: 0.1275\n",
      "Epoch: [3587/10000] Training loss: 3.473015497628064e-05 / Validation loss: 0.0003403476542466587 / Long term Validation loss: 0.1275\n",
      "Epoch: [3588/10000] Training loss: 3.471655418482473e-05 / Validation loss: 0.0003402115043845798 / Long term Validation loss: 0.1275\n",
      "Epoch: [3589/10000] Training loss: 3.470295619440115e-05 / Validation loss: 0.0003400754088903629 / Long term Validation loss: 0.1275\n",
      "Epoch: [3590/10000] Training loss: 3.468936100162097e-05 / Validation loss: 0.00033993938442001707 / Long term Validation loss: 0.1276\n",
      "Epoch: [3591/10000] Training loss: 3.4675768603106666e-05 / Validation loss: 0.00033980341679407604 / Long term Validation loss: 0.1276\n",
      "Epoch: [3592/10000] Training loss: 3.466217899548758e-05 / Validation loss: 0.0003396675166810099 / Long term Validation loss: 0.1276\n",
      "Epoch: [3593/10000] Training loss: 3.464859217540375e-05 / Validation loss: 0.00033953167734886245 / Long term Validation loss: 0.1276\n",
      "Epoch: [3594/10000] Training loss: 3.4635008139503004e-05 / Validation loss: 0.0003393959013533572 / Long term Validation loss: 0.1276\n",
      "Epoch: [3595/10000] Training loss: 3.462142688444279e-05 / Validation loss: 0.00033926019000540835 / Long term Validation loss: 0.1276\n",
      "Epoch: [3596/10000] Training loss: 3.460784840688944e-05 / Validation loss: 0.0003391245385106379 / Long term Validation loss: 0.1276\n",
      "Epoch: [3597/10000] Training loss: 3.4594272703517795e-05 / Validation loss: 0.00033898895436842806 / Long term Validation loss: 0.1276\n",
      "Epoch: [3598/10000] Training loss: 3.458069977101255e-05 / Validation loss: 0.0003388534280692919 / Long term Validation loss: 0.1276\n",
      "Epoch: [3599/10000] Training loss: 3.4567129606066096e-05 / Validation loss: 0.0003387179702060385 / Long term Validation loss: 0.1277\n",
      "Epoch: [3600/10000] Training loss: 3.4553562205381254e-05 / Validation loss: 0.00033858256980260146 / Long term Validation loss: 0.1277\n",
      "Epoch: [3601/10000] Training loss: 3.4539997565668175e-05 / Validation loss: 0.00033844723740229115 / Long term Validation loss: 0.1277\n",
      "Epoch: [3602/10000] Training loss: 3.452643568364753e-05 / Validation loss: 0.0003383119633901806 / Long term Validation loss: 0.1277\n",
      "Epoch: [3603/10000] Training loss: 3.451287655604745e-05 / Validation loss: 0.00033817675589290614 / Long term Validation loss: 0.1277\n",
      "Epoch: [3604/10000] Training loss: 3.449932017960625e-05 / Validation loss: 0.0003380416084786821 / Long term Validation loss: 0.1277\n",
      "Epoch: [3605/10000] Training loss: 3.4485766551070076e-05 / Validation loss: 0.00033790652561269027 / Long term Validation loss: 0.1277\n",
      "Epoch: [3606/10000] Training loss: 3.4472215667194794e-05 / Validation loss: 0.00033777150473229253 / Long term Validation loss: 0.1277\n",
      "Epoch: [3607/10000] Training loss: 3.4458667524744696e-05 / Validation loss: 0.00033763654646403124 / Long term Validation loss: 0.1278\n",
      "Epoch: [3608/10000] Training loss: 3.444512212049313e-05 / Validation loss: 0.0003375016518599502 / Long term Validation loss: 0.1278\n",
      "Epoch: [3609/10000] Training loss: 3.4431579451222366e-05 / Validation loss: 0.0003373668183040167 / Long term Validation loss: 0.1278\n",
      "Epoch: [3610/10000] Training loss: 3.4418039513723377e-05 / Validation loss: 0.00033723204961871774 / Long term Validation loss: 0.1278\n",
      "Epoch: [3611/10000] Training loss: 3.4404502304796415e-05 / Validation loss: 0.00033709734094530293 / Long term Validation loss: 0.1278\n",
      "Epoch: [3612/10000] Training loss: 3.439096782124997e-05 / Validation loss: 0.0003369626978024735 / Long term Validation loss: 0.1278\n",
      "Epoch: [3613/10000] Training loss: 3.437743605990219e-05 / Validation loss: 0.00033682811416650986 / Long term Validation loss: 0.1278\n",
      "Epoch: [3614/10000] Training loss: 3.436390701757929e-05 / Validation loss: 0.00033669359622675904 / Long term Validation loss: 0.1278\n",
      "Epoch: [3615/10000] Training loss: 3.435038069111712e-05 / Validation loss: 0.00033655913772725994 / Long term Validation loss: 0.1279\n",
      "Epoch: [3616/10000] Training loss: 3.433685707735963e-05 / Validation loss: 0.00033642474471597676 / Long term Validation loss: 0.1279\n",
      "Epoch: [3617/10000] Training loss: 3.432333617316042e-05 / Validation loss: 0.0003362904113822223 / Long term Validation loss: 0.1279\n",
      "Epoch: [3618/10000] Training loss: 3.4309817975381174e-05 / Validation loss: 0.0003361561430941768 / Long term Validation loss: 0.1279\n",
      "Epoch: [3619/10000] Training loss: 3.4296302480893135e-05 / Validation loss: 0.00033602193489014445 / Long term Validation loss: 0.1279\n",
      "Epoch: [3620/10000] Training loss: 3.428278968657576e-05 / Validation loss: 0.0003358877911790126 / Long term Validation loss: 0.1279\n",
      "Epoch: [3621/10000] Training loss: 3.426927958931797e-05 / Validation loss: 0.0003357537080174826 / Long term Validation loss: 0.1279\n",
      "Epoch: [3622/10000] Training loss: 3.425577218601696e-05 / Validation loss: 0.0003356196887788598 / Long term Validation loss: 0.1279\n",
      "Epoch: [3623/10000] Training loss: 3.424226747357928e-05 / Validation loss: 0.0003354857305386849 / Long term Validation loss: 0.1279\n",
      "Epoch: [3624/10000] Training loss: 3.422876544891989e-05 / Validation loss: 0.0003353518356929422 / Long term Validation loss: 0.1280\n",
      "Epoch: [3625/10000] Training loss: 3.4215266108962973e-05 / Validation loss: 0.0003352180022350865 / Long term Validation loss: 0.1280\n",
      "Epoch: [3626/10000] Training loss: 3.420176945064123e-05 / Validation loss: 0.00033508423171330927 / Long term Validation loss: 0.1280\n",
      "Epoch: [3627/10000] Training loss: 3.4188275470896465e-05 / Validation loss: 0.00033495052289318604 / Long term Validation loss: 0.1280\n",
      "Epoch: [3628/10000] Training loss: 3.4174784166679163e-05 / Validation loss: 0.00033481687662708406 / Long term Validation loss: 0.1280\n",
      "Epoch: [3629/10000] Training loss: 3.416129553494872e-05 / Validation loss: 0.00033468329230259734 / Long term Validation loss: 0.1280\n",
      "Epoch: [3630/10000] Training loss: 3.414780957267331e-05 / Validation loss: 0.00033454977021819175 / Long term Validation loss: 0.1280\n",
      "Epoch: [3631/10000] Training loss: 3.4134326276830026e-05 / Validation loss: 0.00033441631025429505 / Long term Validation loss: 0.1280\n",
      "Epoch: [3632/10000] Training loss: 3.412084564440477e-05 / Validation loss: 0.0003342829122687088 / Long term Validation loss: 0.1281\n",
      "Epoch: [3633/10000] Training loss: 3.410736767239223e-05 / Validation loss: 0.00033414957653974477 / Long term Validation loss: 0.1281\n",
      "Epoch: [3634/10000] Training loss: 3.4093892357795994e-05 / Validation loss: 0.00033401630255995644 / Long term Validation loss: 0.1281\n",
      "Epoch: [3635/10000] Training loss: 3.408041969762847e-05 / Validation loss: 0.00033388309095076833 / Long term Validation loss: 0.1281\n",
      "Epoch: [3636/10000] Training loss: 3.406694968891094e-05 / Validation loss: 0.0003337499408730651 / Long term Validation loss: 0.1281\n",
      "Epoch: [3637/10000] Training loss: 3.405348232867343e-05 / Validation loss: 0.00033361685327960183 / Long term Validation loss: 0.1281\n",
      "Epoch: [3638/10000] Training loss: 3.404001761395492e-05 / Validation loss: 0.00033348382698857086 / Long term Validation loss: 0.1281\n",
      "Epoch: [3639/10000] Training loss: 3.402655554180309e-05 / Validation loss: 0.0003333508633189544 / Long term Validation loss: 0.1281\n",
      "Epoch: [3640/10000] Training loss: 3.401309610927465e-05 / Validation loss: 0.0003332179606853791 / Long term Validation loss: 0.1281\n",
      "Epoch: [3641/10000] Training loss: 3.3999639313434905e-05 / Validation loss: 0.00033308512086255965 / Long term Validation loss: 0.1282\n",
      "Epoch: [3642/10000] Training loss: 3.39861851513583e-05 / Validation loss: 0.0003329523417395934 / Long term Validation loss: 0.1282\n",
      "Epoch: [3643/10000] Training loss: 3.397273362012777e-05 / Validation loss: 0.0003328196257068075 / Long term Validation loss: 0.1282\n",
      "Epoch: [3644/10000] Training loss: 3.3959284716835474e-05 / Validation loss: 0.00033268696992314306 / Long term Validation loss: 0.1282\n",
      "Epoch: [3645/10000] Training loss: 3.3945838438582025e-05 / Validation loss: 0.00033255437765353707 / Long term Validation loss: 0.1282\n",
      "Epoch: [3646/10000] Training loss: 3.393239478247727e-05 / Validation loss: 0.00033242184500132413 / Long term Validation loss: 0.1282\n",
      "Epoch: [3647/10000] Training loss: 3.391895374563949e-05 / Validation loss: 0.0003322893765139142 / Long term Validation loss: 0.1282\n",
      "Epoch: [3648/10000] Training loss: 3.390551532519633e-05 / Validation loss: 0.0003321569667280065 / Long term Validation loss: 0.1282\n",
      "Epoch: [3649/10000] Training loss: 3.3892079518283587e-05 / Validation loss: 0.0003320246221142168 / Long term Validation loss: 0.1283\n",
      "Epoch: [3650/10000] Training loss: 3.3878646322046755e-05 / Validation loss: 0.0003318923348373266 / Long term Validation loss: 0.1283\n",
      "Epoch: [3651/10000] Training loss: 3.3865215733639186e-05 / Validation loss: 0.0003317601143057487 / Long term Validation loss: 0.1283\n",
      "Epoch: [3652/10000] Training loss: 3.385178775022429e-05 / Validation loss: 0.00033162794903003617 / Long term Validation loss: 0.1283\n",
      "Epoch: [3653/10000] Training loss: 3.383836236897285e-05 / Validation loss: 0.00033149585298252317 / Long term Validation loss: 0.1283\n",
      "Epoch: [3654/10000] Training loss: 3.382493958706631e-05 / Validation loss: 0.0003313638089505173 / Long term Validation loss: 0.1283\n",
      "Epoch: [3655/10000] Training loss: 3.381151940169274e-05 / Validation loss: 0.00033123183811239174 / Long term Validation loss: 0.1283\n",
      "Epoch: [3656/10000] Training loss: 3.379810181005187e-05 / Validation loss: 0.0003310999141464419 / Long term Validation loss: 0.1283\n",
      "Epoch: [3657/10000] Training loss: 3.378468680934881e-05 / Validation loss: 0.0003309680697914998 / Long term Validation loss: 0.1284\n",
      "Epoch: [3658/10000] Training loss: 3.3771274396801815e-05 / Validation loss: 0.00033083626399656765 / Long term Validation loss: 0.1284\n",
      "Epoch: [3659/10000] Training loss: 3.375786456963277e-05 / Validation loss: 0.000330704548340918 / Long term Validation loss: 0.1284\n",
      "Epoch: [3660/10000] Training loss: 3.3744457325078944e-05 / Validation loss: 0.0003305728575806113 / Long term Validation loss: 0.1284\n",
      "Epoch: [3661/10000] Training loss: 3.373105266037825e-05 / Validation loss: 0.0003304412744811838 / Long term Validation loss: 0.1284\n",
      "Epoch: [3662/10000] Training loss: 3.37176505727881e-05 / Validation loss: 0.0003303096934429658 / Long term Validation loss: 0.1284\n",
      "Epoch: [3663/10000] Training loss: 3.370425105956133e-05 / Validation loss: 0.0003301782496519436 / Long term Validation loss: 0.1284\n",
      "Epoch: [3664/10000] Training loss: 3.3690854117976945e-05 / Validation loss: 0.00033004676915851234 / Long term Validation loss: 0.1284\n",
      "Epoch: [3665/10000] Training loss: 3.367745974530116e-05 / Validation loss: 0.00032991547660380223 / Long term Validation loss: 0.1284\n",
      "Epoch: [3666/10000] Training loss: 3.3664067938837636e-05 / Validation loss: 0.0003297840805245113 / Long term Validation loss: 0.1285\n",
      "Epoch: [3667/10000] Training loss: 3.3650678695863126e-05 / Validation loss: 0.0003296529605062938 / Long term Validation loss: 0.1285\n",
      "Epoch: [3668/10000] Training loss: 3.363729201371197e-05 / Validation loss: 0.00032952162003837746 / Long term Validation loss: 0.1285\n",
      "Epoch: [3669/10000] Training loss: 3.362390788966763e-05 / Validation loss: 0.0003293907110462542 / Long term Validation loss: 0.1285\n",
      "Epoch: [3670/10000] Training loss: 3.361052632110793e-05 / Validation loss: 0.0003292593739970098 / Long term Validation loss: 0.1285\n",
      "Epoch: [3671/10000] Training loss: 3.359714730531986e-05 / Validation loss: 0.0003291287464489766 / Long term Validation loss: 0.1285\n",
      "Epoch: [3672/10000] Training loss: 3.3583770839754515e-05 / Validation loss: 0.00032899731690607813 / Long term Validation loss: 0.1285\n",
      "Epoch: [3673/10000] Training loss: 3.3570396921709366e-05 / Validation loss: 0.00032886710127367385 / Long term Validation loss: 0.1285\n",
      "Epoch: [3674/10000] Training loss: 3.355702554878796e-05 / Validation loss: 0.0003287354005805534 / Long term Validation loss: 0.1286\n",
      "Epoch: [3675/10000] Training loss: 3.354365671835634e-05 / Validation loss: 0.00032860584169975025 / Long term Validation loss: 0.1286\n",
      "Epoch: [3676/10000] Training loss: 3.353029042840443e-05 / Validation loss: 0.0003284735326520102 / Long term Validation loss: 0.1286\n",
      "Epoch: [3677/10000] Training loss: 3.3516926676641626e-05 / Validation loss: 0.000328345095844344 / Long term Validation loss: 0.1286\n",
      "Epoch: [3678/10000] Training loss: 3.350356546220586e-05 / Validation loss: 0.0003282115337064438 / Long term Validation loss: 0.1286\n",
      "Epoch: [3679/10000] Training loss: 3.349020678429238e-05 / Validation loss: 0.00032808511457593434 / Long term Validation loss: 0.1286\n",
      "Epoch: [3680/10000] Training loss: 3.347685064585526e-05 / Validation loss: 0.0003279490508471459 / Long term Validation loss: 0.1286\n",
      "Epoch: [3681/10000] Training loss: 3.346349705215334e-05 / Validation loss: 0.00032782639488422317 / Long term Validation loss: 0.1286\n",
      "Epoch: [3682/10000] Training loss: 3.345014601982728e-05 / Validation loss: 0.0003276853814555391 / Long term Validation loss: 0.1287\n",
      "Epoch: [3683/10000] Training loss: 3.343679757846919e-05 / Validation loss: 0.0003275699329984767 / Long term Validation loss: 0.1287\n",
      "Epoch: [3684/10000] Training loss: 3.342345179646269e-05 / Validation loss: 0.0003274191099577013 / Long term Validation loss: 0.1287\n",
      "Epoch: [3685/10000] Training loss: 3.341010880140562e-05 / Validation loss: 0.00032731774966647065 / Long term Validation loss: 0.1287\n",
      "Epoch: [3686/10000] Training loss: 3.339676886513442e-05 / Validation loss: 0.0003271473513479587 / Long term Validation loss: 0.1287\n",
      "Epoch: [3687/10000] Training loss: 3.338343251546198e-05 / Validation loss: 0.0003270739930614569 / Long term Validation loss: 0.1287\n",
      "Epoch: [3688/10000] Training loss: 3.337010085010496e-05 / Validation loss: 0.00032686416025680637 / Long term Validation loss: 0.1287\n",
      "Epoch: [3689/10000] Training loss: 3.3356776062241045e-05 / Validation loss: 0.00032684728341193916 / Long term Validation loss: 0.1288\n",
      "Epoch: [3690/10000] Training loss: 3.334346269678401e-05 / Validation loss: 0.00032655715640893154 / Long term Validation loss: 0.1287\n",
      "Epoch: [3691/10000] Training loss: 3.3330170025517246e-05 / Validation loss: 0.000326655767254394 / Long term Validation loss: 0.1289\n",
      "Epoch: [3692/10000] Training loss: 3.331691735571825e-05 / Validation loss: 0.00032620032375593576 / Long term Validation loss: 0.1287\n",
      "Epoch: [3693/10000] Training loss: 3.330374472562796e-05 / Validation loss: 0.00032653820096335845 / Long term Validation loss: 0.1290\n",
      "Epoch: [3694/10000] Training loss: 3.329073622267266e-05 / Validation loss: 0.0003257386355522178 / Long term Validation loss: 0.1285\n",
      "Epoch: [3695/10000] Training loss: 3.327806868089698e-05 / Validation loss: 0.0003265788387903122 / Long term Validation loss: 0.1292\n",
      "Epoch: [3696/10000] Training loss: 3.326611724709431e-05 / Validation loss: 0.00032505560545167706 / Long term Validation loss: 0.1283\n",
      "Epoch: [3697/10000] Training loss: 3.325568098223009e-05 / Validation loss: 0.00032696540240772094 / Long term Validation loss: 0.1297\n",
      "Epoch: [3698/10000] Training loss: 3.324847316284143e-05 / Validation loss: 0.0003239076070124016 / Long term Validation loss: 0.1277\n",
      "Epoch: [3699/10000] Training loss: 3.324818741655675e-05 / Validation loss: 0.00032813242577564756 / Long term Validation loss: 0.1308\n",
      "Epoch: [3700/10000] Training loss: 3.3262826240290034e-05 / Validation loss: 0.00032180742194865594 / Long term Validation loss: 0.1264\n",
      "Epoch: [3701/10000] Training loss: 3.330983963682055e-05 / Validation loss: 0.0003311507437708454 / Long term Validation loss: 0.1327\n",
      "Epoch: [3702/10000] Training loss: 3.3427371058849076e-05 / Validation loss: 0.000317913183367365 / Long term Validation loss: 0.1228\n",
      "Epoch: [3703/10000] Training loss: 3.369935458305955e-05 / Validation loss: 0.0003389278586117517 / Long term Validation loss: 0.1370\n",
      "Epoch: [3704/10000] Training loss: 3.430969204376384e-05 / Validation loss: 0.0003115353698753685 / Long term Validation loss: 0.1155\n",
      "Epoch: [3705/10000] Training loss: 3.566218781631666e-05 / Validation loss: 0.0003603282496472747 / Long term Validation loss: 0.1375\n",
      "Epoch: [3706/10000] Training loss: 3.861799185418459e-05 / Validation loss: 0.00030678284412932017 / Long term Validation loss: 0.1015\n",
      "Epoch: [3707/10000] Training loss: 4.495591148648401e-05 / Validation loss: 0.0004228410138688478 / Long term Validation loss: 0.1371\n",
      "Epoch: [3708/10000] Training loss: 5.7830361327120385e-05 / Validation loss: 0.0003302784561727002 / Long term Validation loss: 0.0958\n",
      "Epoch: [3709/10000] Training loss: 8.129206871357603e-05 / Validation loss: 0.0005644861083803855 / Long term Validation loss: 0.1335\n",
      "Epoch: [3710/10000] Training loss: 0.000113544308048024 / Validation loss: 0.00038545611119842097 / Long term Validation loss: 0.0974\n",
      "Epoch: [3711/10000] Training loss: 0.00013392864275656708 / Validation loss: 0.0005500995094284199 / Long term Validation loss: 0.1334\n",
      "Epoch: [3712/10000] Training loss: 0.00010719623213181343 / Validation loss: 0.00030733442509141626 / Long term Validation loss: 0.0985\n",
      "Epoch: [3713/10000] Training loss: 5.110461932667902e-05 / Validation loss: 0.0003114375422995779 / Long term Validation loss: 0.1176\n",
      "Epoch: [3714/10000] Training loss: 3.525772404604783e-05 / Validation loss: 0.00045487725375594677 / Long term Validation loss: 0.1362\n",
      "Epoch: [3715/10000] Training loss: 6.876429868314135e-05 / Validation loss: 0.00033127503491268615 / Long term Validation loss: 0.0953\n",
      "Epoch: [3716/10000] Training loss: 8.453073378440296e-05 / Validation loss: 0.00041094151722274115 / Long term Validation loss: 0.1368\n",
      "Epoch: [3717/10000] Training loss: 5.303313254385795e-05 / Validation loss: 0.00033098614319900286 / Long term Validation loss: 0.1356\n",
      "Epoch: [3718/10000] Training loss: 3.328464986959421e-05 / Validation loss: 0.0003086755346540919 / Long term Validation loss: 0.0972\n",
      "Epoch: [3719/10000] Training loss: 5.471386503065096e-05 / Validation loss: 0.0004448415979652601 / Long term Validation loss: 0.1366\n",
      "Epoch: [3720/10000] Training loss: 6.467201035795795e-05 / Validation loss: 0.00030577286632317926 / Long term Validation loss: 0.1042\n",
      "Epoch: [3721/10000] Training loss: 4.2067820353537836e-05 / Validation loss: 0.0003129334473442117 / Long term Validation loss: 0.1202\n",
      "Epoch: [3722/10000] Training loss: 3.468675562868807e-05 / Validation loss: 0.0004070087696438283 / Long term Validation loss: 0.1366\n",
      "Epoch: [3723/10000] Training loss: 5.159245207276102e-05 / Validation loss: 0.0003066079279874135 / Long term Validation loss: 0.0984\n",
      "Epoch: [3724/10000] Training loss: 5.0496106298952305e-05 / Validation loss: 0.0003417977333371076 / Long term Validation loss: 0.1388\n",
      "Epoch: [3725/10000] Training loss: 3.4523299291842985e-05 / Validation loss: 0.0003600432390436282 / Long term Validation loss: 0.1355\n",
      "Epoch: [3726/10000] Training loss: 3.8149485468111935e-05 / Validation loss: 0.00030610602038879975 / Long term Validation loss: 0.0996\n",
      "Epoch: [3727/10000] Training loss: 4.7837941648124475e-05 / Validation loss: 0.0003660100320290931 / Long term Validation loss: 0.1351\n",
      "Epoch: [3728/10000] Training loss: 3.964494955963889e-05 / Validation loss: 0.00032932518812659004 / Long term Validation loss: 0.1352\n",
      "Epoch: [3729/10000] Training loss: 3.3103195297760485e-05 / Validation loss: 0.0003061001641313896 / Long term Validation loss: 0.1054\n",
      "Epoch: [3730/10000] Training loss: 4.0684571031483e-05 / Validation loss: 0.00037307404540966966 / Long term Validation loss: 0.1354\n",
      "Epoch: [3731/10000] Training loss: 4.1679954432741924e-05 / Validation loss: 0.0003144953507284605 / Long term Validation loss: 0.1221\n",
      "Epoch: [3732/10000] Training loss: 3.390969433440349e-05 / Validation loss: 0.0003108912710347802 / Long term Validation loss: 0.1174\n",
      "Epoch: [3733/10000] Training loss: 3.5112825001381946e-05 / Validation loss: 0.0003657464193306577 / Long term Validation loss: 0.1357\n",
      "Epoch: [3734/10000] Training loss: 3.991952871268475e-05 / Validation loss: 0.0003090299417289318 / Long term Validation loss: 0.1146\n",
      "Epoch: [3735/10000] Training loss: 3.606270111800001e-05 / Validation loss: 0.00032038356484624924 / Long term Validation loss: 0.1285\n",
      "Epoch: [3736/10000] Training loss: 3.294910265157099e-05 / Validation loss: 0.00035172640703960706 / Long term Validation loss: 0.1375\n",
      "Epoch: [3737/10000] Training loss: 3.665253019345587e-05 / Validation loss: 0.00030786492993649156 / Long term Validation loss: 0.1120\n",
      "Epoch: [3738/10000] Training loss: 3.696082019272626e-05 / Validation loss: 0.00033087680385243646 / Long term Validation loss: 0.1347\n",
      "Epoch: [3739/10000] Training loss: 3.32225762794573e-05 / Validation loss: 0.0003372954638083579 / Long term Validation loss: 0.1377\n",
      "Epoch: [3740/10000] Training loss: 3.401395613263419e-05 / Validation loss: 0.00030851314602554474 / Long term Validation loss: 0.1135\n",
      "Epoch: [3741/10000] Training loss: 3.622355814643275e-05 / Validation loss: 0.0003383341576892355 / Long term Validation loss: 0.1382\n",
      "Epoch: [3742/10000] Training loss: 3.420380838372214e-05 / Validation loss: 0.00032579081320592313 / Long term Validation loss: 0.1322\n",
      "Epoch: [3743/10000] Training loss: 3.2860767147268975e-05 / Validation loss: 0.00031062654922100287 / Long term Validation loss: 0.1171\n",
      "Epoch: [3744/10000] Training loss: 3.469841484418306e-05 / Validation loss: 0.00034088693070384417 / Long term Validation loss: 0.1390\n",
      "Epoch: [3745/10000] Training loss: 3.468921154751406e-05 / Validation loss: 0.00031829240319455383 / Long term Validation loss: 0.1262\n",
      "Epoch: [3746/10000] Training loss: 3.2903403179810664e-05 / Validation loss: 0.00031454158531531943 / Long term Validation loss: 0.1222\n",
      "Epoch: [3747/10000] Training loss: 3.337758308622785e-05 / Validation loss: 0.0003389502327144196 / Long term Validation loss: 0.1388\n",
      "Epoch: [3748/10000] Training loss: 3.438440227057861e-05 / Validation loss: 0.00031436878973713185 / Long term Validation loss: 0.1221\n",
      "Epoch: [3749/10000] Training loss: 3.3364049572123236e-05 / Validation loss: 0.0003197101236009622 / Long term Validation loss: 0.1277\n",
      "Epoch: [3750/10000] Training loss: 3.275667875339734e-05 / Validation loss: 0.00033407440121274723 / Long term Validation loss: 0.1367\n",
      "Epoch: [3751/10000] Training loss: 3.3643758065246104e-05 / Validation loss: 0.0003128968526819064 / Long term Validation loss: 0.1206\n",
      "Epoch: [3752/10000] Training loss: 3.361173786949024e-05 / Validation loss: 0.00032465884873569427 / Long term Validation loss: 0.1318\n",
      "Epoch: [3753/10000] Training loss: 3.274882123574538e-05 / Validation loss: 0.0003280928718028079 / Long term Validation loss: 0.1338\n",
      "Epoch: [3754/10000] Training loss: 3.297307813669125e-05 / Validation loss: 0.00031306088185064377 / Long term Validation loss: 0.1212\n",
      "Epoch: [3755/10000] Training loss: 3.345525188211004e-05 / Validation loss: 0.00032802468219974033 / Long term Validation loss: 0.1340\n",
      "Epoch: [3756/10000] Training loss: 3.296650189306435e-05 / Validation loss: 0.0003226441824247965 / Long term Validation loss: 0.1307\n",
      "Epoch: [3757/10000] Training loss: 3.265303627660551e-05 / Validation loss: 0.00031458670550329717 / Long term Validation loss: 0.1234\n",
      "Epoch: [3758/10000] Training loss: 3.306737187307384e-05 / Validation loss: 0.0003291715104475764 / Long term Validation loss: 0.1346\n",
      "Epoch: [3759/10000] Training loss: 3.30720639736806e-05 / Validation loss: 0.00031868354031400523 / Long term Validation loss: 0.1278\n",
      "Epoch: [3760/10000] Training loss: 3.264781675357821e-05 / Validation loss: 0.00031713867506835946 / Long term Validation loss: 0.1264\n",
      "Epoch: [3761/10000] Training loss: 3.2720387535297274e-05 / Validation loss: 0.0003281534240892665 / Long term Validation loss: 0.1344\n",
      "Epoch: [3762/10000] Training loss: 3.2963850186824224e-05 / Validation loss: 0.00031634878372947985 / Long term Validation loss: 0.1258\n",
      "Epoch: [3763/10000] Training loss: 3.27480908277046e-05 / Validation loss: 0.00031999036185219413 / Long term Validation loss: 0.1291\n",
      "Epoch: [3764/10000] Training loss: 3.2563007178776765e-05 / Validation loss: 0.00032557669264404103 / Long term Validation loss: 0.1333\n",
      "Epoch: [3765/10000] Training loss: 3.2743122170886094e-05 / Validation loss: 0.00031540172261787416 / Long term Validation loss: 0.1252\n",
      "Epoch: [3766/10000] Training loss: 3.277408138745079e-05 / Validation loss: 0.00032232245468656227 / Long term Validation loss: 0.1314\n",
      "Epoch: [3767/10000] Training loss: 3.256409575913671e-05 / Validation loss: 0.0003224008073145879 / Long term Validation loss: 0.1316\n",
      "Epoch: [3768/10000] Training loss: 3.2560261736178954e-05 / Validation loss: 0.0003156136681899861 / Long term Validation loss: 0.1259\n",
      "Epoch: [3769/10000] Training loss: 3.268458079700515e-05 / Validation loss: 0.0003235602595630026 / Long term Validation loss: 0.1325\n",
      "Epoch: [3770/10000] Training loss: 3.2601655260675996e-05 / Validation loss: 0.00031952051379940884 / Long term Validation loss: 0.1294\n",
      "Epoch: [3771/10000] Training loss: 3.248521413513911e-05 / Validation loss: 0.00031669693288359176 / Long term Validation loss: 0.1273\n",
      "Epoch: [3772/10000] Training loss: 3.2549324502993774e-05 / Validation loss: 0.0003234823048820308 / Long term Validation loss: 0.1326\n",
      "Epoch: [3773/10000] Training loss: 3.2584674689908046e-05 / Validation loss: 0.00031743137994451895 / Long term Validation loss: 0.1280\n",
      "Epoch: [3774/10000] Training loss: 3.2484486245543513e-05 / Validation loss: 0.000318178407369044 / Long term Validation loss: 0.1287\n",
      "Epoch: [3775/10000] Training loss: 3.245098127105015e-05 / Validation loss: 0.0003222568720913112 / Long term Validation loss: 0.1321\n",
      "Epoch: [3776/10000] Training loss: 3.250814955859057e-05 / Validation loss: 0.00031626730884264017 / Long term Validation loss: 0.1274\n",
      "Epoch: [3777/10000] Training loss: 3.248524276642157e-05 / Validation loss: 0.00031951352059953966 / Long term Validation loss: 0.1301\n",
      "Epoch: [3778/10000] Training loss: 3.241362992768703e-05 / Validation loss: 0.0003203859317371122 / Long term Validation loss: 0.1309\n",
      "Epoch: [3779/10000] Training loss: 3.242196287119986e-05 / Validation loss: 0.00031597659181392034 / Long term Validation loss: 0.1275\n",
      "Epoch: [3780/10000] Training loss: 3.244768835105841e-05 / Validation loss: 0.0003202678327583063 / Long term Validation loss: 0.1310\n",
      "Epoch: [3781/10000] Training loss: 3.2404555062419425e-05 / Validation loss: 0.0003184659000970096 / Long term Validation loss: 0.1296\n",
      "Epoch: [3782/10000] Training loss: 3.236706726813409e-05 / Validation loss: 0.0003163641707352186 / Long term Validation loss: 0.1280\n",
      "Epoch: [3783/10000] Training loss: 3.2384394795140624e-05 / Validation loss: 0.00032022528705598666 / Long term Validation loss: 0.1312\n",
      "Epoch: [3784/10000] Training loss: 3.2383089156160276e-05 / Validation loss: 0.00031693542716278384 / Long term Validation loss: 0.1286\n",
      "Epoch: [3785/10000] Training loss: 3.2342791822219666e-05 / Validation loss: 0.00031709457404880607 / Long term Validation loss: 0.1288\n",
      "Epoch: [3786/10000] Training loss: 3.2328032882430705e-05 / Validation loss: 0.00031944486727237826 / Long term Validation loss: 0.1309\n",
      "Epoch: [3787/10000] Training loss: 3.233880421975413e-05 / Validation loss: 0.000316002755861546 / Long term Validation loss: 0.1282\n",
      "Epoch: [3788/10000] Training loss: 3.2322959649956505e-05 / Validation loss: 0.00031778294490568843 / Long term Validation loss: 0.1297\n",
      "Epoch: [3789/10000] Training loss: 3.2293213806008146e-05 / Validation loss: 0.0003182265148785497 / Long term Validation loss: 0.1302\n",
      "Epoch: [3790/10000] Training loss: 3.228859920306953e-05 / Validation loss: 0.0003156846288312649 / Long term Validation loss: 0.1282\n",
      "Epoch: [3791/10000] Training loss: 3.228966106856393e-05 / Validation loss: 0.0003181155810440443 / Long term Validation loss: 0.1303\n",
      "Epoch: [3792/10000] Training loss: 3.226932661377004e-05 / Validation loss: 0.00031695486161703877 / Long term Validation loss: 0.1294\n",
      "Epoch: [3793/10000] Training loss: 3.224936547913882e-05 / Validation loss: 0.0003158364025323441 / Long term Validation loss: 0.1286\n",
      "Epoch: [3794/10000] Training loss: 3.224682865919814e-05 / Validation loss: 0.0003179385335891905 / Long term Validation loss: 0.1305\n",
      "Epoch: [3795/10000] Training loss: 3.224046207180264e-05 / Validation loss: 0.0003159337964875559 / Long term Validation loss: 0.1289\n",
      "Epoch: [3796/10000] Training loss: 3.222094356732027e-05 / Validation loss: 0.0003161996855268798 / Long term Validation loss: 0.1292\n",
      "Epoch: [3797/10000] Training loss: 3.220732446657787e-05 / Validation loss: 0.00031730254663634073 / Long term Validation loss: 0.1303\n",
      "Epoch: [3798/10000] Training loss: 3.220331080075245e-05 / Validation loss: 0.00031531851003448267 / Long term Validation loss: 0.1287\n",
      "Epoch: [3799/10000] Training loss: 3.219278935662494e-05 / Validation loss: 0.00031649723125174237 / Long term Validation loss: 0.1298\n",
      "Epoch: [3800/10000] Training loss: 3.217581640232496e-05 / Validation loss: 0.0003164243054898226 / Long term Validation loss: 0.1298\n",
      "Epoch: [3801/10000] Training loss: 3.216537872138329e-05 / Validation loss: 0.0003151127038389388 / Long term Validation loss: 0.1288\n",
      "Epoch: [3802/10000] Training loss: 3.2159086704467155e-05 / Validation loss: 0.00031652514440416425 / Long term Validation loss: 0.1301\n",
      "Epoch: [3803/10000] Training loss: 3.214686810553273e-05 / Validation loss: 0.00031556380198059406 / Long term Validation loss: 0.1294\n",
      "Epoch: [3804/10000] Training loss: 3.2132354150265676e-05 / Validation loss: 0.0003151850521360209 / Long term Validation loss: 0.1291\n",
      "Epoch: [3805/10000] Training loss: 3.212306944253512e-05 / Validation loss: 0.00031621000357694493 / Long term Validation loss: 0.1302\n",
      "Epoch: [3806/10000] Training loss: 3.21148764122465e-05 / Validation loss: 0.00031490961214718265 / Long term Validation loss: 0.1291\n",
      "Epoch: [3807/10000] Training loss: 3.2102336643204305e-05 / Validation loss: 0.0003153276121246892 / Long term Validation loss: 0.1296\n",
      "Epoch: [3808/10000] Training loss: 3.208959750784004e-05 / Validation loss: 0.0003156255528268646 / Long term Validation loss: 0.1300\n",
      "Epoch: [3809/10000] Training loss: 3.208044440137728e-05 / Validation loss: 0.00031453670817132033 / Long term Validation loss: 0.1291\n",
      "Epoch: [3810/10000] Training loss: 3.207099938563127e-05 / Validation loss: 0.0003153496915283167 / Long term Validation loss: 0.1299\n",
      "Epoch: [3811/10000] Training loss: 3.2058724953781736e-05 / Validation loss: 0.0003149460727639684 / Long term Validation loss: 0.1297\n",
      "Epoch: [3812/10000] Training loss: 3.204707445142668e-05 / Validation loss: 0.00031440860590974584 / Long term Validation loss: 0.1293\n",
      "Epoch: [3813/10000] Training loss: 3.203766466524636e-05 / Validation loss: 0.000315148197844134 / Long term Validation loss: 0.1301\n",
      "Epoch: [3814/10000] Training loss: 3.2027514661743e-05 / Validation loss: 0.0003143476404704065 / Long term Validation loss: 0.1294\n",
      "Epoch: [3815/10000] Training loss: 3.201565639900089e-05 / Validation loss: 0.00031439931024018167 / Long term Validation loss: 0.1296\n",
      "Epoch: [3816/10000] Training loss: 3.20045981051109e-05 / Validation loss: 0.00031472825757210797 / Long term Validation loss: 0.1300\n",
      "Epoch: [3817/10000] Training loss: 3.199485966690779e-05 / Validation loss: 0.00031392832492773564 / Long term Validation loss: 0.1294\n",
      "Epoch: [3818/10000] Training loss: 3.198436496419378e-05 / Validation loss: 0.0003143525769788202 / Long term Validation loss: 0.1298\n",
      "Epoch: [3819/10000] Training loss: 3.197288622345233e-05 / Validation loss: 0.000314185658622799 / Long term Validation loss: 0.1298\n",
      "Epoch: [3820/10000] Training loss: 3.1962116683919244e-05 / Validation loss: 0.00031369105138487666 / Long term Validation loss: 0.1295\n",
      "Epoch: [3821/10000] Training loss: 3.195209932110639e-05 / Validation loss: 0.0003141607825633218 / Long term Validation loss: 0.1300\n",
      "Epoch: [3822/10000] Training loss: 3.1941463947564494e-05 / Validation loss: 0.0003136546201169261 / Long term Validation loss: 0.1296\n",
      "Epoch: [3823/10000] Training loss: 3.193027354671072e-05 / Validation loss: 0.0003135651000205523 / Long term Validation loss: 0.1297\n",
      "Epoch: [3824/10000] Training loss: 3.1919628386336743e-05 / Validation loss: 0.0003138066228671399 / Long term Validation loss: 0.1300\n",
      "Epoch: [3825/10000] Training loss: 3.190940900999317e-05 / Validation loss: 0.0003132363276933978 / Long term Validation loss: 0.1296\n",
      "Epoch: [3826/10000] Training loss: 3.1898734037417626e-05 / Validation loss: 0.00031344235996975494 / Long term Validation loss: 0.1299\n",
      "Epoch: [3827/10000] Training loss: 3.188774429704208e-05 / Validation loss: 0.00031334919782043135 / Long term Validation loss: 0.1299\n",
      "Epoch: [3828/10000] Training loss: 3.187714340553972e-05 / Validation loss: 0.0003129539367487358 / Long term Validation loss: 0.1296\n",
      "Epoch: [3829/10000] Training loss: 3.186678889200728e-05 / Validation loss: 0.0003132299197907388 / Long term Validation loss: 0.1300\n",
      "Epoch: [3830/10000] Training loss: 3.185611791678386e-05 / Validation loss: 0.00031288182553586855 / Long term Validation loss: 0.1298\n",
      "Epoch: [3831/10000] Training loss: 3.1845261640594075e-05 / Validation loss: 0.00031276019381635016 / Long term Validation loss: 0.1298\n",
      "Epoch: [3832/10000] Training loss: 3.1834670450200187e-05 / Validation loss: 0.00031289832806015406 / Long term Validation loss: 0.1300\n",
      "Epoch: [3833/10000] Training loss: 3.182422949963475e-05 / Validation loss: 0.0003124848253935497 / Long term Validation loss: 0.1298\n",
      "Epoch: [3834/10000] Training loss: 3.1813576922140915e-05 / Validation loss: 0.0003125765956092677 / Long term Validation loss: 0.1300\n",
      "Epoch: [3835/10000] Training loss: 3.180280824066458e-05 / Validation loss: 0.00031248746873596293 / Long term Validation loss: 0.1300\n",
      "Epoch: [3836/10000] Training loss: 3.179221452379842e-05 / Validation loss: 0.00031218729294020623 / Long term Validation loss: 0.1298\n",
      "Epoch: [3837/10000] Training loss: 3.1781719232102125e-05 / Validation loss: 0.00031233534677860264 / Long term Validation loss: 0.1301\n",
      "Epoch: [3838/10000] Training loss: 3.177108695584271e-05 / Validation loss: 0.000312069287700183 / Long term Validation loss: 0.1299\n",
      "Epoch: [3839/10000] Training loss: 3.176037609599848e-05 / Validation loss: 0.0003119586217496106 / Long term Validation loss: 0.1299\n",
      "Epoch: [3840/10000] Training loss: 3.1749777425560584e-05 / Validation loss: 0.000312011328980615 / Long term Validation loss: 0.1301\n",
      "Epoch: [3841/10000] Training loss: 3.1739248176618764e-05 / Validation loss: 0.0003117027673732636 / Long term Validation loss: 0.1299\n",
      "Epoch: [3842/10000] Training loss: 3.172863318876725e-05 / Validation loss: 0.00031173687086024466 / Long term Validation loss: 0.1301\n",
      "Epoch: [3843/10000] Training loss: 3.17179615271972e-05 / Validation loss: 0.00031163093824485477 / Long term Validation loss: 0.1301\n",
      "Epoch: [3844/10000] Training loss: 3.170735921067568e-05 / Validation loss: 0.00031140712677351246 / Long term Validation loss: 0.1300\n",
      "Epoch: [3845/10000] Training loss: 3.169680848614027e-05 / Validation loss: 0.00031147079024001074 / Long term Validation loss: 0.1302\n",
      "Epoch: [3846/10000] Training loss: 3.1686206803433754e-05 / Validation loss: 0.0003112488486483401 / Long term Validation loss: 0.1301\n",
      "Epoch: [3847/10000] Training loss: 3.167556255580434e-05 / Validation loss: 0.00031115872422294617 / Long term Validation loss: 0.1301\n",
      "Epoch: [3848/10000] Training loss: 3.1664958896673904e-05 / Validation loss: 0.0003111463451214646 / Long term Validation loss: 0.1302\n",
      "Epoch: [3849/10000] Training loss: 3.165439453546262e-05 / Validation loss: 0.0003109092356616235 / Long term Validation loss: 0.1301\n",
      "Epoch: [3850/10000] Training loss: 3.164380231185272e-05 / Validation loss: 0.0003109109496023739 / Long term Validation loss: 0.1302\n",
      "Epoch: [3851/10000] Training loss: 3.16331779950726e-05 / Validation loss: 0.0003107870938793805 / Long term Validation loss: 0.1302\n",
      "Epoch: [3852/10000] Training loss: 3.162257522828427e-05 / Validation loss: 0.0003106211446621622 / Long term Validation loss: 0.1302\n",
      "Epoch: [3853/10000] Training loss: 3.1612002205439703e-05 / Validation loss: 0.0003106266915356267 / Long term Validation loss: 0.1303\n",
      "Epoch: [3854/10000] Training loss: 3.160141643660058e-05 / Validation loss: 0.00031043351510806784 / Long term Validation loss: 0.1302\n",
      "Epoch: [3855/10000] Training loss: 3.159080698343205e-05 / Validation loss: 0.00031036076278953084 / Long term Validation loss: 0.1303\n",
      "Epoch: [3856/10000] Training loss: 3.158020692818221e-05 / Validation loss: 0.00031030063891131996 / Long term Validation loss: 0.1303\n",
      "Epoch: [3857/10000] Training loss: 3.156962869764363e-05 / Validation loss: 0.00031011481402564735 / Long term Validation loss: 0.1302\n",
      "Epoch: [3858/10000] Training loss: 3.155904707931355e-05 / Validation loss: 0.00031009251328575644 / Long term Validation loss: 0.1303\n",
      "Epoch: [3859/10000] Training loss: 3.1548448964398655e-05 / Validation loss: 0.0003099562626369653 / Long term Validation loss: 0.1303\n",
      "Epoch: [3860/10000] Training loss: 3.153785291221838e-05 / Validation loss: 0.00030983159069301994 / Long term Validation loss: 0.1303\n",
      "Epoch: [3861/10000] Training loss: 3.1527271978102574e-05 / Validation loss: 0.00030979368040918493 / Long term Validation loss: 0.1304\n",
      "Epoch: [3862/10000] Training loss: 3.1516693001742296e-05 / Validation loss: 0.00030962423309062345 / Long term Validation loss: 0.1304\n",
      "Epoch: [3863/10000] Training loss: 3.150610352569958e-05 / Validation loss: 0.0003095606931759154 / Long term Validation loss: 0.1304\n",
      "Epoch: [3864/10000] Training loss: 3.149551228088285e-05 / Validation loss: 0.00030946737824536944 / Long term Validation loss: 0.1304\n",
      "Epoch: [3865/10000] Training loss: 3.1484930615941844e-05 / Validation loss: 0.00030932049078642954 / Long term Validation loss: 0.1304\n",
      "Epoch: [3866/10000] Training loss: 3.147435334176921e-05 / Validation loss: 0.00030927583296140354 / Long term Validation loss: 0.1305\n",
      "Epoch: [3867/10000] Training loss: 3.1463770440841326e-05 / Validation loss: 0.0003091356655858792 / Long term Validation loss: 0.1305\n",
      "Epoch: [3868/10000] Training loss: 3.145318431525329e-05 / Validation loss: 0.00030903804838754395 / Long term Validation loss: 0.1305\n",
      "Epoch: [3869/10000] Training loss: 3.144260347455858e-05 / Validation loss: 0.0003089665508038434 / Long term Validation loss: 0.1305\n",
      "Epoch: [3870/10000] Training loss: 3.143202752927759e-05 / Validation loss: 0.0003088198460577145 / Long term Validation loss: 0.1305\n",
      "Epoch: [3871/10000] Training loss: 3.142144954381578e-05 / Validation loss: 0.00030875608460855615 / Long term Validation loss: 0.1305\n",
      "Epoch: [3872/10000] Training loss: 3.141086847410156e-05 / Validation loss: 0.00030864253629424244 / Long term Validation loss: 0.1306\n",
      "Epoch: [3873/10000] Training loss: 3.1400289658743474e-05 / Validation loss: 0.00030852492486636434 / Long term Validation loss: 0.1306\n",
      "Epoch: [3874/10000] Training loss: 3.138971505662216e-05 / Validation loss: 0.0003084580023804243 / Long term Validation loss: 0.1306\n",
      "Epoch: [3875/10000] Training loss: 3.13791407759791e-05 / Validation loss: 0.0003083226184324914 / Long term Validation loss: 0.1306\n",
      "Epoch: [3876/10000] Training loss: 3.136856436821951e-05 / Validation loss: 0.0003082392195335095 / Long term Validation loss: 0.1306\n",
      "Epoch: [3877/10000] Training loss: 3.135798842311968e-05 / Validation loss: 0.0003081433880076594 / Long term Validation loss: 0.1306\n",
      "Epoch: [3878/10000] Training loss: 3.134741548522368e-05 / Validation loss: 0.0003080185780473432 / Long term Validation loss: 0.1306\n",
      "Epoch: [3879/10000] Training loss: 3.133684407460343e-05 / Validation loss: 0.00030794656532360533 / Long term Validation loss: 0.1307\n",
      "Epoch: [3880/10000] Training loss: 3.13262717815236e-05 / Validation loss: 0.0003078250339288328 / Long term Validation loss: 0.1307\n",
      "Epoch: [3881/10000] Training loss: 3.1315699178397105e-05 / Validation loss: 0.0003077270846366528 / Long term Validation loss: 0.1307\n",
      "Epoch: [3882/10000] Training loss: 3.130512835837979e-05 / Validation loss: 0.00030763990782690074 / Long term Validation loss: 0.1307\n",
      "Epoch: [3883/10000] Training loss: 3.1294559390164644e-05 / Validation loss: 0.0003075160087804329 / Long term Validation loss: 0.1307\n",
      "Epoch: [3884/10000] Training loss: 3.128399060351728e-05 / Validation loss: 0.00030743541473781527 / Long term Validation loss: 0.1308\n",
      "Epoch: [3885/10000] Training loss: 3.1273421491250075e-05 / Validation loss: 0.0003073254428970677 / Long term Validation loss: 0.1308\n",
      "Epoch: [3886/10000] Training loss: 3.1262853238525e-05 / Validation loss: 0.00030721902491850656 / Long term Validation loss: 0.1308\n",
      "Epoch: [3887/10000] Training loss: 3.12522865953417e-05 / Validation loss: 0.00030713406280310005 / Long term Validation loss: 0.1308\n",
      "Epoch: [3888/10000] Training loss: 3.1241720817113234e-05 / Validation loss: 0.0003070149138379871 / Long term Validation loss: 0.1308\n",
      "Epoch: [3889/10000] Training loss: 3.123115507245532e-05 / Validation loss: 0.0003069262282176518 / Long term Validation loss: 0.1308\n",
      "Epoch: [3890/10000] Training loss: 3.122058971379001e-05 / Validation loss: 0.00030682418587182664 / Long term Validation loss: 0.1309\n",
      "Epoch: [3891/10000] Training loss: 3.121002550415162e-05 / Validation loss: 0.0003067142738587843 / Long term Validation loss: 0.1309\n",
      "Epoch: [3892/10000] Training loss: 3.119946240164432e-05 / Validation loss: 0.0003066279988529735 / Long term Validation loss: 0.1309\n",
      "Epoch: [3893/10000] Training loss: 3.118889977536836e-05 / Validation loss: 0.0003065143891906876 / Long term Validation loss: 0.1309\n",
      "Epoch: [3894/10000] Training loss: 3.117833744446184e-05 / Validation loss: 0.00030641955732783947 / Long term Validation loss: 0.1309\n",
      "Epoch: [3895/10000] Training loss: 3.1167775860523887e-05 / Validation loss: 0.0003063219530692672 / Long term Validation loss: 0.1310\n",
      "Epoch: [3896/10000] Training loss: 3.115721531102747e-05 / Validation loss: 0.00030621159925315494 / Long term Validation loss: 0.1310\n",
      "Epoch: [3897/10000] Training loss: 3.1146655531248976e-05 / Validation loss: 0.00030612259852878256 / Long term Validation loss: 0.1310\n",
      "Epoch: [3898/10000] Training loss: 3.1136096196371643e-05 / Validation loss: 0.0003060138115041314 / Long term Validation loss: 0.1310\n",
      "Epoch: [3899/10000] Training loss: 3.112553739479135e-05 / Validation loss: 0.00030591507682974946 / Long term Validation loss: 0.1310\n",
      "Epoch: [3900/10000] Training loss: 3.111497942291692e-05 / Validation loss: 0.0003058194995894247 / Long term Validation loss: 0.1310\n",
      "Epoch: [3901/10000] Training loss: 3.1104422311382213e-05 / Validation loss: 0.0003057103469005663 / Long term Validation loss: 0.1310\n",
      "Epoch: [3902/10000] Training loss: 3.1093865831272213e-05 / Validation loss: 0.0003056185746720509 / Long term Validation loss: 0.1311\n",
      "Epoch: [3903/10000] Training loss: 3.10833098712172e-05 / Validation loss: 0.0003055134162150874 / Long term Validation loss: 0.1311\n",
      "Epoch: [3904/10000] Training loss: 3.107275456433402e-05 / Validation loss: 0.0003054125707370981 / Long term Validation loss: 0.1311\n",
      "Epoch: [3905/10000] Training loss: 3.1062200054524225e-05 / Validation loss: 0.0003053174943684337 / Long term Validation loss: 0.1311\n",
      "Epoch: [3906/10000] Training loss: 3.105164628708438e-05 / Validation loss: 0.00030521005567983735 / Long term Validation loss: 0.1311\n",
      "Epoch: [3907/10000] Training loss: 3.104109312125875e-05 / Validation loss: 0.0003051159484692587 / Long term Validation loss: 0.1312\n",
      "Epoch: [3908/10000] Training loss: 3.103054054395703e-05 / Validation loss: 0.0003050132116676363 / Long term Validation loss: 0.1312\n",
      "Epoch: [3909/10000] Training loss: 3.101998865619611e-05 / Validation loss: 0.0003049114788870734 / Long term Validation loss: 0.1312\n",
      "Epoch: [3910/10000] Training loss: 3.100943751241037e-05 / Validation loss: 0.00030481615555367006 / Long term Validation loss: 0.1312\n",
      "Epoch: [3911/10000] Training loss: 3.099888705130981e-05 / Validation loss: 0.0003047104592563177 / Long term Validation loss: 0.1312\n",
      "Epoch: [3912/10000] Training loss: 3.098833719921021e-05 / Validation loss: 0.0003046146837349467 / Long term Validation loss: 0.1312\n",
      "Epoch: [3913/10000] Training loss: 3.0977787973619634e-05 / Validation loss: 0.0003045134815301116 / Long term Validation loss: 0.1313\n",
      "Epoch: [3914/10000] Training loss: 3.0967239436132e-05 / Validation loss: 0.00030441162564590334 / Long term Validation loss: 0.1313\n",
      "Epoch: [3915/10000] Training loss: 3.095669160110987e-05 / Validation loss: 0.00030431576390545104 / Long term Validation loss: 0.1313\n",
      "Epoch: [3916/10000] Training loss: 3.0946144423206465e-05 / Validation loss: 0.00030421156065043567 / Long term Validation loss: 0.1313\n",
      "Epoch: [3917/10000] Training loss: 3.093559786655957e-05 / Validation loss: 0.0003041146698275608 / Long term Validation loss: 0.1313\n",
      "Epoch: [3918/10000] Training loss: 3.092505195129712e-05 / Validation loss: 0.00030401434748985587 / Long term Validation loss: 0.1314\n",
      "Epoch: [3919/10000] Training loss: 3.091450671143956e-05 / Validation loss: 0.00030391275006274535 / Long term Validation loss: 0.1314\n",
      "Epoch: [3920/10000] Training loss: 3.0903962147328666e-05 / Validation loss: 0.0003038163014798404 / Long term Validation loss: 0.1314\n",
      "Epoch: [3921/10000] Training loss: 3.089341822994706e-05 / Validation loss: 0.0003037133037925812 / Long term Validation loss: 0.1314\n",
      "Epoch: [3922/10000] Training loss: 3.088287494169027e-05 / Validation loss: 0.0003036157554954066 / Long term Validation loss: 0.1314\n",
      "Epoch: [3923/10000] Training loss: 3.0872332297436256e-05 / Validation loss: 0.0003035159345231926 / Long term Validation loss: 0.1314\n",
      "Epoch: [3924/10000] Training loss: 3.086179031530772e-05 / Validation loss: 0.00030341479416924117 / Long term Validation loss: 0.1315\n",
      "Epoch: [3925/10000] Training loss: 3.0851248992466714e-05 / Validation loss: 0.00030331784131868274 / Long term Validation loss: 0.1315\n",
      "Epoch: [3926/10000] Training loss: 3.08407083113797e-05 / Validation loss: 0.00030321579247461183 / Long term Validation loss: 0.1315\n",
      "Epoch: [3927/10000] Training loss: 3.083016826253134e-05 / Validation loss: 0.00030311789615860115 / Long term Validation loss: 0.1315\n",
      "Epoch: [3928/10000] Training loss: 3.0819628855152844e-05 / Validation loss: 0.00030301834282445514 / Long term Validation loss: 0.1315\n",
      "Epoch: [3929/10000] Training loss: 3.080909009891869e-05 / Validation loss: 0.00030291771311150625 / Long term Validation loss: 0.1315\n",
      "Epoch: [3930/10000] Training loss: 3.0798551991360124e-05 / Validation loss: 0.00030282035568812476 / Long term Validation loss: 0.1316\n",
      "Epoch: [3931/10000] Training loss: 3.078801452205405e-05 / Validation loss: 0.00030271905224102136 / Long term Validation loss: 0.1316\n",
      "Epoch: [3932/10000] Training loss: 3.077747768501023e-05 / Validation loss: 0.00030262101239117943 / Long term Validation loss: 0.1316\n",
      "Epoch: [3933/10000] Training loss: 3.076694148527802e-05 / Validation loss: 0.0003025216150473238 / Long term Validation loss: 0.1316\n",
      "Epoch: [3934/10000] Training loss: 3.075640592809314e-05 / Validation loss: 0.0003024215028481783 / Long term Validation loss: 0.1316\n",
      "Epoch: [3935/10000] Training loss: 3.074587101206315e-05 / Validation loss: 0.00030232384833762423 / Long term Validation loss: 0.1317\n",
      "Epoch: [3936/10000] Training loss: 3.0735336731004154e-05 / Validation loss: 0.00030222315237492736 / Long term Validation loss: 0.1317\n",
      "Epoch: [3937/10000] Training loss: 3.0724803080617475e-05 / Validation loss: 0.0003021250910613506 / Long term Validation loss: 0.1317\n",
      "Epoch: [3938/10000] Training loss: 3.07142700632358e-05 / Validation loss: 0.0003020257995382718 / Long term Validation loss: 0.1317\n",
      "Epoch: [3939/10000] Training loss: 3.070373768165025e-05 / Validation loss: 0.00030192617565549514 / Long term Validation loss: 0.1317\n",
      "Epoch: [3940/10000] Training loss: 3.069320593535761e-05 / Validation loss: 0.00030182830617238006 / Long term Validation loss: 0.1317\n",
      "Epoch: [3941/10000] Training loss: 3.0682674820717726e-05 / Validation loss: 0.00030172812419081716 / Long term Validation loss: 0.1318\n",
      "Epoch: [3942/10000] Training loss: 3.067214433445501e-05 / Validation loss: 0.00030163010553658966 / Long term Validation loss: 0.1318\n",
      "Epoch: [3943/10000] Training loss: 3.0661614477259755e-05 / Validation loss: 0.0003015309120537465 / Long term Validation loss: 0.1318\n",
      "Epoch: [3944/10000] Training loss: 3.065108525045824e-05 / Validation loss: 0.00030143173919914716 / Long term Validation loss: 0.1318\n",
      "Epoch: [3945/10000] Training loss: 3.064055665406878e-05 / Validation loss: 0.0003013337209816637 / Long term Validation loss: 0.1318\n",
      "Epoch: [3946/10000] Training loss: 3.063002868600434e-05 / Validation loss: 0.0003012339963486279 / Long term Validation loss: 0.1318\n",
      "Epoch: [3947/10000] Training loss: 3.061950134378476e-05 / Validation loss: 0.00030113604537752164 / Long term Validation loss: 0.1319\n",
      "Epoch: [3948/10000] Training loss: 3.0608974627186176e-05 / Validation loss: 0.00030103696379385166 / Long term Validation loss: 0.1319\n",
      "Epoch: [3949/10000] Training loss: 3.059844853659161e-05 / Validation loss: 0.00030093819661528525 / Long term Validation loss: 0.1319\n",
      "Epoch: [3950/10000] Training loss: 3.058792307221316e-05 / Validation loss: 0.0003008400801280949 / Long term Validation loss: 0.1319\n",
      "Epoch: [3951/10000] Training loss: 3.0577398232903856e-05 / Validation loss: 0.0003007407791958448 / Long term Validation loss: 0.1319\n",
      "Epoch: [3952/10000] Training loss: 3.056687401688128e-05 / Validation loss: 0.0003006428987273149 / Long term Validation loss: 0.1320\n",
      "Epoch: [3953/10000] Training loss: 3.0556350423496146e-05 / Validation loss: 0.00030054396036757085 / Long term Validation loss: 0.1320\n",
      "Epoch: [3954/10000] Training loss: 3.054582745252425e-05 / Validation loss: 0.00030044555223334996 / Long term Validation loss: 0.1320\n",
      "Epoch: [3955/10000] Training loss: 3.053530510414105e-05 / Validation loss: 0.00030034738233381653 / Long term Validation loss: 0.1320\n",
      "Epoch: [3956/10000] Training loss: 3.052478337772609e-05 / Validation loss: 0.00030024848298106836 / Long term Validation loss: 0.1320\n",
      "Epoch: [3957/10000] Training loss: 3.0514262272101314e-05 / Validation loss: 0.0003001506610463946 / Long term Validation loss: 0.1320\n",
      "Epoch: [3958/10000] Training loss: 3.0503741786512805e-05 / Validation loss: 0.000300051902686106 / Long term Validation loss: 0.1321\n",
      "Epoch: [3959/10000] Training loss: 3.0493221920394615e-05 / Validation loss: 0.00029995379993523485 / Long term Validation loss: 0.1321\n",
      "Epoch: [3960/10000] Training loss: 3.0482702673763018e-05 / Validation loss: 0.0002998556193355964 / Long term Validation loss: 0.1321\n",
      "Epoch: [3961/10000] Training loss: 3.0472184046233575e-05 / Validation loss: 0.00029975710185259065 / Long term Validation loss: 0.1321\n",
      "Epoch: [3962/10000] Training loss: 3.0461666037101978e-05 / Validation loss: 0.0002996593254128136 / Long term Validation loss: 0.1321\n",
      "Epoch: [3963/10000] Training loss: 3.04511486456965e-05 / Validation loss: 0.00029956079010168973 / Long term Validation loss: 0.1321\n",
      "Epoch: [3964/10000] Training loss: 3.0440631871336233e-05 / Validation loss: 0.00029946293917994005 / Long term Validation loss: 0.1322\n",
      "Epoch: [3965/10000] Training loss: 3.0430115713857402e-05 / Validation loss: 0.0002993647976934354 / Long term Validation loss: 0.1322\n",
      "Epoch: [3966/10000] Training loss: 3.0419600172926258e-05 / Validation loss: 0.0002992666378734286 / Long term Validation loss: 0.1322\n",
      "Epoch: [3967/10000] Training loss: 3.0409085248150715e-05 / Validation loss: 0.0002991688985254566 / Long term Validation loss: 0.1322\n",
      "Epoch: [3968/10000] Training loss: 3.0398570939012965e-05 / Validation loss: 0.00029907062339870074 / Long term Validation loss: 0.1322\n",
      "Epoch: [3969/10000] Training loss: 3.038805724489024e-05 / Validation loss: 0.00029897296710314794 / Long term Validation loss: 0.1323\n",
      "Epoch: [3970/10000] Training loss: 3.037754416549393e-05 / Validation loss: 0.0002988749178111313 / Long term Validation loss: 0.1323\n",
      "Epoch: [3971/10000] Training loss: 3.0367031700459114e-05 / Validation loss: 0.00029877707932999707 / Long term Validation loss: 0.1323\n",
      "Epoch: [3972/10000] Training loss: 3.0356519849547543e-05 / Validation loss: 0.0002986793817107869 / Long term Validation loss: 0.1323\n",
      "Epoch: [3973/10000] Training loss: 3.0346008612378582e-05 / Validation loss: 0.0002985813946141744 / Long term Validation loss: 0.1323\n",
      "Epoch: [3974/10000] Training loss: 3.0335497988482065e-05 / Validation loss: 0.00029848388362645835 / Long term Validation loss: 0.1323\n",
      "Epoch: [3975/10000] Training loss: 3.032498797753162e-05 / Validation loss: 0.00029838598557357785 / Long term Validation loss: 0.1324\n",
      "Epoch: [3976/10000] Training loss: 3.0314478579139844e-05 / Validation loss: 0.0002982884223567452 / Long term Validation loss: 0.1324\n",
      "Epoch: [3977/10000] Training loss: 3.030396979311056e-05 / Validation loss: 0.00029819078926474936 / Long term Validation loss: 0.1324\n",
      "Epoch: [3978/10000] Training loss: 3.0293461619141993e-05 / Validation loss: 0.0002980930990382668 / Long term Validation loss: 0.1324\n",
      "Epoch: [3979/10000] Training loss: 3.0282954056925133e-05 / Validation loss: 0.00029799569563934946 / Long term Validation loss: 0.1324\n",
      "Epoch: [3980/10000] Training loss: 3.0272447106165328e-05 / Validation loss: 0.00029789800165408274 / Long term Validation loss: 0.1325\n",
      "Epoch: [3981/10000] Training loss: 3.026194076651022e-05 / Validation loss: 0.0002978006582117383 / Long term Validation loss: 0.1325\n",
      "Epoch: [3982/10000] Training loss: 3.0251435037761717e-05 / Validation loss: 0.00029770312864995334 / Long term Validation loss: 0.1325\n",
      "Epoch: [3983/10000] Training loss: 3.024092991964979e-05 / Validation loss: 0.0002976057213473701 / Long term Validation loss: 0.1325\n",
      "Epoch: [3984/10000] Training loss: 3.023042541197696e-05 / Validation loss: 0.00029750841040544074 / Long term Validation loss: 0.1325\n",
      "Epoch: [3985/10000] Training loss: 3.021992151450383e-05 / Validation loss: 0.0002974109606012332 / Long term Validation loss: 0.1325\n",
      "Epoch: [3986/10000] Training loss: 3.0209418226964794e-05 / Validation loss: 0.00029731378532073266 / Long term Validation loss: 0.1326\n",
      "Epoch: [3987/10000] Training loss: 3.0198915549168632e-05 / Validation loss: 0.00029721641011973835 / Long term Validation loss: 0.1326\n",
      "Epoch: [3988/10000] Training loss: 3.018841348087264e-05 / Validation loss: 0.0002971192508747256 / Long term Validation loss: 0.1326\n",
      "Epoch: [3989/10000] Training loss: 3.0177912021933394e-05 / Validation loss: 0.0002970220429371386 / Long term Validation loss: 0.1326\n",
      "Epoch: [3990/10000] Training loss: 3.0167411172156057e-05 / Validation loss: 0.00029692485232206525 / Long term Validation loss: 0.1326\n",
      "Epoch: [3991/10000] Training loss: 3.0156910931370893e-05 / Validation loss: 0.00029682780749301543 / Long term Validation loss: 0.1327\n",
      "Epoch: [3992/10000] Training loss: 3.0146411299415457e-05 / Validation loss: 0.0002967306341903056 / Long term Validation loss: 0.1327\n",
      "Epoch: [3993/10000] Training loss: 3.0135912276100137e-05 / Validation loss: 0.0002966336756448845 / Long term Validation loss: 0.1327\n",
      "Epoch: [3994/10000] Training loss: 3.012541386130701e-05 / Validation loss: 0.0002965366040251803 / Long term Validation loss: 0.1327\n",
      "Epoch: [3995/10000] Training loss: 3.011491605487563e-05 / Validation loss: 0.0002964396599301116 / Long term Validation loss: 0.1327\n",
      "Epoch: [3996/10000] Training loss: 3.0104418856705012e-05 / Validation loss: 0.0002963427347235154 / Long term Validation loss: 0.1327\n",
      "Epoch: [3997/10000] Training loss: 3.0093922266669086e-05 / Validation loss: 0.0002962457941170309 / Long term Validation loss: 0.1328\n",
      "Epoch: [3998/10000] Training loss: 3.008342628465048e-05 / Validation loss: 0.0002961489937695203 / Long term Validation loss: 0.1328\n",
      "Epoch: [3999/10000] Training loss: 3.0072930910556536e-05 / Validation loss: 0.0002960521019063647 / Long term Validation loss: 0.1328\n",
      "Epoch: [4000/10000] Training loss: 3.006243614427075e-05 / Validation loss: 0.00029595536956023233 / Long term Validation loss: 0.1328\n",
      "Epoch: [4001/10000] Training loss: 3.0051941985731853e-05 / Validation loss: 0.00029585858111295867 / Long term Validation loss: 0.1328\n",
      "Epoch: [4002/10000] Training loss: 3.004144843484852e-05 / Validation loss: 0.0002957618752887275 / Long term Validation loss: 0.1329\n",
      "Epoch: [4003/10000] Training loss: 3.003095549156773e-05 / Validation loss: 0.00029566521129281795 / Long term Validation loss: 0.1329\n",
      "Epoch: [4004/10000] Training loss: 3.002046315582743e-05 / Validation loss: 0.0002955685332648868 / Long term Validation loss: 0.1329\n",
      "Epoch: [4005/10000] Training loss: 3.00099714275697e-05 / Validation loss: 0.00029547197296483473 / Long term Validation loss: 0.1329\n",
      "Epoch: [4006/10000] Training loss: 2.9999480306761884e-05 / Validation loss: 0.00029537535642363817 / Long term Validation loss: 0.1329\n",
      "Epoch: [4007/10000] Training loss: 2.99889897933536e-05 / Validation loss: 0.0002952788613258373 / Long term Validation loss: 0.1329\n",
      "Epoch: [4008/10000] Training loss: 2.9978499887335336e-05 / Validation loss: 0.00029518234122742344 / Long term Validation loss: 0.1330\n",
      "Epoch: [4009/10000] Training loss: 2.996801058867808e-05 / Validation loss: 0.0002950858861029746 / Long term Validation loss: 0.1330\n",
      "Epoch: [4010/10000] Training loss: 2.9957521897380633e-05 / Validation loss: 0.00029498947400010393 / Long term Validation loss: 0.1330\n",
      "Epoch: [4011/10000] Training loss: 2.994703381343939e-05 / Validation loss: 0.0002948930613369099 / Long term Validation loss: 0.1330\n",
      "Epoch: [4012/10000] Training loss: 2.9936546336855333e-05 / Validation loss: 0.0002947967426583448 / Long term Validation loss: 0.1330\n",
      "Epoch: [4013/10000] Training loss: 2.9926059467649857e-05 / Validation loss: 0.00029470039480793163 / Long term Validation loss: 0.1331\n",
      "Epoch: [4014/10000] Training loss: 2.991557320583341e-05 / Validation loss: 0.0002946041443513076 / Long term Validation loss: 0.1331\n",
      "Epoch: [4015/10000] Training loss: 2.9905087551446764e-05 / Validation loss: 0.0002945078843240454 / Long term Validation loss: 0.1331\n",
      "Epoch: [4016/10000] Training loss: 2.9894602504518426e-05 / Validation loss: 0.00029441168502137 / Long term Validation loss: 0.1331\n",
      "Epoch: [4017/10000] Training loss: 2.9884118065099574e-05 / Validation loss: 0.0002943155214752919 / Long term Validation loss: 0.1331\n",
      "Epoch: [4018/10000] Training loss: 2.987363423324051e-05 / Validation loss: 0.0002942193734095137 / Long term Validation loss: 0.1331\n",
      "Epoch: [4019/10000] Training loss: 2.986315100899874e-05 / Validation loss: 0.00029412329840492905 / Long term Validation loss: 0.1332\n",
      "Epoch: [4020/10000] Training loss: 2.985266839244623e-05 / Validation loss: 0.0002940272148771362 / Long term Validation loss: 0.1332\n",
      "Epoch: [4021/10000] Training loss: 2.984218638365015e-05 / Validation loss: 0.0002939312124823002 / Long term Validation loss: 0.1332\n",
      "Epoch: [4022/10000] Training loss: 2.9831704982700485e-05 / Validation loss: 0.00029383520893449676 / Long term Validation loss: 0.1332\n",
      "Epoch: [4023/10000] Training loss: 2.9821224189679643e-05 / Validation loss: 0.00029373926655401687 / Long term Validation loss: 0.1332\n",
      "Epoch: [4024/10000] Training loss: 2.981074400468987e-05 / Validation loss: 0.00029364335092430926 / Long term Validation loss: 0.1333\n",
      "Epoch: [4025/10000] Training loss: 2.9800264427832233e-05 / Validation loss: 0.00029354746585408275 / Long term Validation loss: 0.1333\n",
      "Epoch: [4026/10000] Training loss: 2.9789785459217882e-05 / Validation loss: 0.00029345163568865186 / Long term Validation loss: 0.1333\n",
      "Epoch: [4027/10000] Training loss: 2.9779307098967073e-05 / Validation loss: 0.0002933558143528094 / Long term Validation loss: 0.1333\n",
      "Epoch: [4028/10000] Training loss: 2.9768829347200724e-05 / Validation loss: 0.00029326006054528894 / Long term Validation loss: 0.1333\n",
      "Epoch: [4029/10000] Training loss: 2.975835220405626e-05 / Validation loss: 0.00029316431281854695 / Long term Validation loss: 0.1334\n",
      "Epoch: [4030/10000] Training loss: 2.974787566966764e-05 / Validation loss: 0.0002930686260951823 / Long term Validation loss: 0.1334\n",
      "Epoch: [4031/10000] Training loss: 2.9737399744185954e-05 / Validation loss: 0.00029297295917909445 / Long term Validation loss: 0.1334\n",
      "Epoch: [4032/10000] Training loss: 2.972692442776095e-05 / Validation loss: 0.00029287733503066823 / Long term Validation loss: 0.1334\n",
      "Epoch: [4033/10000] Training loss: 2.971644972055462e-05 / Validation loss: 0.0002927817502965479 / Long term Validation loss: 0.1334\n",
      "Epoch: [4034/10000] Training loss: 2.970597562273363e-05 / Validation loss: 0.00029268619015321187 / Long term Validation loss: 0.1334\n",
      "Epoch: [4035/10000] Training loss: 2.9695502134470203e-05 / Validation loss: 0.0002925906837880761 / Long term Validation loss: 0.1335\n",
      "Epoch: [4036/10000] Training loss: 2.9685029255947387e-05 / Validation loss: 0.000292495192873425 / Long term Validation loss: 0.1335\n",
      "Epoch: [4037/10000] Training loss: 2.9674556987348853e-05 / Validation loss: 0.0002923997589502767 / Long term Validation loss: 0.1335\n",
      "Epoch: [4038/10000] Training loss: 2.966408532887217e-05 / Validation loss: 0.0002923043428299584 / Long term Validation loss: 0.1335\n",
      "Epoch: [4039/10000] Training loss: 2.9653614280714466e-05 / Validation loss: 0.00029220897660891675 / Long term Validation loss: 0.1335\n",
      "Epoch: [4040/10000] Training loss: 2.9643143843085687e-05 / Validation loss: 0.0002921136384880755 / Long term Validation loss: 0.1336\n",
      "Epoch: [4041/10000] Training loss: 2.9632674016197707e-05 / Validation loss: 0.00029201833829478266 / Long term Validation loss: 0.1336\n",
      "Epoch: [4042/10000] Training loss: 2.9622204800271606e-05 / Validation loss: 0.00029192307812220676 / Long term Validation loss: 0.1336\n",
      "Epoch: [4043/10000] Training loss: 2.9611736195534396e-05 / Validation loss: 0.00029182784532902486 / Long term Validation loss: 0.1336\n",
      "Epoch: [4044/10000] Training loss: 2.960126820221811e-05 / Validation loss: 0.0002917326605901497 / Long term Validation loss: 0.1336\n",
      "Epoch: [4045/10000] Training loss: 2.959080082056434e-05 / Validation loss: 0.0002916374982564273 / Long term Validation loss: 0.1337\n",
      "Epoch: [4046/10000] Training loss: 2.9580334050816893e-05 / Validation loss: 0.0002915423856109675 / Long term Validation loss: 0.1337\n",
      "Epoch: [4047/10000] Training loss: 2.9569867893230716e-05 / Validation loss: 0.0002914472967756218 / Long term Validation loss: 0.1337\n",
      "Epoch: [4048/10000] Training loss: 2.955940234806242e-05 / Validation loss: 0.0002913522535934778 / Long term Validation loss: 0.1337\n",
      "Epoch: [4049/10000] Training loss: 2.954893741557912e-05 / Validation loss: 0.00029125724005108134 / Long term Validation loss: 0.1337\n",
      "Epoch: [4050/10000] Training loss: 2.9538473096050966e-05 / Validation loss: 0.00029116226524284887 / Long term Validation loss: 0.1338\n",
      "Epoch: [4051/10000] Training loss: 2.9528009389756448e-05 / Validation loss: 0.00029106732715696753 / Long term Validation loss: 0.1338\n",
      "Epoch: [4052/10000] Training loss: 2.951754629697946e-05 / Validation loss: 0.00029097242117663666 / Long term Validation loss: 0.1338\n",
      "Epoch: [4053/10000] Training loss: 2.9507083818009798e-05 / Validation loss: 0.00029087755742119155 / Long term Validation loss: 0.1338\n",
      "Epoch: [4054/10000] Training loss: 2.949662195314473e-05 / Validation loss: 0.0002907827216894786 / Long term Validation loss: 0.1338\n",
      "Epoch: [4055/10000] Training loss: 2.94861607026856e-05 / Validation loss: 0.0002906879305587082 / Long term Validation loss: 0.1338\n",
      "Epoch: [4056/10000] Training loss: 2.9475700066942494e-05 / Validation loss: 0.0002905931667026783 / Long term Validation loss: 0.1339\n",
      "Epoch: [4057/10000] Training loss: 2.9465240046228768e-05 / Validation loss: 0.0002904984466155392 / Long term Validation loss: 0.1339\n",
      "Epoch: [4058/10000] Training loss: 2.9454780640866703e-05 / Validation loss: 0.0002904037558605402 / Long term Validation loss: 0.1339\n",
      "Epoch: [4059/10000] Training loss: 2.9444321851181978e-05 / Validation loss: 0.000290309105817227 / Long term Validation loss: 0.1339\n",
      "Epoch: [4060/10000] Training loss: 2.943386367750857e-05 / Validation loss: 0.0002902144886960345 / Long term Validation loss: 0.1339\n",
      "Epoch: [4061/10000] Training loss: 2.942340612018476e-05 / Validation loss: 0.00029011990841337743 / Long term Validation loss: 0.1340\n",
      "Epoch: [4062/10000] Training loss: 2.941294917955583e-05 / Validation loss: 0.0002900253647843356 / Long term Validation loss: 0.1340\n",
      "Epoch: [4063/10000] Training loss: 2.9402492855972682e-05 / Validation loss: 0.0002899308545692052 / Long term Validation loss: 0.1340\n",
      "Epoch: [4064/10000] Training loss: 2.9392037149791885e-05 / Validation loss: 0.0002898363838309135 / Long term Validation loss: 0.1340\n",
      "Epoch: [4065/10000] Training loss: 2.9381582061376737e-05 / Validation loss: 0.00028974194431671547 / Long term Validation loss: 0.1340\n",
      "Epoch: [4066/10000] Training loss: 2.937112759109511e-05 / Validation loss: 0.0002896475456849727 / Long term Validation loss: 0.1341\n",
      "Epoch: [4067/10000] Training loss: 2.9360673739322406e-05 / Validation loss: 0.00028955317755812837 / Long term Validation loss: 0.1341\n",
      "Epoch: [4068/10000] Training loss: 2.9350220506438045e-05 / Validation loss: 0.00028945885030352175 / Long term Validation loss: 0.1341\n",
      "Epoch: [4069/10000] Training loss: 2.9339767892829125e-05 / Validation loss: 0.0002893645541053049 / Long term Validation loss: 0.1341\n",
      "Epoch: [4070/10000] Training loss: 2.932931589888679e-05 / Validation loss: 0.00028927029770002484 / Long term Validation loss: 0.1341\n",
      "Epoch: [4071/10000] Training loss: 2.931886452500958e-05 / Validation loss: 0.00028917607373341015 / Long term Validation loss: 0.1342\n",
      "Epoch: [4072/10000] Training loss: 2.930841377160033e-05 / Validation loss: 0.00028908188789974494 / Long term Validation loss: 0.1342\n",
      "Epoch: [4073/10000] Training loss: 2.9297963639068885e-05 / Validation loss: 0.0002889877362269661 / Long term Validation loss: 0.1342\n",
      "Epoch: [4074/10000] Training loss: 2.928751412782984e-05 / Validation loss: 0.00028889362090899216 / Long term Validation loss: 0.1342\n",
      "Epoch: [4075/10000] Training loss: 2.927706523830408e-05 / Validation loss: 0.00028879954140524573 / Long term Validation loss: 0.1342\n",
      "Epoch: [4076/10000] Training loss: 2.9266616970917976e-05 / Validation loss: 0.00028870549669850036 / Long term Validation loss: 0.1342\n",
      "Epoch: [4077/10000] Training loss: 2.9256169326103444e-05 / Validation loss: 0.0002886114891273428 / Long term Validation loss: 0.1343\n",
      "Epoch: [4078/10000] Training loss: 2.9245722304298413e-05 / Validation loss: 0.0002885175152001834 / Long term Validation loss: 0.1343\n",
      "Epoch: [4079/10000] Training loss: 2.923527590594588e-05 / Validation loss: 0.00028842357928560185 / Long term Validation loss: 0.1343\n",
      "Epoch: [4080/10000] Training loss: 2.9224830131495167e-05 / Validation loss: 0.0002883296763149149 / Long term Validation loss: 0.1343\n",
      "Epoch: [4081/10000] Training loss: 2.9214384981400314e-05 / Validation loss: 0.00028823581179567525 / Long term Validation loss: 0.1343\n",
      "Epoch: [4082/10000] Training loss: 2.920394045612194e-05 / Validation loss: 0.0002881419799256715 / Long term Validation loss: 0.1344\n",
      "Epoch: [4083/10000] Training loss: 2.919349655612515e-05 / Validation loss: 0.0002880481865865979 / Long term Validation loss: 0.1344\n",
      "Epoch: [4084/10000] Training loss: 2.9183053281881675e-05 / Validation loss: 0.000287954425909408 / Long term Validation loss: 0.1344\n",
      "Epoch: [4085/10000] Training loss: 2.917261063386772e-05 / Validation loss: 0.0002878607035916244 / Long term Validation loss: 0.1344\n",
      "Epoch: [4086/10000] Training loss: 2.916216861256598e-05 / Validation loss: 0.0002877670141442879 / Long term Validation loss: 0.1344\n",
      "Epoch: [4087/10000] Training loss: 2.915172721846365e-05 / Validation loss: 0.0002876733627411117 / Long term Validation loss: 0.1345\n",
      "Epoch: [4088/10000] Training loss: 2.9141286452054474e-05 / Validation loss: 0.0002875797445130744 / Long term Validation loss: 0.1345\n",
      "Epoch: [4089/10000] Training loss: 2.913084631383654e-05 / Validation loss: 0.0002874861639590797 / Long term Validation loss: 0.1345\n",
      "Epoch: [4090/10000] Training loss: 2.9120406804314426e-05 / Validation loss: 0.0002873926169048109 / Long term Validation loss: 0.1345\n",
      "Epoch: [4091/10000] Training loss: 2.9109967923997214e-05 / Validation loss: 0.00028729910716344747 / Long term Validation loss: 0.1345\n",
      "Epoch: [4092/10000] Training loss: 2.9099529673400172e-05 / Validation loss: 0.0002872056312153938 / Long term Validation loss: 0.1346\n",
      "Epoch: [4093/10000] Training loss: 2.9089092053043308e-05 / Validation loss: 0.0002871121922677643 / Long term Validation loss: 0.1346\n",
      "Epoch: [4094/10000] Training loss: 2.9078655063452607e-05 / Validation loss: 0.00028701878734655335 / Long term Validation loss: 0.1346\n",
      "Epoch: [4095/10000] Training loss: 2.9068218705158872e-05 / Validation loss: 0.0002869254191825589 / Long term Validation loss: 0.1346\n",
      "Epoch: [4096/10000] Training loss: 2.9057782978698753e-05 / Validation loss: 0.0002868320852035023 / Long term Validation loss: 0.1346\n",
      "Epoch: [4097/10000] Training loss: 2.9047347884613774e-05 / Validation loss: 0.0002867387878162002 / Long term Validation loss: 0.1347\n",
      "Epoch: [4098/10000] Training loss: 2.9036913423451237e-05 / Validation loss: 0.00028664552469274864 / Long term Validation loss: 0.1347\n",
      "Epoch: [4099/10000] Training loss: 2.9026479595763303e-05 / Validation loss: 0.00028655229807622975 / Long term Validation loss: 0.1347\n",
      "Epoch: [4100/10000] Training loss: 2.9016046402107855e-05 / Validation loss: 0.0002864591057210941 / Long term Validation loss: 0.1347\n",
      "Epoch: [4101/10000] Training loss: 2.9005613843047696e-05 / Validation loss: 0.0002863659498713297 / Long term Validation loss: 0.1347\n",
      "Epoch: [4102/10000] Training loss: 2.899518191915122e-05 / Validation loss: 0.00028627282819530293 / Long term Validation loss: 0.1347\n",
      "Epoch: [4103/10000] Training loss: 2.898475063099171e-05 / Validation loss: 0.00028617974311302225 / Long term Validation loss: 0.1348\n",
      "Epoch: [4104/10000] Training loss: 2.8974319979148125e-05 / Validation loss: 0.00028608669202120543 / Long term Validation loss: 0.1348\n",
      "Epoch: [4105/10000] Training loss: 2.896388996420418e-05 / Validation loss: 0.00028599367771651345 / Long term Validation loss: 0.1348\n",
      "Epoch: [4106/10000] Training loss: 2.8953460586749298e-05 / Validation loss: 0.00028590069710180473 / Long term Validation loss: 0.1348\n",
      "Epoch: [4107/10000] Training loss: 2.8943031847377597e-05 / Validation loss: 0.00028580775360143463 / Long term Validation loss: 0.1348\n",
      "Epoch: [4108/10000] Training loss: 2.893260374668895e-05 / Validation loss: 0.0002857148433349306 / Long term Validation loss: 0.1349\n",
      "Epoch: [4109/10000] Training loss: 2.8922176285287758e-05 / Validation loss: 0.0002856219706938525 / Long term Validation loss: 0.1349\n",
      "Epoch: [4110/10000] Training loss: 2.8911749463784315e-05 / Validation loss: 0.0002855291306107132 / Long term Validation loss: 0.1349\n",
      "Epoch: [4111/10000] Training loss: 2.890132328279324e-05 / Validation loss: 0.00028543632893036637 / Long term Validation loss: 0.1349\n",
      "Epoch: [4112/10000] Training loss: 2.8890897742935268e-05 / Validation loss: 0.00028534355880765956 / Long term Validation loss: 0.1349\n",
      "Epoch: [4113/10000] Training loss: 2.8880472844835092e-05 / Validation loss: 0.0002852508282645454 / Long term Validation loss: 0.1350\n",
      "Epoch: [4114/10000] Training loss: 2.8870048589123962e-05 / Validation loss: 0.0002851581277852627 / Long term Validation loss: 0.1350\n",
      "Epoch: [4115/10000] Training loss: 2.8859624976436515e-05 / Validation loss: 0.0002850654686767075 / Long term Validation loss: 0.1350\n",
      "Epoch: [4116/10000] Training loss: 2.8849202007414483e-05 / Validation loss: 0.0002849728373706501 / Long term Validation loss: 0.1350\n",
      "Epoch: [4117/10000] Training loss: 2.883877968270234e-05 / Validation loss: 0.00028488025019025566 / Long term Validation loss: 0.1350\n",
      "Epoch: [4118/10000] Training loss: 2.8828358002952386e-05 / Validation loss: 0.00028478768733602566 / Long term Validation loss: 0.1350\n",
      "Epoch: [4119/10000] Training loss: 2.881793696881873e-05 / Validation loss: 0.00028469517290041643 / Long term Validation loss: 0.1351\n",
      "Epoch: [4120/10000] Training loss: 2.8807516580964464e-05 / Validation loss: 0.0002846026773605831 / Long term Validation loss: 0.1351\n",
      "Epoch: [4121/10000] Training loss: 2.8797096840052886e-05 / Validation loss: 0.0002845102370248813 / Long term Validation loss: 0.1351\n",
      "Epoch: [4122/10000] Training loss: 2.878667774675827e-05 / Validation loss: 0.00028441780696430893 / Long term Validation loss: 0.1351\n",
      "Epoch: [4123/10000] Training loss: 2.8776259301752613e-05 / Validation loss: 0.0002843254429926414 / Long term Validation loss: 0.1351\n",
      "Epoch: [4124/10000] Training loss: 2.8765841505722e-05 / Validation loss: 0.0002842330753905121 / Long term Validation loss: 0.1352\n",
      "Epoch: [4125/10000] Training loss: 2.8755424359346177e-05 / Validation loss: 0.0002841407916011403 / Long term Validation loss: 0.1352\n",
      "Epoch: [4126/10000] Training loss: 2.874500786332422e-05 / Validation loss: 0.00028404848139522063 / Long term Validation loss: 0.1352\n",
      "Epoch: [4127/10000] Training loss: 2.8734592018342226e-05 / Validation loss: 0.00028395628429850187 / Long term Validation loss: 0.1352\n",
      "Epoch: [4128/10000] Training loss: 2.8724176825114184e-05 / Validation loss: 0.0002838640228663482 / Long term Validation loss: 0.1352\n",
      "Epoch: [4129/10000] Training loss: 2.8713762284330365e-05 / Validation loss: 0.00028377192369729524 / Long term Validation loss: 0.1352\n",
      "Epoch: [4130/10000] Training loss: 2.8703348396723207e-05 / Validation loss: 0.00028367969612742296 / Long term Validation loss: 0.1353\n",
      "Epoch: [4131/10000] Training loss: 2.8692935162983606e-05 / Validation loss: 0.00028358771452056177 / Long term Validation loss: 0.1353\n",
      "Epoch: [4132/10000] Training loss: 2.868252258386928e-05 / Validation loss: 0.000283495494649032 / Long term Validation loss: 0.1353\n",
      "Epoch: [4133/10000] Training loss: 2.8672110660066653e-05 / Validation loss: 0.0002834036653632054 / Long term Validation loss: 0.1353\n",
      "Epoch: [4134/10000] Training loss: 2.8661699392372486e-05 / Validation loss: 0.00028331140663557364 / Long term Validation loss: 0.1353\n",
      "Epoch: [4135/10000] Training loss: 2.8651288781462946e-05 / Validation loss: 0.0002832197920090792 / Long term Validation loss: 0.1354\n",
      "Epoch: [4136/10000] Training loss: 2.8640878828204467e-05 / Validation loss: 0.000283127410453607 / Long term Validation loss: 0.1354\n",
      "Epoch: [4137/10000] Training loss: 2.8630469533263465e-05 / Validation loss: 0.0002830361237503622 / Long term Validation loss: 0.1354\n",
      "Epoch: [4138/10000] Training loss: 2.8620060897651495e-05 / Validation loss: 0.0002829434658726045 / Long term Validation loss: 0.1354\n",
      "Epoch: [4139/10000] Training loss: 2.860965292206369e-05 / Validation loss: 0.00028285271556522046 / Long term Validation loss: 0.1354\n",
      "Epoch: [4140/10000] Training loss: 2.8599245607864787e-05 / Validation loss: 0.000282759497091583 / Long term Validation loss: 0.1354\n",
      "Epoch: [4141/10000] Training loss: 2.8588838955980833e-05 / Validation loss: 0.00028266967185201157 / Long term Validation loss: 0.1355\n",
      "Epoch: [4142/10000] Training loss: 2.8578432968764638e-05 / Validation loss: 0.0002825753594768329 / Long term Validation loss: 0.1355\n",
      "Epoch: [4143/10000] Training loss: 2.8568027648224488e-05 / Validation loss: 0.0002824871932112493 / Long term Validation loss: 0.1355\n",
      "Epoch: [4144/10000] Training loss: 2.8557622999801807e-05 / Validation loss: 0.0002823907736495933 / Long term Validation loss: 0.1355\n",
      "Epoch: [4145/10000] Training loss: 2.854721902994642e-05 / Validation loss: 0.0002823056696987065 / Long term Validation loss: 0.1355\n",
      "Epoch: [4146/10000] Training loss: 2.8536815754579242e-05 / Validation loss: 0.00028220519343344363 / Long term Validation loss: 0.1355\n",
      "Epoch: [4147/10000] Training loss: 2.852641319764523e-05 / Validation loss: 0.00028212586881487344 / Long term Validation loss: 0.1356\n",
      "Epoch: [4148/10000] Training loss: 2.8516011412782875e-05 / Validation loss: 0.0002820175384281356 / Long term Validation loss: 0.1356\n",
      "Epoch: [4149/10000] Training loss: 2.850561049240565e-05 / Validation loss: 0.00028194931879483437 / Long term Validation loss: 0.1356\n",
      "Epoch: [4150/10000] Training loss: 2.8495210631988106e-05 / Validation loss: 0.0002818256467456852 / Long term Validation loss: 0.1356\n",
      "Epoch: [4151/10000] Training loss: 2.848481219433381e-05 / Validation loss: 0.0002817790992053782 / Long term Validation loss: 0.1357\n",
      "Epoch: [4152/10000] Training loss: 2.847441592712192e-05 / Validation loss: 0.0002816251439660753 / Long term Validation loss: 0.1356\n",
      "Epoch: [4153/10000] Training loss: 2.846402327868955e-05 / Validation loss: 0.00028162149166825404 / Long term Validation loss: 0.1357\n",
      "Epoch: [4154/10000] Training loss: 2.845363721094366e-05 / Validation loss: 0.0002814070848953115 / Long term Validation loss: 0.1356\n",
      "Epoch: [4155/10000] Training loss: 2.844326361859663e-05 / Validation loss: 0.0002814894731795524 / Long term Validation loss: 0.1358\n",
      "Epoch: [4156/10000] Training loss: 2.8432914581279482e-05 / Validation loss: 0.00028115300361465173 / Long term Validation loss: 0.1356\n",
      "Epoch: [4157/10000] Training loss: 2.8422614606349145e-05 / Validation loss: 0.0002814102288753809 / Long term Validation loss: 0.1359\n",
      "Epoch: [4158/10000] Training loss: 2.841241426568374e-05 / Validation loss: 0.00028082449991838717 / Long term Validation loss: 0.1355\n",
      "Epoch: [4159/10000] Training loss: 2.8402417748192114e-05 / Validation loss: 0.00028144166021937917 / Long term Validation loss: 0.1361\n",
      "Epoch: [4160/10000] Training loss: 2.8392842062844026e-05 / Validation loss: 0.00028034149284869006 / Long term Validation loss: 0.1354\n",
      "Epoch: [4161/10000] Training loss: 2.838414033416015e-05 / Validation loss: 0.0002817098412458466 / Long term Validation loss: 0.1364\n",
      "Epoch: [4162/10000] Training loss: 2.8377266140308413e-05 / Validation loss: 0.0002795381598620898 / Long term Validation loss: 0.1348\n",
      "Epoch: [4163/10000] Training loss: 2.8374235463374465e-05 / Validation loss: 0.00028249848318762544 / Long term Validation loss: 0.1370\n",
      "Epoch: [4164/10000] Training loss: 2.837933571978692e-05 / Validation loss: 0.00027808159899318135 / Long term Validation loss: 0.1336\n",
      "Epoch: [4165/10000] Training loss: 2.8401737902123883e-05 / Validation loss: 0.0002844808180770085 / Long term Validation loss: 0.1382\n",
      "Epoch: [4166/10000] Training loss: 2.846113827922908e-05 / Validation loss: 0.0002753658491052153 / Long term Validation loss: 0.1306\n",
      "Epoch: [4167/10000] Training loss: 2.8600108846419996e-05 / Validation loss: 0.0002893946211116929 / Long term Validation loss: 0.1406\n",
      "Epoch: [4168/10000] Training loss: 2.8910675835993227e-05 / Validation loss: 0.00027063676843020043 / Long term Validation loss: 0.1236\n",
      "Epoch: [4169/10000] Training loss: 2.9592726839002536e-05 / Validation loss: 0.0003022683166901984 / Long term Validation loss: 0.1425\n",
      "Epoch: [4170/10000] Training loss: 3.107505886795747e-05 / Validation loss: 0.0002649851405417295 / Long term Validation loss: 0.1118\n",
      "Epoch: [4171/10000] Training loss: 3.4268888143609036e-05 / Validation loss: 0.0003390066188895289 / Long term Validation loss: 0.1383\n",
      "Epoch: [4172/10000] Training loss: 4.098828445078114e-05 / Validation loss: 0.0002719599223328573 / Long term Validation loss: 0.1009\n",
      "Epoch: [4173/10000] Training loss: 5.448890979939148e-05 / Validation loss: 0.0004408214293471414 / Long term Validation loss: 0.1345\n",
      "Epoch: [4174/10000] Training loss: 7.855625525585446e-05 / Validation loss: 0.00032831954959667935 / Long term Validation loss: 0.1006\n",
      "Epoch: [4175/10000] Training loss: 0.0001117922386928042 / Validation loss: 0.0005676035476585353 / Long term Validation loss: 0.1364\n",
      "Epoch: [4176/10000] Training loss: 0.00013250961908729692 / Validation loss: 0.00032210831176761903 / Long term Validation loss: 0.1004\n",
      "Epoch: [4177/10000] Training loss: 0.0001073308782564254 / Validation loss: 0.0003658857449240026 / Long term Validation loss: 0.1379\n",
      "Epoch: [4178/10000] Training loss: 4.947840566866056e-05 / Validation loss: 0.0002949159660235448 / Long term Validation loss: 0.1422\n",
      "Epoch: [4179/10000] Training loss: 2.9527100478447798e-05 / Validation loss: 0.00027670113920859064 / Long term Validation loss: 0.1000\n",
      "Epoch: [4180/10000] Training loss: 6.201686626098793e-05 / Validation loss: 0.0004527401460193285 / Long term Validation loss: 0.1345\n",
      "Epoch: [4181/10000] Training loss: 8.190808984527716e-05 / Validation loss: 0.00027047104064498905 / Long term Validation loss: 0.1006\n",
      "Epoch: [4182/10000] Training loss: 5.3059919365897025e-05 / Validation loss: 0.00028336394768201686 / Long term Validation loss: 0.1396\n",
      "Epoch: [4183/10000] Training loss: 2.8262438819762013e-05 / Validation loss: 0.00036024569368682676 / Long term Validation loss: 0.1383\n",
      "Epoch: [4184/10000] Training loss: 4.6676972790315286e-05 / Validation loss: 0.000276773779651426 / Long term Validation loss: 0.0997\n",
      "Epoch: [4185/10000] Training loss: 6.189969860448379e-05 / Validation loss: 0.0003446501761284129 / Long term Validation loss: 0.1380\n",
      "Epoch: [4186/10000] Training loss: 4.157495752747364e-05 / Validation loss: 0.0002885298954968499 / Long term Validation loss: 0.1413\n",
      "Epoch: [4187/10000] Training loss: 2.8548056291995952e-05 / Validation loss: 0.0002658401556808849 / Long term Validation loss: 0.1030\n",
      "Epoch: [4188/10000] Training loss: 4.418037915782432e-05 / Validation loss: 0.00036601070419635845 / Long term Validation loss: 0.1382\n",
      "Epoch: [4189/10000] Training loss: 4.8598030119872496e-05 / Validation loss: 0.00026693873102784463 / Long term Validation loss: 0.1177\n",
      "Epoch: [4190/10000] Training loss: 3.2300496774374016e-05 / Validation loss: 0.000269067235184033 / Long term Validation loss: 0.1228\n",
      "Epoch: [4191/10000] Training loss: 3.067072917613731e-05 / Validation loss: 0.00034638056697048865 / Long term Validation loss: 0.1381\n",
      "Epoch: [4192/10000] Training loss: 4.225843524502611e-05 / Validation loss: 0.0002649105461145328 / Long term Validation loss: 0.1067\n",
      "Epoch: [4193/10000] Training loss: 3.8249862554376956e-05 / Validation loss: 0.0002869718208386582 / Long term Validation loss: 0.1409\n",
      "Epoch: [4194/10000] Training loss: 2.839332941096175e-05 / Validation loss: 0.00031453812466024914 / Long term Validation loss: 0.1392\n",
      "Epoch: [4195/10000] Training loss: 3.336179265884083e-05 / Validation loss: 0.00026470908603257957 / Long term Validation loss: 0.1062\n",
      "Epoch: [4196/10000] Training loss: 3.830337990495765e-05 / Validation loss: 0.0003050651977919623 / Long term Validation loss: 0.1407\n",
      "Epoch: [4197/10000] Training loss: 3.130704958787457e-05 / Validation loss: 0.00028910908419968814 / Long term Validation loss: 0.1414\n",
      "Epoch: [4198/10000] Training loss: 2.8650476878980544e-05 / Validation loss: 0.00026488734000381183 / Long term Validation loss: 0.1112\n",
      "Epoch: [4199/10000] Training loss: 3.4260156201579415e-05 / Validation loss: 0.0003135167119678283 / Long term Validation loss: 0.1395\n",
      "Epoch: [4200/10000] Training loss: 3.346035979999624e-05 / Validation loss: 0.0002748728343746398 / Long term Validation loss: 0.1323\n",
      "Epoch: [4201/10000] Training loss: 2.8377977294822667e-05 / Validation loss: 0.00026851791588579317 / Long term Validation loss: 0.1219\n",
      "Epoch: [4202/10000] Training loss: 3.0152925871924988e-05 / Validation loss: 0.0003109461533143029 / Long term Validation loss: 0.1402\n",
      "Epoch: [4203/10000] Training loss: 3.2947690008389545e-05 / Validation loss: 0.00026894501060448813 / Long term Validation loss: 0.1225\n",
      "Epoch: [4204/10000] Training loss: 2.9869613908309613e-05 / Validation loss: 0.00027588615957450577 / Long term Validation loss: 0.1333\n",
      "Epoch: [4205/10000] Training loss: 2.819600148523181e-05 / Validation loss: 0.0003012373961244842 / Long term Validation loss: 0.1424\n",
      "Epoch: [4206/10000] Training loss: 3.082392023252724e-05 / Validation loss: 0.00026708449911191874 / Long term Validation loss: 0.1194\n",
      "Epoch: [4207/10000] Training loss: 3.0767223612289084e-05 / Validation loss: 0.00028432546122894207 / Long term Validation loss: 0.1393\n",
      "Epoch: [4208/10000] Training loss: 2.8251722754419835e-05 / Validation loss: 0.00028972944676696876 / Long term Validation loss: 0.1419\n",
      "Epoch: [4209/10000] Training loss: 2.886067106490248e-05 / Validation loss: 0.00026728570132136846 / Long term Validation loss: 0.1202\n",
      "Epoch: [4210/10000] Training loss: 3.0350010209726235e-05 / Validation loss: 0.0002906724289049724 / Long term Validation loss: 0.1424\n",
      "Epoch: [4211/10000] Training loss: 2.9014489942524756e-05 / Validation loss: 0.00028038949396704243 / Long term Validation loss: 0.1376\n",
      "Epoch: [4212/10000] Training loss: 2.801914455366833e-05 / Validation loss: 0.0002694468149691501 / Long term Validation loss: 0.1237\n",
      "Epoch: [4213/10000] Training loss: 2.9214363466636937e-05 / Validation loss: 0.0002929413477267471 / Long term Validation loss: 0.1431\n",
      "Epoch: [4214/10000] Training loss: 2.93721774004779e-05 / Validation loss: 0.0002745295071863597 / Long term Validation loss: 0.1318\n",
      "Epoch: [4215/10000] Training loss: 2.8144326272378273e-05 / Validation loss: 0.00027339444031824735 / Long term Validation loss: 0.1303\n",
      "Epoch: [4216/10000] Training loss: 2.8262662415820033e-05 / Validation loss: 0.00029098207382665064 / Long term Validation loss: 0.1428\n",
      "Epoch: [4217/10000] Training loss: 2.9050164118676844e-05 / Validation loss: 0.0002715805873318162 / Long term Validation loss: 0.1276\n",
      "Epoch: [4218/10000] Training loss: 2.8530490584896128e-05 / Validation loss: 0.0002781399618378871 / Long term Validation loss: 0.1364\n",
      "Epoch: [4219/10000] Training loss: 2.792957218534627e-05 / Validation loss: 0.00028633176513993423 / Long term Validation loss: 0.1410\n",
      "Epoch: [4220/10000] Training loss: 2.8427667953063504e-05 / Validation loss: 0.0002707519033403594 / Long term Validation loss: 0.1267\n",
      "Epoch: [4221/10000] Training loss: 2.8632668366521057e-05 / Validation loss: 0.00028230175615933636 / Long term Validation loss: 0.1391\n",
      "Epoch: [4222/10000] Training loss: 2.8054260734379077e-05 / Validation loss: 0.0002811596697886904 / Long term Validation loss: 0.1385\n",
      "Epoch: [4223/10000] Training loss: 2.798015564475303e-05 / Validation loss: 0.00027164500308416844 / Long term Validation loss: 0.1287\n",
      "Epoch: [4224/10000] Training loss: 2.838401843898146e-05 / Validation loss: 0.00028464553824086495 / Long term Validation loss: 0.1404\n",
      "Epoch: [4225/10000] Training loss: 2.8232673702785955e-05 / Validation loss: 0.0002769880881961746 / Long term Validation loss: 0.1360\n",
      "Epoch: [4226/10000] Training loss: 2.7877141114981592e-05 / Validation loss: 0.0002738270690165283 / Long term Validation loss: 0.1323\n",
      "Epoch: [4227/10000] Training loss: 2.8040813966740993e-05 / Validation loss: 0.00028459261262315784 / Long term Validation loss: 0.1405\n",
      "Epoch: [4228/10000] Training loss: 2.821421700073643e-05 / Validation loss: 0.0002743193180579143 / Long term Validation loss: 0.1332\n",
      "Epoch: [4229/10000] Training loss: 2.7968909957262793e-05 / Validation loss: 0.00027657940035678884 / Long term Validation loss: 0.1360\n",
      "Epoch: [4230/10000] Training loss: 2.7845864221619346e-05 / Validation loss: 0.0002825476833093996 / Long term Validation loss: 0.1396\n",
      "Epoch: [4231/10000] Training loss: 2.8029317232088162e-05 / Validation loss: 0.00027313400825243543 / Long term Validation loss: 0.1320\n",
      "Epoch: [4232/10000] Training loss: 2.802742856802145e-05 / Validation loss: 0.0002790571560600514 / Long term Validation loss: 0.1379\n",
      "Epoch: [4233/10000] Training loss: 2.7833713016765743e-05 / Validation loss: 0.000279609797424742 / Long term Validation loss: 0.1381\n",
      "Epoch: [4234/10000] Training loss: 2.7847233924418765e-05 / Validation loss: 0.00027326977782488537 / Long term Validation loss: 0.1326\n",
      "Epoch: [4235/10000] Training loss: 2.796058180792561e-05 / Validation loss: 0.0002805075188878175 / Long term Validation loss: 0.1387\n",
      "Epoch: [4236/10000] Training loss: 2.7880900472634924e-05 / Validation loss: 0.0002768471808949305 / Long term Validation loss: 0.1367\n",
      "Epoch: [4237/10000] Training loss: 2.777451172905545e-05 / Validation loss: 0.0002743799138758303 / Long term Validation loss: 0.1344\n",
      "Epoch: [4238/10000] Training loss: 2.783321414534554e-05 / Validation loss: 0.00028052940033544265 / Long term Validation loss: 0.1388\n",
      "Epoch: [4239/10000] Training loss: 2.787213555084057e-05 / Validation loss: 0.0002748637817276312 / Long term Validation loss: 0.1352\n",
      "Epoch: [4240/10000] Training loss: 2.778394008218253e-05 / Validation loss: 0.0002759156708531742 / Long term Validation loss: 0.1362\n",
      "Epoch: [4241/10000] Training loss: 2.7744932734188115e-05 / Validation loss: 0.0002792948556239665 / Long term Validation loss: 0.1383\n",
      "Epoch: [4242/10000] Training loss: 2.7797902059391717e-05 / Validation loss: 0.0002738669260534488 / Long term Validation loss: 0.1344\n",
      "Epoch: [4243/10000] Training loss: 2.7789932176872725e-05 / Validation loss: 0.0002772725936706864 / Long term Validation loss: 0.1373\n",
      "Epoch: [4244/10000] Training loss: 2.7722858096792847e-05 / Validation loss: 0.0002774358351137506 / Long term Validation loss: 0.1374\n",
      "Epoch: [4245/10000] Training loss: 2.7719653821252855e-05 / Validation loss: 0.00027381521001974905 / Long term Validation loss: 0.1347\n",
      "Epoch: [4246/10000] Training loss: 2.7750581971279212e-05 / Validation loss: 0.0002779563916406633 / Long term Validation loss: 0.1378\n",
      "Epoch: [4247/10000] Training loss: 2.7722941593629235e-05 / Validation loss: 0.00027564073614032416 / Long term Validation loss: 0.1365\n",
      "Epoch: [4248/10000] Training loss: 2.7680992255265813e-05 / Validation loss: 0.0002744423803122684 / Long term Validation loss: 0.1356\n",
      "Epoch: [4249/10000] Training loss: 2.7690025553070162e-05 / Validation loss: 0.0002777475787451531 / Long term Validation loss: 0.1378\n",
      "Epoch: [4250/10000] Training loss: 2.770060381410912e-05 / Validation loss: 0.00027435867428954644 / Long term Validation loss: 0.1357\n",
      "Epoch: [4251/10000] Training loss: 2.7670063236533646e-05 / Validation loss: 0.00027531954761664013 / Long term Validation loss: 0.1365\n",
      "Epoch: [4252/10000] Training loss: 2.7646760864708338e-05 / Validation loss: 0.0002768038806494044 / Long term Validation loss: 0.1375\n",
      "Epoch: [4253/10000] Training loss: 2.7655710151585847e-05 / Validation loss: 0.00027376616486729993 / Long term Validation loss: 0.1355\n",
      "Epoch: [4254/10000] Training loss: 2.765310420875638e-05 / Validation loss: 0.0002760049000225292 / Long term Validation loss: 0.1372\n",
      "Epoch: [4255/10000] Training loss: 2.76267899226598e-05 / Validation loss: 0.00027555348707307946 / Long term Validation loss: 0.1369\n",
      "Epoch: [4256/10000] Training loss: 2.7614280116703216e-05 / Validation loss: 0.0002738120557106996 / Long term Validation loss: 0.1358\n",
      "Epoch: [4257/10000] Training loss: 2.7618818732330876e-05 / Validation loss: 0.00027618973920425356 / Long term Validation loss: 0.1375\n",
      "Epoch: [4258/10000] Training loss: 2.7609556847155622e-05 / Validation loss: 0.000274436286007412 / Long term Validation loss: 0.1364\n",
      "Epoch: [4259/10000] Training loss: 2.7588923520678148e-05 / Validation loss: 0.00027425091393706543 / Long term Validation loss: 0.1364\n",
      "Epoch: [4260/10000] Training loss: 2.75814102642428e-05 / Validation loss: 0.0002758022916188862 / Long term Validation loss: 0.1374\n",
      "Epoch: [4261/10000] Training loss: 2.758126461029295e-05 / Validation loss: 0.0002737234908067002 / Long term Validation loss: 0.1361\n",
      "Epoch: [4262/10000] Training loss: 2.7569563804007695e-05 / Validation loss: 0.0002747390446153563 / Long term Validation loss: 0.1369\n",
      "Epoch: [4263/10000] Training loss: 2.755366348953105e-05 / Validation loss: 0.00027502546902802755 / Long term Validation loss: 0.1371\n",
      "Epoch: [4264/10000] Training loss: 2.7547799506413194e-05 / Validation loss: 0.00027348804679901053 / Long term Validation loss: 0.1362\n",
      "Epoch: [4265/10000] Training loss: 2.754411685699879e-05 / Validation loss: 0.00027498073674346157 / Long term Validation loss: 0.1372\n",
      "Epoch: [4266/10000] Training loss: 2.7532131391965555e-05 / Validation loss: 0.0002741770598033308 / Long term Validation loss: 0.1368\n",
      "Epoch: [4267/10000] Training loss: 2.7519471403027106e-05 / Validation loss: 0.00027362671263516747 / Long term Validation loss: 0.1365\n",
      "Epoch: [4268/10000] Training loss: 2.7513681895140348e-05 / Validation loss: 0.0002748353241071023 / Long term Validation loss: 0.1373\n",
      "Epoch: [4269/10000] Training loss: 2.7507742466227696e-05 / Validation loss: 0.000273527598923037 / Long term Validation loss: 0.1366\n",
      "Epoch: [4270/10000] Training loss: 2.7496321492688224e-05 / Validation loss: 0.00027390703013587133 / Long term Validation loss: 0.1368\n",
      "Epoch: [4271/10000] Training loss: 2.748562934989589e-05 / Validation loss: 0.00027435079092352635 / Long term Validation loss: 0.1372\n",
      "Epoch: [4272/10000] Training loss: 2.7479350752447204e-05 / Validation loss: 0.00027319776292161844 / Long term Validation loss: 0.1365\n",
      "Epoch: [4273/10000] Training loss: 2.747212689249926e-05 / Validation loss: 0.0002740771662231656 / Long term Validation loss: 0.1372\n",
      "Epoch: [4274/10000] Training loss: 2.7461453339550673e-05 / Validation loss: 0.00027372030660332696 / Long term Validation loss: 0.1370\n",
      "Epoch: [4275/10000] Training loss: 2.7451856684429947e-05 / Validation loss: 0.00027315844948712153 / Long term Validation loss: 0.1367\n",
      "Epoch: [4276/10000] Training loss: 2.7445001699816542e-05 / Validation loss: 0.000273988146511735 / Long term Validation loss: 0.1373\n",
      "Epoch: [4277/10000] Training loss: 2.743711295101867e-05 / Validation loss: 0.00027316777140938705 / Long term Validation loss: 0.1368\n",
      "Epoch: [4278/10000] Training loss: 2.742709932599318e-05 / Validation loss: 0.0002732687704821567 / Long term Validation loss: 0.1369\n",
      "Epoch: [4279/10000] Training loss: 2.7418071811979337e-05 / Validation loss: 0.00027364174752728343 / Long term Validation loss: 0.1372\n",
      "Epoch: [4280/10000] Training loss: 2.741073041845554e-05 / Validation loss: 0.0002728281035565244 / Long term Validation loss: 0.1368\n",
      "Epoch: [4281/10000] Training loss: 2.7402527769033686e-05 / Validation loss: 0.0002733408906486803 / Long term Validation loss: 0.1372\n",
      "Epoch: [4282/10000] Training loss: 2.7393016510478135e-05 / Validation loss: 0.0002731567737084697 / Long term Validation loss: 0.1371\n",
      "Epoch: [4283/10000] Training loss: 2.738426762182796e-05 / Validation loss: 0.0002727036245717078 / Long term Validation loss: 0.1369\n",
      "Epoch: [4284/10000] Training loss: 2.7376565731751267e-05 / Validation loss: 0.0002732376719083797 / Long term Validation loss: 0.1373\n",
      "Epoch: [4285/10000] Training loss: 2.7368230635991885e-05 / Validation loss: 0.00027269425297972357 / Long term Validation loss: 0.1370\n",
      "Epoch: [4286/10000] Training loss: 2.735907453714089e-05 / Validation loss: 0.000272698045192452 / Long term Validation loss: 0.1371\n",
      "Epoch: [4287/10000] Training loss: 2.7350455323108997e-05 / Validation loss: 0.00027294186188467514 / Long term Validation loss: 0.1373\n",
      "Epoch: [4288/10000] Training loss: 2.734250274522476e-05 / Validation loss: 0.00027237285234348724 / Long term Validation loss: 0.1370\n",
      "Epoch: [4289/10000] Training loss: 2.7334119785989043e-05 / Validation loss: 0.00027267841527180213 / Long term Validation loss: 0.1372\n",
      "Epoch: [4290/10000] Training loss: 2.7325206154855543e-05 / Validation loss: 0.00027254043508833715 / Long term Validation loss: 0.1372\n",
      "Epoch: [4291/10000] Training loss: 2.7316645282437083e-05 / Validation loss: 0.0002722122268569577 / Long term Validation loss: 0.1371\n",
      "Epoch: [4292/10000] Training loss: 2.7308522944600888e-05 / Validation loss: 0.00027254121564492494 / Long term Validation loss: 0.1373\n",
      "Epoch: [4293/10000] Training loss: 2.7300127323418593e-05 / Validation loss: 0.00027215310182385254 / Long term Validation loss: 0.1371\n",
      "Epoch: [4294/10000] Training loss: 2.729137668373735e-05 / Validation loss: 0.0002721404346043643 / Long term Validation loss: 0.1372\n",
      "Epoch: [4295/10000] Training loss: 2.7282842770173314e-05 / Validation loss: 0.0002722653458802045 / Long term Validation loss: 0.1373\n",
      "Epoch: [4296/10000] Training loss: 2.7274606581866116e-05 / Validation loss: 0.00027186531780393875 / Long term Validation loss: 0.1371\n",
      "Epoch: [4297/10000] Training loss: 2.7266209553351206e-05 / Validation loss: 0.00027205367195623465 / Long term Validation loss: 0.1373\n",
      "Epoch: [4298/10000] Training loss: 2.725756868299165e-05 / Validation loss: 0.00027191456706987245 / Long term Validation loss: 0.1373\n",
      "Epoch: [4299/10000] Training loss: 2.724904948758861e-05 / Validation loss: 0.00027169196104705613 / Long term Validation loss: 0.1372\n",
      "Epoch: [4300/10000] Training loss: 2.724073660101053e-05 / Validation loss: 0.00027187979853645727 / Long term Validation loss: 0.1374\n",
      "Epoch: [4301/10000] Training loss: 2.723233976548606e-05 / Validation loss: 0.0002715828149582459 / Long term Validation loss: 0.1373\n",
      "Epoch: [4302/10000] Training loss: 2.7223773135395958e-05 / Validation loss: 0.0002715793278981136 / Long term Validation loss: 0.1374\n",
      "Epoch: [4303/10000] Training loss: 2.7215264981333394e-05 / Validation loss: 0.00027161126749334795 / Long term Validation loss: 0.1374\n",
      "Epoch: [4304/10000] Training loss: 2.7206900285607443e-05 / Validation loss: 0.0002713304958388251 / Long term Validation loss: 0.1373\n",
      "Epoch: [4305/10000] Training loss: 2.719850142672968e-05 / Validation loss: 0.0002714473733745534 / Long term Validation loss: 0.1375\n",
      "Epoch: [4306/10000] Training loss: 2.718998538653825e-05 / Validation loss: 0.0002712985422806373 / Long term Validation loss: 0.1374\n",
      "Epoch: [4307/10000] Training loss: 2.718148781773976e-05 / Validation loss: 0.0002711574147024616 / Long term Validation loss: 0.1374\n",
      "Epoch: [4308/10000] Training loss: 2.7173088112784944e-05 / Validation loss: 0.000271245591345345 / Long term Validation loss: 0.1375\n",
      "Epoch: [4309/10000] Training loss: 2.7164684596211866e-05 / Validation loss: 0.0002710102472861279 / Long term Validation loss: 0.1374\n",
      "Epoch: [4310/10000] Training loss: 2.715620265874322e-05 / Validation loss: 0.0002710162724832326 / Long term Validation loss: 0.1375\n",
      "Epoch: [4311/10000] Training loss: 2.7147716097483007e-05 / Validation loss: 0.00027097993883341614 / Long term Validation loss: 0.1376\n",
      "Epoch: [4312/10000] Training loss: 2.7139293271642704e-05 / Validation loss: 0.00027078553394665 / Long term Validation loss: 0.1375\n",
      "Epoch: [4313/10000] Training loss: 2.7130882886434615e-05 / Validation loss: 0.00027084955257070586 / Long term Validation loss: 0.1376\n",
      "Epoch: [4314/10000] Training loss: 2.7122423408231784e-05 / Validation loss: 0.0002706965053674557 / Long term Validation loss: 0.1376\n",
      "Epoch: [4315/10000] Training loss: 2.7113947964508313e-05 / Validation loss: 0.0002706132339035436 / Long term Validation loss: 0.1376\n",
      "Epoch: [4316/10000] Training loss: 2.710551065535531e-05 / Validation loss: 0.000270628405024701 / Long term Validation loss: 0.1376\n",
      "Epoch: [4317/10000] Training loss: 2.7097092384741728e-05 / Validation loss: 0.00027044313454343616 / Long term Validation loss: 0.1376\n",
      "Epoch: [4318/10000] Training loss: 2.7088646708248638e-05 / Validation loss: 0.0002704497539907555 / Long term Validation loss: 0.1377\n",
      "Epoch: [4319/10000] Training loss: 2.7080181905963056e-05 / Validation loss: 0.00027036807170626755 / Long term Validation loss: 0.1377\n",
      "Epoch: [4320/10000] Training loss: 2.7071736587263096e-05 / Validation loss: 0.0002702376263078503 / Long term Validation loss: 0.1376\n",
      "Epoch: [4321/10000] Training loss: 2.7063310437831455e-05 / Validation loss: 0.00027025563778876045 / Long term Validation loss: 0.1377\n",
      "Epoch: [4322/10000] Training loss: 2.7054872282886074e-05 / Validation loss: 0.00027010876308697395 / Long term Validation loss: 0.1377\n",
      "Epoch: [4323/10000] Training loss: 2.704641683664208e-05 / Validation loss: 0.0002700607372177826 / Long term Validation loss: 0.1377\n",
      "Epoch: [4324/10000] Training loss: 2.703796826990582e-05 / Validation loss: 0.00027002191960534414 / Long term Validation loss: 0.1378\n",
      "Epoch: [4325/10000] Training loss: 2.70295352868896e-05 / Validation loss: 0.00026988108912850967 / Long term Validation loss: 0.1377\n",
      "Epoch: [4326/10000] Training loss: 2.7021100094534275e-05 / Validation loss: 0.0002698764463783441 / Long term Validation loss: 0.1378\n",
      "Epoch: [4327/10000] Training loss: 2.701265216306308e-05 / Validation loss: 0.0002697705636314738 / Long term Validation loss: 0.1378\n",
      "Epoch: [4328/10000] Training loss: 2.7004203607033368e-05 / Validation loss: 0.0002696856796438066 / Long term Validation loss: 0.1378\n",
      "Epoch: [4329/10000] Training loss: 2.6995765464159958e-05 / Validation loss: 0.0002696622304181708 / Long term Validation loss: 0.1379\n",
      "Epoch: [4330/10000] Training loss: 2.698733032530092e-05 / Validation loss: 0.00026953239831724134 / Long term Validation loss: 0.1378\n",
      "Epoch: [4331/10000] Training loss: 2.697888764509243e-05 / Validation loss: 0.0002694989739622654 / Long term Validation loss: 0.1379\n",
      "Epoch: [4332/10000] Training loss: 2.6970441014209886e-05 / Validation loss: 0.0002694238149744897 / Long term Validation loss: 0.1379\n",
      "Epoch: [4333/10000] Training loss: 2.696199970510425e-05 / Validation loss: 0.00026932153256333023 / Long term Validation loss: 0.1379\n",
      "Epoch: [4334/10000] Training loss: 2.695356304800548e-05 / Validation loss: 0.0002692957867767523 / Long term Validation loss: 0.1379\n",
      "Epoch: [4335/10000] Training loss: 2.694512340778138e-05 / Validation loss: 0.0002691847050475391 / Long term Validation loss: 0.1379\n",
      "Epoch: [4336/10000] Training loss: 2.6936679386416985e-05 / Validation loss: 0.00026912717739853214 / Long term Validation loss: 0.1380\n",
      "Epoch: [4337/10000] Training loss: 2.692823679056501e-05 / Validation loss: 0.0002690695043827004 / Long term Validation loss: 0.1380\n",
      "Epoch: [4338/10000] Training loss: 2.691979822948655e-05 / Validation loss: 0.00026896373868727204 / Long term Validation loss: 0.1380\n",
      "Epoch: [4339/10000] Training loss: 2.6911359728667464e-05 / Validation loss: 0.0002689274686275439 / Long term Validation loss: 0.1380\n",
      "Epoch: [4340/10000] Training loss: 2.6902918108091964e-05 / Validation loss: 0.00026883416467086014 / Long term Validation loss: 0.1380\n",
      "Epoch: [4341/10000] Training loss: 2.689447564934568e-05 / Validation loss: 0.0002687607654052966 / Long term Validation loss: 0.1380\n",
      "Epoch: [4342/10000] Training loss: 2.68860355763715e-05 / Validation loss: 0.0002687100220293364 / Long term Validation loss: 0.1381\n",
      "Epoch: [4343/10000] Training loss: 2.687759697168431e-05 / Validation loss: 0.0002686085468932455 / Long term Validation loss: 0.1381\n",
      "Epoch: [4344/10000] Training loss: 2.686915696311444e-05 / Validation loss: 0.00026856041184742285 / Long term Validation loss: 0.1381\n",
      "Epoch: [4345/10000] Training loss: 2.686071543076486e-05 / Validation loss: 0.0002684804503122452 / Long term Validation loss: 0.1381\n",
      "Epoch: [4346/10000] Training loss: 2.6852274623094325e-05 / Validation loss: 0.0002683987939105597 / Long term Validation loss: 0.1381\n",
      "Epoch: [4347/10000] Training loss: 2.684383535327311e-05 / Validation loss: 0.0002683483114966608 / Long term Validation loss: 0.1382\n",
      "Epoch: [4348/10000] Training loss: 2.6835396065029833e-05 / Validation loss: 0.00026825381411187176 / Long term Validation loss: 0.1382\n",
      "Epoch: [4349/10000] Training loss: 2.6826955586489698e-05 / Validation loss: 0.0002681956151916703 / Long term Validation loss: 0.1382\n",
      "Epoch: [4350/10000] Training loss: 2.6818514791620024e-05 / Validation loss: 0.0002681239482908571 / Long term Validation loss: 0.1382\n",
      "Epoch: [4351/10000] Training loss: 2.6810074899107626e-05 / Validation loss: 0.0002680393800040724 / Long term Validation loss: 0.1382\n",
      "Epoch: [4352/10000] Training loss: 2.680163563062277e-05 / Validation loss: 0.0002679859935880529 / Long term Validation loss: 0.1382\n",
      "Epoch: [4353/10000] Training loss: 2.6793195907339615e-05 / Validation loss: 0.00026789846148249195 / Long term Validation loss: 0.1382\n",
      "Epoch: [4354/10000] Training loss: 2.678475555749403e-05 / Validation loss: 0.00026783322514709035 / Long term Validation loss: 0.1383\n",
      "Epoch: [4355/10000] Training loss: 2.6776315396387032e-05 / Validation loss: 0.0002677659735547613 / Long term Validation loss: 0.1383\n",
      "Epoch: [4356/10000] Training loss: 2.67678758486416e-05 / Validation loss: 0.0002676816811602021 / Long term Validation loss: 0.1383\n",
      "Epoch: [4357/10000] Training loss: 2.6759436419352173e-05 / Validation loss: 0.0002676244793387447 / Long term Validation loss: 0.1383\n",
      "Epoch: [4358/10000] Training loss: 2.675099658116774e-05 / Validation loss: 0.00026754255590879015 / Long term Validation loss: 0.1383\n",
      "Epoch: [4359/10000] Training loss: 2.6742556517369706e-05 / Validation loss: 0.0002674729325987315 / Long term Validation loss: 0.1384\n",
      "Epoch: [4360/10000] Training loss: 2.6734116731764136e-05 / Validation loss: 0.00026740737814108983 / Long term Validation loss: 0.1384\n",
      "Epoch: [4361/10000] Training loss: 2.6725677268785893e-05 / Validation loss: 0.0002673247511203741 / Long term Validation loss: 0.1384\n",
      "Epoch: [4362/10000] Training loss: 2.671723774101282e-05 / Validation loss: 0.0002672639733053257 / Long term Validation loss: 0.1384\n",
      "Epoch: [4363/10000] Training loss: 2.6708797957312716e-05 / Validation loss: 0.000267186090687007 / Long term Validation loss: 0.1384\n",
      "Epoch: [4364/10000] Training loss: 2.670035813996175e-05 / Validation loss: 0.0002671141699642252 / Long term Validation loss: 0.1384\n",
      "Epoch: [4365/10000] Training loss: 2.6691918542609672e-05 / Validation loss: 0.00026704885986888707 / Long term Validation loss: 0.1385\n",
      "Epoch: [4366/10000] Training loss: 2.6683479089845485e-05 / Validation loss: 0.00026696837504376755 / Long term Validation loss: 0.1385\n",
      "Epoch: [4367/10000] Training loss: 2.6675039543454914e-05 / Validation loss: 0.0002669047838283568 / Long term Validation loss: 0.1385\n",
      "Epoch: [4368/10000] Training loss: 2.6666599862886003e-05 / Validation loss: 0.0002668295758941488 / Long term Validation loss: 0.1385\n",
      "Epoch: [4369/10000] Training loss: 2.6658160213670643e-05 / Validation loss: 0.0002667566826395187 / Long term Validation loss: 0.1385\n",
      "Epoch: [4370/10000] Training loss: 2.6649720708007042e-05 / Validation loss: 0.000266690843285708 / Long term Validation loss: 0.1386\n",
      "Epoch: [4371/10000] Training loss: 2.6641281261123222e-05 / Validation loss: 0.0002666123475896303 / Long term Validation loss: 0.1386\n",
      "Epoch: [4372/10000] Training loss: 2.663284174139105e-05 / Validation loss: 0.00026654668427472607 / Long term Validation loss: 0.1386\n",
      "Epoch: [4373/10000] Training loss: 2.6624402158477304e-05 / Validation loss: 0.0002664731061564702 / Long term Validation loss: 0.1386\n",
      "Epoch: [4374/10000] Training loss: 2.6615962616641927e-05 / Validation loss: 0.00026640003790789327 / Long term Validation loss: 0.1386\n",
      "Epoch: [4375/10000] Training loss: 2.66075231620641e-05 / Validation loss: 0.00026633342662438057 / Long term Validation loss: 0.1386\n",
      "Epoch: [4376/10000] Training loss: 2.659908373166964e-05 / Validation loss: 0.0002662566439801154 / Long term Validation loss: 0.1386\n",
      "Epoch: [4377/10000] Training loss: 2.6590644255073106e-05 / Validation loss: 0.0002661895983792198 / Long term Validation loss: 0.1387\n",
      "Epoch: [4378/10000] Training loss: 2.6582204750774685e-05 / Validation loss: 0.0002661169779002695 / Long term Validation loss: 0.1387\n",
      "Epoch: [4379/10000] Training loss: 2.657376528002297e-05 / Validation loss: 0.00026604415794795924 / Long term Validation loss: 0.1387\n",
      "Epoch: [4380/10000] Training loss: 2.65653258622306e-05 / Validation loss: 0.0002659767409555802 / Long term Validation loss: 0.1387\n",
      "Epoch: [4381/10000] Training loss: 2.6556886457155545e-05 / Validation loss: 0.00026590131275482207 / Long term Validation loss: 0.1387\n",
      "Epoch: [4382/10000] Training loss: 2.654844702630892e-05 / Validation loss: 0.0002658333516729987 / Long term Validation loss: 0.1388\n",
      "Epoch: [4383/10000] Training loss: 2.6540007584207903e-05 / Validation loss: 0.0002657612382515999 / Long term Validation loss: 0.1388\n",
      "Epoch: [4384/10000] Training loss: 2.653156816602564e-05 / Validation loss: 0.00026568884694512464 / Long term Validation loss: 0.1388\n",
      "Epoch: [4385/10000] Training loss: 2.652312878155542e-05 / Validation loss: 0.0002656207079076635 / Long term Validation loss: 0.1388\n",
      "Epoch: [4386/10000] Training loss: 2.6514689407226734e-05 / Validation loss: 0.00026554635606720466 / Long term Validation loss: 0.1388\n",
      "Epoch: [4387/10000] Training loss: 2.6506250020761913e-05 / Validation loss: 0.00026547783115673894 / Long term Validation loss: 0.1388\n",
      "Epoch: [4388/10000] Training loss: 2.6497810630796884e-05 / Validation loss: 0.0002654059927140005 / Long term Validation loss: 0.1389\n",
      "Epoch: [4389/10000] Training loss: 2.6489371257457726e-05 / Validation loss: 0.0002653341013438959 / Long term Validation loss: 0.1389\n",
      "Epoch: [4390/10000] Training loss: 2.648093190737419e-05 / Validation loss: 0.0002652653546509393 / Long term Validation loss: 0.1389\n",
      "Epoch: [4391/10000] Training loss: 2.6472492567746854e-05 / Validation loss: 0.00026519185559552934 / Long term Validation loss: 0.1389\n",
      "Epoch: [4392/10000] Training loss: 2.64640532249158e-05 / Validation loss: 0.0002651229717621423 / Long term Validation loss: 0.1389\n",
      "Epoch: [4393/10000] Training loss: 2.6455613882910095e-05 / Validation loss: 0.0002650512746282838 / Long term Validation loss: 0.1389\n",
      "Epoch: [4394/10000] Training loss: 2.64471745531216e-05 / Validation loss: 0.0002649798724988002 / Long term Validation loss: 0.1390\n",
      "Epoch: [4395/10000] Training loss: 2.643873524100883e-05 / Validation loss: 0.0002649106262023695 / Long term Validation loss: 0.1390\n",
      "Epoch: [4396/10000] Training loss: 2.643029594038454e-05 / Validation loss: 0.0002648378305701492 / Long term Validation loss: 0.1390\n",
      "Epoch: [4397/10000] Training loss: 2.6421856642680328e-05 / Validation loss: 0.00026476871888980296 / Long term Validation loss: 0.1390\n",
      "Epoch: [4398/10000] Training loss: 2.6413417349034132e-05 / Validation loss: 0.00026469712365569976 / Long term Validation loss: 0.1390\n",
      "Epoch: [4399/10000] Training loss: 2.6404978065602992e-05 / Validation loss: 0.00026462617875347395 / Long term Validation loss: 0.1390\n",
      "Epoch: [4400/10000] Training loss: 2.639653879704851e-05 / Validation loss: 0.00026455653182634424 / Long term Validation loss: 0.1391\n",
      "Epoch: [4401/10000] Training loss: 2.638809954110089e-05 / Validation loss: 0.0002644843383651233 / Long term Validation loss: 0.1391\n",
      "Epoch: [4402/10000] Training loss: 2.6379660292598304e-05 / Validation loss: 0.0002644150598539277 / Long term Validation loss: 0.1391\n",
      "Epoch: [4403/10000] Training loss: 2.6371221051203975e-05 / Validation loss: 0.00026434356514929065 / Long term Validation loss: 0.1391\n",
      "Epoch: [4404/10000] Training loss: 2.6362781819866465e-05 / Validation loss: 0.00026427301936548375 / Long term Validation loss: 0.1391\n",
      "Epoch: [4405/10000] Training loss: 2.635434260236329e-05 / Validation loss: 0.0002642030549492846 / Long term Validation loss: 0.1391\n",
      "Epoch: [4406/10000] Training loss: 2.6345903398590462e-05 / Validation loss: 0.00026413139871559133 / Long term Validation loss: 0.1392\n",
      "Epoch: [4407/10000] Training loss: 2.6337464205853405e-05 / Validation loss: 0.00026406198116422116 / Long term Validation loss: 0.1392\n",
      "Epoch: [4408/10000] Training loss: 2.6329025023349507e-05 / Validation loss: 0.0002639906219175535 / Long term Validation loss: 0.1392\n",
      "Epoch: [4409/10000] Training loss: 2.632058585212639e-05 / Validation loss: 0.0002639204105156798 / Long term Validation loss: 0.1392\n",
      "Epoch: [4410/10000] Training loss: 2.631214669497622e-05 / Validation loss: 0.0002638502065753353 / Long term Validation loss: 0.1392\n",
      "Epoch: [4411/10000] Training loss: 2.6303707552795236e-05 / Validation loss: 0.0002637790377459991 / Long term Validation loss: 0.1392\n",
      "Epoch: [4412/10000] Training loss: 2.629526842467103e-05 / Validation loss: 0.0002637094823091664 / Long term Validation loss: 0.1393\n",
      "Epoch: [4413/10000] Training loss: 2.628682930996768e-05 / Validation loss: 0.0002636383032836221 / Long term Validation loss: 0.1393\n",
      "Epoch: [4414/10000] Training loss: 2.6278390208772993e-05 / Validation loss: 0.0002635683480943722 / Long term Validation loss: 0.1393\n",
      "Epoch: [4415/10000] Training loss: 2.626995112293512e-05 / Validation loss: 0.0002634979812906351 / Long term Validation loss: 0.1393\n",
      "Epoch: [4416/10000] Training loss: 2.6261512053592847e-05 / Validation loss: 0.0002634272551581199 / Long term Validation loss: 0.1393\n",
      "Epoch: [4417/10000] Training loss: 2.625307300100102e-05 / Validation loss: 0.00026335756120285977 / Long term Validation loss: 0.1393\n",
      "Epoch: [4418/10000] Training loss: 2.6244633964988692e-05 / Validation loss: 0.0002632866165805204 / Long term Validation loss: 0.1394\n",
      "Epoch: [4419/10000] Training loss: 2.6236194945378292e-05 / Validation loss: 0.000263216837656605 / Long term Validation loss: 0.1394\n",
      "Epoch: [4420/10000] Training loss: 2.6227755943282385e-05 / Validation loss: 0.0002631463908884061 / Long term Validation loss: 0.1394\n",
      "Epoch: [4421/10000] Training loss: 2.621931695966667e-05 / Validation loss: 0.0002630760532447534 / Long term Validation loss: 0.1394\n",
      "Epoch: [4422/10000] Training loss: 2.621087799539277e-05 / Validation loss: 0.00026300622356535355 / Long term Validation loss: 0.1394\n",
      "Epoch: [4423/10000] Training loss: 2.620243905077221e-05 / Validation loss: 0.00026293555602376756 / Long term Validation loss: 0.1394\n",
      "Epoch: [4424/10000] Training loss: 2.6194000125810044e-05 / Validation loss: 0.0002628658704019804 / Long term Validation loss: 0.1395\n",
      "Epoch: [4425/10000] Training loss: 2.618556122119475e-05 / Validation loss: 0.0002627954305334863 / Long term Validation loss: 0.1395\n",
      "Epoch: [4426/10000] Training loss: 2.6177122337604145e-05 / Validation loss: 0.00026272541402901816 / Long term Validation loss: 0.1395\n",
      "Epoch: [4427/10000] Training loss: 2.6168683476066356e-05 / Validation loss: 0.0002626554727718472 / Long term Validation loss: 0.1395\n",
      "Epoch: [4428/10000] Training loss: 2.616024463720828e-05 / Validation loss: 0.00026258511109460386 / Long term Validation loss: 0.1395\n",
      "Epoch: [4429/10000] Training loss: 2.6151805821402045e-05 / Validation loss: 0.00026251545061365284 / Long term Validation loss: 0.1395\n",
      "Epoch: [4430/10000] Training loss: 2.614336702921381e-05 / Validation loss: 0.00026244510733626746 / Long term Validation loss: 0.1395\n",
      "Epoch: [4431/10000] Training loss: 2.6134928261140535e-05 / Validation loss: 0.00026237533083296635 / Long term Validation loss: 0.1396\n",
      "Epoch: [4432/10000] Training loss: 2.612648951813008e-05 / Validation loss: 0.0002623053243462975 / Long term Validation loss: 0.1396\n",
      "Epoch: [4433/10000] Training loss: 2.611805080091038e-05 / Validation loss: 0.000262235266095921 / Long term Validation loss: 0.1396\n",
      "Epoch: [4434/10000] Training loss: 2.6109612110175928e-05 / Validation loss: 0.00026216558247655047 / Long term Validation loss: 0.1396\n",
      "Epoch: [4435/10000] Training loss: 2.6101173446553682e-05 / Validation loss: 0.00026209541246415897 / Long term Validation loss: 0.1396\n",
      "Epoch: [4436/10000] Training loss: 2.6092734810551053e-05 / Validation loss: 0.000262025788160295 / Long term Validation loss: 0.1396\n",
      "Epoch: [4437/10000] Training loss: 2.6084296202988433e-05 / Validation loss: 0.00026195578645223 / Long term Validation loss: 0.1397\n",
      "Epoch: [4438/10000] Training loss: 2.6075857624563692e-05 / Validation loss: 0.00026188599898215904 / Long term Validation loss: 0.1397\n",
      "Epoch: [4439/10000] Training loss: 2.606741907613196e-05 / Validation loss: 0.00026181628129537935 / Long term Validation loss: 0.1397\n",
      "Epoch: [4440/10000] Training loss: 2.6058980558413133e-05 / Validation loss: 0.0002617463383761929 / Long term Validation loss: 0.1397\n",
      "Epoch: [4441/10000] Training loss: 2.605054207205829e-05 / Validation loss: 0.0002616767887508802 / Long term Validation loss: 0.1397\n",
      "Epoch: [4442/10000] Training loss: 2.6042103617831504e-05 / Validation loss: 0.00026160687277234957 / Long term Validation loss: 0.1397\n",
      "Epoch: [4443/10000] Training loss: 2.6033665196402968e-05 / Validation loss: 0.0002615372934606969 / Long term Validation loss: 0.1398\n",
      "Epoch: [4444/10000] Training loss: 2.6025226808647488e-05 / Validation loss: 0.00026146756738682934 / Long term Validation loss: 0.1398\n",
      "Epoch: [4445/10000] Training loss: 2.601678845532396e-05 / Validation loss: 0.0002613978651411994 / Long term Validation loss: 0.1398\n",
      "Epoch: [4446/10000] Training loss: 2.6008350137233584e-05 / Validation loss: 0.0002613283386633162 / Long term Validation loss: 0.1398\n",
      "Epoch: [4447/10000] Training loss: 2.5999911855155443e-05 / Validation loss: 0.00026125858069678986 / Long term Validation loss: 0.1398\n",
      "Epoch: [4448/10000] Training loss: 2.5991473609808092e-05 / Validation loss: 0.0002611891348391586 / Long term Validation loss: 0.1398\n",
      "Epoch: [4449/10000] Training loss: 2.598303540203985e-05 / Validation loss: 0.00026111945897393387 / Long term Validation loss: 0.1399\n",
      "Epoch: [4450/10000] Training loss: 2.5974597232612437e-05 / Validation loss: 0.00026104997091483173 / Long term Validation loss: 0.1399\n",
      "Epoch: [4451/10000] Training loss: 2.596615910240245e-05 / Validation loss: 0.0002609804586648566 / Long term Validation loss: 0.1399\n",
      "Epoch: [4452/10000] Training loss: 2.59577210122167e-05 / Validation loss: 0.0002609109012607764 / Long term Validation loss: 0.1399\n",
      "Epoch: [4453/10000] Training loss: 2.5949282962869615e-05 / Validation loss: 0.00026084152505344895 / Long term Validation loss: 0.1399\n",
      "Epoch: [4454/10000] Training loss: 2.5940844955202796e-05 / Validation loss: 0.0002607719676830711 / Long term Validation loss: 0.1399\n",
      "Epoch: [4455/10000] Training loss: 2.593240699000455e-05 / Validation loss: 0.00026070263596691685 / Long term Validation loss: 0.1400\n",
      "Epoch: [4456/10000] Training loss: 2.5923969068163353e-05 / Validation loss: 0.0002606331699378312 / Long term Validation loss: 0.1400\n",
      "Epoch: [4457/10000] Training loss: 2.5915531190499507e-05 / Validation loss: 0.000260563811236247 / Long term Validation loss: 0.1400\n",
      "Epoch: [4458/10000] Training loss: 2.5907093357900906e-05 / Validation loss: 0.00026049447558109236 / Long term Validation loss: 0.1400\n",
      "Epoch: [4459/10000] Training loss: 2.5898655571220825e-05 / Validation loss: 0.0002604250878621725 / Long term Validation loss: 0.1400\n",
      "Epoch: [4460/10000] Training loss: 2.589021783131083e-05 / Validation loss: 0.0002603558515402324 / Long term Validation loss: 0.1401\n",
      "Epoch: [4461/10000] Training loss: 2.5881780139057818e-05 / Validation loss: 0.00026028648854048696 / Long term Validation loss: 0.1401\n",
      "Epoch: [4462/10000] Training loss: 2.587334249530557e-05 / Validation loss: 0.0002602172883710849 / Long term Validation loss: 0.1401\n",
      "Epoch: [4463/10000] Training loss: 2.586490490096877e-05 / Validation loss: 0.0002601480085271029 / Long term Validation loss: 0.1401\n",
      "Epoch: [4464/10000] Training loss: 2.585646735691341e-05 / Validation loss: 0.00026007880145641015 / Long term Validation loss: 0.1401\n",
      "Epoch: [4465/10000] Training loss: 2.5848029864050276e-05 / Validation loss: 0.0002600096254611206 / Long term Validation loss: 0.1401\n",
      "Epoch: [4466/10000] Training loss: 2.583959242327219e-05 / Validation loss: 0.0002599404143118731 / Long term Validation loss: 0.1402\n",
      "Epoch: [4467/10000] Training loss: 2.5831155035470502e-05 / Validation loss: 0.0002598713190289379 / Long term Validation loss: 0.1402\n",
      "Epoch: [4468/10000] Training loss: 2.5822717701565138e-05 / Validation loss: 0.0002598021403940722 / Long term Validation loss: 0.1402\n",
      "Epoch: [4469/10000] Training loss: 2.5814280422444533e-05 / Validation loss: 0.00025973308418844566 / Long term Validation loss: 0.1402\n",
      "Epoch: [4470/10000] Training loss: 2.5805843199047163e-05 / Validation loss: 0.00025966397632031673 / Long term Validation loss: 0.1402\n",
      "Epoch: [4471/10000] Training loss: 2.579740603227679e-05 / Validation loss: 0.00025959493075913614 / Long term Validation loss: 0.1403\n",
      "Epoch: [4472/10000] Training loss: 2.5788968923071138e-05 / Validation loss: 0.00025952590804606014 / Long term Validation loss: 0.1403\n",
      "Epoch: [4473/10000] Training loss: 2.578053187235527e-05 / Validation loss: 0.0002594568733925553 / Long term Validation loss: 0.1403\n",
      "Epoch: [4474/10000] Training loss: 2.5772094881056977e-05 / Validation loss: 0.0002593879224405235 / Long term Validation loss: 0.1403\n",
      "Epoch: [4475/10000] Training loss: 2.576365795012263e-05 / Validation loss: 0.00025931892105506254 / Long term Validation loss: 0.1403\n",
      "Epoch: [4476/10000] Training loss: 2.5755221080478146e-05 / Validation loss: 0.0002592500152944832 / Long term Validation loss: 0.1404\n",
      "Epoch: [4477/10000] Training loss: 2.574678427308458e-05 / Validation loss: 0.0002591810727952189 / Long term Validation loss: 0.1404\n",
      "Epoch: [4478/10000] Training loss: 2.5738347528877776e-05 / Validation loss: 0.00025911219166320867 / Long term Validation loss: 0.1404\n",
      "Epoch: [4479/10000] Training loss: 2.572991084882174e-05 / Validation loss: 0.0002590433206696175 / Long term Validation loss: 0.1404\n",
      "Epoch: [4480/10000] Training loss: 2.5721474233868243e-05 / Validation loss: 0.00025897446047378495 / Long term Validation loss: 0.1405\n",
      "Epoch: [4481/10000] Training loss: 2.5713037684977256e-05 / Validation loss: 0.00025890565608619294 / Long term Validation loss: 0.1405\n",
      "Epoch: [4482/10000] Training loss: 2.5704601203117322e-05 / Validation loss: 0.0002588368282849381 / Long term Validation loss: 0.1405\n",
      "Epoch: [4483/10000] Training loss: 2.5696164789246928e-05 / Validation loss: 0.0002587680748767962 / Long term Validation loss: 0.1405\n",
      "Epoch: [4484/10000] Training loss: 2.568772844434746e-05 / Validation loss: 0.00025869929611318604 / Long term Validation loss: 0.1406\n",
      "Epoch: [4485/10000] Training loss: 2.5679292169382982e-05 / Validation loss: 0.00025863057851483265 / Long term Validation loss: 0.1406\n",
      "Epoch: [4486/10000] Training loss: 2.5670855965340792e-05 / Validation loss: 0.00025856186023811455 / Long term Validation loss: 0.1406\n",
      "Epoch: [4487/10000] Training loss: 2.566241983319577e-05 / Validation loss: 0.0002584931718568064 / Long term Validation loss: 0.1406\n",
      "Epoch: [4488/10000] Training loss: 2.565398377393545e-05 / Validation loss: 0.00025842451536081294 / Long term Validation loss: 0.1407\n",
      "Epoch: [4489/10000] Training loss: 2.5645547788547517e-05 / Validation loss: 0.0002583558596392513 / Long term Validation loss: 0.1407\n",
      "Epoch: [4490/10000] Training loss: 2.5637111878018875e-05 / Validation loss: 0.00025828725772039077 / Long term Validation loss: 0.1407\n",
      "Epoch: [4491/10000] Training loss: 2.562867604334865e-05 / Validation loss: 0.00025821864399516915 / Long term Validation loss: 0.1407\n",
      "Epoch: [4492/10000] Training loss: 2.562024028552629e-05 / Validation loss: 0.000258150086562562 / Long term Validation loss: 0.1408\n",
      "Epoch: [4493/10000] Training loss: 2.561180460555852e-05 / Validation loss: 0.00025808152401423506 / Long term Validation loss: 0.1408\n",
      "Epoch: [4494/10000] Training loss: 2.5603369004441226e-05 / Validation loss: 0.0002580130037068822 / Long term Validation loss: 0.1408\n",
      "Epoch: [4495/10000] Training loss: 2.559493348318456e-05 / Validation loss: 0.0002579444969589718 / Long term Validation loss: 0.1408\n",
      "Epoch: [4496/10000] Training loss: 2.55864980427934e-05 / Validation loss: 0.0002578760119801405 / Long term Validation loss: 0.1409\n",
      "Epoch: [4497/10000] Training loss: 2.5578062684278908e-05 / Validation loss: 0.0002578075600108343 / Long term Validation loss: 0.1409\n",
      "Epoch: [4498/10000] Training loss: 2.556962740865535e-05 / Validation loss: 0.000257739113596658 / Long term Validation loss: 0.1409\n",
      "Epoch: [4499/10000] Training loss: 2.5561192216935245e-05 / Validation loss: 0.0002576707115316747 / Long term Validation loss: 0.1409\n",
      "Epoch: [4500/10000] Training loss: 2.5552757110140808e-05 / Validation loss: 0.00025760230926796167 / Long term Validation loss: 0.1409\n",
      "Epoch: [4501/10000] Training loss: 2.5544322089287775e-05 / Validation loss: 0.0002575339514157334 / Long term Validation loss: 0.1409\n",
      "Epoch: [4502/10000] Training loss: 2.553588715540382e-05 / Validation loss: 0.000257465598247207 / Long term Validation loss: 0.1410\n",
      "Epoch: [4503/10000] Training loss: 2.5527452309509984e-05 / Validation loss: 0.0002573972806786183 / Long term Validation loss: 0.1410\n",
      "Epoch: [4504/10000] Training loss: 2.5519017552637065e-05 / Validation loss: 0.0002573289790102425 / Long term Validation loss: 0.1410\n",
      "Epoch: [4505/10000] Training loss: 2.5510582885812836e-05 / Validation loss: 0.00025726070070672494 / Long term Validation loss: 0.1410\n",
      "Epoch: [4506/10000] Training loss: 2.5502148310069816e-05 / Validation loss: 0.0002571924500671792 / Long term Validation loss: 0.1410\n",
      "Epoch: [4507/10000] Training loss: 2.5493713826442608e-05 / Validation loss: 0.00025712421257691144 / Long term Validation loss: 0.1410\n",
      "Epoch: [4508/10000] Training loss: 2.548527943596562e-05 / Validation loss: 0.00025705601050369744 / Long term Validation loss: 0.1410\n",
      "Epoch: [4509/10000] Training loss: 2.547684513967934e-05 / Validation loss: 0.00025698781669022155 / Long term Validation loss: 0.1410\n",
      "Epoch: [4510/10000] Training loss: 2.5468410938620925e-05 / Validation loss: 0.00025691966011684165 / Long term Validation loss: 0.1410\n",
      "Epoch: [4511/10000] Training loss: 2.5459976833835447e-05 / Validation loss: 0.0002568515127691293 / Long term Validation loss: 0.1411\n",
      "Epoch: [4512/10000] Training loss: 2.545154282636377e-05 / Validation loss: 0.0002567833992400645 / Long term Validation loss: 0.1411\n",
      "Epoch: [4513/10000] Training loss: 2.5443108917254197e-05 / Validation loss: 0.0002567153001144304 / Long term Validation loss: 0.1411\n",
      "Epoch: [4514/10000] Training loss: 2.543467510755213e-05 / Validation loss: 0.0002566472284407026 / Long term Validation loss: 0.1411\n",
      "Epoch: [4515/10000] Training loss: 2.5426241398308095e-05 / Validation loss: 0.00025657917794345386 / Long term Validation loss: 0.1411\n",
      "Epoch: [4516/10000] Training loss: 2.541780779057228e-05 / Validation loss: 0.000256511148240816 / Long term Validation loss: 0.1411\n",
      "Epoch: [4517/10000] Training loss: 2.5409374285397062e-05 / Validation loss: 0.0002564431456497273 / Long term Validation loss: 0.1411\n",
      "Epoch: [4518/10000] Training loss: 2.540094088383718e-05 / Validation loss: 0.0002563751589395237 / Long term Validation loss: 0.1411\n",
      "Epoch: [4519/10000] Training loss: 2.5392507586947002e-05 / Validation loss: 0.0002563072029102503 / Long term Validation loss: 0.1411\n",
      "Epoch: [4520/10000] Training loss: 2.5384074395785156e-05 / Validation loss: 0.00025623926055666686 / Long term Validation loss: 0.1412\n",
      "Epoch: [4521/10000] Training loss: 2.5375641311408316e-05 / Validation loss: 0.00025617134965961726 / Long term Validation loss: 0.1412\n",
      "Epoch: [4522/10000] Training loss: 2.5367208334878286e-05 / Validation loss: 0.0002561034528801762 / Long term Validation loss: 0.1412\n",
      "Epoch: [4523/10000] Training loss: 2.535877546725448e-05 / Validation loss: 0.0002560355859969961 / Long term Validation loss: 0.1412\n",
      "Epoch: [4524/10000] Training loss: 2.5350342709601105e-05 / Validation loss: 0.00025596773557331294 / Long term Validation loss: 0.1412\n",
      "Epoch: [4525/10000] Training loss: 2.534191006298056e-05 / Validation loss: 0.00025589991208466807 / Long term Validation loss: 0.1412\n",
      "Epoch: [4526/10000] Training loss: 2.5333477528458985e-05 / Validation loss: 0.0002558321082865923 / Long term Validation loss: 0.1412\n",
      "Epoch: [4527/10000] Training loss: 2.5325045107101806e-05 / Validation loss: 0.00025576432806722384 / Long term Validation loss: 0.1412\n",
      "Epoch: [4528/10000] Training loss: 2.5316612799976754e-05 / Validation loss: 0.0002556965707321906 / Long term Validation loss: 0.1413\n",
      "Epoch: [4529/10000] Training loss: 2.530818060815215e-05 / Validation loss: 0.0002556288340202211 / Long term Validation loss: 0.1413\n",
      "Epoch: [4530/10000] Training loss: 2.5299748532697193e-05 / Validation loss: 0.00025556112270944 / Long term Validation loss: 0.1413\n",
      "Epoch: [4531/10000] Training loss: 2.5291316574682835e-05 / Validation loss: 0.00025549342993092054 / Long term Validation loss: 0.1413\n",
      "Epoch: [4532/10000] Training loss: 2.5282884735179713e-05 / Validation loss: 0.0002554257640963311 / Long term Validation loss: 0.1413\n",
      "Epoch: [4533/10000] Training loss: 2.5274453015261018e-05 / Validation loss: 0.0002553581157089226 / Long term Validation loss: 0.1413\n",
      "Epoch: [4534/10000] Training loss: 2.5266021415998886e-05 / Validation loss: 0.0002552904948279015 / Long term Validation loss: 0.1413\n",
      "Epoch: [4535/10000] Training loss: 2.5257589938468345e-05 / Validation loss: 0.00025522289121550313 / Long term Validation loss: 0.1413\n",
      "Epoch: [4536/10000] Training loss: 2.5249158583743063e-05 / Validation loss: 0.0002551553148729435 / Long term Validation loss: 0.1414\n",
      "Epoch: [4537/10000] Training loss: 2.5240727352899585e-05 / Validation loss: 0.0002550877562946224 / Long term Validation loss: 0.1414\n",
      "Epoch: [4538/10000] Training loss: 2.523229624701306e-05 / Validation loss: 0.00025502022421181514 / Long term Validation loss: 0.1414\n",
      "Epoch: [4539/10000] Training loss: 2.5223865267161227e-05 / Validation loss: 0.0002549527107934648 / Long term Validation loss: 0.1414\n",
      "Epoch: [4540/10000] Training loss: 2.521543441442068e-05 / Validation loss: 0.0002548852228176015 / Long term Validation loss: 0.1414\n",
      "Epoch: [4541/10000] Training loss: 2.520700368987011e-05 / Validation loss: 0.0002548177545714844 / Long term Validation loss: 0.1414\n",
      "Epoch: [4542/10000] Training loss: 2.519857309458743e-05 / Validation loss: 0.0002547503106449975 / Long term Validation loss: 0.1414\n",
      "Epoch: [4543/10000] Training loss: 2.519014262965204e-05 / Validation loss: 0.0002546828875034997 / Long term Validation loss: 0.1415\n",
      "Epoch: [4544/10000] Training loss: 2.518171229614302e-05 / Validation loss: 0.0002546154876289641 / Long term Validation loss: 0.1415\n",
      "Epoch: [4545/10000] Training loss: 2.517328209514042e-05 / Validation loss: 0.00025454810948065355 / Long term Validation loss: 0.1415\n",
      "Epoch: [4546/10000] Training loss: 2.5164852027724146e-05 / Validation loss: 0.00025448075368953425 / Long term Validation loss: 0.1415\n",
      "Epoch: [4547/10000] Training loss: 2.5156422094974778e-05 / Validation loss: 0.0002544134204088788 / Long term Validation loss: 0.1415\n",
      "Epoch: [4548/10000] Training loss: 2.5147992297973003e-05 / Validation loss: 0.00025434610873691495 / Long term Validation loss: 0.1415\n",
      "Epoch: [4549/10000] Training loss: 2.5139562637799612e-05 / Validation loss: 0.00025427882020418564 / Long term Validation loss: 0.1415\n",
      "Epoch: [4550/10000] Training loss: 2.5131133115535895e-05 / Validation loss: 0.00025421155267425327 / Long term Validation loss: 0.1415\n",
      "Epoch: [4551/10000] Training loss: 2.5122703732262813e-05 / Validation loss: 0.0002541443087870627 / Long term Validation loss: 0.1416\n",
      "Epoch: [4552/10000] Training loss: 2.5114274489061933e-05 / Validation loss: 0.00025407708539944594 / Long term Validation loss: 0.1416\n",
      "Epoch: [4553/10000] Training loss: 2.510584538701431e-05 / Validation loss: 0.00025400988607926815 / Long term Validation loss: 0.1416\n",
      "Epoch: [4554/10000] Training loss: 2.5097416427201648e-05 / Validation loss: 0.00025394270680764464 / Long term Validation loss: 0.1416\n",
      "Epoch: [4555/10000] Training loss: 2.5088987610704806e-05 / Validation loss: 0.000253875552003779 / Long term Validation loss: 0.1416\n",
      "Epoch: [4556/10000] Training loss: 2.5080558938605493e-05 / Validation loss: 0.0002538084167935082 / Long term Validation loss: 0.1416\n",
      "Epoch: [4557/10000] Training loss: 2.5072130411984244e-05 / Validation loss: 0.0002537413064858528 / Long term Validation loss: 0.1416\n",
      "Epoch: [4558/10000] Training loss: 2.5063702031922538e-05 / Validation loss: 0.00025367421525114195 / Long term Validation loss: 0.1417\n",
      "Epoch: [4559/10000] Training loss: 2.5055273799500436e-05 / Validation loss: 0.00025360714945354404 / Long term Validation loss: 0.1417\n",
      "Epoch: [4560/10000] Training loss: 2.5046845715799048e-05 / Validation loss: 0.0002535401020714278 / Long term Validation loss: 0.1417\n",
      "Epoch: [4561/10000] Training loss: 2.503841778189773e-05 / Validation loss: 0.0002534730808385258 / Long term Validation loss: 0.1417\n",
      "Epoch: [4562/10000] Training loss: 2.5029989998877073e-05 / Validation loss: 0.00025340607713829225 / Long term Validation loss: 0.1417\n",
      "Epoch: [4563/10000] Training loss: 2.5021562367815476e-05 / Validation loss: 0.00025333910057923244 / Long term Validation loss: 0.1417\n",
      "Epoch: [4564/10000] Training loss: 2.5013134889792948e-05 / Validation loss: 0.0002532721403247851 / Long term Validation loss: 0.1417\n",
      "Epoch: [4565/10000] Training loss: 2.5004707565886687e-05 / Validation loss: 0.00025320520862725176 / Long term Validation loss: 0.1418\n",
      "Epoch: [4566/10000] Training loss: 2.4996280397175953e-05 / Validation loss: 0.00025313829148738135 / Long term Validation loss: 0.1418\n",
      "Epoch: [4567/10000] Training loss: 2.4987853384736466e-05 / Validation loss: 0.0002530714049566875 / Long term Validation loss: 0.1418\n",
      "Epoch: [4568/10000] Training loss: 2.4979426529646725e-05 / Validation loss: 0.00025300453045546236 / Long term Validation loss: 0.1418\n",
      "Epoch: [4569/10000] Training loss: 2.497099983298068e-05 / Validation loss: 0.0002529376895774978 / Long term Validation loss: 0.1418\n",
      "Epoch: [4570/10000] Training loss: 2.4962573295815984e-05 / Validation loss: 0.0002528708570129966 / Long term Validation loss: 0.1418\n",
      "Epoch: [4571/10000] Training loss: 2.4954146919224405e-05 / Validation loss: 0.0002528040625569541 / Long term Validation loss: 0.1418\n",
      "Epoch: [4572/10000] Training loss: 2.4945720704282876e-05 / Validation loss: 0.00025273727086887676 / Long term Validation loss: 0.1419\n",
      "Epoch: [4573/10000] Training loss: 2.493729465206042e-05 / Validation loss: 0.00025267052405665824 / Long term Validation loss: 0.1419\n",
      "Epoch: [4574/10000] Training loss: 2.4928868763633575e-05 / Validation loss: 0.0002526037716086539 / Long term Validation loss: 0.1419\n",
      "Epoch: [4575/10000] Training loss: 2.492044304006781e-05 / Validation loss: 0.0002525370743962644 / Long term Validation loss: 0.1419\n",
      "Epoch: [4576/10000] Training loss: 2.4912017482439783e-05 / Validation loss: 0.0002524703586128504 / Long term Validation loss: 0.1419\n",
      "Epoch: [4577/10000] Training loss: 2.49035920918104e-05 / Validation loss: 0.0002524037141620902 / Long term Validation loss: 0.1419\n",
      "Epoch: [4578/10000] Training loss: 2.4895166869257383e-05 / Validation loss: 0.00025233703091524074 / Long term Validation loss: 0.1420\n",
      "Epoch: [4579/10000] Training loss: 2.4886741815835493e-05 / Validation loss: 0.0002522704443941029 / Long term Validation loss: 0.1420\n",
      "Epoch: [4580/10000] Training loss: 2.4878316932625263e-05 / Validation loss: 0.00025220378695488325 / Long term Validation loss: 0.1420\n",
      "Epoch: [4581/10000] Training loss: 2.486989222067287e-05 / Validation loss: 0.00025213726691345536 / Long term Validation loss: 0.1420\n",
      "Epoch: [4582/10000] Training loss: 2.4861467681064745e-05 / Validation loss: 0.0002520706241390467 / Long term Validation loss: 0.1420\n",
      "Epoch: [4583/10000] Training loss: 2.4853043314834813e-05 / Validation loss: 0.0002520041849044978 / Long term Validation loss: 0.1420\n",
      "Epoch: [4584/10000] Training loss: 2.484461912308093e-05 / Validation loss: 0.0002519375380642259 / Long term Validation loss: 0.1420\n",
      "Epoch: [4585/10000] Training loss: 2.4836195106819363e-05 / Validation loss: 0.0002518712039604051 / Long term Validation loss: 0.1421\n",
      "Epoch: [4586/10000] Training loss: 2.4827771267169547e-05 / Validation loss: 0.0002518045211099528 / Long term Validation loss: 0.1421\n",
      "Epoch: [4587/10000] Training loss: 2.4819347605122506e-05 / Validation loss: 0.0002517383339820085 / Long term Validation loss: 0.1421\n",
      "Epoch: [4588/10000] Training loss: 2.481092412183969e-05 / Validation loss: 0.00025167155986961066 / Long term Validation loss: 0.1421\n",
      "Epoch: [4589/10000] Training loss: 2.4802500818278474e-05 / Validation loss: 0.0002516055926656205 / Long term Validation loss: 0.1421\n",
      "Epoch: [4590/10000] Training loss: 2.4794077695686138e-05 / Validation loss: 0.0002515386304025144 / Long term Validation loss: 0.1421\n",
      "Epoch: [4591/10000] Training loss: 2.478565475498682e-05 / Validation loss: 0.00025147301198487397 / Long term Validation loss: 0.1421\n",
      "Epoch: [4592/10000] Training loss: 2.4777231997617193e-05 / Validation loss: 0.00025140568935996403 / Long term Validation loss: 0.1422\n",
      "Epoch: [4593/10000] Training loss: 2.4768809424516864e-05 / Validation loss: 0.00025134065037623986 / Long term Validation loss: 0.1422\n",
      "Epoch: [4594/10000] Training loss: 2.4760387037583864e-05 / Validation loss: 0.00025127265721058215 / Long term Validation loss: 0.1422\n",
      "Epoch: [4595/10000] Training loss: 2.475196483802054e-05 / Validation loss: 0.00025120861590900094 / Long term Validation loss: 0.1422\n",
      "Epoch: [4596/10000] Training loss: 2.4743542828971074e-05 / Validation loss: 0.0002511393861703252 / Long term Validation loss: 0.1422\n",
      "Epoch: [4597/10000] Training loss: 2.4735121012895003e-05 / Validation loss: 0.00025107711084258153 / Long term Validation loss: 0.1422\n",
      "Epoch: [4598/10000] Training loss: 2.472669939663464e-05 / Validation loss: 0.0002510055982018785 / Long term Validation loss: 0.1423\n",
      "Epoch: [4599/10000] Training loss: 2.4718277987644896e-05 / Validation loss: 0.0002509465182872116 / Long term Validation loss: 0.1423\n",
      "Epoch: [4600/10000] Training loss: 2.470985680461114e-05 / Validation loss: 0.0002508707638221415 / Long term Validation loss: 0.1423\n",
      "Epoch: [4601/10000] Training loss: 2.4701435873797582e-05 / Validation loss: 0.0002508175726722432 / Long term Validation loss: 0.1423\n",
      "Epoch: [4602/10000] Training loss: 2.4693015254097423e-05 / Validation loss: 0.0002507338626337614 / Long term Validation loss: 0.1423\n",
      "Epoch: [4603/10000] Training loss: 2.4684595041827284e-05 / Validation loss: 0.00025069169892233227 / Long term Validation loss: 0.1423\n",
      "Epoch: [4604/10000] Training loss: 2.4676175438657473e-05 / Validation loss: 0.0002505929051656045 / Long term Validation loss: 0.1423\n",
      "Epoch: [4605/10000] Training loss: 2.4667756803668737e-05 / Validation loss: 0.00025057169523408417 / Long term Validation loss: 0.1424\n",
      "Epoch: [4606/10000] Training loss: 2.4659339864116104e-05 / Validation loss: 0.00025044396858762036 / Long term Validation loss: 0.1424\n",
      "Epoch: [4607/10000] Training loss: 2.4650925980198822e-05 / Validation loss: 0.00025046312448754725 / Long term Validation loss: 0.1424\n",
      "Epoch: [4608/10000] Training loss: 2.4642517874028765e-05 / Validation loss: 0.00025027923472330607 / Long term Validation loss: 0.1424\n",
      "Epoch: [4609/10000] Training loss: 2.4634120795974858e-05 / Validation loss: 0.00025037718822477266 / Long term Validation loss: 0.1425\n",
      "Epoch: [4610/10000] Training loss: 2.462574525012038e-05 / Validation loss: 0.0002500829705453817 / Long term Validation loss: 0.1424\n",
      "Epoch: [4611/10000] Training loss: 2.4617411934749825e-05 / Validation loss: 0.0002503367582418533 / Long term Validation loss: 0.1425\n",
      "Epoch: [4612/10000] Training loss: 2.460916249774744e-05 / Validation loss: 0.0002498232703311624 / Long term Validation loss: 0.1424\n",
      "Epoch: [4613/10000] Training loss: 2.4601080380967924e-05 / Validation loss: 0.0002503893012913211 / Long term Validation loss: 0.1426\n",
      "Epoch: [4614/10000] Training loss: 2.4593334955055893e-05 / Validation loss: 0.00024943518935869696 / Long term Validation loss: 0.1423\n",
      "Epoch: [4615/10000] Training loss: 2.458627039536469e-05 / Validation loss: 0.0002506354231631266 / Long term Validation loss: 0.1429\n",
      "Epoch: [4616/10000] Training loss: 2.4580592419298464e-05 / Validation loss: 0.0002487871671928209 / Long term Validation loss: 0.1422\n",
      "Epoch: [4617/10000] Training loss: 2.4577753690197535e-05 / Validation loss: 0.0002512949445628372 / Long term Validation loss: 0.1431\n",
      "Epoch: [4618/10000] Training loss: 2.4580764507173878e-05 / Validation loss: 0.0002476191674573444 / Long term Validation loss: 0.1416\n",
      "Epoch: [4619/10000] Training loss: 2.4595896636283562e-05 / Validation loss: 0.00025287156212406333 / Long term Validation loss: 0.1434\n",
      "Epoch: [4620/10000] Training loss: 2.4636280050474987e-05 / Validation loss: 0.0002454552205222679 / Long term Validation loss: 0.1401\n",
      "Epoch: [4621/10000] Training loss: 2.4729573107600127e-05 / Validation loss: 0.0002566089779493395 / Long term Validation loss: 0.1450\n",
      "Epoch: [4622/10000] Training loss: 2.493414369816914e-05 / Validation loss: 0.00024162798881377477 / Long term Validation loss: 0.1342\n",
      "Epoch: [4623/10000] Training loss: 2.537385410682484e-05 / Validation loss: 0.0002659181100384813 / Long term Validation loss: 0.1467\n",
      "Epoch: [4624/10000] Training loss: 2.6309829068345777e-05 / Validation loss: 0.00023632334324324837 / Long term Validation loss: 0.1229\n",
      "Epoch: [4625/10000] Training loss: 2.829125859278401e-05 / Validation loss: 0.00029117181655303163 / Long term Validation loss: 0.1411\n",
      "Epoch: [4626/10000] Training loss: 3.2430060239318585e-05 / Validation loss: 0.00023680267522035843 / Long term Validation loss: 0.1094\n",
      "Epoch: [4627/10000] Training loss: 4.086451397224529e-05 / Validation loss: 0.00036242863872628045 / Long term Validation loss: 0.1379\n",
      "Epoch: [4628/10000] Training loss: 5.695546910569755e-05 / Validation loss: 0.0002735739362240687 / Long term Validation loss: 0.1074\n",
      "Epoch: [4629/10000] Training loss: 8.379648642367458e-05 / Validation loss: 0.0005033401264516055 / Long term Validation loss: 0.1386\n",
      "Epoch: [4630/10000] Training loss: 0.00011575405916019 / Validation loss: 0.0003231902216441656 / Long term Validation loss: 0.1092\n",
      "Epoch: [4631/10000] Training loss: 0.0001278417050490226 / Validation loss: 0.000449475493992208 / Long term Validation loss: 0.1372\n",
      "Epoch: [4632/10000] Training loss: 9.169368333863793e-05 / Validation loss: 0.00023520755386280054 / Long term Validation loss: 0.1108\n",
      "Epoch: [4633/10000] Training loss: 3.728315413809963e-05 / Validation loss: 0.0002367353125976267 / Long term Validation loss: 0.1235\n",
      "Epoch: [4634/10000] Training loss: 2.8499751179382402e-05 / Validation loss: 0.0003817005956951963 / Long term Validation loss: 0.1372\n",
      "Epoch: [4635/10000] Training loss: 6.285867365246695e-05 / Validation loss: 0.0002652524593997003 / Long term Validation loss: 0.1053\n",
      "Epoch: [4636/10000] Training loss: 7.636400260712929e-05 / Validation loss: 0.00033570652365577777 / Long term Validation loss: 0.1400\n",
      "Epoch: [4637/10000] Training loss: 4.5313859404192746e-05 / Validation loss: 0.0002546439389602611 / Long term Validation loss: 0.1451\n",
      "Epoch: [4638/10000] Training loss: 2.4528356322794163e-05 / Validation loss: 0.00023935891804108213 / Long term Validation loss: 0.1082\n",
      "Epoch: [4639/10000] Training loss: 4.374114456897444e-05 / Validation loss: 0.0003689214533103468 / Long term Validation loss: 0.1380\n",
      "Epoch: [4640/10000] Training loss: 5.711185673985974e-05 / Validation loss: 0.00023684011245548488 / Long term Validation loss: 0.1108\n",
      "Epoch: [4641/10000] Training loss: 3.7537949014061925e-05 / Validation loss: 0.00024832268850593823 / Long term Validation loss: 0.1434\n",
      "Epoch: [4642/10000] Training loss: 2.4672675350273894e-05 / Validation loss: 0.0003177085745180822 / Long term Validation loss: 0.1402\n",
      "Epoch: [4643/10000] Training loss: 3.9028871157576645e-05 / Validation loss: 0.00023985670417739013 / Long term Validation loss: 0.1076\n",
      "Epoch: [4644/10000] Training loss: 4.4897662391115225e-05 / Validation loss: 0.0002859286181157669 / Long term Validation loss: 0.1412\n",
      "Epoch: [4645/10000] Training loss: 2.9841267914243756e-05 / Validation loss: 0.000266389408375736 / Long term Validation loss: 0.1464\n",
      "Epoch: [4646/10000] Training loss: 2.5744539342082513e-05 / Validation loss: 0.00023681334555746667 / Long term Validation loss: 0.1106\n",
      "Epoch: [4647/10000] Training loss: 3.6834998639240834e-05 / Validation loss: 0.00030683709891167254 / Long term Validation loss: 0.1404\n",
      "Epoch: [4648/10000] Training loss: 3.593837287688294e-05 / Validation loss: 0.00024275616233625065 / Long term Validation loss: 0.1363\n",
      "Epoch: [4649/10000] Training loss: 2.5554998682506175e-05 / Validation loss: 0.00023889697787683338 / Long term Validation loss: 0.1266\n",
      "Epoch: [4650/10000] Training loss: 2.7398495462240447e-05 / Validation loss: 0.0002993982561732373 / Long term Validation loss: 0.1406\n",
      "Epoch: [4651/10000] Training loss: 3.405451649938529e-05 / Validation loss: 0.0002365549297234273 / Long term Validation loss: 0.1189\n",
      "Epoch: [4652/10000] Training loss: 2.9655454964696864e-05 / Validation loss: 0.0002508839170932021 / Long term Validation loss: 0.1440\n",
      "Epoch: [4653/10000] Training loss: 2.437821567954433e-05 / Validation loss: 0.0002775749597670022 / Long term Validation loss: 0.1438\n",
      "Epoch: [4654/10000] Training loss: 2.8456528018905163e-05 / Validation loss: 0.0002356444582136086 / Long term Validation loss: 0.1158\n",
      "Epoch: [4655/10000] Training loss: 3.075809158243492e-05 / Validation loss: 0.0002656111965054983 / Long term Validation loss: 0.1462\n",
      "Epoch: [4656/10000] Training loss: 2.6029035086600594e-05 / Validation loss: 0.0002572452450366263 / Long term Validation loss: 0.1458\n",
      "Epoch: [4657/10000] Training loss: 2.4808220208253542e-05 / Validation loss: 0.00023660718511975395 / Long term Validation loss: 0.1213\n",
      "Epoch: [4658/10000] Training loss: 2.8365736524843993e-05 / Validation loss: 0.0002736283568757374 / Long term Validation loss: 0.1451\n",
      "Epoch: [4659/10000] Training loss: 2.7781205635646563e-05 / Validation loss: 0.0002449364517159385 / Long term Validation loss: 0.1403\n",
      "Epoch: [4660/10000] Training loss: 2.4552286886864838e-05 / Validation loss: 0.00024052760634966764 / Long term Validation loss: 0.1323\n",
      "Epoch: [4661/10000] Training loss: 2.54806685378728e-05 / Validation loss: 0.0002715619004766347 / Long term Validation loss: 0.1456\n",
      "Epoch: [4662/10000] Training loss: 2.7424032529158394e-05 / Validation loss: 0.0002395146879122394 / Long term Validation loss: 0.1306\n",
      "Epoch: [4663/10000] Training loss: 2.5752764093249644e-05 / Validation loss: 0.00024768460232402727 / Long term Validation loss: 0.1427\n",
      "Epoch: [4664/10000] Training loss: 2.4302565818906836e-05 / Validation loss: 0.0002629190545235199 / Long term Validation loss: 0.1467\n",
      "Epoch: [4665/10000] Training loss: 2.5755745453272586e-05 / Validation loss: 0.00023817802597696667 / Long term Validation loss: 0.1283\n",
      "Epoch: [4666/10000] Training loss: 2.6257038192165304e-05 / Validation loss: 0.00025550602648620615 / Long term Validation loss: 0.1456\n",
      "Epoch: [4667/10000] Training loss: 2.4689618586536212e-05 / Validation loss: 0.00025337499229765426 / Long term Validation loss: 0.1448\n",
      "Epoch: [4668/10000] Training loss: 2.447943154781745e-05 / Validation loss: 0.00023946176387236906 / Long term Validation loss: 0.1309\n",
      "Epoch: [4669/10000] Training loss: 2.5600893149222277e-05 / Validation loss: 0.00026020867449350754 / Long term Validation loss: 0.1469\n",
      "Epoch: [4670/10000] Training loss: 2.5287571593063927e-05 / Validation loss: 0.0002463937991867221 / Long term Validation loss: 0.1421\n",
      "Epoch: [4671/10000] Training loss: 2.428982761955566e-05 / Validation loss: 0.00024287830165402546 / Long term Validation loss: 0.1379\n",
      "Epoch: [4672/10000] Training loss: 2.464671110199708e-05 / Validation loss: 0.00025993141410870666 / Long term Validation loss: 0.1470\n",
      "Epoch: [4673/10000] Training loss: 2.5224837860506816e-05 / Validation loss: 0.00024262476892892613 / Long term Validation loss: 0.1377\n",
      "Epoch: [4674/10000] Training loss: 2.466271770664066e-05 / Validation loss: 0.00024769613124148236 / Long term Validation loss: 0.1433\n",
      "Epoch: [4675/10000] Training loss: 2.4216400296415284e-05 / Validation loss: 0.000255902042534665 / Long term Validation loss: 0.1460\n",
      "Epoch: [4676/10000] Training loss: 2.4675536899226223e-05 / Validation loss: 0.0002415673701236907 / Long term Validation loss: 0.1360\n",
      "Epoch: [4677/10000] Training loss: 2.483075647739535e-05 / Validation loss: 0.0002522900279105452 / Long term Validation loss: 0.1445\n",
      "Epoch: [4678/10000] Training loss: 2.4333019876596983e-05 / Validation loss: 0.00025081184779958404 / Long term Validation loss: 0.1441\n",
      "Epoch: [4679/10000] Training loss: 2.4242001421724615e-05 / Validation loss: 0.00024263516539108207 / Long term Validation loss: 0.1384\n",
      "Epoch: [4680/10000] Training loss: 2.4592334723419918e-05 / Validation loss: 0.00025475429090496737 / Long term Validation loss: 0.1457\n",
      "Epoch: [4681/10000] Training loss: 2.451936554980543e-05 / Validation loss: 0.0002466849536703345 / Long term Validation loss: 0.1429\n",
      "Epoch: [4682/10000] Training loss: 2.4188527190722378e-05 / Validation loss: 0.0002451661282246223 / Long term Validation loss: 0.1417\n",
      "Epoch: [4683/10000] Training loss: 2.4261184106441325e-05 / Validation loss: 0.0002542677031486644 / Long term Validation loss: 0.1455\n",
      "Epoch: [4684/10000] Training loss: 2.4460775710258288e-05 / Validation loss: 0.0002442888151658456 / Long term Validation loss: 0.1410\n",
      "Epoch: [4685/10000] Training loss: 2.4311126919843357e-05 / Validation loss: 0.0002482230884915545 / Long term Validation loss: 0.1434\n",
      "Epoch: [4686/10000] Training loss: 2.4133510364174317e-05 / Validation loss: 0.0002516367987840575 / Long term Validation loss: 0.1443\n",
      "Epoch: [4687/10000] Training loss: 2.4247348339438694e-05 / Validation loss: 0.00024367332679379455 / Long term Validation loss: 0.1406\n",
      "Epoch: [4688/10000] Training loss: 2.4329680565853327e-05 / Validation loss: 0.00025062792112603803 / Long term Validation loss: 0.1440\n",
      "Epoch: [4689/10000] Training loss: 2.418459435365228e-05 / Validation loss: 0.0002484469001200456 / Long term Validation loss: 0.1436\n",
      "Epoch: [4690/10000] Training loss: 2.410957895165773e-05 / Validation loss: 0.00024454156354558707 / Long term Validation loss: 0.1416\n",
      "Epoch: [4691/10000] Training loss: 2.4207939332890154e-05 / Validation loss: 0.00025141455803247585 / Long term Validation loss: 0.1442\n",
      "Epoch: [4692/10000] Training loss: 2.4220898713853254e-05 / Validation loss: 0.0002459273298248398 / Long term Validation loss: 0.1428\n",
      "Epoch: [4693/10000] Training loss: 2.4109867393662405e-05 / Validation loss: 0.0002462722009561975 / Long term Validation loss: 0.1429\n",
      "Epoch: [4694/10000] Training loss: 2.4090039319176175e-05 / Validation loss: 0.00025043280899375467 / Long term Validation loss: 0.1439\n",
      "Epoch: [4695/10000] Training loss: 2.4157200079899354e-05 / Validation loss: 0.0002446053977459166 / Long term Validation loss: 0.1419\n",
      "Epoch: [4696/10000] Training loss: 2.4138194041132983e-05 / Validation loss: 0.00024801634599401094 / Long term Validation loss: 0.1436\n",
      "Epoch: [4697/10000] Training loss: 2.4062915376268638e-05 / Validation loss: 0.00024841809337041614 / Long term Validation loss: 0.1436\n",
      "Epoch: [4698/10000] Training loss: 2.4066586646577365e-05 / Validation loss: 0.00024452043262361675 / Long term Validation loss: 0.1420\n",
      "Epoch: [4699/10000] Training loss: 2.4105528199581272e-05 / Validation loss: 0.00024898109606520696 / Long term Validation loss: 0.1436\n",
      "Epoch: [4700/10000] Training loss: 2.4076585615157378e-05 / Validation loss: 0.000246391480186857 / Long term Validation loss: 0.1430\n",
      "Epoch: [4701/10000] Training loss: 2.402871766960177e-05 / Validation loss: 0.00024535890953664933 / Long term Validation loss: 0.1427\n",
      "Epoch: [4702/10000] Training loss: 2.403878975802385e-05 / Validation loss: 0.000248785331504373 / Long term Validation loss: 0.1436\n",
      "Epoch: [4703/10000] Training loss: 2.405775791297803e-05 / Validation loss: 0.00024504721376033243 / Long term Validation loss: 0.1426\n",
      "Epoch: [4704/10000] Training loss: 2.402934372989064e-05 / Validation loss: 0.00024651836685458516 / Long term Validation loss: 0.1432\n",
      "Epoch: [4705/10000] Training loss: 2.399942482347735e-05 / Validation loss: 0.0002476530007447038 / Long term Validation loss: 0.1435\n",
      "Epoch: [4706/10000] Training loss: 2.40084140921073e-05 / Validation loss: 0.00024461294011998805 / Long term Validation loss: 0.1426\n",
      "Epoch: [4707/10000] Training loss: 2.4015095104653407e-05 / Validation loss: 0.00024734023971893256 / Long term Validation loss: 0.1435\n",
      "Epoch: [4708/10000] Training loss: 2.399085376769731e-05 / Validation loss: 0.0002462255289395 / Long term Validation loss: 0.1432\n",
      "Epoch: [4709/10000] Training loss: 2.3971561861413486e-05 / Validation loss: 0.0002449632667241234 / Long term Validation loss: 0.1427\n",
      "Epoch: [4710/10000] Training loss: 2.3977133821634503e-05 / Validation loss: 0.00024742382705427314 / Long term Validation loss: 0.1435\n",
      "Epoch: [4711/10000] Training loss: 2.3976967079318104e-05 / Validation loss: 0.00024512457603026975 / Long term Validation loss: 0.1429\n",
      "Epoch: [4712/10000] Training loss: 2.3957335381489817e-05 / Validation loss: 0.0002457031556622529 / Long term Validation loss: 0.1431\n",
      "Epoch: [4713/10000] Training loss: 2.394389108406455e-05 / Validation loss: 0.0002467944268998919 / Long term Validation loss: 0.1436\n",
      "Epoch: [4714/10000] Training loss: 2.394597359575143e-05 / Validation loss: 0.0002446519666158726 / Long term Validation loss: 0.1428\n",
      "Epoch: [4715/10000] Training loss: 2.3942297643904727e-05 / Validation loss: 0.0002463167737823295 / Long term Validation loss: 0.1435\n",
      "Epoch: [4716/10000] Training loss: 2.3926517532707147e-05 / Validation loss: 0.00024582751927576887 / Long term Validation loss: 0.1434\n",
      "Epoch: [4717/10000] Training loss: 2.391611306732241e-05 / Validation loss: 0.0002447748775739261 / Long term Validation loss: 0.1430\n",
      "Epoch: [4718/10000] Training loss: 2.3915382664846794e-05 / Validation loss: 0.00024643812607711194 / Long term Validation loss: 0.1436\n",
      "Epoch: [4719/10000] Training loss: 2.3910041688857285e-05 / Validation loss: 0.0002449908078472577 / Long term Validation loss: 0.1431\n",
      "Epoch: [4720/10000] Training loss: 2.389712395420117e-05 / Validation loss: 0.0002452271733634994 / Long term Validation loss: 0.1432\n",
      "Epoch: [4721/10000] Training loss: 2.388823078846489e-05 / Validation loss: 0.00024603382462285334 / Long term Validation loss: 0.1436\n",
      "Epoch: [4722/10000] Training loss: 2.3885451512925917e-05 / Validation loss: 0.00024457878125890676 / Long term Validation loss: 0.1431\n",
      "Epoch: [4723/10000] Training loss: 2.3879381233047473e-05 / Validation loss: 0.0002456327631659279 / Long term Validation loss: 0.1436\n",
      "Epoch: [4724/10000] Training loss: 2.3868468460283317e-05 / Validation loss: 0.00024535165832030207 / Long term Validation loss: 0.1435\n",
      "Epoch: [4725/10000] Training loss: 2.3860304916355208e-05 / Validation loss: 0.00024460538898495287 / Long term Validation loss: 0.1432\n",
      "Epoch: [4726/10000] Training loss: 2.3856103342001355e-05 / Validation loss: 0.0002456957125939483 / Long term Validation loss: 0.1437\n",
      "Epoch: [4727/10000] Training loss: 2.3849738943162164e-05 / Validation loss: 0.00024472619959929826 / Long term Validation loss: 0.1433\n",
      "Epoch: [4728/10000] Training loss: 2.3840191067988578e-05 / Validation loss: 0.0002448689049676863 / Long term Validation loss: 0.1434\n",
      "Epoch: [4729/10000] Training loss: 2.3832382363860973e-05 / Validation loss: 0.0002453697438754607 / Long term Validation loss: 0.1437\n",
      "Epoch: [4730/10000] Training loss: 2.3827214569480006e-05 / Validation loss: 0.00024439061712146405 / Long term Validation loss: 0.1433\n",
      "Epoch: [4731/10000] Training loss: 2.3820736159645157e-05 / Validation loss: 0.0002450914295563167 / Long term Validation loss: 0.1436\n",
      "Epoch: [4732/10000] Training loss: 2.3812109143949936e-05 / Validation loss: 0.00024484424766944867 / Long term Validation loss: 0.1436\n",
      "Epoch: [4733/10000] Training loss: 2.3804488211004743e-05 / Validation loss: 0.00024437343333289673 / Long term Validation loss: 0.1434\n",
      "Epoch: [4734/10000] Training loss: 2.3798667290556925e-05 / Validation loss: 0.00024506627226874173 / Long term Validation loss: 0.1437\n",
      "Epoch: [4735/10000] Training loss: 2.379213611662553e-05 / Validation loss: 0.0002443736574679633 / Long term Validation loss: 0.1435\n",
      "Epoch: [4736/10000] Training loss: 2.378412996921637e-05 / Validation loss: 0.0002445169039168036 / Long term Validation loss: 0.1436\n",
      "Epoch: [4737/10000] Training loss: 2.3776629272883273e-05 / Validation loss: 0.00024476640222828046 / Long term Validation loss: 0.1437\n",
      "Epoch: [4738/10000] Training loss: 2.37703665759864e-05 / Validation loss: 0.00024411837960563638 / Long term Validation loss: 0.1435\n",
      "Epoch: [4739/10000] Training loss: 2.3763792230032003e-05 / Validation loss: 0.00024460242420446063 / Long term Validation loss: 0.1437\n",
      "Epoch: [4740/10000] Training loss: 2.3756205724203644e-05 / Validation loss: 0.00024433921646018267 / Long term Validation loss: 0.1437\n",
      "Epoch: [4741/10000] Training loss: 2.374880117053312e-05 / Validation loss: 0.00024408282358374435 / Long term Validation loss: 0.1436\n",
      "Epoch: [4742/10000] Training loss: 2.374223941550498e-05 / Validation loss: 0.0002444937661993797 / Long term Validation loss: 0.1438\n",
      "Epoch: [4743/10000] Training loss: 2.3735614558788183e-05 / Validation loss: 0.0002439800889843414 / Long term Validation loss: 0.1436\n",
      "Epoch: [4744/10000] Training loss: 2.372830967104002e-05 / Validation loss: 0.00024414015622611048 / Long term Validation loss: 0.1437\n",
      "Epoch: [4745/10000] Training loss: 2.372099401266211e-05 / Validation loss: 0.000244203186980419 / Long term Validation loss: 0.1438\n",
      "Epoch: [4746/10000] Training loss: 2.371423261919343e-05 / Validation loss: 0.00024379468414318804 / Long term Validation loss: 0.1436\n",
      "Epoch: [4747/10000] Training loss: 2.3707547148601158e-05 / Validation loss: 0.00024412718067438015 / Long term Validation loss: 0.1438\n",
      "Epoch: [4748/10000] Training loss: 2.3700427456552186e-05 / Validation loss: 0.0002438545555114618 / Long term Validation loss: 0.1438\n",
      "Epoch: [4749/10000] Training loss: 2.369319719802828e-05 / Validation loss: 0.00024375404834376242 / Long term Validation loss: 0.1437\n",
      "Epoch: [4750/10000] Training loss: 2.3686307586026714e-05 / Validation loss: 0.0002439600233521986 / Long term Validation loss: 0.1439\n",
      "Epoch: [4751/10000] Training loss: 2.3679556244227455e-05 / Validation loss: 0.00024358404695998395 / Long term Validation loss: 0.1437\n",
      "Epoch: [4752/10000] Training loss: 2.3672551832321462e-05 / Validation loss: 0.00024374397022338374 / Long term Validation loss: 0.1439\n",
      "Epoch: [4753/10000] Training loss: 2.366540188875456e-05 / Validation loss: 0.00024367973301558308 / Long term Validation loss: 0.1439\n",
      "Epoch: [4754/10000] Training loss: 2.365843688094743e-05 / Validation loss: 0.00024344752489650887 / Long term Validation loss: 0.1438\n",
      "Epoch: [4755/10000] Training loss: 2.3651620487646216e-05 / Validation loss: 0.0002436541413104699 / Long term Validation loss: 0.1439\n",
      "Epoch: [4756/10000] Training loss: 2.3644680613598117e-05 / Validation loss: 0.00024339636571393927 / Long term Validation loss: 0.1439\n",
      "Epoch: [4757/10000] Training loss: 2.363760164433543e-05 / Validation loss: 0.00024339573079737515 / Long term Validation loss: 0.1439\n",
      "Epoch: [4758/10000] Training loss: 2.363060005101956e-05 / Validation loss: 0.00024345190457863273 / Long term Validation loss: 0.1439\n",
      "Epoch: [4759/10000] Training loss: 2.3623725902293872e-05 / Validation loss: 0.00024319631454158017 / Long term Validation loss: 0.1439\n",
      "Epoch: [4760/10000] Training loss: 2.361681379637051e-05 / Validation loss: 0.00024332939026858606 / Long term Validation loss: 0.1440\n",
      "Epoch: [4761/10000] Training loss: 2.360979262370746e-05 / Validation loss: 0.0002431935689368311 / Long term Validation loss: 0.1440\n",
      "Epoch: [4762/10000] Training loss: 2.3602781637599157e-05 / Validation loss: 0.00024308814526200409 / Long term Validation loss: 0.1440\n",
      "Epoch: [4763/10000] Training loss: 2.3595861609290723e-05 / Validation loss: 0.00024318255096831355 / Long term Validation loss: 0.1440\n",
      "Epoch: [4764/10000] Training loss: 2.3588952743653602e-05 / Validation loss: 0.00024296623153902797 / Long term Validation loss: 0.1440\n",
      "Epoch: [4765/10000] Training loss: 2.35819732431762e-05 / Validation loss: 0.0002430117981181647 / Long term Validation loss: 0.1440\n",
      "Epoch: [4766/10000] Training loss: 2.357496986107597e-05 / Validation loss: 0.00024296486866678106 / Long term Validation loss: 0.1440\n",
      "Epoch: [4767/10000] Training loss: 2.3568018491575264e-05 / Validation loss: 0.00024281408757595256 / Long term Validation loss: 0.1440\n",
      "Epoch: [4768/10000] Training loss: 2.3561098602087456e-05 / Validation loss: 0.00024289515004908592 / Long term Validation loss: 0.1441\n",
      "Epoch: [4769/10000] Training loss: 2.3554144311018188e-05 / Validation loss: 0.00024273815739060401 / Long term Validation loss: 0.1441\n",
      "Epoch: [4770/10000] Training loss: 2.3547156399619567e-05 / Validation loss: 0.000242712790732471 / Long term Validation loss: 0.1441\n",
      "Epoch: [4771/10000] Training loss: 2.354018812500693e-05 / Validation loss: 0.00024271322081643467 / Long term Validation loss: 0.1441\n",
      "Epoch: [4772/10000] Training loss: 2.3533251937641545e-05 / Validation loss: 0.00024255797461140998 / Long term Validation loss: 0.1441\n",
      "Epoch: [4773/10000] Training loss: 2.3526308113486695e-05 / Validation loss: 0.00024260334795044223 / Long term Validation loss: 0.1441\n",
      "Epoch: [4774/10000] Training loss: 2.3519336328654156e-05 / Validation loss: 0.0002425002207959125 / Long term Validation loss: 0.1441\n",
      "Epoch: [4775/10000] Training loss: 2.3512363022473886e-05 / Validation loss: 0.0002424305281105848 / Long term Validation loss: 0.1441\n",
      "Epoch: [4776/10000] Training loss: 2.3505411754946778e-05 / Validation loss: 0.0002424466735943499 / Long term Validation loss: 0.1442\n",
      "Epoch: [4777/10000] Training loss: 2.3498467705816595e-05 / Validation loss: 0.00024230870174145926 / Long term Validation loss: 0.1442\n",
      "Epoch: [4778/10000] Training loss: 2.349150768275311e-05 / Validation loss: 0.00024231651818873478 / Long term Validation loss: 0.1442\n",
      "Epoch: [4779/10000] Training loss: 2.348453694868294e-05 / Validation loss: 0.0002422518180749404 / Long term Validation loss: 0.1442\n",
      "Epoch: [4780/10000] Training loss: 2.3477575604477316e-05 / Validation loss: 0.00024216093821599141 / Long term Validation loss: 0.1442\n",
      "Epoch: [4781/10000] Training loss: 2.3470625470152695e-05 / Validation loss: 0.0002421734556347824 / Long term Validation loss: 0.1442\n",
      "Epoch: [4782/10000] Training loss: 2.346367090448708e-05 / Validation loss: 0.00024205894409772225 / Long term Validation loss: 0.1442\n",
      "Epoch: [4783/10000] Training loss: 2.345670550809648e-05 / Validation loss: 0.0002420364011430217 / Long term Validation loss: 0.1443\n",
      "Epoch: [4784/10000] Training loss: 2.3449739862256572e-05 / Validation loss: 0.0002419941321344557 / Long term Validation loss: 0.1443\n",
      "Epoch: [4785/10000] Training loss: 2.3442782459534208e-05 / Validation loss: 0.0002418979007312113 / Long term Validation loss: 0.1443\n",
      "Epoch: [4786/10000] Training loss: 2.34358275915611e-05 / Validation loss: 0.00024189857026882478 / Long term Validation loss: 0.1443\n",
      "Epoch: [4787/10000] Training loss: 2.342886651003615e-05 / Validation loss: 0.00024180586065274527 / Long term Validation loss: 0.1443\n",
      "Epoch: [4788/10000] Training loss: 2.3421900761110105e-05 / Validation loss: 0.00024176291336257884 / Long term Validation loss: 0.1443\n",
      "Epoch: [4789/10000] Training loss: 2.3414937944355166e-05 / Validation loss: 0.00024173097478867703 / Long term Validation loss: 0.1443\n",
      "Epoch: [4790/10000] Training loss: 2.3407979446431933e-05 / Validation loss: 0.00024163791800541643 / Long term Validation loss: 0.1443\n",
      "Epoch: [4791/10000] Training loss: 2.34010196463072e-05 / Validation loss: 0.0002416249667940104 / Long term Validation loss: 0.1444\n",
      "Epoch: [4792/10000] Training loss: 2.339405538517524e-05 / Validation loss: 0.000241548759600458 / Long term Validation loss: 0.1444\n",
      "Epoch: [4793/10000] Training loss: 2.33870899563791e-05 / Validation loss: 0.0002414938041724646 / Long term Validation loss: 0.1444\n",
      "Epoch: [4794/10000] Training loss: 2.3380127160621042e-05 / Validation loss: 0.000241464414086072 / Long term Validation loss: 0.1444\n",
      "Epoch: [4795/10000] Training loss: 2.3373165770818484e-05 / Validation loss: 0.0002413779915814961 / Long term Validation loss: 0.1444\n",
      "Epoch: [4796/10000] Training loss: 2.336620223836435e-05 / Validation loss: 0.0002413532259188511 / Long term Validation loss: 0.1444\n",
      "Epoch: [4797/10000] Training loss: 2.3359236159571832e-05 / Validation loss: 0.0002412882511458344 / Long term Validation loss: 0.1444\n",
      "Epoch: [4798/10000] Training loss: 2.3352270178096458e-05 / Validation loss: 0.00024122756790344367 / Long term Validation loss: 0.1444\n",
      "Epoch: [4799/10000] Training loss: 2.3345305706239204e-05 / Validation loss: 0.00024119673469174697 / Long term Validation loss: 0.1444\n",
      "Epoch: [4800/10000] Training loss: 2.333834117995508e-05 / Validation loss: 0.00024111743464824025 / Long term Validation loss: 0.1445\n",
      "Epoch: [4801/10000] Training loss: 2.3331374788493874e-05 / Validation loss: 0.00024108360507607752 / Long term Validation loss: 0.1445\n",
      "Epoch: [4802/10000] Training loss: 2.3324407029595947e-05 / Validation loss: 0.00024102534179037255 / Long term Validation loss: 0.1445\n",
      "Epoch: [4803/10000] Training loss: 2.33174395336185e-05 / Validation loss: 0.00024096265726021812 / Long term Validation loss: 0.1445\n",
      "Epoch: [4804/10000] Training loss: 2.331047261900109e-05 / Validation loss: 0.00024092861417444953 / Long term Validation loss: 0.1445\n",
      "Epoch: [4805/10000] Training loss: 2.3303505101574102e-05 / Validation loss: 0.00024085551128170572 / Long term Validation loss: 0.1445\n",
      "Epoch: [4806/10000] Training loss: 2.329653617138459e-05 / Validation loss: 0.00024081539490355969 / Long term Validation loss: 0.1445\n",
      "Epoch: [4807/10000] Training loss: 2.3289566418902402e-05 / Validation loss: 0.0002407608178131169 / Long term Validation loss: 0.1445\n",
      "Epoch: [4808/10000] Training loss: 2.3282596734596962e-05 / Validation loss: 0.0002406983637077588 / Long term Validation loss: 0.1446\n",
      "Epoch: [4809/10000] Training loss: 2.3275627061414986e-05 / Validation loss: 0.00024066087817733352 / Long term Validation loss: 0.1446\n",
      "Epoch: [4810/10000] Training loss: 2.3268656645442527e-05 / Validation loss: 0.00024059271825237511 / Long term Validation loss: 0.1446\n",
      "Epoch: [4811/10000] Training loss: 2.326168514883374e-05 / Validation loss: 0.00024054851765128264 / Long term Validation loss: 0.1446\n",
      "Epoch: [4812/10000] Training loss: 2.3254713017933678e-05 / Validation loss: 0.00024049559957584936 / Long term Validation loss: 0.1446\n",
      "Epoch: [4813/10000] Training loss: 2.3247740716139256e-05 / Validation loss: 0.0002404343104905222 / Long term Validation loss: 0.1446\n",
      "Epoch: [4814/10000] Training loss: 2.3240768113402437e-05 / Validation loss: 0.0002403936122579048 / Long term Validation loss: 0.1446\n",
      "Epoch: [4815/10000] Training loss: 2.323379476191291e-05 / Validation loss: 0.00024032910099993016 / Long term Validation loss: 0.1446\n",
      "Epoch: [4816/10000] Training loss: 2.3226820516552774e-05 / Validation loss: 0.0002402823822061039 / Long term Validation loss: 0.1447\n",
      "Epoch: [4817/10000] Training loss: 2.3219845663992418e-05 / Validation loss: 0.00024022999435542307 / Long term Validation loss: 0.1447\n",
      "Epoch: [4818/10000] Training loss: 2.3212870446704276e-05 / Validation loss: 0.0002401702863770413 / Long term Validation loss: 0.1447\n",
      "Epoch: [4819/10000] Training loss: 2.32058947580325e-05 / Validation loss: 0.0002401269484884554 / Long term Validation loss: 0.1447\n",
      "Epoch: [4820/10000] Training loss: 2.319891833632461e-05 / Validation loss: 0.00024006513724461725 / Long term Validation loss: 0.1447\n",
      "Epoch: [4821/10000] Training loss: 2.3191941105937407e-05 / Validation loss: 0.00024001696437269852 / Long term Validation loss: 0.1447\n",
      "Epoch: [4822/10000] Training loss: 2.318496323488904e-05 / Validation loss: 0.00023996448478595927 / Long term Validation loss: 0.1447\n",
      "Epoch: [4823/10000] Training loss: 2.3177984855456067e-05 / Validation loss: 0.000239906349484207 / Long term Validation loss: 0.1447\n",
      "Epoch: [4824/10000] Training loss: 2.3171005900535955e-05 / Validation loss: 0.00023986082574763812 / Long term Validation loss: 0.1447\n",
      "Epoch: [4825/10000] Training loss: 2.3164026215503076e-05 / Validation loss: 0.00023980093724320812 / Long term Validation loss: 0.1448\n",
      "Epoch: [4826/10000] Training loss: 2.3157045747430006e-05 / Validation loss: 0.00023975190796701363 / Long term Validation loss: 0.1448\n",
      "Epoch: [4827/10000] Training loss: 2.3150064586775568e-05 / Validation loss: 0.00023969904104225608 / Long term Validation loss: 0.1448\n",
      "Epoch: [4828/10000] Training loss: 2.314308280899509e-05 / Validation loss: 0.0002396423631421999 / Long term Validation loss: 0.1448\n",
      "Epoch: [4829/10000] Training loss: 2.3136100378727137e-05 / Validation loss: 0.00023959509082919115 / Long term Validation loss: 0.1448\n",
      "Epoch: [4830/10000] Training loss: 2.3129117202888824e-05 / Validation loss: 0.00023953665697801772 / Long term Validation loss: 0.1448\n",
      "Epoch: [4831/10000] Training loss: 2.312213323661222e-05 / Validation loss: 0.00023948713423417527 / Long term Validation loss: 0.1448\n",
      "Epoch: [4832/10000] Training loss: 2.3115148522035604e-05 / Validation loss: 0.0002394337927389647 / Long term Validation loss: 0.1448\n",
      "Epoch: [4833/10000] Training loss: 2.3108163102164522e-05 / Validation loss: 0.00023937837884766897 / Long term Validation loss: 0.1448\n",
      "Epoch: [4834/10000] Training loss: 2.3101176961778162e-05 / Validation loss: 0.00023932964385625356 / Long term Validation loss: 0.1448\n",
      "Epoch: [4835/10000] Training loss: 2.309419004425388e-05 / Validation loss: 0.00023927232078375165 / Long term Validation loss: 0.1449\n",
      "Epoch: [4836/10000] Training loss: 2.308720230982489e-05 / Validation loss: 0.0002392224312188407 / Long term Validation loss: 0.1449\n",
      "Epoch: [4837/10000] Training loss: 2.308021377121192e-05 / Validation loss: 0.0002391686198629627 / Long term Validation loss: 0.1449\n",
      "Epoch: [4838/10000] Training loss: 2.3073224450192035e-05 / Validation loss: 0.000239114270354976 / Long term Validation loss: 0.1449\n",
      "Epoch: [4839/10000] Training loss: 2.3066234341886146e-05 / Validation loss: 0.00023906430540714495 / Long term Validation loss: 0.1449\n",
      "Epoch: [4840/10000] Training loss: 2.305924341184173e-05 / Validation loss: 0.0002390079183217959 / Long term Validation loss: 0.1449\n",
      "Epoch: [4841/10000] Training loss: 2.3052251625819416e-05 / Validation loss: 0.0002389577055617388 / Long term Validation loss: 0.1449\n",
      "Epoch: [4842/10000] Training loss: 2.3045258979298812e-05 / Validation loss: 0.00023890352194617045 / Long term Validation loss: 0.1449\n",
      "Epoch: [4843/10000] Training loss: 2.3038265478705255e-05 / Validation loss: 0.00023885004015530395 / Long term Validation loss: 0.1449\n",
      "Epoch: [4844/10000] Training loss: 2.303127112236885e-05 / Validation loss: 0.00023879901178669978 / Long term Validation loss: 0.1449\n",
      "Epoch: [4845/10000] Training loss: 2.302427588902398e-05 / Validation loss: 0.00023874344341506034 / Long term Validation loss: 0.1450\n",
      "Epoch: [4846/10000] Training loss: 2.3017279750596098e-05 / Validation loss: 0.0002386928562352381 / Long term Validation loss: 0.1450\n",
      "Epoch: [4847/10000] Training loss: 2.30102826933891e-05 / Validation loss: 0.00023863842670327712 / Long term Validation loss: 0.1450\n",
      "Epoch: [4848/10000] Training loss: 2.300328471252189e-05 / Validation loss: 0.00023858561005827468 / Long term Validation loss: 0.1450\n",
      "Epoch: [4849/10000] Training loss: 2.299628580485237e-05 / Validation loss: 0.0002385336729854281 / Long term Validation loss: 0.1450\n",
      "Epoch: [4850/10000] Training loss: 2.298928595596135e-05 / Validation loss: 0.00023847886381443697 / Long term Validation loss: 0.1450\n",
      "Epoch: [4851/10000] Training loss: 2.2982285143666475e-05 / Validation loss: 0.00023842783710319198 / Long term Validation loss: 0.1450\n",
      "Epoch: [4852/10000] Training loss: 2.2975283350422115e-05 / Validation loss: 0.00023837332782998023 / Long term Validation loss: 0.1450\n",
      "Epoch: [4853/10000] Training loss: 2.296828056361147e-05 / Validation loss: 0.00023832096951655445 / Long term Validation loss: 0.1450\n",
      "Epoch: [4854/10000] Training loss: 2.2961276776114637e-05 / Validation loss: 0.0002382682769964316 / Long term Validation loss: 0.1450\n",
      "Epoch: [4855/10000] Training loss: 2.2954271975664074e-05 / Validation loss: 0.00023821415896268483 / Long term Validation loss: 0.1451\n",
      "Epoch: [4856/10000] Training loss: 2.294726614456234e-05 / Validation loss: 0.00023816261235072892 / Long term Validation loss: 0.1451\n",
      "Epoch: [4857/10000] Training loss: 2.294025926454236e-05 / Validation loss: 0.00023810818477330633 / Long term Validation loss: 0.1451\n",
      "Epoch: [4858/10000] Training loss: 2.2933251318548246e-05 / Validation loss: 0.00023805606347769177 / Long term Validation loss: 0.1451\n",
      "Epoch: [4859/10000] Training loss: 2.2926242294851006e-05 / Validation loss: 0.00023800279102720588 / Long term Validation loss: 0.1451\n",
      "Epoch: [4860/10000] Training loss: 2.291923218015083e-05 / Validation loss: 0.00023794927718127814 / Long term Validation loss: 0.1451\n",
      "Epoch: [4861/10000] Training loss: 2.2912220959027737e-05 / Validation loss: 0.0002378971664412306 / Long term Validation loss: 0.1451\n",
      "Epoch: [4862/10000] Training loss: 2.2905208613688456e-05 / Validation loss: 0.00023784296992380646 / Long term Validation loss: 0.1451\n",
      "Epoch: [4863/10000] Training loss: 2.2898195125277643e-05 / Validation loss: 0.0002377908647524053 / Long term Validation loss: 0.1451\n",
      "Epoch: [4864/10000] Training loss: 2.2891180478188036e-05 / Validation loss: 0.00023773720682412875 / Long term Validation loss: 0.1451\n",
      "Epoch: [4865/10000] Training loss: 2.28841646565193e-05 / Validation loss: 0.00023768415652990993 / Long term Validation loss: 0.1452\n",
      "Epoch: [4866/10000] Training loss: 2.2877147644823716e-05 / Validation loss: 0.00023763147943870158 / Long term Validation loss: 0.1452\n",
      "Epoch: [4867/10000] Training loss: 2.2870129425467762e-05 / Validation loss: 0.00023757760810965017 / Long term Validation loss: 0.1452\n",
      "Epoch: [4868/10000] Training loss: 2.286310997927893e-05 / Validation loss: 0.00023752531402142517 / Long term Validation loss: 0.1452\n",
      "Epoch: [4869/10000] Training loss: 2.2856089288063535e-05 / Validation loss: 0.00023747147608460203 / Long term Validation loss: 0.1452\n",
      "Epoch: [4870/10000] Training loss: 2.2849067333213567e-05 / Validation loss: 0.0002374187050023937 / Long term Validation loss: 0.1452\n",
      "Epoch: [4871/10000] Training loss: 2.2842044097601116e-05 / Validation loss: 0.00023736553215505208 / Long term Validation loss: 0.1452\n",
      "Epoch: [4872/10000] Training loss: 2.283501956269495e-05 / Validation loss: 0.0002373120116291548 / Long term Validation loss: 0.1452\n",
      "Epoch: [4873/10000] Training loss: 2.282799370917875e-05 / Validation loss: 0.00023725937643452754 / Long term Validation loss: 0.1452\n",
      "Epoch: [4874/10000] Training loss: 2.2820966517275097e-05 / Validation loss: 0.0002372055445901318 / Long term Validation loss: 0.1453\n",
      "Epoch: [4875/10000] Training loss: 2.28139379663993e-05 / Validation loss: 0.00023715283367638178 / Long term Validation loss: 0.1453\n",
      "Epoch: [4876/10000] Training loss: 2.280690803711723e-05 / Validation loss: 0.00023709929359285036 / Long term Validation loss: 0.1453\n",
      "Epoch: [4877/10000] Training loss: 2.279987670908964e-05 / Validation loss: 0.0002370460550688234 / Long term Validation loss: 0.1453\n",
      "Epoch: [4878/10000] Training loss: 2.2792843962148276e-05 / Validation loss: 0.00023699300402343633 / Long term Validation loss: 0.1453\n",
      "Epoch: [4879/10000] Training loss: 2.2785809775181252e-05 / Validation loss: 0.00023693930758637486 / Long term Validation loss: 0.1453\n",
      "Epoch: [4880/10000] Training loss: 2.2778774126329338e-05 / Validation loss: 0.00023688644865009584 / Long term Validation loss: 0.1453\n",
      "Epoch: [4881/10000] Training loss: 2.2771736994000756e-05 / Validation loss: 0.00023683271224281523 / Long term Validation loss: 0.1453\n",
      "Epoch: [4882/10000] Training loss: 2.2764698355808367e-05 / Validation loss: 0.00023677961099106946 / Long term Validation loss: 0.1453\n",
      "Epoch: [4883/10000] Training loss: 2.2757658189915672e-05 / Validation loss: 0.00023672616921617467 / Long term Validation loss: 0.1453\n",
      "Epoch: [4884/10000] Training loss: 2.2750616473562767e-05 / Validation loss: 0.0002366726466134106 / Long term Validation loss: 0.1454\n",
      "Epoch: [4885/10000] Training loss: 2.2743573183726135e-05 / Validation loss: 0.00023661948660829422 / Long term Validation loss: 0.1454\n",
      "Epoch: [4886/10000] Training loss: 2.273652829694938e-05 / Validation loss: 0.0002365657105344721 / Long term Validation loss: 0.1454\n",
      "Epoch: [4887/10000] Training loss: 2.272948178907423e-05 / Validation loss: 0.00023651255441420896 / Long term Validation loss: 0.1454\n",
      "Epoch: [4888/10000] Training loss: 2.272243363621688e-05 / Validation loss: 0.00023645882637513596 / Long term Validation loss: 0.1454\n",
      "Epoch: [4889/10000] Training loss: 2.2715383813670196e-05 / Validation loss: 0.0002364054097256456 / Long term Validation loss: 0.1454\n",
      "Epoch: [4890/10000] Training loss: 2.2708332296866818e-05 / Validation loss: 0.00023635189242614298 / Long term Validation loss: 0.1454\n",
      "Epoch: [4891/10000] Training loss: 2.2701279060510866e-05 / Validation loss: 0.00023629816834062743 / Long term Validation loss: 0.1454\n",
      "Epoch: [4892/10000] Training loss: 2.269422407891331e-05 / Validation loss: 0.00023624478505769275 / Long term Validation loss: 0.1454\n",
      "Epoch: [4893/10000] Training loss: 2.268716732615568e-05 / Validation loss: 0.00023619091115610098 / Long term Validation loss: 0.1454\n",
      "Epoch: [4894/10000] Training loss: 2.2680108775629244e-05 / Validation loss: 0.00023613745396904663 / Long term Validation loss: 0.1455\n",
      "Epoch: [4895/10000] Training loss: 2.2673048400844955e-05 / Validation loss: 0.000236083625269452 / Long term Validation loss: 0.1455\n",
      "Epoch: [4896/10000] Training loss: 2.2665986174549785e-05 / Validation loss: 0.00023602993826988647 / Long term Validation loss: 0.1455\n",
      "Epoch: [4897/10000] Training loss: 2.2658922069427738e-05 / Validation loss: 0.00023597623038870417 / Long term Validation loss: 0.1455\n",
      "Epoch: [4898/10000] Training loss: 2.2651856057587817e-05 / Validation loss: 0.00023592231151282924 / Long term Validation loss: 0.1455\n",
      "Epoch: [4899/10000] Training loss: 2.2644788110712872e-05 / Validation loss: 0.0002358686483692946 / Long term Validation loss: 0.1455\n",
      "Epoch: [4900/10000] Training loss: 2.263771820028826e-05 / Validation loss: 0.0002358146146275934 / Long term Validation loss: 0.1455\n",
      "Epoch: [4901/10000] Training loss: 2.2630646297149447e-05 / Validation loss: 0.00023576085351816723 / Long term Validation loss: 0.1455\n",
      "Epoch: [4902/10000] Training loss: 2.262357237212526e-05 / Validation loss: 0.00023570682866548947 / Long term Validation loss: 0.1455\n",
      "Epoch: [4903/10000] Training loss: 2.2616496395368344e-05 / Validation loss: 0.00023565287271335838 / Long term Validation loss: 0.1456\n",
      "Epoch: [4904/10000] Training loss: 2.2609418336886392e-05 / Validation loss: 0.00023559889648561903 / Long term Validation loss: 0.1456\n",
      "Epoch: [4905/10000] Training loss: 2.2602338166196226e-05 / Validation loss: 0.00023554474872199553 / Long term Validation loss: 0.1456\n",
      "Epoch: [4906/10000] Training loss: 2.2595255852413264e-05 / Validation loss: 0.0002354907653616933 / Long term Validation loss: 0.1456\n",
      "Epoch: [4907/10000] Training loss: 2.258817136443854e-05 / Validation loss: 0.0002354365018314477 / Long term Validation loss: 0.1456\n",
      "Epoch: [4908/10000] Training loss: 2.2581084670599938e-05 / Validation loss: 0.00023538241565770744 / Long term Validation loss: 0.1456\n",
      "Epoch: [4909/10000] Training loss: 2.2573995739149746e-05 / Validation loss: 0.00023532811602369253 / Long term Validation loss: 0.1456\n",
      "Epoch: [4910/10000] Training loss: 2.2566904537755953e-05 / Validation loss: 0.0002352738599457692 / Long term Validation loss: 0.1456\n",
      "Epoch: [4911/10000] Training loss: 2.2559811033935064e-05 / Validation loss: 0.00023521955212996937 / Long term Validation loss: 0.1456\n",
      "Epoch: [4912/10000] Training loss: 2.2552715194768117e-05 / Validation loss: 0.00023516512115349445 / Long term Validation loss: 0.1456\n",
      "Epoch: [4913/10000] Training loss: 2.254561698700966e-05 / Validation loss: 0.0002351107722000377 / Long term Validation loss: 0.1457\n",
      "Epoch: [4914/10000] Training loss: 2.2538516377191017e-05 / Validation loss: 0.00023505620962766083 / Long term Validation loss: 0.1457\n",
      "Epoch: [4915/10000] Training loss: 2.253141333138364e-05 / Validation loss: 0.00023500175670927544 / Long term Validation loss: 0.1457\n",
      "Epoch: [4916/10000] Training loss: 2.2524307815567557e-05 / Validation loss: 0.0002349471138936103 / Long term Validation loss: 0.1457\n",
      "Epoch: [4917/10000] Training loss: 2.2517199795246694e-05 / Validation loss: 0.00023489250592932198 / Long term Validation loss: 0.1457\n",
      "Epoch: [4918/10000] Training loss: 2.251008923582239e-05 / Validation loss: 0.00023483780692160484 / Long term Validation loss: 0.1457\n",
      "Epoch: [4919/10000] Training loss: 2.2502976102327074e-05 / Validation loss: 0.00023478302893244755 / Long term Validation loss: 0.1457\n",
      "Epoch: [4920/10000] Training loss: 2.2495860359588867e-05 / Validation loss: 0.0002347282597094831 / Long term Validation loss: 0.1457\n",
      "Epoch: [4921/10000] Training loss: 2.2488741972236708e-05 / Validation loss: 0.00023467333013701177 / Long term Validation loss: 0.1457\n",
      "Epoch: [4922/10000] Training loss: 2.2481620904597018e-05 / Validation loss: 0.00023461845217617356 / Long term Validation loss: 0.1458\n",
      "Epoch: [4923/10000] Training loss: 2.247449712093618e-05 / Validation loss: 0.00023456340192155433 / Long term Validation loss: 0.1458\n",
      "Epoch: [4924/10000] Training loss: 2.2467370585192446e-05 / Validation loss: 0.00023450837601781852 / Long term Validation loss: 0.1458\n",
      "Epoch: [4925/10000] Training loss: 2.246024126129421e-05 / Validation loss: 0.00023445322584692173 / Long term Validation loss: 0.1458\n",
      "Epoch: [4926/10000] Training loss: 2.245310911290985e-05 / Validation loss: 0.0002343980303232949 / Long term Validation loss: 0.1458\n",
      "Epoch: [4927/10000] Training loss: 2.2445974103668268e-05 / Validation loss: 0.0002343427793680717 / Long term Validation loss: 0.1458\n",
      "Epoch: [4928/10000] Training loss: 2.243883619707222e-05 / Validation loss: 0.00023428741440323475 / Long term Validation loss: 0.1458\n",
      "Epoch: [4929/10000] Training loss: 2.2431695356529182e-05 / Validation loss: 0.00023423204279269941 / Long term Validation loss: 0.1458\n",
      "Epoch: [4930/10000] Training loss: 2.242455154546257e-05 / Validation loss: 0.00023417652237535454 / Long term Validation loss: 0.1459\n",
      "Epoch: [4931/10000] Training loss: 2.2417404727174445e-05 / Validation loss: 0.0002341210027492931 / Long term Validation loss: 0.1459\n",
      "Epoch: [4932/10000] Training loss: 2.2410254865082317e-05 / Validation loss: 0.00023406534177709408 / Long term Validation loss: 0.1459\n",
      "Epoch: [4933/10000] Training loss: 2.2403101922517645e-05 / Validation loss: 0.00023400965148979365 / Long term Validation loss: 0.1459\n",
      "Epoch: [4934/10000] Training loss: 2.2395945862966992e-05 / Validation loss: 0.0002339538557802927 / Long term Validation loss: 0.1459\n",
      "Epoch: [4935/10000] Training loss: 2.2388786649926483e-05 / Validation loss: 0.0002338979837106662 / Long term Validation loss: 0.1459\n",
      "Epoch: [4936/10000] Training loss: 2.2381624247045177e-05 / Validation loss: 0.00023384204692086295 / Long term Validation loss: 0.1459\n",
      "Epoch: [4937/10000] Training loss: 2.237445861810975e-05 / Validation loss: 0.00023378599321440793 / Long term Validation loss: 0.1459\n",
      "Epoch: [4938/10000] Training loss: 2.236728972705082e-05 / Validation loss: 0.00023372990012884593 / Long term Validation loss: 0.1460\n",
      "Epoch: [4939/10000] Training loss: 2.236011753806014e-05 / Validation loss: 0.00023367367100088097 / Long term Validation loss: 0.1460\n",
      "Epoch: [4940/10000] Training loss: 2.2352942015489026e-05 / Validation loss: 0.0002336174038619642 / Long term Validation loss: 0.1460\n",
      "Epoch: [4941/10000] Training loss: 2.2345763124041823e-05 / Validation loss: 0.00023356100520001584 / Long term Validation loss: 0.1460\n",
      "Epoch: [4942/10000] Training loss: 2.233858082863794e-05 / Validation loss: 0.00023350454944959711 / Long term Validation loss: 0.1460\n",
      "Epoch: [4943/10000] Training loss: 2.2331395094602712e-05 / Validation loss: 0.00023344798234943085 / Long term Validation loss: 0.1460\n",
      "Epoch: [4944/10000] Training loss: 2.2324205887567766e-05 / Validation loss: 0.00023339132957368386 / Long term Validation loss: 0.1460\n",
      "Epoch: [4945/10000] Training loss: 2.2317013173595156e-05 / Validation loss: 0.00023333458913384482 / Long term Validation loss: 0.1460\n",
      "Epoch: [4946/10000] Training loss: 2.2309816919162205e-05 / Validation loss: 0.00023327773688026622 / Long term Validation loss: 0.1461\n",
      "Epoch: [4947/10000] Training loss: 2.2302617091191545e-05 / Validation loss: 0.00023322081378272298 / Long term Validation loss: 0.1461\n",
      "Epoch: [4948/10000] Training loss: 2.229541365712556e-05 / Validation loss: 0.00023316376328033525 / Long term Validation loss: 0.1461\n",
      "Epoch: [4949/10000] Training loss: 2.228820658487445e-05 / Validation loss: 0.00023310664669889714 / Long term Validation loss: 0.1461\n",
      "Epoch: [4950/10000] Training loss: 2.228099584295065e-05 / Validation loss: 0.0002330493999989287 / Long term Validation loss: 0.1461\n",
      "Epoch: [4951/10000] Training loss: 2.2273781400373624e-05 / Validation loss: 0.0002329920803477774 / Long term Validation loss: 0.1461\n",
      "Epoch: [4952/10000] Training loss: 2.226656322681757e-05 / Validation loss: 0.00023293463813937388 / Long term Validation loss: 0.1461\n",
      "Epoch: [4953/10000] Training loss: 2.225934129251858e-05 / Validation loss: 0.00023287710875424903 / Long term Validation loss: 0.1462\n",
      "Epoch: [4954/10000] Training loss: 2.225211556839137e-05 / Validation loss: 0.00023281946945724735 / Long term Validation loss: 0.1462\n",
      "Epoch: [4955/10000] Training loss: 2.2244886025973743e-05 / Validation loss: 0.0002327617270084524 / Long term Validation loss: 0.1462\n",
      "Epoch: [4956/10000] Training loss: 2.223765263748459e-05 / Validation loss: 0.00023270388708110016 / Long term Validation loss: 0.1462\n",
      "Epoch: [4957/10000] Training loss: 2.2230415375820374e-05 / Validation loss: 0.00023264593102162855 / Long term Validation loss: 0.1462\n",
      "Epoch: [4958/10000] Training loss: 2.2223174214547784e-05 / Validation loss: 0.0002325878860066407 / Long term Validation loss: 0.1462\n",
      "Epoch: [4959/10000] Training loss: 2.2215929127946615e-05 / Validation loss: 0.00023252971756624127 / Long term Validation loss: 0.1462\n",
      "Epoch: [4960/10000] Training loss: 2.220868009094806e-05 / Validation loss: 0.00023247146331232556 / Long term Validation loss: 0.1463\n",
      "Epoch: [4961/10000] Training loss: 2.220142707920427e-05 / Validation loss: 0.00023241308451725157 / Long term Validation loss: 0.1463\n",
      "Epoch: [4962/10000] Training loss: 2.219417006899346e-05 / Validation loss: 0.00023235461816758487 / Long term Validation loss: 0.1463\n",
      "Epoch: [4963/10000] Training loss: 2.2186909037290626e-05 / Validation loss: 0.00023229603119779768 / Long term Validation loss: 0.1463\n",
      "Epoch: [4964/10000] Training loss: 2.217964396166239e-05 / Validation loss: 0.00023223735177053189 / Long term Validation loss: 0.1463\n",
      "Epoch: [4965/10000] Training loss: 2.2172374820315492e-05 / Validation loss: 0.00023217855875554448 / Long term Validation loss: 0.1463\n",
      "Epoch: [4966/10000] Training loss: 2.2165101591999396e-05 / Validation loss: 0.00023211966732818395 / Long term Validation loss: 0.1463\n",
      "Epoch: [4967/10000] Training loss: 2.215782425601661e-05 / Validation loss: 0.0002320606705077896 / Long term Validation loss: 0.1464\n",
      "Epoch: [4968/10000] Training loss: 2.215054279214281e-05 / Validation loss: 0.00023200157011574662 / Long term Validation loss: 0.1464\n",
      "Epoch: [4969/10000] Training loss: 2.21432571805921e-05 / Validation loss: 0.00023194237220367794 / Long term Validation loss: 0.1464\n",
      "Epoch: [4970/10000] Training loss: 2.2135967401956262e-05 / Validation loss: 0.00023188306759192745 / Long term Validation loss: 0.1464\n",
      "Epoch: [4971/10000] Training loss: 2.2128673437126118e-05 / Validation loss: 0.0002318236721831948 / Long term Validation loss: 0.1464\n",
      "Epoch: [4972/10000] Training loss: 2.2121375267245274e-05 / Validation loss: 0.00023176416953666213 / Long term Validation loss: 0.1464\n",
      "Epoch: [4973/10000] Training loss: 2.2114072873594632e-05 / Validation loss: 0.00023170458145427205 / Long term Validation loss: 0.1465\n",
      "Epoch: [4974/10000] Training loss: 2.2106766237552342e-05 / Validation loss: 0.00023164488819235932 / Long term Validation loss: 0.1465\n",
      "Epoch: [4975/10000] Training loss: 2.2099455340451514e-05 / Validation loss: 0.00023158511372737482 / Long term Validation loss: 0.1465\n",
      "Epoch: [4976/10000] Training loss: 2.2092140163538048e-05 / Validation loss: 0.00023152523839407892 / Long term Validation loss: 0.1465\n",
      "Epoch: [4977/10000] Training loss: 2.208482068781158e-05 / Validation loss: 0.000231465285429827 / Long term Validation loss: 0.1465\n",
      "Epoch: [4978/10000] Training loss: 2.2077496893974326e-05 / Validation loss: 0.00023140523766344845 / Long term Validation loss: 0.1465\n",
      "Epoch: [4979/10000] Training loss: 2.2070168762264787e-05 / Validation loss: 0.00023134511569395904 / Long term Validation loss: 0.1465\n",
      "Epoch: [4980/10000] Training loss: 2.2062836272393277e-05 / Validation loss: 0.00023128490623749372 / Long term Validation loss: 0.1466\n",
      "Epoch: [4981/10000] Training loss: 2.205549940337605e-05 / Validation loss: 0.00023122462630232305 / Long term Validation loss: 0.1466\n",
      "Epoch: [4982/10000] Training loss: 2.2048158133456443e-05 / Validation loss: 0.0002311642670178917 / Long term Validation loss: 0.1466\n",
      "Epoch: [4983/10000] Training loss: 2.2040812439945105e-05 / Validation loss: 0.00023110384158170572 / Long term Validation loss: 0.1466\n",
      "Epoch: [4984/10000] Training loss: 2.2033462299128592e-05 / Validation loss: 0.000231043345444662 / Long term Validation loss: 0.1466\n",
      "Epoch: [4985/10000] Training loss: 2.2026107686120013e-05 / Validation loss: 0.00023098278824556546 / Long term Validation loss: 0.1466\n",
      "Epoch: [4986/10000] Training loss: 2.2018748574758897e-05 / Validation loss: 0.00023092216930266818 / Long term Validation loss: 0.1467\n",
      "Epoch: [4987/10000] Training loss: 2.201138493747574e-05 / Validation loss: 0.0002308614951803102 / Long term Validation loss: 0.1467\n",
      "Epoch: [4988/10000] Training loss: 2.200401674518822e-05 / Validation loss: 0.00023080076846027355 / Long term Validation loss: 0.1467\n",
      "Epoch: [4989/10000] Training loss: 2.199664396718264e-05 / Validation loss: 0.00023073999316293304 / Long term Validation loss: 0.1467\n",
      "Epoch: [4990/10000] Training loss: 2.1989266571012e-05 / Validation loss: 0.00023067917453295283 / Long term Validation loss: 0.1467\n",
      "Epoch: [4991/10000] Training loss: 2.198188452239728e-05 / Validation loss: 0.00023061831449939726 / Long term Validation loss: 0.1467\n",
      "Epoch: [4992/10000] Training loss: 2.197449778513285e-05 / Validation loss: 0.000230557420469669 / Long term Validation loss: 0.1468\n",
      "Epoch: [4993/10000] Training loss: 2.1967106321009917e-05 / Validation loss: 0.00023049649258453513 / Long term Validation loss: 0.1468\n",
      "Epoch: [4994/10000] Training loss: 2.1959710089734494e-05 / Validation loss: 0.00023043554006968092 / Long term Validation loss: 0.1468\n",
      "Epoch: [4995/10000] Training loss: 2.1952309048875307e-05 / Validation loss: 0.00023037456139351196 / Long term Validation loss: 0.1468\n",
      "Epoch: [4996/10000] Training loss: 2.1944903153797868e-05 / Validation loss: 0.00023031356744119077 / Long term Validation loss: 0.1468\n",
      "Epoch: [4997/10000] Training loss: 2.193749235763904e-05 / Validation loss: 0.00023025255491558317 / Long term Validation loss: 0.1468\n",
      "Epoch: [4998/10000] Training loss: 2.1930076611259598e-05 / Validation loss: 0.00023019153641079402 / Long term Validation loss: 0.1469\n",
      "Epoch: [4999/10000] Training loss: 2.1922655863246987e-05 / Validation loss: 0.00023013050653850406 / Long term Validation loss: 0.1469\n",
      "Epoch: [5000/10000] Training loss: 2.191523005988693e-05 / Validation loss: 0.0002300694798923477 / Long term Validation loss: 0.1469\n",
      "Epoch: [5001/10000] Training loss: 2.1907799145195316e-05 / Validation loss: 0.00023000844839457995 / Long term Validation loss: 0.1469\n",
      "Epoch: [5002/10000] Training loss: 2.190036306090785e-05 / Validation loss: 0.00022994742923030517 / Long term Validation loss: 0.1469\n",
      "Epoch: [5003/10000] Training loss: 2.1892921746541154e-05 / Validation loss: 0.00022988641068681862 / Long term Validation loss: 0.1469\n",
      "Epoch: [5004/10000] Training loss: 2.188547513939756e-05 / Validation loss: 0.00022982541354084435 / Long term Validation loss: 0.1470\n",
      "Epoch: [5005/10000] Training loss: 2.1878023174655343e-05 / Validation loss: 0.00022976442101886585 / Long term Validation loss: 0.1470\n",
      "Epoch: [5006/10000] Training loss: 2.187056578538307e-05 / Validation loss: 0.0002297034590779261 / Long term Validation loss: 0.1470\n",
      "Epoch: [5007/10000] Training loss: 2.1863102902659362e-05 / Validation loss: 0.0002296425037511806 / Long term Validation loss: 0.1470\n",
      "Epoch: [5008/10000] Training loss: 2.185563445558916e-05 / Validation loss: 0.00022958158865063504 / Long term Validation loss: 0.1470\n",
      "Epoch: [5009/10000] Training loss: 2.184816037145358e-05 / Validation loss: 0.00022952067940149772 / Long term Validation loss: 0.1470\n",
      "Epoch: [5010/10000] Training loss: 2.184068057571796e-05 / Validation loss: 0.0002294598211186047 / Long term Validation loss: 0.1471\n",
      "Epoch: [5011/10000] Training loss: 2.1833194992214787e-05 / Validation loss: 0.0002293989641042716 / Long term Validation loss: 0.1471\n",
      "Epoch: [5012/10000] Training loss: 2.1825703543129613e-05 / Validation loss: 0.0002293381709973038 / Long term Validation loss: 0.1471\n",
      "Epoch: [5013/10000] Training loss: 2.1818206149223543e-05 / Validation loss: 0.0002292773691400659 / Long term Validation loss: 0.1471\n",
      "Epoch: [5014/10000] Training loss: 2.1810702729778515e-05 / Validation loss: 0.00022921664821285497 / Long term Validation loss: 0.1471\n",
      "Epoch: [5015/10000] Training loss: 2.1803193202872244e-05 / Validation loss: 0.0002291559005356405 / Long term Validation loss: 0.1472\n",
      "Epoch: [5016/10000] Training loss: 2.1795677485257107e-05 / Validation loss: 0.00022909525805531221 / Long term Validation loss: 0.1472\n",
      "Epoch: [5017/10000] Training loss: 2.1788155492710262e-05 / Validation loss: 0.0002290345587130152 / Long term Validation loss: 0.1472\n",
      "Epoch: [5018/10000] Training loss: 2.1780627139809e-05 / Validation loss: 0.00022897400139505132 / Long term Validation loss: 0.1472\n",
      "Epoch: [5019/10000] Training loss: 2.1773092340395482e-05 / Validation loss: 0.00022891333812734792 / Long term Validation loss: 0.1472\n",
      "Epoch: [5020/10000] Training loss: 2.176555100719193e-05 / Validation loss: 0.00022885287526145298 / Long term Validation loss: 0.1473\n",
      "Epoch: [5021/10000] Training loss: 2.1758003052446216e-05 / Validation loss: 0.00022879222677037867 / Long term Validation loss: 0.1473\n",
      "Epoch: [5022/10000] Training loss: 2.1750448387298208e-05 / Validation loss: 0.00022873187395536893 / Long term Validation loss: 0.1473\n",
      "Epoch: [5023/10000] Training loss: 2.1742886922716642e-05 / Validation loss: 0.0002286712053055284 / Long term Validation loss: 0.1473\n",
      "Epoch: [5024/10000] Training loss: 2.1735318568473323e-05 / Validation loss: 0.00022861099100720324 / Long term Validation loss: 0.1473\n",
      "Epoch: [5025/10000] Training loss: 2.1727743234556013e-05 / Validation loss: 0.00022855024539566426 / Long term Validation loss: 0.1474\n",
      "Epoch: [5026/10000] Training loss: 2.172016082951383e-05 / Validation loss: 0.00022849022256080597 / Long term Validation loss: 0.1474\n",
      "Epoch: [5027/10000] Training loss: 2.171257126265815e-05 / Validation loss: 0.00022842930639738396 / Long term Validation loss: 0.1474\n",
      "Epoch: [5028/10000] Training loss: 2.1704974441382345e-05 / Validation loss: 0.00022836957328151552 / Long term Validation loss: 0.1474\n",
      "Epoch: [5029/10000] Training loss: 2.169737027468601e-05 / Validation loss: 0.00022830832885831277 / Long term Validation loss: 0.1474\n",
      "Epoch: [5030/10000] Training loss: 2.1689758668795167e-05 / Validation loss: 0.0002282490668968887 / Long term Validation loss: 0.1475\n",
      "Epoch: [5031/10000] Training loss: 2.1682139532946602e-05 / Validation loss: 0.00022818722183097566 / Long term Validation loss: 0.1475\n",
      "Epoch: [5032/10000] Training loss: 2.167451277215987e-05 / Validation loss: 0.00022812876546091337 / Long term Validation loss: 0.1475\n",
      "Epoch: [5033/10000] Training loss: 2.1666878296964206e-05 / Validation loss: 0.00022806583823194057 / Long term Validation loss: 0.1475\n",
      "Epoch: [5034/10000] Training loss: 2.1659236011376065e-05 / Validation loss: 0.0002280088053500603 / Long term Validation loss: 0.1476\n",
      "Epoch: [5035/10000] Training loss: 2.1651585829705774e-05 / Validation loss: 0.00022794392695828697 / Long term Validation loss: 0.1476\n",
      "Epoch: [5036/10000] Training loss: 2.164392765654138e-05 / Validation loss: 0.00022788946581548222 / Long term Validation loss: 0.1476\n",
      "Epoch: [5037/10000] Training loss: 2.1636261416817915e-05 / Validation loss: 0.0002278210394312819 / Long term Validation loss: 0.1476\n",
      "Epoch: [5038/10000] Training loss: 2.162858702302291e-05 / Validation loss: 0.0002277713016742318 / Long term Validation loss: 0.1477\n",
      "Epoch: [5039/10000] Training loss: 2.162090443176543e-05 / Validation loss: 0.00022769634592405822 / Long term Validation loss: 0.1477\n",
      "Epoch: [5040/10000] Training loss: 2.1613213593160538e-05 / Validation loss: 0.00022765540384743515 / Long term Validation loss: 0.1477\n",
      "Epoch: [5041/10000] Training loss: 2.1605514565666905e-05 / Validation loss: 0.000227568271537367 / Long term Validation loss: 0.1477\n",
      "Epoch: [5042/10000] Training loss: 2.159780745242546e-05 / Validation loss: 0.00022754391771868888 / Long term Validation loss: 0.1478\n",
      "Epoch: [5043/10000] Training loss: 2.159009266172199e-05 / Validation loss: 0.00022743376821790485 / Long term Validation loss: 0.1477\n",
      "Epoch: [5044/10000] Training loss: 2.1582370894045813e-05 / Validation loss: 0.00022744108779800137 / Long term Validation loss: 0.1478\n",
      "Epoch: [5045/10000] Training loss: 2.1574643822723216e-05 / Validation loss: 0.00022728684613866002 / Long term Validation loss: 0.1478\n",
      "Epoch: [5046/10000] Training loss: 2.1566914470134558e-05 / Validation loss: 0.00022735539141652348 / Long term Validation loss: 0.1479\n",
      "Epoch: [5047/10000] Training loss: 2.1559189271014234e-05 / Validation loss: 0.00022711558865818 / Long term Validation loss: 0.1478\n",
      "Epoch: [5048/10000] Training loss: 2.1551480369172392e-05 / Validation loss: 0.0002273039628524775 / Long term Validation loss: 0.1480\n",
      "Epoch: [5049/10000] Training loss: 2.1543812714887837e-05 / Validation loss: 0.00022689605753523244 / Long term Validation loss: 0.1478\n",
      "Epoch: [5050/10000] Training loss: 2.153623490993007e-05 / Validation loss: 0.000227321939978196 / Long term Validation loss: 0.1481\n",
      "Epoch: [5051/10000] Training loss: 2.1528845970025698e-05 / Validation loss: 0.0002265798718086204 / Long term Validation loss: 0.1477\n",
      "Epoch: [5052/10000] Training loss: 2.152184321755748e-05 / Validation loss: 0.00022748273024923745 / Long term Validation loss: 0.1484\n",
      "Epoch: [5053/10000] Training loss: 2.151562967489896e-05 / Validation loss: 0.0002260692967315529 / Long term Validation loss: 0.1475\n",
      "Epoch: [5054/10000] Training loss: 2.1511022658799287e-05 / Validation loss: 0.00022794371949244943 / Long term Validation loss: 0.1488\n",
      "Epoch: [5055/10000] Training loss: 2.1509703586959738e-05 / Validation loss: 0.0002251697594200551 / Long term Validation loss: 0.1473\n",
      "Epoch: [5056/10000] Training loss: 2.151513220602216e-05 / Validation loss: 0.00022905595026119323 / Long term Validation loss: 0.1495\n",
      "Epoch: [5057/10000] Training loss: 2.15344901091016e-05 / Validation loss: 0.00022351248912267144 / Long term Validation loss: 0.1468\n",
      "Epoch: [5058/10000] Training loss: 2.158274293062112e-05 / Validation loss: 0.00023165285840859933 / Long term Validation loss: 0.1504\n",
      "Epoch: [5059/10000] Training loss: 2.1691223109544687e-05 / Validation loss: 0.00022049890340121746 / Long term Validation loss: 0.1457\n",
      "Epoch: [5060/10000] Training loss: 2.192589329116775e-05 / Validation loss: 0.00023790622064637264 / Long term Validation loss: 0.1519\n",
      "Epoch: [5061/10000] Training loss: 2.2425409041983352e-05 / Validation loss: 0.0002156927051035361 / Long term Validation loss: 0.1380\n",
      "Epoch: [5062/10000] Training loss: 2.3482171175856238e-05 / Validation loss: 0.000254145575605255 / Long term Validation loss: 0.1476\n",
      "Epoch: [5063/10000] Training loss: 2.5700960627625695e-05 / Validation loss: 0.00021190138760369446 / Long term Validation loss: 0.1215\n",
      "Epoch: [5064/10000] Training loss: 3.031075602541352e-05 / Validation loss: 0.00029981613876897196 / Long term Validation loss: 0.1430\n",
      "Epoch: [5065/10000] Training loss: 3.9587252937665135e-05 / Validation loss: 0.0002279105525645735 / Long term Validation loss: 0.1136\n",
      "Epoch: [5066/10000] Training loss: 5.7125756868193024e-05 / Validation loss: 0.00041661697240610233 / Long term Validation loss: 0.1415\n",
      "Epoch: [5067/10000] Training loss: 8.555600918145364e-05 / Validation loss: 0.0002935155496706418 / Long term Validation loss: 0.1202\n",
      "Epoch: [5068/10000] Training loss: 0.00011850116317390936 / Validation loss: 0.0005144219518282178 / Long term Validation loss: 0.1450\n",
      "Epoch: [5069/10000] Training loss: 0.00012797444652646678 / Validation loss: 0.00025828502313682066 / Long term Validation loss: 0.1166\n",
      "Epoch: [5070/10000] Training loss: 8.852027321661902e-05 / Validation loss: 0.00028303070643333135 / Long term Validation loss: 0.1431\n",
      "Epoch: [5071/10000] Training loss: 3.305440779419654e-05 / Validation loss: 0.00026040051221297797 / Long term Validation loss: 0.1449\n",
      "Epoch: [5072/10000] Training loss: 2.642133591355399e-05 / Validation loss: 0.00023266946100005255 / Long term Validation loss: 0.1125\n",
      "Epoch: [5073/10000] Training loss: 6.23022790756571e-05 / Validation loss: 0.00039760430878719755 / Long term Validation loss: 0.1405\n",
      "Epoch: [5074/10000] Training loss: 7.471345076131421e-05 / Validation loss: 0.0002184420764964551 / Long term Validation loss: 0.1161\n",
      "Epoch: [5075/10000] Training loss: 4.2413294764703804e-05 / Validation loss: 0.00022847557893443157 / Long term Validation loss: 0.1504\n",
      "Epoch: [5076/10000] Training loss: 2.150635928681916e-05 / Validation loss: 0.0003123159527829171 / Long term Validation loss: 0.1424\n",
      "Epoch: [5077/10000] Training loss: 4.137663269388765e-05 / Validation loss: 0.0002277428750301645 / Long term Validation loss: 0.1127\n",
      "Epoch: [5078/10000] Training loss: 5.524125323363748e-05 / Validation loss: 0.0002938635119046395 / Long term Validation loss: 0.1430\n",
      "Epoch: [5079/10000] Training loss: 3.519743390826024e-05 / Validation loss: 0.00023528441624128736 / Long term Validation loss: 0.1518\n",
      "Epoch: [5080/10000] Training loss: 2.1581726469264884e-05 / Validation loss: 0.00021584745100520987 / Long term Validation loss: 0.1181\n",
      "Epoch: [5081/10000] Training loss: 3.6009748866078734e-05 / Validation loss: 0.00031543485451442744 / Long term Validation loss: 0.1422\n",
      "Epoch: [5082/10000] Training loss: 4.26635640027908e-05 / Validation loss: 0.00021546664373336505 / Long term Validation loss: 0.1264\n",
      "Epoch: [5083/10000] Training loss: 2.755460765153337e-05 / Validation loss: 0.000221975935794397 / Long term Validation loss: 0.1462\n",
      "Epoch: [5084/10000] Training loss: 2.240652562112504e-05 / Validation loss: 0.0002871991137412756 / Long term Validation loss: 0.1434\n",
      "Epoch: [5085/10000] Training loss: 3.362059386012818e-05 / Validation loss: 0.0002153220979749305 / Long term Validation loss: 0.1185\n",
      "Epoch: [5086/10000] Training loss: 3.3802656332086294e-05 / Validation loss: 0.0002455244545148516 / Long term Validation loss: 0.1509\n",
      "Epoch: [5087/10000] Training loss: 2.30154813139548e-05 / Validation loss: 0.00024934522316472785 / Long term Validation loss: 0.1495\n",
      "Epoch: [5088/10000] Training loss: 2.3826266107079723e-05 / Validation loss: 0.00021427851826588977 / Long term Validation loss: 0.1199\n",
      "Epoch: [5089/10000] Training loss: 3.106953699612507e-05 / Validation loss: 0.00026360641749711847 / Long term Validation loss: 0.1450\n",
      "Epoch: [5090/10000] Training loss: 2.748776326939933e-05 / Validation loss: 0.0002253755746790931 / Long term Validation loss: 0.1487\n",
      "Epoch: [5091/10000] Training loss: 2.1414380476908428e-05 / Validation loss: 0.00021518782962363277 / Long term Validation loss: 0.1306\n",
      "Epoch: [5092/10000] Training loss: 2.486822832703532e-05 / Validation loss: 0.0002642028365795117 / Long term Validation loss: 0.1450\n",
      "Epoch: [5093/10000] Training loss: 2.8050837698693796e-05 / Validation loss: 0.00021617953170108674 / Long term Validation loss: 0.1359\n",
      "Epoch: [5094/10000] Training loss: 2.3655812027992312e-05 / Validation loss: 0.00022295604874456547 / Long term Validation loss: 0.1477\n",
      "Epoch: [5095/10000] Training loss: 2.1493892174837933e-05 / Validation loss: 0.0002514478069310398 / Long term Validation loss: 0.1489\n",
      "Epoch: [5096/10000] Training loss: 2.4985395049099494e-05 / Validation loss: 0.00021412569303935316 / Long term Validation loss: 0.1279\n",
      "Epoch: [5097/10000] Training loss: 2.526969390411221e-05 / Validation loss: 0.0002343905436672163 / Long term Validation loss: 0.1514\n",
      "Epoch: [5098/10000] Training loss: 2.1843533983211825e-05 / Validation loss: 0.00023567092010508484 / Long term Validation loss: 0.1515\n",
      "Epoch: [5099/10000] Training loss: 2.2031276370590668e-05 / Validation loss: 0.00021446804361700643 / Long term Validation loss: 0.1308\n",
      "Epoch: [5100/10000] Training loss: 2.4342198670603716e-05 / Validation loss: 0.0002424209588048477 / Long term Validation loss: 0.1522\n",
      "Epoch: [5101/10000] Training loss: 2.3219401766275687e-05 / Validation loss: 0.00022414758413011565 / Long term Validation loss: 0.1483\n",
      "Epoch: [5102/10000] Training loss: 2.128514251211941e-05 / Validation loss: 0.0002173933259757189 / Long term Validation loss: 0.1422\n",
      "Epoch: [5103/10000] Training loss: 2.237053066405468e-05 / Validation loss: 0.00024311530088064832 / Long term Validation loss: 0.1520\n",
      "Epoch: [5104/10000] Training loss: 2.338748342070469e-05 / Validation loss: 0.00021847514288343654 / Long term Validation loss: 0.1444\n",
      "Epoch: [5105/10000] Training loss: 2.2001815727326037e-05 / Validation loss: 0.0002234352607162275 / Long term Validation loss: 0.1481\n",
      "Epoch: [5106/10000] Training loss: 2.1283773763883318e-05 / Validation loss: 0.00023775428446571256 / Long term Validation loss: 0.1524\n",
      "Epoch: [5107/10000] Training loss: 2.2372711498246447e-05 / Validation loss: 0.0002169225822248419 / Long term Validation loss: 0.1411\n",
      "Epoch: [5108/10000] Training loss: 2.2506474321712574e-05 / Validation loss: 0.00023034387475608366 / Long term Validation loss: 0.1513\n",
      "Epoch: [5109/10000] Training loss: 2.1419168341186937e-05 / Validation loss: 0.00023031531047051608 / Long term Validation loss: 0.1514\n",
      "Epoch: [5110/10000] Training loss: 2.1407873283853167e-05 / Validation loss: 0.00021786386225049501 / Long term Validation loss: 0.1436\n",
      "Epoch: [5111/10000] Training loss: 2.2150783521366612e-05 / Validation loss: 0.00023471152483168768 / Long term Validation loss: 0.1520\n",
      "Epoch: [5112/10000] Training loss: 2.1863177913088833e-05 / Validation loss: 0.000224221762187228 / Long term Validation loss: 0.1484\n",
      "Epoch: [5113/10000] Training loss: 2.1207467064801927e-05 / Validation loss: 0.00022086229717228095 / Long term Validation loss: 0.1471\n",
      "Epoch: [5114/10000] Training loss: 2.147747373439623e-05 / Validation loss: 0.0002348791628765458 / Long term Validation loss: 0.1520\n",
      "Epoch: [5115/10000] Training loss: 2.1854180068341874e-05 / Validation loss: 0.00022086031106183918 / Long term Validation loss: 0.1471\n",
      "Epoch: [5116/10000] Training loss: 2.146717406210531e-05 / Validation loss: 0.00022524280471798572 / Long term Validation loss: 0.1489\n",
      "Epoch: [5117/10000] Training loss: 2.116068854787609e-05 / Validation loss: 0.00023171868694365377 / Long term Validation loss: 0.1517\n",
      "Epoch: [5118/10000] Training loss: 2.146291348797091e-05 / Validation loss: 0.00022000734829443524 / Long term Validation loss: 0.1468\n",
      "Epoch: [5119/10000] Training loss: 2.1583664255601556e-05 / Validation loss: 0.00022922461347891517 / Long term Validation loss: 0.1511\n",
      "Epoch: [5120/10000] Training loss: 2.125259890620716e-05 / Validation loss: 0.00022738501477467745 / Long term Validation loss: 0.1506\n",
      "Epoch: [5121/10000] Training loss: 2.1160325704196287e-05 / Validation loss: 0.00022102315209319184 / Long term Validation loss: 0.1473\n",
      "Epoch: [5122/10000] Training loss: 2.139314858362704e-05 / Validation loss: 0.00023101945470773054 / Long term Validation loss: 0.1515\n",
      "Epoch: [5123/10000] Training loss: 2.1376964286562912e-05 / Validation loss: 0.00022378940970358178 / Long term Validation loss: 0.1484\n",
      "Epoch: [5124/10000] Training loss: 2.1145779948755237e-05 / Validation loss: 0.00022332929574257206 / Long term Validation loss: 0.1482\n",
      "Epoch: [5125/10000] Training loss: 2.1156691105270805e-05 / Validation loss: 0.00023018250011992052 / Long term Validation loss: 0.1514\n",
      "Epoch: [5126/10000] Training loss: 2.1303474488769414e-05 / Validation loss: 0.00022185731394581113 / Long term Validation loss: 0.1477\n",
      "Epoch: [5127/10000] Training loss: 2.1234578716045653e-05 / Validation loss: 0.00022600891376190472 / Long term Validation loss: 0.1500\n",
      "Epoch: [5128/10000] Training loss: 2.109245699437777e-05 / Validation loss: 0.00022765242183843756 / Long term Validation loss: 0.1507\n",
      "Epoch: [5129/10000] Training loss: 2.1138338914148173e-05 / Validation loss: 0.00022161476936499467 / Long term Validation loss: 0.1477\n",
      "Epoch: [5130/10000] Training loss: 2.1217177636315464e-05 / Validation loss: 0.00022781130469075927 / Long term Validation loss: 0.1508\n",
      "Epoch: [5131/10000] Training loss: 2.1140804736989317e-05 / Validation loss: 0.00022483898005563093 / Long term Validation loss: 0.1494\n",
      "Epoch: [5132/10000] Training loss: 2.1061014780584096e-05 / Validation loss: 0.00022263418308282628 / Long term Validation loss: 0.1480\n",
      "Epoch: [5133/10000] Training loss: 2.110862995984862e-05 / Validation loss: 0.00022794470071234237 / Long term Validation loss: 0.1508\n",
      "Epoch: [5134/10000] Training loss: 2.1143738254170125e-05 / Validation loss: 0.0002228196530929722 / Long term Validation loss: 0.1482\n",
      "Epoch: [5135/10000] Training loss: 2.1078452324130043e-05 / Validation loss: 0.0002242617933682275 / Long term Validation loss: 0.1492\n",
      "Epoch: [5136/10000] Training loss: 2.103626353788276e-05 / Validation loss: 0.00022661858573792986 / Long term Validation loss: 0.1505\n",
      "Epoch: [5137/10000] Training loss: 2.107356327880727e-05 / Validation loss: 0.0002220467341023923 / Long term Validation loss: 0.1479\n",
      "Epoch: [5138/10000] Training loss: 2.108430602765598e-05 / Validation loss: 0.0002256535924801216 / Long term Validation loss: 0.1502\n",
      "Epoch: [5139/10000] Training loss: 2.10342177984971e-05 / Validation loss: 0.0002247119099104489 / Long term Validation loss: 0.1498\n",
      "Epoch: [5140/10000] Training loss: 2.1012275030597924e-05 / Validation loss: 0.00022240310343790975 / Long term Validation loss: 0.1482\n",
      "Epoch: [5141/10000] Training loss: 2.10374804051446e-05 / Validation loss: 0.00022609048008980336 / Long term Validation loss: 0.1505\n",
      "Epoch: [5142/10000] Training loss: 2.1036263860194443e-05 / Validation loss: 0.0002231022609261069 / Long term Validation loss: 0.1488\n",
      "Epoch: [5143/10000] Training loss: 2.0999576386794515e-05 / Validation loss: 0.0002233999108937521 / Long term Validation loss: 0.1491\n",
      "Epoch: [5144/10000] Training loss: 2.0987449610616964e-05 / Validation loss: 0.00022544332624929416 / Long term Validation loss: 0.1504\n",
      "Epoch: [5145/10000] Training loss: 2.100259781787856e-05 / Validation loss: 0.00022229466969638298 / Long term Validation loss: 0.1485\n",
      "Epoch: [5146/10000] Training loss: 2.0996366362459528e-05 / Validation loss: 0.00022439016493376704 / Long term Validation loss: 0.1500\n",
      "Epoch: [5147/10000] Training loss: 2.0969717605370324e-05 / Validation loss: 0.0002242018675697571 / Long term Validation loss: 0.1500\n",
      "Epoch: [5148/10000] Training loss: 2.096181124197202e-05 / Validation loss: 0.00022235784731954573 / Long term Validation loss: 0.1488\n",
      "Epoch: [5149/10000] Training loss: 2.0969681929669097e-05 / Validation loss: 0.0002248140769486103 / Long term Validation loss: 0.1504\n",
      "Epoch: [5150/10000] Training loss: 2.0961869980172126e-05 / Validation loss: 0.00022303212199268713 / Long term Validation loss: 0.1495\n",
      "Epoch: [5151/10000] Training loss: 2.094215780689406e-05 / Validation loss: 0.0002229820031074894 / Long term Validation loss: 0.1495\n",
      "Epoch: [5152/10000] Training loss: 2.0935749561284157e-05 / Validation loss: 0.00022447618230581813 / Long term Validation loss: 0.1503\n",
      "Epoch: [5153/10000] Training loss: 2.093870534840381e-05 / Validation loss: 0.00022237367825900654 / Long term Validation loss: 0.1492\n",
      "Epoch: [5154/10000] Training loss: 2.0930797845011132e-05 / Validation loss: 0.00022365642288864262 / Long term Validation loss: 0.1500\n",
      "Epoch: [5155/10000] Training loss: 2.0915680639182452e-05 / Validation loss: 0.00022363803167599523 / Long term Validation loss: 0.1500\n",
      "Epoch: [5156/10000] Training loss: 2.0909581651680273e-05 / Validation loss: 0.00022233084863958208 / Long term Validation loss: 0.1494\n",
      "Epoch: [5157/10000] Training loss: 2.090931826840333e-05 / Validation loss: 0.00022394205027697454 / Long term Validation loss: 0.1503\n",
      "Epoch: [5158/10000] Training loss: 2.0901821860723115e-05 / Validation loss: 0.00022278496671812475 / Long term Validation loss: 0.1497\n",
      "Epoch: [5159/10000] Training loss: 2.0889692934416348e-05 / Validation loss: 0.00022271138265256244 / Long term Validation loss: 0.1497\n",
      "Epoch: [5160/10000] Training loss: 2.0883465775709197e-05 / Validation loss: 0.00022368972815055939 / Long term Validation loss: 0.1503\n",
      "Epoch: [5161/10000] Training loss: 2.088110842744513e-05 / Validation loss: 0.00022228414248149728 / Long term Validation loss: 0.1496\n",
      "Epoch: [5162/10000] Training loss: 2.0874095683422208e-05 / Validation loss: 0.00022313866000144898 / Long term Validation loss: 0.1500\n",
      "Epoch: [5163/10000] Training loss: 2.0863911576384784e-05 / Validation loss: 0.00022307523353631077 / Long term Validation loss: 0.1500\n",
      "Epoch: [5164/10000] Training loss: 2.0857455687708237e-05 / Validation loss: 0.00022222362966622909 / Long term Validation loss: 0.1496\n",
      "Epoch: [5165/10000] Training loss: 2.0853720559259654e-05 / Validation loss: 0.00022327301048455242 / Long term Validation loss: 0.1503\n",
      "Epoch: [5166/10000] Training loss: 2.0847106467910828e-05 / Validation loss: 0.0002224430382001237 / Long term Validation loss: 0.1498\n",
      "Epoch: [5167/10000] Training loss: 2.0838204675599422e-05 / Validation loss: 0.00022244988653717939 / Long term Validation loss: 0.1499\n",
      "Epoch: [5168/10000] Training loss: 2.0831552531866026e-05 / Validation loss: 0.00022301697176430835 / Long term Validation loss: 0.1503\n",
      "Epoch: [5169/10000] Training loss: 2.082689270621463e-05 / Validation loss: 0.0002220727879005712 / Long term Validation loss: 0.1498\n",
      "Epoch: [5170/10000] Training loss: 2.0820551201728338e-05 / Validation loss: 0.0002226864596638995 / Long term Validation loss: 0.1502\n",
      "Epoch: [5171/10000] Training loss: 2.0812513762728707e-05 / Validation loss: 0.0002225283663432441 / Long term Validation loss: 0.1501\n",
      "Epoch: [5172/10000] Training loss: 2.0805737127544498e-05 / Validation loss: 0.00022202725509762596 / Long term Validation loss: 0.1499\n",
      "Epoch: [5173/10000] Training loss: 2.0800440019161936e-05 / Validation loss: 0.0002226962287834823 / Long term Validation loss: 0.1503\n",
      "Epoch: [5174/10000] Training loss: 2.0794254377129903e-05 / Validation loss: 0.00022206420331760625 / Long term Validation loss: 0.1500\n",
      "Epoch: [5175/10000] Training loss: 2.0786809174161375e-05 / Validation loss: 0.00022216348802507269 / Long term Validation loss: 0.1501\n",
      "Epoch: [5176/10000] Training loss: 2.077997977959232e-05 / Validation loss: 0.00022242854673429327 / Long term Validation loss: 0.1503\n",
      "Epoch: [5177/10000] Training loss: 2.0774235651051523e-05 / Validation loss: 0.00022180863111925336 / Long term Validation loss: 0.1500\n",
      "Epoch: [5178/10000] Training loss: 2.0768110316183693e-05 / Validation loss: 0.00022226221364808724 / Long term Validation loss: 0.1503\n",
      "Epoch: [5179/10000] Training loss: 2.076107405139774e-05 / Validation loss: 0.00022202711643363692 / Long term Validation loss: 0.1502\n",
      "Epoch: [5180/10000] Training loss: 2.0754249236435013e-05 / Validation loss: 0.0002217810394411768 / Long term Validation loss: 0.1501\n",
      "Epoch: [5181/10000] Training loss: 2.0748189853644926e-05 / Validation loss: 0.00022217481471434054 / Long term Validation loss: 0.1504\n",
      "Epoch: [5182/10000] Training loss: 2.0742056609780173e-05 / Validation loss: 0.00022169070966732626 / Long term Validation loss: 0.1502\n",
      "Epoch: [5183/10000] Training loss: 2.073529768801645e-05 / Validation loss: 0.00022185082252350455 / Long term Validation loss: 0.1503\n",
      "Epoch: [5184/10000] Training loss: 2.072851742135811e-05 / Validation loss: 0.0002219045384983274 / Long term Validation loss: 0.1504\n",
      "Epoch: [5185/10000] Training loss: 2.0722240932442385e-05 / Validation loss: 0.00022152717400265856 / Long term Validation loss: 0.1503\n",
      "Epoch: [5186/10000] Training loss: 2.0716055084902795e-05 / Validation loss: 0.00022184807258206747 / Long term Validation loss: 0.1505\n",
      "Epoch: [5187/10000] Training loss: 2.0709476175712773e-05 / Validation loss: 0.0002215788921269419 / Long term Validation loss: 0.1504\n",
      "Epoch: [5188/10000] Training loss: 2.0702762637357992e-05 / Validation loss: 0.0002215067825373638 / Long term Validation loss: 0.1504\n",
      "Epoch: [5189/10000] Training loss: 2.0696344794390874e-05 / Validation loss: 0.00022169097696157083 / Long term Validation loss: 0.1506\n",
      "Epoch: [5190/10000] Training loss: 2.069008391252612e-05 / Validation loss: 0.00022133828535392104 / Long term Validation loss: 0.1504\n",
      "Epoch: [5191/10000] Training loss: 2.0683609728624e-05 / Validation loss: 0.0002215102509366498 / Long term Validation loss: 0.1506\n",
      "Epoch: [5192/10000] Training loss: 2.0676969709939957e-05 / Validation loss: 0.00022142673681141114 / Long term Validation loss: 0.1506\n",
      "Epoch: [5193/10000] Training loss: 2.0670469817066284e-05 / Validation loss: 0.00022123148551884253 / Long term Validation loss: 0.1505\n",
      "Epoch: [5194/10000] Training loss: 2.0664128434612857e-05 / Validation loss: 0.00022142477614516267 / Long term Validation loss: 0.1508\n",
      "Epoch: [5195/10000] Training loss: 2.0657702484583273e-05 / Validation loss: 0.00022116821071251153 / Long term Validation loss: 0.1506\n",
      "Epoch: [5196/10000] Training loss: 2.0651129132790337e-05 / Validation loss: 0.00022119788357267189 / Long term Validation loss: 0.1508\n",
      "Epoch: [5197/10000] Training loss: 2.064459161795169e-05 / Validation loss: 0.00022122610425579154 / Long term Validation loss: 0.1509\n",
      "Epoch: [5198/10000] Training loss: 2.0638177995009493e-05 / Validation loss: 0.00022099806114172116 / Long term Validation loss: 0.1507\n",
      "Epoch: [5199/10000] Training loss: 2.063175943466668e-05 / Validation loss: 0.00022113556639264043 / Long term Validation loss: 0.1510\n",
      "Epoch: [5200/10000] Training loss: 2.0625236970037275e-05 / Validation loss: 0.00022098253673042396 / Long term Validation loss: 0.1509\n",
      "Epoch: [5201/10000] Training loss: 2.061869176462784e-05 / Validation loss: 0.00022091451685351944 / Long term Validation loss: 0.1510\n",
      "Epoch: [5202/10000] Training loss: 2.061222227478032e-05 / Validation loss: 0.00022098711589784326 / Long term Validation loss: 0.1511\n",
      "Epoch: [5203/10000] Training loss: 2.0605786077208876e-05 / Validation loss: 0.0002207819711906676 / Long term Validation loss: 0.1510\n",
      "Epoch: [5204/10000] Training loss: 2.059929360558103e-05 / Validation loss: 0.00022084787301398704 / Long term Validation loss: 0.1512\n",
      "Epoch: [5205/10000] Training loss: 2.0592757080149462e-05 / Validation loss: 0.00022077478062134976 / Long term Validation loss: 0.1512\n",
      "Epoch: [5206/10000] Training loss: 2.0586251349685415e-05 / Validation loss: 0.00022065688358804145 / Long term Validation loss: 0.1512\n",
      "Epoch: [5207/10000] Training loss: 2.0579785978459374e-05 / Validation loss: 0.00022072849004777979 / Long term Validation loss: 0.1513\n",
      "Epoch: [5208/10000] Training loss: 2.0573303187185987e-05 / Validation loss: 0.00022056702248093082 / Long term Validation loss: 0.1512\n",
      "Epoch: [5209/10000] Training loss: 2.0566779582931944e-05 / Validation loss: 0.00022057075249070303 / Long term Validation loss: 0.1513\n",
      "Epoch: [5210/10000] Training loss: 2.0560255390377052e-05 / Validation loss: 0.00022054578256675667 / Long term Validation loss: 0.1514\n",
      "Epoch: [5211/10000] Training loss: 2.055376049911946e-05 / Validation loss: 0.00022041363843246708 / Long term Validation loss: 0.1514\n",
      "Epoch: [5212/10000] Training loss: 2.0547271149340267e-05 / Validation loss: 0.00022046289896622274 / Long term Validation loss: 0.1515\n",
      "Epoch: [5213/10000] Training loss: 2.054075639198424e-05 / Validation loss: 0.00022034573006822277 / Long term Validation loss: 0.1515\n",
      "Epoch: [5214/10000] Training loss: 2.0534225973180916e-05 / Validation loss: 0.00022030708033823757 / Long term Validation loss: 0.1515\n",
      "Epoch: [5215/10000] Training loss: 2.052770788572783e-05 / Validation loss: 0.00022030441716167225 / Long term Validation loss: 0.1515\n",
      "Epoch: [5216/10000] Training loss: 2.052120282364116e-05 / Validation loss: 0.0002201787107364811 / Long term Validation loss: 0.1515\n",
      "Epoch: [5217/10000] Training loss: 2.0514688492469474e-05 / Validation loss: 0.00022020012758481477 / Long term Validation loss: 0.1516\n",
      "Epoch: [5218/10000] Training loss: 2.0508157263599557e-05 / Validation loss: 0.0002201174966081886 / Long term Validation loss: 0.1516\n",
      "Epoch: [5219/10000] Training loss: 2.0501624443556117e-05 / Validation loss: 0.00022005455420693514 / Long term Validation loss: 0.1516\n",
      "Epoch: [5220/10000] Training loss: 2.0495101362238897e-05 / Validation loss: 0.00022005672063317016 / Long term Validation loss: 0.1516\n",
      "Epoch: [5221/10000] Training loss: 2.048857949241746e-05 / Validation loss: 0.00021994584415859107 / Long term Validation loss: 0.1516\n",
      "Epoch: [5222/10000] Training loss: 2.048204676761754e-05 / Validation loss: 0.0002199425794748914 / Long term Validation loss: 0.1517\n",
      "Epoch: [5223/10000] Training loss: 2.047550591880335e-05 / Validation loss: 0.00021988316941340375 / Long term Validation loss: 0.1517\n",
      "Epoch: [5224/10000] Training loss: 2.04689675204753e-05 / Validation loss: 0.0002198097488194932 / Long term Validation loss: 0.1517\n",
      "Epoch: [5225/10000] Training loss: 2.0462433278848186e-05 / Validation loss: 0.00021980783554379968 / Long term Validation loss: 0.1517\n",
      "Epoch: [5226/10000] Training loss: 2.0455895317681e-05 / Validation loss: 0.0002197130747710678 / Long term Validation loss: 0.1517\n",
      "Epoch: [5227/10000] Training loss: 2.0449349293745864e-05 / Validation loss: 0.0002196912493989965 / Long term Validation loss: 0.1518\n",
      "Epoch: [5228/10000] Training loss: 2.044279998527567e-05 / Validation loss: 0.00021964512016453867 / Long term Validation loss: 0.1518\n",
      "Epoch: [5229/10000] Training loss: 2.0436252658468443e-05 / Validation loss: 0.0002195693270602673 / Long term Validation loss: 0.1518\n",
      "Epoch: [5230/10000] Training loss: 2.042970558831898e-05 / Validation loss: 0.00021955952833076927 / Long term Validation loss: 0.1518\n",
      "Epoch: [5231/10000] Training loss: 2.0423153883682676e-05 / Validation loss: 0.00021947837386159273 / Long term Validation loss: 0.1519\n",
      "Epoch: [5232/10000] Training loss: 2.0416596930354724e-05 / Validation loss: 0.00021944406335296034 / Long term Validation loss: 0.1519\n",
      "Epoch: [5233/10000] Training loss: 2.041003849283277e-05 / Validation loss: 0.00021940431843267213 / Long term Validation loss: 0.1519\n",
      "Epoch: [5234/10000] Training loss: 2.0403480589332068e-05 / Validation loss: 0.0002193305694511245 / Long term Validation loss: 0.1519\n",
      "Epoch: [5235/10000] Training loss: 2.0396921085908865e-05 / Validation loss: 0.00021931263061450776 / Long term Validation loss: 0.1520\n",
      "Epoch: [5236/10000] Training loss: 2.0390357490938256e-05 / Validation loss: 0.00021924180849876166 / Long term Validation loss: 0.1520\n",
      "Epoch: [5237/10000] Training loss: 2.0383790417896297e-05 / Validation loss: 0.00021919994761746773 / Long term Validation loss: 0.1520\n",
      "Epoch: [5238/10000] Training loss: 2.037722221278117e-05 / Validation loss: 0.00021916223319505413 / Long term Validation loss: 0.1520\n",
      "Epoch: [5239/10000] Training loss: 2.037065341111968e-05 / Validation loss: 0.00021909212665996897 / Long term Validation loss: 0.1521\n",
      "Epoch: [5240/10000] Training loss: 2.0364082403640342e-05 / Validation loss: 0.0002190669818276226 / Long term Validation loss: 0.1521\n",
      "Epoch: [5241/10000] Training loss: 2.0357508072238356e-05 / Validation loss: 0.00021900322166070504 / Long term Validation loss: 0.1521\n",
      "Epoch: [5242/10000] Training loss: 2.0350931178826338e-05 / Validation loss: 0.00021895720435482778 / Long term Validation loss: 0.1521\n",
      "Epoch: [5243/10000] Training loss: 2.0344353045452875e-05 / Validation loss: 0.00021891919342240153 / Long term Validation loss: 0.1522\n",
      "Epoch: [5244/10000] Training loss: 2.033777368899039e-05 / Validation loss: 0.0002188531076295684 / Long term Validation loss: 0.1522\n",
      "Epoch: [5245/10000] Training loss: 2.033119208659831e-05 / Validation loss: 0.0002188223802726913 / Long term Validation loss: 0.1522\n",
      "Epoch: [5246/10000] Training loss: 2.032460777575748e-05 / Validation loss: 0.00021876332645916213 / Long term Validation loss: 0.1522\n",
      "Epoch: [5247/10000] Training loss: 2.0318021346366404e-05 / Validation loss: 0.00021871544979192834 / Long term Validation loss: 0.1523\n",
      "Epoch: [5248/10000] Training loss: 2.031143352482773e-05 / Validation loss: 0.0002186760922659123 / Long term Validation loss: 0.1523\n",
      "Epoch: [5249/10000] Training loss: 2.0304844216529444e-05 / Validation loss: 0.0002186136671101885 / Long term Validation loss: 0.1523\n",
      "Epoch: [5250/10000] Training loss: 2.029825282050687e-05 / Validation loss: 0.00021857873666294517 / Long term Validation loss: 0.1524\n",
      "Epoch: [5251/10000] Training loss: 2.0291659150790935e-05 / Validation loss: 0.00021852259388821497 / Long term Validation loss: 0.1524\n",
      "Epoch: [5252/10000] Training loss: 2.0285063598133233e-05 / Validation loss: 0.00021847413112788876 / Long term Validation loss: 0.1524\n",
      "Epoch: [5253/10000] Training loss: 2.027846657509102e-05 / Validation loss: 0.00021843306202225913 / Long term Validation loss: 0.1525\n",
      "Epoch: [5254/10000] Training loss: 2.0271868008657064e-05 / Validation loss: 0.00021837378641634665 / Long term Validation loss: 0.1525\n",
      "Epoch: [5255/10000] Training loss: 2.0265267560451063e-05 / Validation loss: 0.00021833586753112783 / Long term Validation loss: 0.1525\n",
      "Epoch: [5256/10000] Training loss: 2.0258665151746224e-05 / Validation loss: 0.00021828157970607835 / Long term Validation loss: 0.1525\n",
      "Epoch: [5257/10000] Training loss: 2.025206102408667e-05 / Validation loss: 0.0002182333024315223 / Long term Validation loss: 0.1526\n",
      "Epoch: [5258/10000] Training loss: 2.0245455429619757e-05 / Validation loss: 0.00021819055733379573 / Long term Validation loss: 0.1526\n",
      "Epoch: [5259/10000] Training loss: 2.0238848342246727e-05 / Validation loss: 0.00021813389122885056 / Long term Validation loss: 0.1526\n",
      "Epoch: [5260/10000] Training loss: 2.023223957967618e-05 / Validation loss: 0.00021809378978003297 / Long term Validation loss: 0.1527\n",
      "Epoch: [5261/10000] Training loss: 2.022562910436863e-05 / Validation loss: 0.00021804062686328965 / Long term Validation loss: 0.1527\n",
      "Epoch: [5262/10000] Training loss: 2.0219017060987775e-05 / Validation loss: 0.00021799281761626326 / Long term Validation loss: 0.1527\n",
      "Epoch: [5263/10000] Training loss: 2.0212403617177226e-05 / Validation loss: 0.00021794851976888213 / Long term Validation loss: 0.1528\n",
      "Epoch: [5264/10000] Training loss: 2.0205788785707107e-05 / Validation loss: 0.0002178939968001793 / Long term Validation loss: 0.1528\n",
      "Epoch: [5265/10000] Training loss: 2.019917247775672e-05 / Validation loss: 0.0002178522814595937 / Long term Validation loss: 0.1528\n",
      "Epoch: [5266/10000] Training loss: 2.019255467487439e-05 / Validation loss: 0.0002177998876934708 / Long term Validation loss: 0.1529\n",
      "Epoch: [5267/10000] Training loss: 2.0185935462977714e-05 / Validation loss: 0.00021775266844499308 / Long term Validation loss: 0.1529\n",
      "Epoch: [5268/10000] Training loss: 2.017931496127843e-05 / Validation loss: 0.00021770704018983695 / Long term Validation loss: 0.1529\n",
      "Epoch: [5269/10000] Training loss: 2.0172693207256208e-05 / Validation loss: 0.00021765430237776932 / Long term Validation loss: 0.1529\n",
      "Epoch: [5270/10000] Training loss: 2.0166070169878395e-05 / Validation loss: 0.00021761130276075553 / Long term Validation loss: 0.1530\n",
      "Epoch: [5271/10000] Training loss: 2.015944584411101e-05 / Validation loss: 0.00021755947106008502 / Long term Validation loss: 0.1530\n",
      "Epoch: [5272/10000] Training loss: 2.0152820282365524e-05 / Validation loss: 0.00021751276697921796 / Long term Validation loss: 0.1530\n",
      "Epoch: [5273/10000] Training loss: 2.014619357304866e-05 / Validation loss: 0.00021746599765609973 / Long term Validation loss: 0.1531\n",
      "Epoch: [5274/10000] Training loss: 2.013956576610432e-05 / Validation loss: 0.00021741475217227317 / Long term Validation loss: 0.1531\n",
      "Epoch: [5275/10000] Training loss: 2.0132936867143485e-05 / Validation loss: 0.00021737066847143905 / Long term Validation loss: 0.1531\n",
      "Epoch: [5276/10000] Training loss: 2.0126306883835736e-05 / Validation loss: 0.00021731936047931838 / Long term Validation loss: 0.1532\n",
      "Epoch: [5277/10000] Training loss: 2.0119675851660964e-05 / Validation loss: 0.0002172730639580114 / Long term Validation loss: 0.1532\n",
      "Epoch: [5278/10000] Training loss: 2.0113043838057403e-05 / Validation loss: 0.00021722539010953109 / Long term Validation loss: 0.1532\n",
      "Epoch: [5279/10000] Training loss: 2.0106410896661497e-05 / Validation loss: 0.0002171754226731456 / Long term Validation loss: 0.1533\n",
      "Epoch: [5280/10000] Training loss: 2.009977705596166e-05 / Validation loss: 0.00021713037310693058 / Long term Validation loss: 0.1533\n",
      "Epoch: [5281/10000] Training loss: 2.0093142336062067e-05 / Validation loss: 0.00021707961164020565 / Long term Validation loss: 0.1533\n",
      "Epoch: [5282/10000] Training loss: 2.0086506766514613e-05 / Validation loss: 0.0002170335414718409 / Long term Validation loss: 0.1534\n",
      "Epoch: [5283/10000] Training loss: 2.007987040069057e-05 / Validation loss: 0.0002169851835924665 / Long term Validation loss: 0.1534\n",
      "Epoch: [5284/10000] Training loss: 2.0073233290578972e-05 / Validation loss: 0.0002169362885437123 / Long term Validation loss: 0.1534\n",
      "Epoch: [5285/10000] Training loss: 2.0066595477714126e-05 / Validation loss: 0.00021689035806882168 / Long term Validation loss: 0.1535\n",
      "Epoch: [5286/10000] Training loss: 2.0059956993244943e-05 / Validation loss: 0.00021684021764885184 / Long term Validation loss: 0.1535\n",
      "Epoch: [5287/10000] Training loss: 2.005331786782252e-05 / Validation loss: 0.00021679419507641045 / Long term Validation loss: 0.1535\n",
      "Epoch: [5288/10000] Training loss: 2.0046678146563712e-05 / Validation loss: 0.00021674541143823492 / Long term Validation loss: 0.1536\n",
      "Epoch: [5289/10000] Training loss: 2.004003787757165e-05 / Validation loss: 0.0002166973957694608 / Long term Validation loss: 0.1536\n",
      "Epoch: [5290/10000] Training loss: 2.0033397108266978e-05 / Validation loss: 0.0002166506786265264 / Long term Validation loss: 0.1536\n",
      "Epoch: [5291/10000] Training loss: 2.002675587812754e-05 / Validation loss: 0.00021660122314445582 / Long term Validation loss: 0.1537\n",
      "Epoch: [5292/10000] Training loss: 2.0020114222599083e-05 / Validation loss: 0.00021655505499902338 / Long term Validation loss: 0.1537\n",
      "Epoch: [5293/10000] Training loss: 2.001347218350388e-05 / Validation loss: 0.0002165060949767057 / Long term Validation loss: 0.1537\n",
      "Epoch: [5294/10000] Training loss: 2.0006829805375693e-05 / Validation loss: 0.00021645873105653307 / Long term Validation loss: 0.1538\n",
      "Epoch: [5295/10000] Training loss: 2.0000187136847673e-05 / Validation loss: 0.0002164113501853657 / Long term Validation loss: 0.1538\n",
      "Epoch: [5296/10000] Training loss: 1.9993544222304164e-05 / Validation loss: 0.00021636260930751837 / Long term Validation loss: 0.1538\n",
      "Epoch: [5297/10000] Training loss: 1.9986901102785974e-05 / Validation loss: 0.00021631614143743312 / Long term Validation loss: 0.1539\n",
      "Epoch: [5298/10000] Training loss: 1.9980257820417517e-05 / Validation loss: 0.00021626725876779896 / Long term Validation loss: 0.1539\n",
      "Epoch: [5299/10000] Training loss: 1.9973614418079575e-05 / Validation loss: 0.00021622030530148425 / Long term Validation loss: 0.1539\n",
      "Epoch: [5300/10000] Training loss: 1.996697094343229e-05 / Validation loss: 0.0002161724359519881 / Long term Validation loss: 0.1540\n",
      "Epoch: [5301/10000] Training loss: 1.9960327442691724e-05 / Validation loss: 0.00021612436978895097 / Long term Validation loss: 0.1540\n",
      "Epoch: [5302/10000] Training loss: 1.995368396118374e-05 / Validation loss: 0.00021607749847993006 / Long term Validation loss: 0.1540\n",
      "Epoch: [5303/10000] Training loss: 1.9947040543052047e-05 / Validation loss: 0.00021602889986946074 / Long term Validation loss: 0.1540\n",
      "Epoch: [5304/10000] Training loss: 1.9940397231731706e-05 / Validation loss: 0.00021598210092465061 / Long term Validation loss: 0.1541\n",
      "Epoch: [5305/10000] Training loss: 1.9933754073778856e-05 / Validation loss: 0.00021593395967875676 / Long term Validation loss: 0.1541\n",
      "Epoch: [5306/10000] Training loss: 1.9927111115455247e-05 / Validation loss: 0.0002158864504694853 / Long term Validation loss: 0.1541\n",
      "Epoch: [5307/10000] Training loss: 1.9920468404209194e-05 / Validation loss: 0.0002158391642204022 / Long term Validation loss: 0.1542\n",
      "Epoch: [5308/10000] Training loss: 1.9913825986127725e-05 / Validation loss: 0.00021579099321631677 / Long term Validation loss: 0.1542\n",
      "Epoch: [5309/10000] Training loss: 1.9907183906492005e-05 / Validation loss: 0.00021574412970630722 / Long term Validation loss: 0.1542\n",
      "Epoch: [5310/10000] Training loss: 1.9900542211638506e-05 / Validation loss: 0.00021569595736154167 / Long term Validation loss: 0.1543\n",
      "Epoch: [5311/10000] Training loss: 1.989390094760224e-05 / Validation loss: 0.00021564881770218066 / Long term Validation loss: 0.1543\n",
      "Epoch: [5312/10000] Training loss: 1.9887260162227747e-05 / Validation loss: 0.00021560119921038045 / Long term Validation loss: 0.1543\n",
      "Epoch: [5313/10000] Training loss: 1.9880619902525815e-05 / Validation loss: 0.00021555349208767882 / Long term Validation loss: 0.1544\n",
      "Epoch: [5314/10000] Training loss: 1.987398021553355e-05 / Validation loss: 0.00021550641308926087 / Long term Validation loss: 0.1544\n",
      "Epoch: [5315/10000] Training loss: 1.986734114807743e-05 / Validation loss: 0.00021545842172953538 / Long term Validation loss: 0.1544\n",
      "Epoch: [5316/10000] Training loss: 1.986070274652983e-05 / Validation loss: 0.00021541143151141798 / Long term Validation loss: 0.1544\n",
      "Epoch: [5317/10000] Training loss: 1.985406505846183e-05 / Validation loss: 0.00021536364891352108 / Long term Validation loss: 0.1545\n",
      "Epoch: [5318/10000] Training loss: 1.984742813094133e-05 / Validation loss: 0.00021531633568373714 / Long term Validation loss: 0.1545\n",
      "Epoch: [5319/10000] Training loss: 1.984079201179449e-05 / Validation loss: 0.0002152690046142894 / Long term Validation loss: 0.1545\n",
      "Epoch: [5320/10000] Training loss: 1.9834156748282103e-05 / Validation loss: 0.00021522133096924743 / Long term Validation loss: 0.1546\n",
      "Epoch: [5321/10000] Training loss: 1.9827522387479812e-05 / Validation loss: 0.00021517429641342332 / Long term Validation loss: 0.1546\n",
      "Epoch: [5322/10000] Training loss: 1.9820888976736798e-05 / Validation loss: 0.00021512655006347285 / Long term Validation loss: 0.1546\n",
      "Epoch: [5323/10000] Training loss: 1.9814256562983248e-05 / Validation loss: 0.00021507947072538217 / Long term Validation loss: 0.1547\n",
      "Epoch: [5324/10000] Training loss: 1.9807625193923692e-05 / Validation loss: 0.000215031962632327 / Long term Validation loss: 0.1547\n",
      "Epoch: [5325/10000] Training loss: 1.9800994916740516e-05 / Validation loss: 0.0002149846239263837 / Long term Validation loss: 0.1547\n",
      "Epoch: [5326/10000] Training loss: 1.9794365778886134e-05 / Validation loss: 0.0002149374365461892 / Long term Validation loss: 0.1547\n",
      "Epoch: [5327/10000] Training loss: 1.9787737827520864e-05 / Validation loss: 0.0002148898916794484 / Long term Validation loss: 0.1548\n",
      "Epoch: [5328/10000] Training loss: 1.9781111109556278e-05 / Validation loss: 0.0002148428637002864 / Long term Validation loss: 0.1548\n",
      "Epoch: [5329/10000] Training loss: 1.9774485672179717e-05 / Validation loss: 0.000214795334464506 / Long term Validation loss: 0.1548\n",
      "Epoch: [5330/10000] Training loss: 1.9767861562140474e-05 / Validation loss: 0.0002147482366381876 / Long term Validation loss: 0.1549\n",
      "Epoch: [5331/10000] Training loss: 1.976123882659122e-05 / Validation loss: 0.00021470090869584116 / Long term Validation loss: 0.1549\n",
      "Epoch: [5332/10000] Training loss: 1.9754617512225725e-05 / Validation loss: 0.00021465363051937866 / Long term Validation loss: 0.1549\n",
      "Epoch: [5333/10000] Training loss: 1.9747997665777454e-05 / Validation loss: 0.0002146065231619599 / Long term Validation loss: 0.1550\n",
      "Epoch: [5334/10000] Training loss: 1.9741379333775074e-05 / Validation loss: 0.0002145591282901685 / Long term Validation loss: 0.1550\n",
      "Epoch: [5335/10000] Training loss: 1.973476256245263e-05 / Validation loss: 0.00021451211647771816 / Long term Validation loss: 0.1550\n",
      "Epoch: [5336/10000] Training loss: 1.972814739817082e-05 / Validation loss: 0.0002144647585005555 / Long term Validation loss: 0.1550\n",
      "Epoch: [5337/10000] Training loss: 1.9721533886847126e-05 / Validation loss: 0.00021441769349405437 / Long term Validation loss: 0.1551\n",
      "Epoch: [5338/10000] Training loss: 1.971492207454848e-05 / Validation loss: 0.00021437048720517605 / Long term Validation loss: 0.1551\n",
      "Epoch: [5339/10000] Training loss: 1.970831200691575e-05 / Validation loss: 0.00021432330560645392 / Long term Validation loss: 0.1551\n",
      "Epoch: [5340/10000] Training loss: 1.970170372949762e-05 / Validation loss: 0.00021427625565802666 / Long term Validation loss: 0.1552\n",
      "Epoch: [5341/10000] Training loss: 1.9695097287606123e-05 / Validation loss: 0.00021422900387156079 / Long term Validation loss: 0.1552\n",
      "Epoch: [5342/10000] Training loss: 1.9688492726224427e-05 / Validation loss: 0.0002141820257481177 / Long term Validation loss: 0.1552\n",
      "Epoch: [5343/10000] Training loss: 1.9681890090296082e-05 / Validation loss: 0.00021413480440502744 / Long term Validation loss: 0.1552\n",
      "Epoch: [5344/10000] Training loss: 1.967528942431675e-05 / Validation loss: 0.00021408780018900003 / Long term Validation loss: 0.1553\n",
      "Epoch: [5345/10000] Training loss: 1.9668690772752284e-05 / Validation loss: 0.00021404068582525187 / Long term Validation loss: 0.1553\n",
      "Epoch: [5346/10000] Training loss: 1.9662094179632408e-05 / Validation loss: 0.00021399361059904398 / Long term Validation loss: 0.1553\n",
      "Epoch: [5347/10000] Training loss: 1.9655499688803712e-05 / Validation loss: 0.0002139466115894165 / Long term Validation loss: 0.1554\n",
      "Epoch: [5348/10000] Training loss: 1.9648907343797338e-05 / Validation loss: 0.00021389948985096915 / Long term Validation loss: 0.1554\n",
      "Epoch: [5349/10000] Training loss: 1.964231718778892e-05 / Validation loss: 0.0002138525561172122 / Long term Validation loss: 0.1554\n",
      "Epoch: [5350/10000] Training loss: 1.963572926376399e-05 / Validation loss: 0.00021380545074262827 / Long term Validation loss: 0.1554\n",
      "Epoch: [5351/10000] Training loss: 1.9629143614249563e-05 / Validation loss: 0.00021375851738930258 / Long term Validation loss: 0.1555\n",
      "Epoch: [5352/10000] Training loss: 1.962256028160343e-05 / Validation loss: 0.00021371148269615358 / Long term Validation loss: 0.1555\n",
      "Epoch: [5353/10000] Training loss: 1.9615979307714526e-05 / Validation loss: 0.00021366451206038798 / Long term Validation loss: 0.1555\n",
      "Epoch: [5354/10000] Training loss: 1.96094007342151e-05 / Validation loss: 0.0002136175634705996 / Long term Validation loss: 0.1556\n",
      "Epoch: [5355/10000] Training loss: 1.960282460233434e-05 / Validation loss: 0.00021357056077463558 / Long term Validation loss: 0.1556\n",
      "Epoch: [5356/10000] Training loss: 1.959625095292218e-05 / Validation loss: 0.00021352367456928744 / Long term Validation loss: 0.1556\n",
      "Epoch: [5357/10000] Training loss: 1.9589679826507735e-05 / Validation loss: 0.00021347667493187036 / Long term Validation loss: 0.1556\n",
      "Epoch: [5358/10000] Training loss: 1.9583111263149012e-05 / Validation loss: 0.00021342981027239668 / Long term Validation loss: 0.1557\n",
      "Epoch: [5359/10000] Training loss: 1.9576545302615525e-05 / Validation loss: 0.00021338285210736408 / Long term Validation loss: 0.1557\n",
      "Epoch: [5360/10000] Training loss: 1.9569981984173065e-05 / Validation loss: 0.0002133359770631833 / Long term Validation loss: 0.1557\n",
      "Epoch: [5361/10000] Training loss: 1.9563421346761313e-05 / Validation loss: 0.0002132890803462161 / Long term Validation loss: 0.1557\n",
      "Epoch: [5362/10000] Training loss: 1.9556863428839514e-05 / Validation loss: 0.00021324218662830522 / Long term Validation loss: 0.1558\n",
      "Epoch: [5363/10000] Training loss: 1.9550308268462397e-05 / Validation loss: 0.00021319534649537835 / Long term Validation loss: 0.1558\n",
      "Epoch: [5364/10000] Training loss: 1.9543755903253578e-05 / Validation loss: 0.00021314844806438296 / Long term Validation loss: 0.1558\n",
      "Epoch: [5365/10000] Training loss: 1.9537206370355334e-05 / Validation loss: 0.00021310164289874514 / Long term Validation loss: 0.1558\n",
      "Epoch: [5366/10000] Training loss: 1.9530659706514214e-05 / Validation loss: 0.00021305476352642848 / Long term Validation loss: 0.1559\n",
      "Epoch: [5367/10000] Training loss: 1.9524115947948774e-05 / Validation loss: 0.00021300796937055063 / Long term Validation loss: 0.1559\n",
      "Epoch: [5368/10000] Training loss: 1.9517575130479028e-05 / Validation loss: 0.00021296112848090802 / Long term Validation loss: 0.1559\n",
      "Epoch: [5369/10000] Training loss: 1.951103728938855e-05 / Validation loss: 0.00021291433085015557 / Long term Validation loss: 0.1559\n",
      "Epoch: [5370/10000] Training loss: 1.950450245952398e-05 / Validation loss: 0.00021286753511923456 / Long term Validation loss: 0.1560\n",
      "Epoch: [5371/10000] Training loss: 1.949797067521546e-05 / Validation loss: 0.00021282073326415012 / Long term Validation loss: 0.1560\n",
      "Epoch: [5372/10000] Training loss: 1.9491441970302563e-05 / Validation loss: 0.00021277397640556228 / Long term Validation loss: 0.1560\n",
      "Epoch: [5373/10000] Training loss: 1.948491637813558e-05 / Validation loss: 0.00021272718016645617 / Long term Validation loss: 0.1561\n",
      "Epoch: [5374/10000] Training loss: 1.9478393931527103e-05 / Validation loss: 0.0002126804486654147 / Long term Validation loss: 0.1561\n",
      "Epoch: [5375/10000] Training loss: 1.9471874662815283e-05 / Validation loss: 0.00021263367144451882 / Long term Validation loss: 0.1561\n",
      "Epoch: [5376/10000] Training loss: 1.94653586037732e-05 / Validation loss: 0.00021258695191891383 / Long term Validation loss: 0.1561\n",
      "Epoch: [5377/10000] Training loss: 1.9458845785692746e-05 / Validation loss: 0.00021254020397540644 / Long term Validation loss: 0.1561\n",
      "Epoch: [5378/10000] Training loss: 1.945233623929565e-05 / Validation loss: 0.00021249348847315257 / Long term Validation loss: 0.1562\n",
      "Epoch: [5379/10000] Training loss: 1.9445829994797362e-05 / Validation loss: 0.00021244677332977734 / Long term Validation loss: 0.1562\n",
      "Epoch: [5380/10000] Training loss: 1.9439327081853546e-05 / Validation loss: 0.00021240006095837253 / Long term Validation loss: 0.1562\n",
      "Epoch: [5381/10000] Training loss: 1.9432827529580715e-05 / Validation loss: 0.00021235337551901208 / Long term Validation loss: 0.1562\n",
      "Epoch: [5382/10000] Training loss: 1.9426331366550434e-05 / Validation loss: 0.00021230667085880486 / Long term Validation loss: 0.1563\n",
      "Epoch: [5383/10000] Training loss: 1.9419838620765325e-05 / Validation loss: 0.0002122600080883387 / Long term Validation loss: 0.1563\n",
      "Epoch: [5384/10000] Training loss: 1.9413349319692606e-05 / Validation loss: 0.00021221331798097493 / Long term Validation loss: 0.1563\n",
      "Epoch: [5385/10000] Training loss: 1.9406863490211066e-05 / Validation loss: 0.00021216667031202498 / Long term Validation loss: 0.1563\n",
      "Epoch: [5386/10000] Training loss: 1.940038115866325e-05 / Validation loss: 0.00021212000072165217 / Long term Validation loss: 0.1564\n",
      "Epoch: [5387/10000] Training loss: 1.9393902350796326e-05 / Validation loss: 0.00021207336266904262 / Long term Validation loss: 0.1564\n",
      "Epoch: [5388/10000] Training loss: 1.93874270918103e-05 / Validation loss: 0.00021202671673857036 / Long term Validation loss: 0.1564\n",
      "Epoch: [5389/10000] Training loss: 1.9380955406312936e-05 / Validation loss: 0.00021198008602826433 / Long term Validation loss: 0.1564\n",
      "Epoch: [5390/10000] Training loss: 1.9374487318348226e-05 / Validation loss: 0.00021193346365800758 / Long term Validation loss: 0.1565\n",
      "Epoch: [5391/10000] Training loss: 1.9368022851376107e-05 / Validation loss: 0.0002118868409678651 / Long term Validation loss: 0.1565\n",
      "Epoch: [5392/10000] Training loss: 1.9361562028275556e-05 / Validation loss: 0.0002118402395906852 / Long term Validation loss: 0.1565\n",
      "Epoch: [5393/10000] Training loss: 1.9355104871349977e-05 / Validation loss: 0.0002117936274461905 / Long term Validation loss: 0.1565\n",
      "Epoch: [5394/10000] Training loss: 1.9348651402308238e-05 / Validation loss: 0.00021174704335371022 / Long term Validation loss: 0.1565\n",
      "Epoch: [5395/10000] Training loss: 1.9342201642288928e-05 / Validation loss: 0.00021170044480145494 / Long term Validation loss: 0.1566\n",
      "Epoch: [5396/10000] Training loss: 1.933575561182845e-05 / Validation loss: 0.00021165387440426258 / Long term Validation loss: 0.1566\n",
      "Epoch: [5397/10000] Training loss: 1.932931333089375e-05 / Validation loss: 0.00021160729193578823 / Long term Validation loss: 0.1566\n",
      "Epoch: [5398/10000] Training loss: 1.932287481884814e-05 / Validation loss: 0.0002115607325884791 / Long term Validation loss: 0.1566\n",
      "Epoch: [5399/10000] Training loss: 1.931644009448239e-05 / Validation loss: 0.00021151416755218864 / Long term Validation loss: 0.1567\n",
      "Epoch: [5400/10000] Training loss: 1.931000917598709e-05 / Validation loss: 0.0002114676178588074 / Long term Validation loss: 0.1567\n",
      "Epoch: [5401/10000] Training loss: 1.9303582080975103e-05 / Validation loss: 0.00021142107037284017 / Long term Validation loss: 0.1567\n",
      "Epoch: [5402/10000] Training loss: 1.9297158826465504e-05 / Validation loss: 0.00021137453007925082 / Long term Validation loss: 0.1567\n",
      "Epoch: [5403/10000] Training loss: 1.9290739428894247e-05 / Validation loss: 0.00021132799930024637 / Long term Validation loss: 0.1567\n",
      "Epoch: [5404/10000] Training loss: 1.9284323904111063e-05 / Validation loss: 0.00021128146894928623 / Long term Validation loss: 0.1568\n",
      "Epoch: [5405/10000] Training loss: 1.9277912267378455e-05 / Validation loss: 0.0002112349534924291 / Long term Validation loss: 0.1568\n",
      "Epoch: [5406/10000] Training loss: 1.9271504533380063e-05 / Validation loss: 0.00021118843400797855 / Long term Validation loss: 0.1568\n",
      "Epoch: [5407/10000] Training loss: 1.9265100716210643e-05 / Validation loss: 0.00021114193234694393 / Long term Validation loss: 0.1568\n",
      "Epoch: [5408/10000] Training loss: 1.925870082939242e-05 / Validation loss: 0.00021109542467159754 / Long term Validation loss: 0.1569\n",
      "Epoch: [5409/10000] Training loss: 1.925230488585976e-05 / Validation loss: 0.00021104893542739145 / Long term Validation loss: 0.1569\n",
      "Epoch: [5410/10000] Training loss: 1.9245912897979815e-05 / Validation loss: 0.00021100244028539143 / Long term Validation loss: 0.1569\n",
      "Epoch: [5411/10000] Training loss: 1.923952487753557e-05 / Validation loss: 0.0002109559623854365 / Long term Validation loss: 0.1569\n",
      "Epoch: [5412/10000] Training loss: 1.9233140835747107e-05 / Validation loss: 0.0002109094801853637 / Long term Validation loss: 0.1569\n",
      "Epoch: [5413/10000] Training loss: 1.922676078325627e-05 / Validation loss: 0.00021086301291152947 / Long term Validation loss: 0.1570\n",
      "Epoch: [5414/10000] Training loss: 1.9220384730146022e-05 / Validation loss: 0.00021081654375695004 / Long term Validation loss: 0.1570\n",
      "Epoch: [5415/10000] Training loss: 1.921401268592853e-05 / Validation loss: 0.00021077008671234568 / Long term Validation loss: 0.1570\n",
      "Epoch: [5416/10000] Training loss: 1.9207644659561235e-05 / Validation loss: 0.00021072363046997867 / Long term Validation loss: 0.1570\n",
      "Epoch: [5417/10000] Training loss: 1.9201280659439382e-05 / Validation loss: 0.00021067718349798725 / Long term Validation loss: 0.1570\n",
      "Epoch: [5418/10000] Training loss: 1.9194920693408072e-05 / Validation loss: 0.00021063073988257394 / Long term Validation loss: 0.1571\n",
      "Epoch: [5419/10000] Training loss: 1.9188564768759547e-05 / Validation loss: 0.000210584302972923 / Long term Validation loss: 0.1571\n",
      "Epoch: [5420/10000] Training loss: 1.9182212892241363e-05 / Validation loss: 0.00021053787162785592 / Long term Validation loss: 0.1571\n",
      "Epoch: [5421/10000] Training loss: 1.917586507005786e-05 / Validation loss: 0.0002104914448372746 / Long term Validation loss: 0.1571\n",
      "Epoch: [5422/10000] Training loss: 1.9169521307875088e-05 / Validation loss: 0.0002104450254024751 / Long term Validation loss: 0.1571\n",
      "Epoch: [5423/10000] Training loss: 1.9163181610825902e-05 / Validation loss: 0.00021039860880123657 / Long term Validation loss: 0.1572\n",
      "Epoch: [5424/10000] Training loss: 1.915684598351212e-05 / Validation loss: 0.00021035220096271397 / Long term Validation loss: 0.1572\n",
      "Epoch: [5425/10000] Training loss: 1.915051443001264e-05 / Validation loss: 0.00021030579460309295 / Long term Validation loss: 0.1572\n",
      "Epoch: [5426/10000] Training loss: 1.914418695388353e-05 / Validation loss: 0.00021025939812111376 / Long term Validation loss: 0.1572\n",
      "Epoch: [5427/10000] Training loss: 1.9137863558168445e-05 / Validation loss: 0.00021021300201895155 / Long term Validation loss: 0.1573\n",
      "Epoch: [5428/10000] Training loss: 1.9131544245397142e-05 / Validation loss: 0.0002101666167385173 / Long term Validation loss: 0.1573\n",
      "Epoch: [5429/10000] Training loss: 1.912522901759783e-05 / Validation loss: 0.00021012023086268753 / Long term Validation loss: 0.1573\n",
      "Epoch: [5430/10000] Training loss: 1.9118917876294416e-05 / Validation loss: 0.00021007385671602307 / Long term Validation loss: 0.1573\n",
      "Epoch: [5431/10000] Training loss: 1.9112610822520493e-05 / Validation loss: 0.00021002748098368783 / Long term Validation loss: 0.1573\n",
      "Epoch: [5432/10000] Training loss: 1.9106307856815422e-05 / Validation loss: 0.00020998111799402705 / Long term Validation loss: 0.1574\n",
      "Epoch: [5433/10000] Training loss: 1.9100008979239946e-05 / Validation loss: 0.0002099347522675709 / Long term Validation loss: 0.1574\n",
      "Epoch: [5434/10000] Training loss: 1.909371418937107e-05 / Validation loss: 0.0002098884005585997 / Long term Validation loss: 0.1574\n",
      "Epoch: [5435/10000] Training loss: 1.9087423486319405e-05 / Validation loss: 0.00020984204463669686 / Long term Validation loss: 0.1574\n",
      "Epoch: [5436/10000] Training loss: 1.90811368687224e-05 / Validation loss: 0.00020979570444936593 / Long term Validation loss: 0.1574\n",
      "Epoch: [5437/10000] Training loss: 1.9074854334764036e-05 / Validation loss: 0.00020974935804458122 / Long term Validation loss: 0.1574\n",
      "Epoch: [5438/10000] Training loss: 1.9068575882165808e-05 / Validation loss: 0.00020970302976567476 / Long term Validation loss: 0.1575\n",
      "Epoch: [5439/10000] Training loss: 1.9062301508209317e-05 / Validation loss: 0.000209656692463011 / Long term Validation loss: 0.1575\n",
      "Epoch: [5440/10000] Training loss: 1.905603120972421e-05 / Validation loss: 0.00020961037667503174 / Long term Validation loss: 0.1575\n",
      "Epoch: [5441/10000] Training loss: 1.9049764983114733e-05 / Validation loss: 0.00020956404786459757 / Long term Validation loss: 0.1575\n",
      "Epoch: [5442/10000] Training loss: 1.904350282434321e-05 / Validation loss: 0.000209517745430581 / Long term Validation loss: 0.1575\n",
      "Epoch: [5443/10000] Training loss: 1.9037244728962275e-05 / Validation loss: 0.0002094714242002627 / Long term Validation loss: 0.1575\n",
      "Epoch: [5444/10000] Training loss: 1.9030990692091858e-05 / Validation loss: 0.00020942513640209376 / Long term Validation loss: 0.1576\n",
      "Epoch: [5445/10000] Training loss: 1.902474070845936e-05 / Validation loss: 0.00020937882136396152 / Long term Validation loss: 0.1576\n",
      "Epoch: [5446/10000] Training loss: 1.9018494772367387e-05 / Validation loss: 0.00020933255012430446 / Long term Validation loss: 0.1576\n",
      "Epoch: [5447/10000] Training loss: 1.9012252877745416e-05 / Validation loss: 0.00020928623913173865 / Long term Validation loss: 0.1576\n",
      "Epoch: [5448/10000] Training loss: 1.9006015018103637e-05 / Validation loss: 0.00020923998737309524 / Long term Validation loss: 0.1576\n",
      "Epoch: [5449/10000] Training loss: 1.8999781186601723e-05 / Validation loss: 0.00020919367705866268 / Long term Validation loss: 0.1577\n",
      "Epoch: [5450/10000] Training loss: 1.8993551375982174e-05 / Validation loss: 0.00020914744929299584 / Long term Validation loss: 0.1577\n",
      "Epoch: [5451/10000] Training loss: 1.898732557866423e-05 / Validation loss: 0.00020910113430779704 / Long term Validation loss: 0.1577\n",
      "Epoch: [5452/10000] Training loss: 1.8981103786646456e-05 / Validation loss: 0.00020905493761645986 / Long term Validation loss: 0.1577\n",
      "Epoch: [5453/10000] Training loss: 1.897488599163891e-05 / Validation loss: 0.00020900860936245741 / Long term Validation loss: 0.1577\n",
      "Epoch: [5454/10000] Training loss: 1.896867218491853e-05 / Validation loss: 0.0002089624550417547 / Long term Validation loss: 0.1577\n",
      "Epoch: [5455/10000] Training loss: 1.896246235752016e-05 / Validation loss: 0.0002089160995310105 / Long term Validation loss: 0.1578\n",
      "Epoch: [5456/10000] Training loss: 1.895625650001908e-05 / Validation loss: 0.00020887000588684185 / Long term Validation loss: 0.1578\n",
      "Epoch: [5457/10000] Training loss: 1.8950054602813803e-05 / Validation loss: 0.00020882360008176424 / Long term Validation loss: 0.1578\n",
      "Epoch: [5458/10000] Training loss: 1.8943856655794232e-05 / Validation loss: 0.0002087775972339273 / Long term Validation loss: 0.1578\n",
      "Epoch: [5459/10000] Training loss: 1.8937662648770462e-05 / Validation loss: 0.00020873110271689527 / Long term Validation loss: 0.1578\n",
      "Epoch: [5460/10000] Training loss: 1.8931472570958726e-05 / Validation loss: 0.00020868524095987817 / Long term Validation loss: 0.1578\n",
      "Epoch: [5461/10000] Training loss: 1.892528641164636e-05 / Validation loss: 0.00020863859285284757 / Long term Validation loss: 0.1579\n",
      "Epoch: [5462/10000] Training loss: 1.891910415938478e-05 / Validation loss: 0.00020859295738073902 / Long term Validation loss: 0.1579\n",
      "Epoch: [5463/10000] Training loss: 1.891292580304325e-05 / Validation loss: 0.00020854604471749608 / Long term Validation loss: 0.1579\n",
      "Epoch: [5464/10000] Training loss: 1.8906751330526567e-05 / Validation loss: 0.00020850078186492138 / Long term Validation loss: 0.1579\n",
      "Epoch: [5465/10000] Training loss: 1.890058073048418e-05 / Validation loss: 0.0002084534124070585 / Long term Validation loss: 0.1579\n",
      "Epoch: [5466/10000] Training loss: 1.8894413990254962e-05 / Validation loss: 0.0002084087769657895 / Long term Validation loss: 0.1579\n",
      "Epoch: [5467/10000] Training loss: 1.888825109871345e-05 / Validation loss: 0.00020836061338781606 / Long term Validation loss: 0.1579\n",
      "Epoch: [5468/10000] Training loss: 1.888209204297072e-05 / Validation loss: 0.00020831705493176404 / Long term Validation loss: 0.1579\n",
      "Epoch: [5469/10000] Training loss: 1.8875936813275955e-05 / Validation loss: 0.00020826749772173228 / Long term Validation loss: 0.1580\n",
      "Epoch: [5470/10000] Training loss: 1.8869785397794718e-05 / Validation loss: 0.00020822581994323254 / Long term Validation loss: 0.1580\n",
      "Epoch: [5471/10000] Training loss: 1.8863637791453245e-05 / Validation loss: 0.00020817379002991253 / Long term Validation loss: 0.1580\n",
      "Epoch: [5472/10000] Training loss: 1.885749398808868e-05 / Validation loss: 0.00020813544825637413 / Long term Validation loss: 0.1580\n",
      "Epoch: [5473/10000] Training loss: 1.885135399753786e-05 / Validation loss: 0.00020807897884870115 / Long term Validation loss: 0.1580\n",
      "Epoch: [5474/10000] Training loss: 1.8845217835637566e-05 / Validation loss: 0.00020804664198986145 / Long term Validation loss: 0.1580\n",
      "Epoch: [5475/10000] Training loss: 1.8839085560633316e-05 / Validation loss: 0.0002079821034377879 / Long term Validation loss: 0.1580\n",
      "Epoch: [5476/10000] Training loss: 1.8832957268155276e-05 / Validation loss: 0.00020796072759400906 / Long term Validation loss: 0.1580\n",
      "Epoch: [5477/10000] Training loss: 1.882683317917309e-05 / Validation loss: 0.00020788133875973723 / Long term Validation loss: 0.1581\n",
      "Epoch: [5478/10000] Training loss: 1.8820713676009497e-05 / Validation loss: 0.00020788024192287788 / Long term Validation loss: 0.1581\n",
      "Epoch: [5479/10000] Training loss: 1.881459954587365e-05 / Validation loss: 0.0002077731797380304 / Long term Validation loss: 0.1581\n",
      "Epoch: [5480/10000] Training loss: 1.8808492211195766e-05 / Validation loss: 0.00020781009732297152 / Long term Validation loss: 0.1581\n",
      "Epoch: [5481/10000] Training loss: 1.8802394491945712e-05 / Validation loss: 0.0002076508241802418 / Long term Validation loss: 0.1581\n",
      "Epoch: [5482/10000] Training loss: 1.8796311649713138e-05 / Validation loss: 0.00020775992962122688 / Long term Validation loss: 0.1581\n",
      "Epoch: [5483/10000] Training loss: 1.8790254010227238e-05 / Validation loss: 0.00020750094674136006 / Long term Validation loss: 0.1581\n",
      "Epoch: [5484/10000] Training loss: 1.8784241299671764e-05 / Validation loss: 0.0002077489074730626 / Long term Validation loss: 0.1581\n",
      "Epoch: [5485/10000] Training loss: 1.8778312289677413e-05 / Validation loss: 0.00020729724961939983 / Long term Validation loss: 0.1581\n",
      "Epoch: [5486/10000] Training loss: 1.8772542413270216e-05 / Validation loss: 0.00020781579450330288 / Long term Validation loss: 0.1582\n",
      "Epoch: [5487/10000] Training loss: 1.8767080945600834e-05 / Validation loss: 0.00020698765721306335 / Long term Validation loss: 0.1581\n",
      "Epoch: [5488/10000] Training loss: 1.8762222745462318e-05 / Validation loss: 0.00020804065252009275 / Long term Validation loss: 0.1582\n",
      "Epoch: [5489/10000] Training loss: 1.8758556698319037e-05 / Validation loss: 0.00020646956381184403 / Long term Validation loss: 0.1579\n",
      "Epoch: [5490/10000] Training loss: 1.875726172753151e-05 / Validation loss: 0.00020859388623965176 / Long term Validation loss: 0.1583\n",
      "Epoch: [5491/10000] Training loss: 1.8760716335576614e-05 / Validation loss: 0.00020554559543028777 / Long term Validation loss: 0.1575\n",
      "Epoch: [5492/10000] Training loss: 1.8773741549654745e-05 / Validation loss: 0.00020985527539731083 / Long term Validation loss: 0.1582\n",
      "Epoch: [5493/10000] Training loss: 1.880616264917151e-05 / Validation loss: 0.00020386274214075078 / Long term Validation loss: 0.1568\n",
      "Epoch: [5494/10000] Training loss: 1.8878122525677316e-05 / Validation loss: 0.00021273351396956507 / Long term Validation loss: 0.1584\n",
      "Epoch: [5495/10000] Training loss: 1.9031025905680983e-05 / Validation loss: 0.00020092543833808257 / Long term Validation loss: 0.1531\n",
      "Epoch: [5496/10000] Training loss: 1.9350501343561815e-05 / Validation loss: 0.00021963113776595034 / Long term Validation loss: 0.1570\n",
      "Epoch: [5497/10000] Training loss: 2.0012970037383475e-05 / Validation loss: 0.000196749495674619 / Long term Validation loss: 0.1459\n",
      "Epoch: [5498/10000] Training loss: 2.1382538006274954e-05 / Validation loss: 0.00023761998148029816 / Long term Validation loss: 0.1478\n",
      "Epoch: [5499/10000] Training loss: 2.41921555329679e-05 / Validation loss: 0.00019577699597619376 / Long term Validation loss: 0.1308\n",
      "Epoch: [5500/10000] Training loss: 2.9879226996355587e-05 / Validation loss: 0.00028792176903691424 / Long term Validation loss: 0.1430\n",
      "Epoch: [5501/10000] Training loss: 4.095141365705209e-05 / Validation loss: 0.0002201401950605287 / Long term Validation loss: 0.1234\n",
      "Epoch: [5502/10000] Training loss: 6.092570446058966e-05 / Validation loss: 0.00040862220973925963 / Long term Validation loss: 0.1443\n",
      "Epoch: [5503/10000] Training loss: 9.090685747441791e-05 / Validation loss: 0.00028861246372372266 / Long term Validation loss: 0.1359\n",
      "Epoch: [5504/10000] Training loss: 0.00012061046406331664 / Validation loss: 0.00047497629765618085 / Long term Validation loss: 0.1475\n",
      "Epoch: [5505/10000] Training loss: 0.00012003265540984745 / Validation loss: 0.0002327004974526851 / Long term Validation loss: 0.1248\n",
      "Epoch: [5506/10000] Training loss: 7.349968440429896e-05 / Validation loss: 0.0002421825048908812 / Long term Validation loss: 0.1464\n",
      "Epoch: [5507/10000] Training loss: 2.454902849431819e-05 / Validation loss: 0.0002539678729779091 / Long term Validation loss: 0.1448\n",
      "Epoch: [5508/10000] Training loss: 2.7677105182794247e-05 / Validation loss: 0.00022319322078241793 / Long term Validation loss: 0.1227\n",
      "Epoch: [5509/10000] Training loss: 6.314434371023558e-05 / Validation loss: 0.0003653602689283423 / Long term Validation loss: 0.1421\n",
      "Epoch: [5510/10000] Training loss: 6.891325388030063e-05 / Validation loss: 0.00020173978108225035 / Long term Validation loss: 0.1297\n",
      "Epoch: [5511/10000] Training loss: 3.568566340113177e-05 / Validation loss: 0.00020732119884315933 / Long term Validation loss: 0.1587\n",
      "Epoch: [5512/10000] Training loss: 1.9122918589031284e-05 / Validation loss: 0.0002925782762800548 / Long term Validation loss: 0.1429\n",
      "Epoch: [5513/10000] Training loss: 3.965700273191302e-05 / Validation loss: 0.00021352354447470411 / Long term Validation loss: 0.1235\n",
      "Epoch: [5514/10000] Training loss: 5.132129949280442e-05 / Validation loss: 0.0002690414247943365 / Long term Validation loss: 0.1441\n",
      "Epoch: [5515/10000] Training loss: 3.1563610150755064e-05 / Validation loss: 0.00021627178651032082 / Long term Validation loss: 0.1583\n",
      "Epoch: [5516/10000] Training loss: 1.886185692394197e-05 / Validation loss: 0.00020043914575349506 / Long term Validation loss: 0.1303\n",
      "Epoch: [5517/10000] Training loss: 3.2465457807968274e-05 / Validation loss: 0.00029064820543840225 / Long term Validation loss: 0.1429\n",
      "Epoch: [5518/10000] Training loss: 3.9441001199904146e-05 / Validation loss: 0.00019894551318661643 / Long term Validation loss: 0.1374\n",
      "Epoch: [5519/10000] Training loss: 2.551978802668565e-05 / Validation loss: 0.0002058059952372284 / Long term Validation loss: 0.1566\n",
      "Epoch: [5520/10000] Training loss: 1.9203994200473252e-05 / Validation loss: 0.00025988107795143226 / Long term Validation loss: 0.1447\n",
      "Epoch: [5521/10000] Training loss: 2.9412550807034808e-05 / Validation loss: 0.00019989843970734755 / Long term Validation loss: 0.1290\n",
      "Epoch: [5522/10000] Training loss: 3.1499329647063286e-05 / Validation loss: 0.00022996681023637253 / Long term Validation loss: 0.1531\n",
      "Epoch: [5523/10000] Training loss: 2.1363203007557903e-05 / Validation loss: 0.00022221741934164838 / Long term Validation loss: 0.1569\n",
      "Epoch: [5524/10000] Training loss: 1.991994992171443e-05 / Validation loss: 0.0001977104049990147 / Long term Validation loss: 0.1325\n",
      "Epoch: [5525/10000] Training loss: 2.714957737333e-05 / Validation loss: 0.00024546428077690507 / Long term Validation loss: 0.1465\n",
      "Epoch: [5526/10000] Training loss: 2.595716008693215e-05 / Validation loss: 0.00020229932413784205 / Long term Validation loss: 0.1528\n",
      "Epoch: [5527/10000] Training loss: 1.9300778888222584e-05 / Validation loss: 0.0001990080417696369 / Long term Validation loss: 0.1473\n",
      "Epoch: [5528/10000] Training loss: 2.05845416313263e-05 / Validation loss: 0.00024046545277866518 / Long term Validation loss: 0.1475\n",
      "Epoch: [5529/10000] Training loss: 2.4934472039890014e-05 / Validation loss: 0.00019691999264234624 / Long term Validation loss: 0.1405\n",
      "Epoch: [5530/10000] Training loss: 2.2348339174622992e-05 / Validation loss: 0.0002088803951143325 / Long term Validation loss: 0.1600\n",
      "Epoch: [5531/10000] Training loss: 1.8645670483746677e-05 / Validation loss: 0.00022385982891453244 / Long term Validation loss: 0.1557\n",
      "Epoch: [5532/10000] Training loss: 2.0861447951768326e-05 / Validation loss: 0.00019625268299236967 / Long term Validation loss: 0.1381\n",
      "Epoch: [5533/10000] Training loss: 2.290927672062861e-05 / Validation loss: 0.00022051881483921187 / Long term Validation loss: 0.1572\n",
      "Epoch: [5534/10000] Training loss: 2.0235424273535773e-05 / Validation loss: 0.00020870673931464107 / Long term Validation loss: 0.1601\n",
      "Epoch: [5535/10000] Training loss: 1.8646929733641733e-05 / Validation loss: 0.00019740268072818625 / Long term Validation loss: 0.1457\n",
      "Epoch: [5536/10000] Training loss: 2.072164481860905e-05 / Validation loss: 0.00022500223131767584 / Long term Validation loss: 0.1550\n",
      "Epoch: [5537/10000] Training loss: 2.1264428220432338e-05 / Validation loss: 0.00020056428892988512 / Long term Validation loss: 0.1524\n",
      "Epoch: [5538/10000] Training loss: 1.9159275455183152e-05 / Validation loss: 0.00020226286444972028 / Long term Validation loss: 0.1558\n",
      "Epoch: [5539/10000] Training loss: 1.8815436519896468e-05 / Validation loss: 0.00022071325882065727 / Long term Validation loss: 0.1568\n",
      "Epoch: [5540/10000] Training loss: 2.0323790838302348e-05 / Validation loss: 0.00019808441390326164 / Long term Validation loss: 0.1486\n",
      "Epoch: [5541/10000] Training loss: 2.009618927799031e-05 / Validation loss: 0.0002099255377734401 / Long term Validation loss: 0.1600\n",
      "Epoch: [5542/10000] Training loss: 1.8697711036497178e-05 / Validation loss: 0.000212481531548063 / Long term Validation loss: 0.1596\n",
      "Epoch: [5543/10000] Training loss: 1.89324805569503e-05 / Validation loss: 0.00019867166758586567 / Long term Validation loss: 0.1500\n",
      "Epoch: [5544/10000] Training loss: 1.9851764791755072e-05 / Validation loss: 0.00021578919996093516 / Long term Validation loss: 0.1587\n",
      "Epoch: [5545/10000] Training loss: 1.9348511714512694e-05 / Validation loss: 0.00020545791210745554 / Long term Validation loss: 0.1590\n",
      "Epoch: [5546/10000] Training loss: 1.8541726707582176e-05 / Validation loss: 0.0002015306018380999 / Long term Validation loss: 0.1543\n",
      "Epoch: [5547/10000] Training loss: 1.8948915154344255e-05 / Validation loss: 0.00021649890346772012 / Long term Validation loss: 0.1584\n",
      "Epoch: [5548/10000] Training loss: 1.9421301197720013e-05 / Validation loss: 0.00020169075402867858 / Long term Validation loss: 0.1549\n",
      "Epoch: [5549/10000] Training loss: 1.8911756033105833e-05 / Validation loss: 0.0002062550034928453 / Long term Validation loss: 0.1592\n",
      "Epoch: [5550/10000] Training loss: 1.8506759638980322e-05 / Validation loss: 0.00021285164259014699 / Long term Validation loss: 0.1591\n",
      "Epoch: [5551/10000] Training loss: 1.8890225590328685e-05 / Validation loss: 0.00020088920019610407 / Long term Validation loss: 0.1532\n",
      "Epoch: [5552/10000] Training loss: 1.908434229711404e-05 / Validation loss: 0.00021082439483226226 / Long term Validation loss: 0.1593\n",
      "Epoch: [5553/10000] Training loss: 1.867318446623109e-05 / Validation loss: 0.00020796360175205734 / Long term Validation loss: 0.1594\n",
      "Epoch: [5554/10000] Training loss: 1.8502761734662204e-05 / Validation loss: 0.0002022442589507818 / Long term Validation loss: 0.1562\n",
      "Epoch: [5555/10000] Training loss: 1.879658604165879e-05 / Validation loss: 0.00021270433184596043 / Long term Validation loss: 0.1589\n",
      "Epoch: [5556/10000] Training loss: 1.884281446247811e-05 / Validation loss: 0.00020426570698939435 / Long term Validation loss: 0.1580\n",
      "Epoch: [5557/10000] Training loss: 1.8547714109328725e-05 / Validation loss: 0.00020501765129123903 / Long term Validation loss: 0.1582\n",
      "Epoch: [5558/10000] Training loss: 1.8496293399217025e-05 / Validation loss: 0.00021127083463946784 / Long term Validation loss: 0.1589\n",
      "Epoch: [5559/10000] Training loss: 1.8697961832219354e-05 / Validation loss: 0.00020259725283881604 / Long term Validation loss: 0.1568\n",
      "Epoch: [5560/10000] Training loss: 1.8679178996290184e-05 / Validation loss: 0.00020802851516963622 / Long term Validation loss: 0.1589\n",
      "Epoch: [5561/10000] Training loss: 1.848069991432324e-05 / Validation loss: 0.00020808480600232487 / Long term Validation loss: 0.1589\n",
      "Epoch: [5562/10000] Training loss: 1.848049698203056e-05 / Validation loss: 0.00020284575110886895 / Long term Validation loss: 0.1571\n",
      "Epoch: [5563/10000] Training loss: 1.8610124984798637e-05 / Validation loss: 0.0002096973580997325 / Long term Validation loss: 0.1588\n",
      "Epoch: [5564/10000] Training loss: 1.8570426413390376e-05 / Validation loss: 0.0002051117615523716 / Long term Validation loss: 0.1582\n",
      "Epoch: [5565/10000] Training loss: 1.8441279893000018e-05 / Validation loss: 0.00020443293586559728 / Long term Validation loss: 0.1579\n",
      "Epoch: [5566/10000] Training loss: 1.8457567417786778e-05 / Validation loss: 0.00020919988324675785 / Long term Validation loss: 0.1587\n",
      "Epoch: [5567/10000] Training loss: 1.853761372087138e-05 / Validation loss: 0.00020340843165310652 / Long term Validation loss: 0.1576\n",
      "Epoch: [5568/10000] Training loss: 1.849731127631792e-05 / Validation loss: 0.00020639049718276212 / Long term Validation loss: 0.1586\n",
      "Epoch: [5569/10000] Training loss: 1.841388137852085e-05 / Validation loss: 0.00020717576531970348 / Long term Validation loss: 0.1586\n",
      "Epoch: [5570/10000] Training loss: 1.843132096781544e-05 / Validation loss: 0.00020317947024040927 / Long term Validation loss: 0.1576\n",
      "Epoch: [5571/10000] Training loss: 1.8479683984826738e-05 / Validation loss: 0.00020760330349313551 / Long term Validation loss: 0.1585\n",
      "Epoch: [5572/10000] Training loss: 1.844601820209085e-05 / Validation loss: 0.00020499628601123023 / Long term Validation loss: 0.1583\n",
      "Epoch: [5573/10000] Training loss: 1.839140335830884e-05 / Validation loss: 0.00020410849120839405 / Long term Validation loss: 0.1579\n",
      "Epoch: [5574/10000] Training loss: 1.8404596828183648e-05 / Validation loss: 0.00020742941262432273 / Long term Validation loss: 0.1585\n",
      "Epoch: [5575/10000] Training loss: 1.8433466760638903e-05 / Validation loss: 0.00020364061296669202 / Long term Validation loss: 0.1578\n",
      "Epoch: [5576/10000] Training loss: 1.840774982545788e-05 / Validation loss: 0.00020543984777083744 / Long term Validation loss: 0.1585\n",
      "Epoch: [5577/10000] Training loss: 1.837091523177408e-05 / Validation loss: 0.0002061392324797457 / Long term Validation loss: 0.1586\n",
      "Epoch: [5578/10000] Training loss: 1.8378878352104472e-05 / Validation loss: 0.00020337585797240392 / Long term Validation loss: 0.1578\n",
      "Epoch: [5579/10000] Training loss: 1.8395919176584326e-05 / Validation loss: 0.00020628147162888796 / Long term Validation loss: 0.1585\n",
      "Epoch: [5580/10000] Training loss: 1.8377110404179797e-05 / Validation loss: 0.00020461717951161413 / Long term Validation loss: 0.1584\n",
      "Epoch: [5581/10000] Training loss: 1.8351243983327222e-05 / Validation loss: 0.00020398095181154547 / Long term Validation loss: 0.1582\n",
      "Epoch: [5582/10000] Training loss: 1.8354639355740264e-05 / Validation loss: 0.00020615872340110582 / Long term Validation loss: 0.1585\n",
      "Epoch: [5583/10000] Training loss: 1.836440781774467e-05 / Validation loss: 0.00020364568584625416 / Long term Validation loss: 0.1581\n",
      "Epoch: [5584/10000] Training loss: 1.835089275525671e-05 / Validation loss: 0.00020489404811694026 / Long term Validation loss: 0.1585\n",
      "Epoch: [5585/10000] Training loss: 1.8331918885325625e-05 / Validation loss: 0.00020527218339512225 / Long term Validation loss: 0.1585\n",
      "Epoch: [5586/10000] Training loss: 1.8331828314366595e-05 / Validation loss: 0.00020348904835359479 / Long term Validation loss: 0.1581\n",
      "Epoch: [5587/10000] Training loss: 1.8336978040876994e-05 / Validation loss: 0.00020544108404457788 / Long term Validation loss: 0.1585\n",
      "Epoch: [5588/10000] Training loss: 1.832723544625643e-05 / Validation loss: 0.00020423007377342746 / Long term Validation loss: 0.1584\n",
      "Epoch: [5589/10000] Training loss: 1.8312750729699696e-05 / Validation loss: 0.0002039446240597796 / Long term Validation loss: 0.1584\n",
      "Epoch: [5590/10000] Training loss: 1.8310209810469372e-05 / Validation loss: 0.00020527806410014336 / Long term Validation loss: 0.1585\n",
      "Epoch: [5591/10000] Training loss: 1.831228316476742e-05 / Validation loss: 0.00020358437406158196 / Long term Validation loss: 0.1583\n",
      "Epoch: [5592/10000] Training loss: 1.8305091351182297e-05 / Validation loss: 0.0002045531708117406 / Long term Validation loss: 0.1584\n",
      "Epoch: [5593/10000] Training loss: 1.8293665323440386e-05 / Validation loss: 0.00020460022175046552 / Long term Validation loss: 0.1584\n",
      "Epoch: [5594/10000] Training loss: 1.8289521780994392e-05 / Validation loss: 0.00020353508169656907 / Long term Validation loss: 0.1583\n",
      "Epoch: [5595/10000] Training loss: 1.8289438369620816e-05 / Validation loss: 0.00020484370377082302 / Long term Validation loss: 0.1585\n",
      "Epoch: [5596/10000] Training loss: 1.8283873891485567e-05 / Validation loss: 0.00020388151387654244 / Long term Validation loss: 0.1584\n",
      "Epoch: [5597/10000] Training loss: 1.8274619382631984e-05 / Validation loss: 0.00020389703102777044 / Long term Validation loss: 0.1584\n",
      "Epoch: [5598/10000] Training loss: 1.8269516913210428e-05 / Validation loss: 0.0002046202993807538 / Long term Validation loss: 0.1584\n",
      "Epoch: [5599/10000] Training loss: 1.8267856765983856e-05 / Validation loss: 0.00020349314670928715 / Long term Validation loss: 0.1583\n",
      "Epoch: [5600/10000] Training loss: 1.826324384459115e-05 / Validation loss: 0.00020427811785808458 / Long term Validation loss: 0.1584\n",
      "Epoch: [5601/10000] Training loss: 1.8255573902072594e-05 / Validation loss: 0.00020407478412954588 / Long term Validation loss: 0.1584\n",
      "Epoch: [5602/10000] Training loss: 1.824998316694024e-05 / Validation loss: 0.0002035239229961761 / Long term Validation loss: 0.1583\n",
      "Epoch: [5603/10000] Training loss: 1.8247146845411644e-05 / Validation loss: 0.00020435563079321598 / Long term Validation loss: 0.1584\n",
      "Epoch: [5604/10000] Training loss: 1.8243005254386313e-05 / Validation loss: 0.0002035776427259335 / Long term Validation loss: 0.1583\n",
      "Epoch: [5605/10000] Training loss: 1.8236497357184e-05 / Validation loss: 0.00020379044957855066 / Long term Validation loss: 0.1584\n",
      "Epoch: [5606/10000] Training loss: 1.8230751477519887e-05 / Validation loss: 0.00020407554086892402 / Long term Validation loss: 0.1584\n",
      "Epoch: [5607/10000] Training loss: 1.8227048965491536e-05 / Validation loss: 0.00020337222371667616 / Long term Validation loss: 0.1583\n",
      "Epoch: [5608/10000] Training loss: 1.822305073455826e-05 / Validation loss: 0.0002039841873912063 / Long term Validation loss: 0.1584\n",
      "Epoch: [5609/10000] Training loss: 1.8217376595361953e-05 / Validation loss: 0.00020363912157335206 / Long term Validation loss: 0.1584\n",
      "Epoch: [5610/10000] Training loss: 1.8211696437839905e-05 / Validation loss: 0.00020344799797019646 / Long term Validation loss: 0.1583\n",
      "Epoch: [5611/10000] Training loss: 1.820738619524544e-05 / Validation loss: 0.0002039052999306246 / Long term Validation loss: 0.1584\n",
      "Epoch: [5612/10000] Training loss: 1.8203325543450688e-05 / Validation loss: 0.00020331202613398317 / Long term Validation loss: 0.1583\n",
      "Epoch: [5613/10000] Training loss: 1.819821472951752e-05 / Validation loss: 0.00020361044314696437 / Long term Validation loss: 0.1584\n",
      "Epoch: [5614/10000] Training loss: 1.8192727409102843e-05 / Validation loss: 0.00020359790714833127 / Long term Validation loss: 0.1584\n",
      "Epoch: [5615/10000] Training loss: 1.8188030488082263e-05 / Validation loss: 0.00020322352986030498 / Long term Validation loss: 0.1583\n",
      "Epoch: [5616/10000] Training loss: 1.8183798027046353e-05 / Validation loss: 0.00020364306162876702 / Long term Validation loss: 0.1584\n",
      "Epoch: [5617/10000] Training loss: 1.817902712892669e-05 / Validation loss: 0.00020326360716825496 / Long term Validation loss: 0.1584\n",
      "Epoch: [5618/10000] Training loss: 1.8173781444388202e-05 / Validation loss: 0.00020329955118544483 / Long term Validation loss: 0.1584\n",
      "Epoch: [5619/10000] Training loss: 1.8168880976448174e-05 / Validation loss: 0.00020346599268603228 / Long term Validation loss: 0.1584\n",
      "Epoch: [5620/10000] Training loss: 1.8164444245394036e-05 / Validation loss: 0.0002030727973110226 / Long term Validation loss: 0.1584\n",
      "Epoch: [5621/10000] Training loss: 1.8159835871213404e-05 / Validation loss: 0.00020335724626795455 / Long term Validation loss: 0.1584\n",
      "Epoch: [5622/10000] Training loss: 1.8154820824425176e-05 / Validation loss: 0.00020317832893008698 / Long term Validation loss: 0.1584\n",
      "Epoch: [5623/10000] Training loss: 1.814985699218121e-05 / Validation loss: 0.0002030500489331393 / Long term Validation loss: 0.1584\n",
      "Epoch: [5624/10000] Training loss: 1.8145238834644925e-05 / Validation loss: 0.00020326844279943172 / Long term Validation loss: 0.1584\n",
      "Epoch: [5625/10000] Training loss: 1.8140666763937874e-05 / Validation loss: 0.00020294596242143934 / Long term Validation loss: 0.1584\n",
      "Epoch: [5626/10000] Training loss: 1.8135831032279326e-05 / Validation loss: 0.00020308644635865525 / Long term Validation loss: 0.1584\n",
      "Epoch: [5627/10000] Training loss: 1.8130896170066396e-05 / Validation loss: 0.0002030504289527231 / Long term Validation loss: 0.1584\n",
      "Epoch: [5628/10000] Training loss: 1.8126152504200395e-05 / Validation loss: 0.0002028518916399927 / Long term Validation loss: 0.1584\n",
      "Epoch: [5629/10000] Training loss: 1.8121542474042532e-05 / Validation loss: 0.00020304696104513252 / Long term Validation loss: 0.1584\n",
      "Epoch: [5630/10000] Training loss: 1.811681706624205e-05 / Validation loss: 0.0002028208811866982 / Long term Validation loss: 0.1584\n",
      "Epoch: [5631/10000] Training loss: 1.811195517388029e-05 / Validation loss: 0.00020284722811006106 / Long term Validation loss: 0.1584\n",
      "Epoch: [5632/10000] Training loss: 1.8107151365808445e-05 / Validation loss: 0.00020288841998666453 / Long term Validation loss: 0.1584\n",
      "Epoch: [5633/10000] Training loss: 1.8102476904417023e-05 / Validation loss: 0.0002026810920222025 / Long term Validation loss: 0.1584\n",
      "Epoch: [5634/10000] Training loss: 1.8097795546851465e-05 / Validation loss: 0.00020282084152655055 / Long term Validation loss: 0.1584\n",
      "Epoch: [5635/10000] Training loss: 1.809301013035365e-05 / Validation loss: 0.00020268120899176106 / Long term Validation loss: 0.1584\n",
      "Epoch: [5636/10000] Training loss: 1.8088201064320304e-05 / Validation loss: 0.00020263524456389445 / Long term Validation loss: 0.1584\n",
      "Epoch: [5637/10000] Training loss: 1.8083470924341616e-05 / Validation loss: 0.0002027056506513591 / Long term Validation loss: 0.1584\n",
      "Epoch: [5638/10000] Training loss: 1.8078786532989497e-05 / Validation loss: 0.00020252266993730722 / Long term Validation loss: 0.1584\n",
      "Epoch: [5639/10000] Training loss: 1.8074054838427175e-05 / Validation loss: 0.000202604526419905 / Long term Validation loss: 0.1584\n",
      "Epoch: [5640/10000] Training loss: 1.8069272731057664e-05 / Validation loss: 0.000202527672770138 / Long term Validation loss: 0.1584\n",
      "Epoch: [5641/10000] Training loss: 1.8064513823896776e-05 / Validation loss: 0.0002024441353301039 / Long term Validation loss: 0.1584\n",
      "Epoch: [5642/10000] Training loss: 1.8059805064900887e-05 / Validation loss: 0.00020251333018374623 / Long term Validation loss: 0.1584\n",
      "Epoch: [5643/10000] Training loss: 1.8055096258034532e-05 / Validation loss: 0.00020236429674520595 / Long term Validation loss: 0.1584\n",
      "Epoch: [5644/10000] Training loss: 1.8050348401165155e-05 / Validation loss: 0.00020239788458744878 / Long term Validation loss: 0.1584\n",
      "Epoch: [5645/10000] Training loss: 1.8045588286445946e-05 / Validation loss: 0.00020235953786368764 / Long term Validation loss: 0.1584\n",
      "Epoch: [5646/10000] Training loss: 1.804085662595166e-05 / Validation loss: 0.00020226270317646523 / Long term Validation loss: 0.1584\n",
      "Epoch: [5647/10000] Training loss: 1.8036146567645514e-05 / Validation loss: 0.00020231714545751526 / Long term Validation loss: 0.1585\n",
      "Epoch: [5648/10000] Training loss: 1.803142283424031e-05 / Validation loss: 0.00020220093514795973 / Long term Validation loss: 0.1584\n",
      "Epoch: [5649/10000] Training loss: 1.802667771308436e-05 / Validation loss: 0.00020220059426530795 / Long term Validation loss: 0.1585\n",
      "Epoch: [5650/10000] Training loss: 1.8021936692383077e-05 / Validation loss: 0.00020218235002244364 / Long term Validation loss: 0.1585\n",
      "Epoch: [5651/10000] Training loss: 1.801721564633662e-05 / Validation loss: 0.00020208601130587982 / Long term Validation loss: 0.1585\n",
      "Epoch: [5652/10000] Training loss: 1.8012500270280913e-05 / Validation loss: 0.0002021214683339013 / Long term Validation loss: 0.1585\n",
      "Epoch: [5653/10000] Training loss: 1.8007772143868808e-05 / Validation loss: 0.00020203106851192376 / Long term Validation loss: 0.1585\n",
      "Epoch: [5654/10000] Training loss: 1.8003035290820813e-05 / Validation loss: 0.00020200899579664213 / Long term Validation loss: 0.1585\n",
      "Epoch: [5655/10000] Training loss: 1.7998306191156308e-05 / Validation loss: 0.0002019984502510959 / Long term Validation loss: 0.1585\n",
      "Epoch: [5656/10000] Training loss: 1.799358834592861e-05 / Validation loss: 0.00020190916153746554 / Long term Validation loss: 0.1585\n",
      "Epoch: [5657/10000] Training loss: 1.7988869969621822e-05 / Validation loss: 0.00020192724828336296 / Long term Validation loss: 0.1585\n",
      "Epoch: [5658/10000] Training loss: 1.7984143252270915e-05 / Validation loss: 0.00020185569123212934 / Long term Validation loss: 0.1585\n",
      "Epoch: [5659/10000] Training loss: 1.7979414231804113e-05 / Validation loss: 0.00020182175785405396 / Long term Validation loss: 0.1585\n",
      "Epoch: [5660/10000] Training loss: 1.79746918492682e-05 / Validation loss: 0.00020181222529696557 / Long term Validation loss: 0.1585\n",
      "Epoch: [5661/10000] Training loss: 1.7969975163165465e-05 / Validation loss: 0.00020173212506974507 / Long term Validation loss: 0.1585\n",
      "Epoch: [5662/10000] Training loss: 1.7965256555376605e-05 / Validation loss: 0.00020173615643717858 / Long term Validation loss: 0.1585\n",
      "Epoch: [5663/10000] Training loss: 1.7960533334726223e-05 / Validation loss: 0.0002016769988029707 / Long term Validation loss: 0.1585\n",
      "Epoch: [5664/10000] Training loss: 1.795581032622638e-05 / Validation loss: 0.0002016373364339022 / Long term Validation loss: 0.1585\n",
      "Epoch: [5665/10000] Training loss: 1.7951091963995587e-05 / Validation loss: 0.00020162526319772063 / Long term Validation loss: 0.1585\n",
      "Epoch: [5666/10000] Training loss: 1.7946376435286663e-05 / Validation loss: 0.0002015540280242253 / Long term Validation loss: 0.1585\n",
      "Epoch: [5667/10000] Training loss: 1.7941659297312013e-05 / Validation loss: 0.00020154769752767435 / Long term Validation loss: 0.1585\n",
      "Epoch: [5668/10000] Training loss: 1.7936939868314526e-05 / Validation loss: 0.00020149645007406424 / Long term Validation loss: 0.1585\n",
      "Epoch: [5669/10000] Training loss: 1.7932221337175974e-05 / Validation loss: 0.00020145509323216682 / Long term Validation loss: 0.1585\n",
      "Epoch: [5670/10000] Training loss: 1.7927505850580082e-05 / Validation loss: 0.00020143934117792362 / Long term Validation loss: 0.1585\n",
      "Epoch: [5671/10000] Training loss: 1.79227919043336e-05 / Validation loss: 0.00020137579733763624 / Long term Validation loss: 0.1585\n",
      "Epoch: [5672/10000] Training loss: 1.791807699610048e-05 / Validation loss: 0.0002013620920146917 / Long term Validation loss: 0.1585\n",
      "Epoch: [5673/10000] Training loss: 1.7913361058365195e-05 / Validation loss: 0.00020131541536049684 / Long term Validation loss: 0.1585\n",
      "Epoch: [5674/10000] Training loss: 1.7908646029429044e-05 / Validation loss: 0.00020127424931870014 / Long term Validation loss: 0.1585\n",
      "Epoch: [5675/10000] Training loss: 1.7903932996463502e-05 / Validation loss: 0.0002012544198907572 / Long term Validation loss: 0.1585\n",
      "Epoch: [5676/10000] Training loss: 1.7899220966316983e-05 / Validation loss: 0.0002011969939456078 / Long term Validation loss: 0.1585\n",
      "Epoch: [5677/10000] Training loss: 1.789450851898119e-05 / Validation loss: 0.00020117811519320338 / Long term Validation loss: 0.1585\n",
      "Epoch: [5678/10000] Training loss: 1.788979568749358e-05 / Validation loss: 0.00020113398427858172 / Long term Validation loss: 0.1585\n",
      "Epoch: [5679/10000] Training loss: 1.7885083601502978e-05 / Validation loss: 0.00020109404771587045 / Long term Validation loss: 0.1585\n",
      "Epoch: [5680/10000] Training loss: 1.7880372873101578e-05 / Validation loss: 0.00020107047512592024 / Long term Validation loss: 0.1585\n",
      "Epoch: [5681/10000] Training loss: 1.787566292737114e-05 / Validation loss: 0.00020101790873132706 / Long term Validation loss: 0.1585\n",
      "Epoch: [5682/10000] Training loss: 1.787095293565683e-05 / Validation loss: 0.00020099537370646514 / Long term Validation loss: 0.1585\n",
      "Epoch: [5683/10000] Training loss: 1.7866242889276852e-05 / Validation loss: 0.0002009524968521358 / Long term Validation loss: 0.1586\n",
      "Epoch: [5684/10000] Training loss: 1.7861533425440633e-05 / Validation loss: 0.00020091401994936452 / Long term Validation loss: 0.1586\n",
      "Epoch: [5685/10000] Training loss: 1.7856824932819164e-05 / Validation loss: 0.00020088702718549408 / Long term Validation loss: 0.1586\n",
      "Epoch: [5686/10000] Training loss: 1.7852117119693573e-05 / Validation loss: 0.00020083822158520933 / Long term Validation loss: 0.1586\n",
      "Epoch: [5687/10000] Training loss: 1.7847409491280826e-05 / Validation loss: 0.00020081297840302546 / Long term Validation loss: 0.1586\n",
      "Epoch: [5688/10000] Training loss: 1.7842701985952385e-05 / Validation loss: 0.00020077072912296376 / Long term Validation loss: 0.1586\n",
      "Epoch: [5689/10000] Training loss: 1.783799494468512e-05 / Validation loss: 0.00020073377218458874 / Long term Validation loss: 0.1586\n",
      "Epoch: [5690/10000] Training loss: 1.7833288633509956e-05 / Validation loss: 0.00020070390376060733 / Long term Validation loss: 0.1586\n",
      "Epoch: [5691/10000] Training loss: 1.7828582934871492e-05 / Validation loss: 0.00020065819781732737 / Long term Validation loss: 0.1586\n",
      "Epoch: [5692/10000] Training loss: 1.7823877555511774e-05 / Validation loss: 0.00020063088549709368 / Long term Validation loss: 0.1586\n",
      "Epoch: [5693/10000] Training loss: 1.781917240495407e-05 / Validation loss: 0.00020058901855806048 / Long term Validation loss: 0.1586\n",
      "Epoch: [5694/10000] Training loss: 1.7814467645639758e-05 / Validation loss: 0.00020055339801195354 / Long term Validation loss: 0.1586\n",
      "Epoch: [5695/10000] Training loss: 1.7809763460121093e-05 / Validation loss: 0.00020052108147999314 / Long term Validation loss: 0.1586\n",
      "Epoch: [5696/10000] Training loss: 1.780505982829674e-05 / Validation loss: 0.000200477948270467 / Long term Validation loss: 0.1586\n",
      "Epoch: [5697/10000] Training loss: 1.780035658674931e-05 / Validation loss: 0.00020044889958989963 / Long term Validation loss: 0.1586\n",
      "Epoch: [5698/10000] Training loss: 1.7795653642583407e-05 / Validation loss: 0.00020040742898503518 / Long term Validation loss: 0.1586\n",
      "Epoch: [5699/10000] Training loss: 1.779095105269349e-05 / Validation loss: 0.00020037292959578808 / Long term Validation loss: 0.1586\n",
      "Epoch: [5700/10000] Training loss: 1.7786248933663e-05 / Validation loss: 0.00020033866044560865 / Long term Validation loss: 0.1586\n",
      "Epoch: [5701/10000] Training loss: 1.7781547311013057e-05 / Validation loss: 0.00020029779825299682 / Long term Validation loss: 0.1586\n",
      "Epoch: [5702/10000] Training loss: 1.7776846106892568e-05 / Validation loss: 0.00020026723039028 / Long term Validation loss: 0.1586\n",
      "Epoch: [5703/10000] Training loss: 1.777214524496905e-05 / Validation loss: 0.00020022631117877753 / Long term Validation loss: 0.1586\n",
      "Epoch: [5704/10000] Training loss: 1.7767444724963515e-05 / Validation loss: 0.00020019260750689564 / Long term Validation loss: 0.1586\n",
      "Epoch: [5705/10000] Training loss: 1.77627446101361e-05 / Validation loss: 0.00020015679664060125 / Long term Validation loss: 0.1586\n",
      "Epoch: [5706/10000] Training loss: 1.775804493786748e-05 / Validation loss: 0.0002001178892120931 / Long term Validation loss: 0.1586\n",
      "Epoch: [5707/10000] Training loss: 1.7753345683702702e-05 / Validation loss: 0.00020008587796149988 / Long term Validation loss: 0.1586\n",
      "Epoch: [5708/10000] Training loss: 1.7748646797099678e-05 / Validation loss: 0.00020004570443045642 / Long term Validation loss: 0.1586\n",
      "Epoch: [5709/10000] Training loss: 1.774394825394996e-05 / Validation loss: 0.000200012437894363 / Long term Validation loss: 0.1586\n",
      "Epoch: [5710/10000] Training loss: 1.7739250077977397e-05 / Validation loss: 0.00019997553504922392 / Long term Validation loss: 0.1586\n",
      "Epoch: [5711/10000] Training loss: 1.7734552298857103e-05 / Validation loss: 0.0001999383036857779 / Long term Validation loss: 0.1586\n",
      "Epoch: [5712/10000] Training loss: 1.7729854919964092e-05 / Validation loss: 0.0001999049385459136 / Long term Validation loss: 0.1586\n",
      "Epoch: [5713/10000] Training loss: 1.7725157916429838e-05 / Validation loss: 0.00019986570900744645 / Long term Validation loss: 0.1587\n",
      "Epoch: [5714/10000] Training loss: 1.7720461262051188e-05 / Validation loss: 0.0001998324852400663 / Long term Validation loss: 0.1587\n",
      "Epoch: [5715/10000] Training loss: 1.771576495645007e-05 / Validation loss: 0.0001997949150954286 / Long term Validation loss: 0.1587\n",
      "Epoch: [5716/10000] Training loss: 1.7711069013964997e-05 / Validation loss: 0.00019975899373240267 / Long term Validation loss: 0.1587\n",
      "Epoch: [5717/10000] Training loss: 1.7706373446451574e-05 / Validation loss: 0.0001997243752923784 / Long term Validation loss: 0.1587\n",
      "Epoch: [5718/10000] Training loss: 1.770167824759263e-05 / Validation loss: 0.0001996862145012864 / Long term Validation loss: 0.1587\n",
      "Epoch: [5719/10000] Training loss: 1.7696983400156894e-05 / Validation loss: 0.00019965267171690758 / Long term Validation loss: 0.1587\n",
      "Epoch: [5720/10000] Training loss: 1.769228889422865e-05 / Validation loss: 0.00019961488500450987 / Long term Validation loss: 0.1587\n",
      "Epoch: [5721/10000] Training loss: 1.768759473045937e-05 / Validation loss: 0.00019957988100173342 / Long term Validation loss: 0.1587\n",
      "Epoch: [5722/10000] Training loss: 1.7682900917457604e-05 / Validation loss: 0.00019954422667635522 / Long term Validation loss: 0.1587\n",
      "Epoch: [5723/10000] Training loss: 1.767820745739985e-05 / Validation loss: 0.00019950716922222597 / Long term Validation loss: 0.1587\n",
      "Epoch: [5724/10000] Training loss: 1.7673514343776136e-05 / Validation loss: 0.00019947302888972124 / Long term Validation loss: 0.1587\n",
      "Epoch: [5725/10000] Training loss: 1.7668821567385447e-05 / Validation loss: 0.0001994354331680731 / Long term Validation loss: 0.1587\n",
      "Epoch: [5726/10000] Training loss: 1.76641291219481e-05 / Validation loss: 0.00019940089768790634 / Long term Validation loss: 0.1587\n",
      "Epoch: [5727/10000] Training loss: 1.7659437009160438e-05 / Validation loss: 0.00019936451207990747 / Long term Validation loss: 0.1587\n",
      "Epoch: [5728/10000] Training loss: 1.765474523135998e-05 / Validation loss: 0.00019932844548767233 / Long term Validation loss: 0.1587\n",
      "Epoch: [5729/10000] Training loss: 1.7650053788511733e-05 / Validation loss: 0.0001992935712191726 / Long term Validation loss: 0.1587\n",
      "Epoch: [5730/10000] Training loss: 1.7645362676054968e-05 / Validation loss: 0.0001992565002600525 / Long term Validation loss: 0.1587\n",
      "Epoch: [5731/10000] Training loss: 1.7640671887593434e-05 / Validation loss: 0.0001992220176893595 / Long term Validation loss: 0.1587\n",
      "Epoch: [5732/10000] Training loss: 1.763598142016418e-05 / Validation loss: 0.0001991853017506853 / Long term Validation loss: 0.1587\n",
      "Epoch: [5733/10000] Training loss: 1.763129127275634e-05 / Validation loss: 0.00019914998373597994 / Long term Validation loss: 0.1587\n",
      "Epoch: [5734/10000] Training loss: 1.7626601446355068e-05 / Validation loss: 0.00019911441804998823 / Long term Validation loss: 0.1587\n",
      "Epoch: [5735/10000] Training loss: 1.762191193970441e-05 / Validation loss: 0.00019907804853430892 / Long term Validation loss: 0.1587\n",
      "Epoch: [5736/10000] Training loss: 1.7617222749465985e-05 / Validation loss: 0.00019904327996882372 / Long term Validation loss: 0.1587\n",
      "Epoch: [5737/10000] Training loss: 1.7612533871985913e-05 / Validation loss: 0.00019900663116886855 / Long term Validation loss: 0.1588\n",
      "Epoch: [5738/10000] Training loss: 1.76078453039665e-05 / Validation loss: 0.00019897170942049903 / Long term Validation loss: 0.1588\n",
      "Epoch: [5739/10000] Training loss: 1.7603157044596407e-05 / Validation loss: 0.00019893566494146476 / Long term Validation loss: 0.1588\n",
      "Epoch: [5740/10000] Training loss: 1.759846909292327e-05 / Validation loss: 0.00019889998453643385 / Long term Validation loss: 0.1588\n",
      "Epoch: [5741/10000] Training loss: 1.7593781447882822e-05 / Validation loss: 0.0001988647634051564 / Long term Validation loss: 0.1588\n",
      "Epoch: [5742/10000] Training loss: 1.7589094107167284e-05 / Validation loss: 0.000198828501658815 / Long term Validation loss: 0.1588\n",
      "Epoch: [5743/10000] Training loss: 1.7584407067732125e-05 / Validation loss: 0.0001987936163105267 / Long term Validation loss: 0.1588\n",
      "Epoch: [5744/10000] Training loss: 1.7579720327408617e-05 / Validation loss: 0.00019875741975446782 / Long term Validation loss: 0.1588\n",
      "Epoch: [5745/10000] Training loss: 1.757503388419317e-05 / Validation loss: 0.00019872222876062324 / Long term Validation loss: 0.1588\n",
      "Epoch: [5746/10000] Training loss: 1.7570347737192935e-05 / Validation loss: 0.00019868658546366697 / Long term Validation loss: 0.1588\n",
      "Epoch: [5747/10000] Training loss: 1.7565661884938936e-05 / Validation loss: 0.00019865084627154747 / Long term Validation loss: 0.1588\n",
      "Epoch: [5748/10000] Training loss: 1.756097632567723e-05 / Validation loss: 0.00019861572804648052 / Long term Validation loss: 0.1588\n",
      "Epoch: [5749/10000] Training loss: 1.7556291057334385e-05 / Validation loss: 0.00019857970448230658 / Long term Validation loss: 0.1588\n",
      "Epoch: [5750/10000] Training loss: 1.7551606077580435e-05 / Validation loss: 0.00019854469958304583 / Long term Validation loss: 0.1588\n",
      "Epoch: [5751/10000] Training loss: 1.7546921384857596e-05 / Validation loss: 0.0001985088463536463 / Long term Validation loss: 0.1588\n",
      "Epoch: [5752/10000] Training loss: 1.7542236977486868e-05 / Validation loss: 0.00019847356073056428 / Long term Validation loss: 0.1588\n",
      "Epoch: [5753/10000] Training loss: 1.7537552854281853e-05 / Validation loss: 0.00019843813285316158 / Long term Validation loss: 0.1588\n",
      "Epoch: [5754/10000] Training loss: 1.7532869013650517e-05 / Validation loss: 0.00019840248614766358 / Long term Validation loss: 0.1588\n",
      "Epoch: [5755/10000] Training loss: 1.7528185453848878e-05 / Validation loss: 0.0001983673899274225 / Long term Validation loss: 0.1588\n",
      "Epoch: [5756/10000] Training loss: 1.7523502173153092e-05 / Validation loss: 0.00019833160778465688 / Long term Validation loss: 0.1588\n",
      "Epoch: [5757/10000] Training loss: 1.7518819169680104e-05 / Validation loss: 0.00019829654585962428 / Long term Validation loss: 0.1588\n",
      "Epoch: [5758/10000] Training loss: 1.7514136442054255e-05 / Validation loss: 0.00019826092817000298 / Long term Validation loss: 0.1588\n",
      "Epoch: [5759/10000] Training loss: 1.750945398871223e-05 / Validation loss: 0.00019822565946906479 / Long term Validation loss: 0.1589\n",
      "Epoch: [5760/10000] Training loss: 1.7504771808368458e-05 / Validation loss: 0.0001981903484021652 / Long term Validation loss: 0.1589\n",
      "Epoch: [5761/10000] Training loss: 1.7500089899510825e-05 / Validation loss: 0.00019815484706582625 / Long term Validation loss: 0.1589\n",
      "Epoch: [5762/10000] Training loss: 1.7495408260568528e-05 / Validation loss: 0.00019811976105506098 / Long term Validation loss: 0.1589\n",
      "Epoch: [5763/10000] Training loss: 1.7490726890051433e-05 / Validation loss: 0.0001980841871450776 / Long term Validation loss: 0.1589\n",
      "Epoch: [5764/10000] Training loss: 1.748604578634155e-05 / Validation loss: 0.00019804912616591352 / Long term Validation loss: 0.1589\n",
      "Epoch: [5765/10000] Training loss: 1.7481364948142945e-05 / Validation loss: 0.00019801367553532012 / Long term Validation loss: 0.1589\n",
      "Epoch: [5766/10000] Training loss: 1.7476684373997025e-05 / Validation loss: 0.0001979784825085434 / Long term Validation loss: 0.1589\n",
      "Epoch: [5767/10000] Training loss: 1.7472004062645048e-05 / Validation loss: 0.00019794324804328474 / Long term Validation loss: 0.1589\n",
      "Epoch: [5768/10000] Training loss: 1.746732401269563e-05 / Validation loss: 0.00019790790334035512 / Long term Validation loss: 0.1589\n",
      "Epoch: [5769/10000] Training loss: 1.746264422275432e-05 / Validation loss: 0.0001978728369075476 / Long term Validation loss: 0.1589\n",
      "Epoch: [5770/10000] Training loss: 1.7457964691484738e-05 / Validation loss: 0.00019783744043258637 / Long term Validation loss: 0.1589\n",
      "Epoch: [5771/10000] Training loss: 1.74532854174582e-05 / Validation loss: 0.0001978024148216121 / Long term Validation loss: 0.1589\n",
      "Epoch: [5772/10000] Training loss: 1.7448606399451096e-05 / Validation loss: 0.00019776709596460695 / Long term Validation loss: 0.1589\n",
      "Epoch: [5773/10000] Training loss: 1.744392763611668e-05 / Validation loss: 0.00019773200198181594 / Long term Validation loss: 0.1589\n",
      "Epoch: [5774/10000] Training loss: 1.7439249126269514e-05 / Validation loss: 0.00019769683294417216 / Long term Validation loss: 0.1589\n",
      "Epoch: [5775/10000] Training loss: 1.743457086863188e-05 / Validation loss: 0.00019766164215134644 / Long term Validation loss: 0.1589\n",
      "Epoch: [5776/10000] Training loss: 1.7429892861962437e-05 / Validation loss: 0.00019762660726797303 / Long term Validation loss: 0.1589\n",
      "Epoch: [5777/10000] Training loss: 1.7425215105041157e-05 / Validation loss: 0.0001975913708683228 / Long term Validation loss: 0.1589\n",
      "Epoch: [5778/10000] Training loss: 1.742053759659755e-05 / Validation loss: 0.00019755639572377167 / Long term Validation loss: 0.1589\n",
      "Epoch: [5779/10000] Training loss: 1.7415860335485787e-05 / Validation loss: 0.00019752119672037692 / Long term Validation loss: 0.1590\n",
      "Epoch: [5780/10000] Training loss: 1.7411183320473503e-05 / Validation loss: 0.00019748620416941174 / Long term Validation loss: 0.1590\n",
      "Epoch: [5781/10000] Training loss: 1.7406506550458638e-05 / Validation loss: 0.00019745110288340713 / Long term Validation loss: 0.1590\n",
      "Epoch: [5782/10000] Training loss: 1.740183002426563e-05 / Validation loss: 0.0001974160567601203 / Long term Validation loss: 0.1590\n",
      "Epoch: [5783/10000] Training loss: 1.7397153740782654e-05 / Validation loss: 0.00019738106253893697 / Long term Validation loss: 0.1590\n",
      "Epoch: [5784/10000] Training loss: 1.7392477698887654e-05 / Validation loss: 0.0001973459779473728 / Long term Validation loss: 0.1590\n",
      "Epoch: [5785/10000] Training loss: 1.7387801897450983e-05 / Validation loss: 0.00019731105584549677 / Long term Validation loss: 0.1590\n",
      "Epoch: [5786/10000] Training loss: 1.7383126335406478e-05 / Validation loss: 0.000197275979364577 / Long term Validation loss: 0.1590\n",
      "Epoch: [5787/10000] Training loss: 1.7378451011637534e-05 / Validation loss: 0.0001972410787342062 / Long term Validation loss: 0.1590\n",
      "Epoch: [5788/10000] Training loss: 1.7373775925122813e-05 / Validation loss: 0.0001972060570918081 / Long term Validation loss: 0.1590\n",
      "Epoch: [5789/10000] Training loss: 1.7369101074783382e-05 / Validation loss: 0.0001971711409993351 / Long term Validation loss: 0.1590\n",
      "Epoch: [5790/10000] Training loss: 1.7364426459615e-05 / Validation loss: 0.00019713619752199859 / Long term Validation loss: 0.1590\n",
      "Epoch: [5791/10000] Training loss: 1.735975207858349e-05 / Validation loss: 0.0001971012576934816 / Long term Validation loss: 0.1590\n",
      "Epoch: [5792/10000] Training loss: 1.7355077930683752e-05 / Validation loss: 0.00019706638632436063 / Long term Validation loss: 0.1590\n",
      "Epoch: [5793/10000] Training loss: 1.735040401492746e-05 / Validation loss: 0.00019703144026276452 / Long term Validation loss: 0.1590\n",
      "Epoch: [5794/10000] Training loss: 1.734573033031373e-05 / Validation loss: 0.00019699661530490515 / Long term Validation loss: 0.1590\n",
      "Epoch: [5795/10000] Training loss: 1.734105687589639e-05 / Validation loss: 0.00019696169201035782 / Long term Validation loss: 0.1590\n",
      "Epoch: [5796/10000] Training loss: 1.73363836506944e-05 / Validation loss: 0.0001969268845646854 / Long term Validation loss: 0.1590\n",
      "Epoch: [5797/10000] Training loss: 1.7331710653791697e-05 / Validation loss: 0.00019689200874178068 / Long term Validation loss: 0.1590\n",
      "Epoch: [5798/10000] Training loss: 1.7327037884238795e-05 / Validation loss: 0.00019685720026868365 / Long term Validation loss: 0.1591\n",
      "Epoch: [5799/10000] Training loss: 1.732236534113539e-05 / Validation loss: 0.00019682238257366297 / Long term Validation loss: 0.1591\n",
      "Epoch: [5800/10000] Training loss: 1.7317693023568576e-05 / Validation loss: 0.0001967875702556791 / Long term Validation loss: 0.1591\n",
      "Epoch: [5801/10000] Training loss: 1.731302093064686e-05 / Validation loss: 0.00019675280613913704 / Long term Validation loss: 0.1591\n",
      "Epoch: [5802/10000] Training loss: 1.7308349061494408e-05 / Validation loss: 0.00019671800010817562 / Long term Validation loss: 0.1591\n",
      "Epoch: [5803/10000] Training loss: 1.7303677415231152e-05 / Validation loss: 0.00019668327530238152 / Long term Validation loss: 0.1591\n",
      "Epoch: [5804/10000] Training loss: 1.729900599101463e-05 / Validation loss: 0.00019664849134557003 / Long term Validation loss: 0.1591\n",
      "Epoch: [5805/10000] Training loss: 1.729433478798352e-05 / Validation loss: 0.00019661378981222774 / Long term Validation loss: 0.1591\n",
      "Epoch: [5806/10000] Training loss: 1.7289663805321515e-05 / Validation loss: 0.00019657904188908414 / Long term Validation loss: 0.1591\n",
      "Epoch: [5807/10000] Training loss: 1.72849930421926e-05 / Validation loss: 0.00019654435228849183 / Long term Validation loss: 0.1591\n",
      "Epoch: [5808/10000] Training loss: 1.7280322497799123e-05 / Validation loss: 0.00019650964782442598 / Long term Validation loss: 0.1591\n",
      "Epoch: [5809/10000] Training loss: 1.727565217133421e-05 / Validation loss: 0.00019647496639442485 / Long term Validation loss: 0.1591\n",
      "Epoch: [5810/10000] Training loss: 1.7270982062014465e-05 / Validation loss: 0.00019644030525945902 / Long term Validation loss: 0.1591\n",
      "Epoch: [5811/10000] Training loss: 1.7266312169063044e-05 / Validation loss: 0.00019640563511146952 / Long term Validation loss: 0.1591\n",
      "Epoch: [5812/10000] Training loss: 1.7261642491710526e-05 / Validation loss: 0.00019637101153563273 / Long term Validation loss: 0.1591\n",
      "Epoch: [5813/10000] Training loss: 1.7256973029208587e-05 / Validation loss: 0.00019633635977007758 / Long term Validation loss: 0.1591\n",
      "Epoch: [5814/10000] Training loss: 1.7252303780804304e-05 / Validation loss: 0.00019630176563769104 / Long term Validation loss: 0.1591\n",
      "Epoch: [5815/10000] Training loss: 1.7247634745774346e-05 / Validation loss: 0.00019626714000492535 / Long term Validation loss: 0.1591\n",
      "Epoch: [5816/10000] Training loss: 1.7242965923385664e-05 / Validation loss: 0.00019623256797648387 / Long term Validation loss: 0.1591\n",
      "Epoch: [5817/10000] Training loss: 1.723829731293597e-05 / Validation loss: 0.00019619797434076926 / Long term Validation loss: 0.1592\n",
      "Epoch: [5818/10000] Training loss: 1.723362891371479e-05 / Validation loss: 0.00019616341980319477 / Long term Validation loss: 0.1592\n",
      "Epoch: [5819/10000] Training loss: 1.7228960725037517e-05 / Validation loss: 0.00019612886093481473 / Long term Validation loss: 0.1592\n",
      "Epoch: [5820/10000] Training loss: 1.722429274621782e-05 / Validation loss: 0.0001960943225191695 / Long term Validation loss: 0.1592\n",
      "Epoch: [5821/10000] Training loss: 1.7219624976587015e-05 / Validation loss: 0.0001960597981385042 / Long term Validation loss: 0.1592\n",
      "Epoch: [5822/10000] Training loss: 1.721495741548304e-05 / Validation loss: 0.00019602527713170304 / Long term Validation loss: 0.1592\n",
      "Epoch: [5823/10000] Training loss: 1.7210290062252913e-05 / Validation loss: 0.00019599078479065247 / Long term Validation loss: 0.1592\n",
      "Epoch: [5824/10000] Training loss: 1.7205622916257864e-05 / Validation loss: 0.0001959562840158062 / Long term Validation loss: 0.1592\n",
      "Epoch: [5825/10000] Training loss: 1.7200955976861448e-05 / Validation loss: 0.0001959218202976271 / Long term Validation loss: 0.1592\n",
      "Epoch: [5826/10000] Training loss: 1.719628924344647e-05 / Validation loss: 0.00019588734297289258 / Long term Validation loss: 0.1592\n",
      "Epoch: [5827/10000] Training loss: 1.719162271539414e-05 / Validation loss: 0.0001958529045629914 / Long term Validation loss: 0.1592\n",
      "Epoch: [5828/10000] Training loss: 1.7186956392107075e-05 / Validation loss: 0.0001958184534401087 / Long term Validation loss: 0.1592\n",
      "Epoch: [5829/10000] Training loss: 1.718229027298523e-05 / Validation loss: 0.00019578403780800996 / Long term Validation loss: 0.1592\n",
      "Epoch: [5830/10000] Training loss: 1.7177624357449295e-05 / Validation loss: 0.00019574961469701724 / Long term Validation loss: 0.1592\n",
      "Epoch: [5831/10000] Training loss: 1.7172958644918728e-05 / Validation loss: 0.00019571522034725493 / Long term Validation loss: 0.1592\n",
      "Epoch: [5832/10000] Training loss: 1.7168293134831108e-05 / Validation loss: 0.00019568082600813304 / Long term Validation loss: 0.1592\n",
      "Epoch: [5833/10000] Training loss: 1.7163627826625667e-05 / Validation loss: 0.0001956464524094863 / Long term Validation loss: 0.1592\n",
      "Epoch: [5834/10000] Training loss: 1.7158962719756153e-05 / Validation loss: 0.0001956120867165567 / Long term Validation loss: 0.1592\n",
      "Epoch: [5835/10000] Training loss: 1.715429781368139e-05 / Validation loss: 0.00019557773406127864 / Long term Validation loss: 0.1592\n",
      "Epoch: [5836/10000] Training loss: 1.7149633107870997e-05 / Validation loss: 0.0001955433963060535 / Long term Validation loss: 0.1592\n",
      "Epoch: [5837/10000] Training loss: 1.7144968601802992e-05 / Validation loss: 0.00019550906521693246 / Long term Validation loss: 0.1593\n",
      "Epoch: [5838/10000] Training loss: 1.714030429496282e-05 / Validation loss: 0.0001954747544193711 / Long term Validation loss: 0.1593\n",
      "Epoch: [5839/10000] Training loss: 1.7135640186846976e-05 / Validation loss: 0.0001954404456761111 / Long term Validation loss: 0.1593\n",
      "Epoch: [5840/10000] Training loss: 1.713097627695684e-05 / Validation loss: 0.0001954061608234366 / Long term Validation loss: 0.1593\n",
      "Epoch: [5841/10000] Training loss: 1.7126312564806738e-05 / Validation loss: 0.00019537187515222397 / Long term Validation loss: 0.1593\n",
      "Epoch: [5842/10000] Training loss: 1.712164904991405e-05 / Validation loss: 0.00019533761534675358 / Long term Validation loss: 0.1593\n",
      "Epoch: [5843/10000] Training loss: 1.711698573181027e-05 / Validation loss: 0.00019530335329684975 / Long term Validation loss: 0.1593\n",
      "Epoch: [5844/10000] Training loss: 1.7112322610028865e-05 / Validation loss: 0.00019526911783107379 / Long term Validation loss: 0.1593\n",
      "Epoch: [5845/10000] Training loss: 1.7107659684117873e-05 / Validation loss: 0.00019523487973483277 / Long term Validation loss: 0.1593\n",
      "Epoch: [5846/10000] Training loss: 1.7102996953626814e-05 / Validation loss: 0.00019520066811496431 / Long term Validation loss: 0.1593\n",
      "Epoch: [5847/10000] Training loss: 1.709833441811983e-05 / Validation loss: 0.0001951664541020811 / Long term Validation loss: 0.1593\n",
      "Epoch: [5848/10000] Training loss: 1.709367207716237e-05 / Validation loss: 0.00019513226603304308 / Long term Validation loss: 0.1593\n",
      "Epoch: [5849/10000] Training loss: 1.7089009930334246e-05 / Validation loss: 0.00019509807606435628 / Long term Validation loss: 0.1593\n",
      "Epoch: [5850/10000] Training loss: 1.7084347977216687e-05 / Validation loss: 0.00019506391141014374 / Long term Validation loss: 0.1593\n",
      "Epoch: [5851/10000] Training loss: 1.7079686217404877e-05 / Validation loss: 0.00019502974531116952 / Long term Validation loss: 0.1593\n",
      "Epoch: [5852/10000] Training loss: 1.7075024650495558e-05 / Validation loss: 0.00019499560405104407 / Long term Validation loss: 0.1593\n",
      "Epoch: [5853/10000] Training loss: 1.7070363276099043e-05 / Validation loss: 0.00019496146154135215 / Long term Validation loss: 0.1593\n",
      "Epoch: [5854/10000] Training loss: 1.7065702093827266e-05 / Validation loss: 0.0001949273437401925 / Long term Validation loss: 0.1593\n",
      "Epoch: [5855/10000] Training loss: 1.7061041103305534e-05 / Validation loss: 0.0001948932244571398 / Long term Validation loss: 0.1593\n",
      "Epoch: [5856/10000] Training loss: 1.7056380304160644e-05 / Validation loss: 0.00019485913025582094 / Long term Validation loss: 0.1593\n",
      "Epoch: [5857/10000] Training loss: 1.7051719696032716e-05 / Validation loss: 0.0001948250337651266 / Long term Validation loss: 0.1593\n",
      "Epoch: [5858/10000] Training loss: 1.7047059278563027e-05 / Validation loss: 0.00019479096338687136 / Long term Validation loss: 0.1593\n",
      "Epoch: [5859/10000] Training loss: 1.7042399051406435e-05 / Validation loss: 0.0001947568891711351 / Long term Validation loss: 0.1594\n",
      "Epoch: [5860/10000] Training loss: 1.703773901421832e-05 / Validation loss: 0.00019472284294191032 / Long term Validation loss: 0.1594\n",
      "Epoch: [5861/10000] Training loss: 1.70330791666682e-05 / Validation loss: 0.0001946887903637768 / Long term Validation loss: 0.1594\n",
      "Epoch: [5862/10000] Training loss: 1.7028419508425143e-05 / Validation loss: 0.00019465476875391615 / Long term Validation loss: 0.1594\n",
      "Epoch: [5863/10000] Training loss: 1.7023760039173304e-05 / Validation loss: 0.0001946207369935921 / Long term Validation loss: 0.1594\n",
      "Epoch: [5864/10000] Training loss: 1.7019100758594936e-05 / Validation loss: 0.0001945867406936398 / Long term Validation loss: 0.1594\n",
      "Epoch: [5865/10000] Training loss: 1.7014441666388975e-05 / Validation loss: 0.0001945527286533548 / Long term Validation loss: 0.1594\n",
      "Epoch: [5866/10000] Training loss: 1.7009782762250237e-05 / Validation loss: 0.00019451875869862554 / Long term Validation loss: 0.1594\n",
      "Epoch: [5867/10000] Training loss: 1.700512404589263e-05 / Validation loss: 0.0001944847648524681 / Long term Validation loss: 0.1594\n",
      "Epoch: [5868/10000] Training loss: 1.7000465517022824e-05 / Validation loss: 0.00019445082281716215 / Long term Validation loss: 0.1594\n",
      "Epoch: [5869/10000] Training loss: 1.699580717537017e-05 / Validation loss: 0.0001944168449696429 / Long term Validation loss: 0.1594\n",
      "Epoch: [5870/10000] Training loss: 1.6991149020652166e-05 / Validation loss: 0.0001943829332706335 / Long term Validation loss: 0.1594\n",
      "Epoch: [5871/10000] Training loss: 1.6986491052614376e-05 / Validation loss: 0.00019434896816859578 / Long term Validation loss: 0.1594\n",
      "Epoch: [5872/10000] Training loss: 1.6981833270983835e-05 / Validation loss: 0.00019431509055299297 / Long term Validation loss: 0.1594\n",
      "Epoch: [5873/10000] Training loss: 1.6977175675523567e-05 / Validation loss: 0.00019428113326039968 / Long term Validation loss: 0.1594\n",
      "Epoch: [5874/10000] Training loss: 1.6972518265968288e-05 / Validation loss: 0.00019424729560223977 / Long term Validation loss: 0.1594\n",
      "Epoch: [5875/10000] Training loss: 1.696786104210064e-05 / Validation loss: 0.00019421333847985002 / Long term Validation loss: 0.1594\n",
      "Epoch: [5876/10000] Training loss: 1.69632040036603e-05 / Validation loss: 0.00019417955009517324 / Long term Validation loss: 0.1594\n",
      "Epoch: [5877/10000] Training loss: 1.6958547150453196e-05 / Validation loss: 0.00019414558110840565 / Long term Validation loss: 0.1594\n",
      "Epoch: [5878/10000] Training loss: 1.695389048222008e-05 / Validation loss: 0.00019411185694824678 / Long term Validation loss: 0.1594\n",
      "Epoch: [5879/10000] Training loss: 1.694923399879642e-05 / Validation loss: 0.0001940778568227338 / Long term Validation loss: 0.1594\n",
      "Epoch: [5880/10000] Training loss: 1.6944577699918704e-05 / Validation loss: 0.00019404422117563338 / Long term Validation loss: 0.1594\n",
      "Epoch: [5881/10000] Training loss: 1.6939921585463167e-05 / Validation loss: 0.0001940101585602829 / Long term Validation loss: 0.1594\n",
      "Epoch: [5882/10000] Training loss: 1.6935265655155158e-05 / Validation loss: 0.0001939766513843886 / Long term Validation loss: 0.1594\n",
      "Epoch: [5883/10000] Training loss: 1.6930609908933286e-05 / Validation loss: 0.00019394247453076495 / Long term Validation loss: 0.1594\n",
      "Epoch: [5884/10000] Training loss: 1.6925954346505706e-05 / Validation loss: 0.00019390916241606514 / Long term Validation loss: 0.1595\n",
      "Epoch: [5885/10000] Training loss: 1.692129896791774e-05 / Validation loss: 0.00019387478469263487 / Long term Validation loss: 0.1595\n",
      "Epoch: [5886/10000] Training loss: 1.6916643772864954e-05 / Validation loss: 0.00019384178006338145 / Long term Validation loss: 0.1595\n",
      "Epoch: [5887/10000] Training loss: 1.6911988761598992e-05 / Validation loss: 0.0001938070544320198 / Long term Validation loss: 0.1595\n",
      "Epoch: [5888/10000] Training loss: 1.6907333933854387e-05 / Validation loss: 0.00019377454957992708 / Long term Validation loss: 0.1595\n",
      "Epoch: [5889/10000] Training loss: 1.690267929033407e-05 / Validation loss: 0.00019373922309028256 / Long term Validation loss: 0.1595\n",
      "Epoch: [5890/10000] Training loss: 1.68980248310322e-05 / Validation loss: 0.00019370755120589006 / Long term Validation loss: 0.1595\n",
      "Epoch: [5891/10000] Training loss: 1.6893370557758346e-05 / Validation loss: 0.00019367118291270432 / Long term Validation loss: 0.1595\n",
      "Epoch: [5892/10000] Training loss: 1.6888716471562893e-05 / Validation loss: 0.0001936409288194177 / Long term Validation loss: 0.1595\n",
      "Epoch: [5893/10000] Training loss: 1.688406257724381e-05 / Validation loss: 0.00019360273999838973 / Long term Validation loss: 0.1595\n",
      "Epoch: [5894/10000] Training loss: 1.6879408879632482e-05 / Validation loss: 0.00019357494340889786 / Long term Validation loss: 0.1595\n",
      "Epoch: [5895/10000] Training loss: 1.6874755392235876e-05 / Validation loss: 0.00019353354106959719 / Long term Validation loss: 0.1595\n",
      "Epoch: [5896/10000] Training loss: 1.687010213286578e-05 / Validation loss: 0.00019351007399014957 / Long term Validation loss: 0.1595\n",
      "Epoch: [5897/10000] Training loss: 1.6865449141937857e-05 / Validation loss: 0.00019346293465929805 / Long term Validation loss: 0.1595\n",
      "Epoch: [5898/10000] Training loss: 1.6860796481460163e-05 / Validation loss: 0.0001934472102238893 / Long term Validation loss: 0.1595\n",
      "Epoch: [5899/10000] Training loss: 1.6856144278851455e-05 / Validation loss: 0.00019338970522752696 / Long term Validation loss: 0.1595\n",
      "Epoch: [5900/10000] Training loss: 1.6851492747724607e-05 / Validation loss: 0.00019338802431613356 / Long term Validation loss: 0.1595\n",
      "Epoch: [5901/10000] Training loss: 1.6846842307330913e-05 / Validation loss: 0.0001933115588017642 / Long term Validation loss: 0.1595\n",
      "Epoch: [5902/10000] Training loss: 1.684219369937342e-05 / Validation loss: 0.00019333569773569972 / Long term Validation loss: 0.1595\n",
      "Epoch: [5903/10000] Training loss: 1.683754835314256e-05 / Validation loss: 0.00019322411885429813 / Long term Validation loss: 0.1595\n",
      "Epoch: [5904/10000] Training loss: 1.6832908885951548e-05 / Validation loss: 0.00019329635975963075 / Long term Validation loss: 0.1595\n",
      "Epoch: [5905/10000] Training loss: 1.6828280322281743e-05 / Validation loss: 0.00019311895127643725 / Long term Validation loss: 0.1595\n",
      "Epoch: [5906/10000] Training loss: 1.6823672085780144e-05 / Validation loss: 0.00019328197753171558 / Long term Validation loss: 0.1596\n",
      "Epoch: [5907/10000] Training loss: 1.6819102335684847e-05 / Validation loss: 0.00019297966212484952 / Long term Validation loss: 0.1595\n",
      "Epoch: [5908/10000] Training loss: 1.6814605756371406e-05 / Validation loss: 0.0001933162697878125 / Long term Validation loss: 0.1596\n",
      "Epoch: [5909/10000] Training loss: 1.681024969994759e-05 / Validation loss: 0.00019277419953473964 / Long term Validation loss: 0.1595\n",
      "Epoch: [5910/10000] Training loss: 1.6806164770886437e-05 / Validation loss: 0.00019344711416311542 / Long term Validation loss: 0.1597\n",
      "Epoch: [5911/10000] Training loss: 1.680260703582447e-05 / Validation loss: 0.00019243988543415905 / Long term Validation loss: 0.1594\n",
      "Epoch: [5912/10000] Training loss: 1.6800079771651577e-05 / Validation loss: 0.00019377355718027325 / Long term Validation loss: 0.1598\n",
      "Epoch: [5913/10000] Training loss: 1.67995801043806e-05 / Validation loss: 0.00019185558373992388 / Long term Validation loss: 0.1591\n",
      "Epoch: [5914/10000] Training loss: 1.680309249666682e-05 / Validation loss: 0.00019450802659488122 / Long term Validation loss: 0.1599\n",
      "Epoch: [5915/10000] Training loss: 1.6814590753012133e-05 / Validation loss: 0.00019079631211790457 / Long term Validation loss: 0.1588\n",
      "Epoch: [5916/10000] Training loss: 1.6842077592120103e-05 / Validation loss: 0.0001961320886233307 / Long term Validation loss: 0.1603\n",
      "Epoch: [5917/10000] Training loss: 1.6901739657032203e-05 / Validation loss: 0.00018889097150790121 / Long term Validation loss: 0.1584\n",
      "Epoch: [5918/10000] Training loss: 1.702652388103903e-05 / Validation loss: 0.0001998314373437522 / Long term Validation loss: 0.1596\n",
      "Epoch: [5919/10000] Training loss: 1.7283569374478416e-05 / Validation loss: 0.00018576079447056514 / Long term Validation loss: 0.1560\n",
      "Epoch: [5920/10000] Training loss: 1.7810433384555914e-05 / Validation loss: 0.0002088546196115843 / Long term Validation loss: 0.1567\n",
      "Epoch: [5921/10000] Training loss: 1.8886794873736627e-05 / Validation loss: 0.0001822782607369738 / Long term Validation loss: 0.1477\n",
      "Epoch: [5922/10000] Training loss: 2.107959133741346e-05 / Validation loss: 0.00023296993309255046 / Long term Validation loss: 0.1470\n",
      "Epoch: [5923/10000] Training loss: 2.549375478910282e-05 / Validation loss: 0.00018667619941425037 / Long term Validation loss: 0.1338\n",
      "Epoch: [5924/10000] Training loss: 3.417562375954191e-05 / Validation loss: 0.0002997959771693559 / Long term Validation loss: 0.1440\n",
      "Epoch: [5925/10000] Training loss: 5.0241310174742386e-05 / Validation loss: 0.00022826632297439527 / Long term Validation loss: 0.1379\n",
      "Epoch: [5926/10000] Training loss: 7.658287384828893e-05 / Validation loss: 0.0004340221784513801 / Long term Validation loss: 0.1499\n",
      "Epoch: [5927/10000] Training loss: 0.00010881753672059375 / Validation loss: 0.00028749993620946757 / Long term Validation loss: 0.1483\n",
      "Epoch: [5928/10000] Training loss: 0.00012556516794256546 / Validation loss: 0.0004115517100852384 / Long term Validation loss: 0.1482\n",
      "Epoch: [5929/10000] Training loss: 9.736839967559342e-05 / Validation loss: 0.00019198705197628028 / Long term Validation loss: 0.1320\n",
      "Epoch: [5930/10000] Training loss: 4.068882783930437e-05 / Validation loss: 0.00019123108522049284 / Long term Validation loss: 0.1590\n",
      "Epoch: [5931/10000] Training loss: 1.710181208200493e-05 / Validation loss: 0.0002923847780400501 / Long term Validation loss: 0.1436\n",
      "Epoch: [5932/10000] Training loss: 4.467734527315596e-05 / Validation loss: 0.00022294236375784506 / Long term Validation loss: 0.1334\n",
      "Epoch: [5933/10000] Training loss: 7.098592760471473e-05 / Validation loss: 0.0003130289687670191 / Long term Validation loss: 0.1436\n",
      "Epoch: [5934/10000] Training loss: 5.216578193697671e-05 / Validation loss: 0.00018791441876677567 / Long term Validation loss: 0.1533\n",
      "Epoch: [5935/10000] Training loss: 2.0072498831023712e-05 / Validation loss: 0.0001868714475876712 / Long term Validation loss: 0.1441\n",
      "Epoch: [5936/10000] Training loss: 2.4793683297309164e-05 / Validation loss: 0.00030250952959303166 / Long term Validation loss: 0.1434\n",
      "Epoch: [5937/10000] Training loss: 4.756383370060475e-05 / Validation loss: 0.0001963582309520369 / Long term Validation loss: 0.1320\n",
      "Epoch: [5938/10000] Training loss: 4.240834157484771e-05 / Validation loss: 0.00022140046329025895 / Long term Validation loss: 0.1516\n",
      "Epoch: [5939/10000] Training loss: 2.0168821593684803e-05 / Validation loss: 0.00022539013511840988 / Long term Validation loss: 0.1497\n",
      "Epoch: [5940/10000] Training loss: 2.1233993434422658e-05 / Validation loss: 0.0001919441391620146 / Long term Validation loss: 0.1335\n",
      "Epoch: [5941/10000] Training loss: 3.661307089535602e-05 / Validation loss: 0.0002610875345529396 / Long term Validation loss: 0.1454\n",
      "Epoch: [5942/10000] Training loss: 3.2706357676430406e-05 / Validation loss: 0.00018955512758046014 / Long term Validation loss: 0.1567\n",
      "Epoch: [5943/10000] Training loss: 1.8285327688717488e-05 / Validation loss: 0.00018677960613425694 / Long term Validation loss: 0.1498\n",
      "Epoch: [5944/10000] Training loss: 2.0735279281718215e-05 / Validation loss: 0.0002526714519074024 / Long term Validation loss: 0.1459\n",
      "Epoch: [5945/10000] Training loss: 3.0438144234744952e-05 / Validation loss: 0.00018608588720491273 / Long term Validation loss: 0.1390\n",
      "Epoch: [5946/10000] Training loss: 2.5805865454704793e-05 / Validation loss: 0.00020140226465444492 / Long term Validation loss: 0.1604\n",
      "Epoch: [5947/10000] Training loss: 1.7054392022853656e-05 / Validation loss: 0.000219253564252902 / Long term Validation loss: 0.1521\n",
      "Epoch: [5948/10000] Training loss: 2.070962757870037e-05 / Validation loss: 0.00018513179990725553 / Long term Validation loss: 0.1370\n",
      "Epoch: [5949/10000] Training loss: 2.6151790676060165e-05 / Validation loss: 0.00022039473073477058 / Long term Validation loss: 0.1510\n",
      "Epoch: [5950/10000] Training loss: 2.13507318194625e-05 / Validation loss: 0.00019407791646579233 / Long term Validation loss: 0.1605\n",
      "Epoch: [5951/10000] Training loss: 1.671995732199136e-05 / Validation loss: 0.000184028739151085 / Long term Validation loss: 0.1468\n",
      "Epoch: [5952/10000] Training loss: 2.048381465137104e-05 / Validation loss: 0.0002244241658201083 / Long term Validation loss: 0.1494\n",
      "Epoch: [5953/10000] Training loss: 2.2893927264070112e-05 / Validation loss: 0.00018490725397709852 / Long term Validation loss: 0.1513\n",
      "Epoch: [5954/10000] Training loss: 1.8777642904150645e-05 / Validation loss: 0.00019003042383011737 / Long term Validation loss: 0.1600\n",
      "Epoch: [5955/10000] Training loss: 1.6855215800570387e-05 / Validation loss: 0.00021305669428026074 / Long term Validation loss: 0.1551\n",
      "Epoch: [5956/10000] Training loss: 1.9947245115672724e-05 / Validation loss: 0.00018327135426326444 / Long term Validation loss: 0.1459\n",
      "Epoch: [5957/10000] Training loss: 2.0519113952544816e-05 / Validation loss: 0.00020103584554100547 / Long term Validation loss: 0.1605\n",
      "Epoch: [5958/10000] Training loss: 1.7464901611300536e-05 / Validation loss: 0.00019818190439179278 / Long term Validation loss: 0.1613\n",
      "Epoch: [5959/10000] Training loss: 1.7074322248160945e-05 / Validation loss: 0.0001835990424003268 / Long term Validation loss: 0.1494\n",
      "Epoch: [5960/10000] Training loss: 1.924410091955435e-05 / Validation loss: 0.00020819847374609188 / Long term Validation loss: 0.1576\n",
      "Epoch: [5961/10000] Training loss: 1.889354396892907e-05 / Validation loss: 0.00018873216113048175 / Long term Validation loss: 0.1596\n",
      "Epoch: [5962/10000] Training loss: 1.688641107608663e-05 / Validation loss: 0.0001871369373496391 / Long term Validation loss: 0.1572\n",
      "Epoch: [5963/10000] Training loss: 1.720028419371325e-05 / Validation loss: 0.00020680378565476141 / Long term Validation loss: 0.1582\n",
      "Epoch: [5964/10000] Training loss: 1.8537409573756328e-05 / Validation loss: 0.00018538671964790656 / Long term Validation loss: 0.1540\n",
      "Epoch: [5965/10000] Training loss: 1.7869147434369795e-05 / Validation loss: 0.000194104168940648 / Long term Validation loss: 0.1610\n",
      "Epoch: [5966/10000] Training loss: 1.6678891409287986e-05 / Validation loss: 0.00019977984254135382 / Long term Validation loss: 0.1607\n",
      "Epoch: [5967/10000] Training loss: 1.7206394624185882e-05 / Validation loss: 0.00018539552075206374 / Long term Validation loss: 0.1540\n",
      "Epoch: [5968/10000] Training loss: 1.793789315233049e-05 / Validation loss: 0.0002004196389163523 / Long term Validation loss: 0.1604\n",
      "Epoch: [5969/10000] Training loss: 1.72659110196808e-05 / Validation loss: 0.00019270358656233415 / Long term Validation loss: 0.1606\n",
      "Epoch: [5970/10000] Training loss: 1.6625395221144732e-05 / Validation loss: 0.00018766735176308346 / Long term Validation loss: 0.1581\n",
      "Epoch: [5971/10000] Training loss: 1.7129219895351028e-05 / Validation loss: 0.00020205699765157492 / Long term Validation loss: 0.1595\n",
      "Epoch: [5972/10000] Training loss: 1.74785857377623e-05 / Validation loss: 0.00018863270931791036 / Long term Validation loss: 0.1591\n",
      "Epoch: [5973/10000] Training loss: 1.693019778319553e-05 / Validation loss: 0.00019202241312465634 / Long term Validation loss: 0.1601\n",
      "Epoch: [5974/10000] Training loss: 1.6618122068866387e-05 / Validation loss: 0.00019901672105690328 / Long term Validation loss: 0.1603\n",
      "Epoch: [5975/10000] Training loss: 1.701495707258717e-05 / Validation loss: 0.00018758940084332944 / Long term Validation loss: 0.1579\n",
      "Epoch: [5976/10000] Training loss: 1.7152964535879593e-05 / Validation loss: 0.00019653666391445906 / Long term Validation loss: 0.1605\n",
      "Epoch: [5977/10000] Training loss: 1.6749755981317844e-05 / Validation loss: 0.0001942875682253266 / Long term Validation loss: 0.1602\n",
      "Epoch: [5978/10000] Training loss: 1.6614249760310053e-05 / Validation loss: 0.00018871253426261733 / Long term Validation loss: 0.1589\n",
      "Epoch: [5979/10000] Training loss: 1.6897313452864204e-05 / Validation loss: 0.00019847345907423065 / Long term Validation loss: 0.1602\n",
      "Epoch: [5980/10000] Training loss: 1.6932184164080507e-05 / Validation loss: 0.00019059829862804167 / Long term Validation loss: 0.1594\n",
      "Epoch: [5981/10000] Training loss: 1.6653102899822467e-05 / Validation loss: 0.00019132890692254452 / Long term Validation loss: 0.1595\n",
      "Epoch: [5982/10000] Training loss: 1.660279958843676e-05 / Validation loss: 0.0001970682359805251 / Long term Validation loss: 0.1602\n",
      "Epoch: [5983/10000] Training loss: 1.6793545588402087e-05 / Validation loss: 0.00018898405950310495 / Long term Validation loss: 0.1588\n",
      "Epoch: [5984/10000] Training loss: 1.6786228737336355e-05 / Validation loss: 0.0001942510968627432 / Long term Validation loss: 0.1598\n",
      "Epoch: [5985/10000] Training loss: 1.6598921966883443e-05 / Validation loss: 0.00019392938811741168 / Long term Validation loss: 0.1598\n",
      "Epoch: [5986/10000] Training loss: 1.6584638722282305e-05 / Validation loss: 0.00018931618745014197 / Long term Validation loss: 0.1588\n",
      "Epoch: [5987/10000] Training loss: 1.670923399215899e-05 / Validation loss: 0.0001957586200253332 / Long term Validation loss: 0.1601\n",
      "Epoch: [5988/10000] Training loss: 1.6689443275704482e-05 / Validation loss: 0.00019109006030965122 / Long term Validation loss: 0.1592\n",
      "Epoch: [5989/10000] Training loss: 1.6565144183443442e-05 / Validation loss: 0.00019095733632899803 / Long term Validation loss: 0.1591\n",
      "Epoch: [5990/10000] Training loss: 1.656302188285525e-05 / Validation loss: 0.0001950449889280936 / Long term Validation loss: 0.1599\n",
      "Epoch: [5991/10000] Training loss: 1.6643126433211207e-05 / Validation loss: 0.00018960841004893504 / Long term Validation loss: 0.1588\n",
      "Epoch: [5992/10000] Training loss: 1.662363636740403e-05 / Validation loss: 0.0001928759222540169 / Long term Validation loss: 0.1595\n",
      "Epoch: [5993/10000] Training loss: 1.654087212389823e-05 / Validation loss: 0.00019294416226468123 / Long term Validation loss: 0.1595\n",
      "Epoch: [5994/10000] Training loss: 1.6540723572055236e-05 / Validation loss: 0.00018964180083849556 / Long term Validation loss: 0.1588\n",
      "Epoch: [5995/10000] Training loss: 1.659186652201466e-05 / Validation loss: 0.00019389600312717762 / Long term Validation loss: 0.1597\n",
      "Epoch: [5996/10000] Training loss: 1.6576812306581803e-05 / Validation loss: 0.00019091699925778387 / Long term Validation loss: 0.1591\n",
      "Epoch: [5997/10000] Training loss: 1.6521064279464154e-05 / Validation loss: 0.0001907651829137105 / Long term Validation loss: 0.1591\n",
      "Epoch: [5998/10000] Training loss: 1.651933609379088e-05 / Validation loss: 0.00019345513398013732 / Long term Validation loss: 0.1597\n",
      "Epoch: [5999/10000] Training loss: 1.655173466463013e-05 / Validation loss: 0.00018985513079263243 / Long term Validation loss: 0.1588\n",
      "Epoch: [6000/10000] Training loss: 1.6541574093277535e-05 / Validation loss: 0.00019209682498658352 / Long term Validation loss: 0.1594\n",
      "Epoch: [6001/10000] Training loss: 1.6503439265412507e-05 / Validation loss: 0.00019203586021395816 / Long term Validation loss: 0.1594\n",
      "Epoch: [6002/10000] Training loss: 1.6499454096575428e-05 / Validation loss: 0.00018990639774063965 / Long term Validation loss: 0.1588\n",
      "Epoch: [6003/10000] Training loss: 1.651958054476576e-05 / Validation loss: 0.00019272933805291413 / Long term Validation loss: 0.1596\n",
      "Epoch: [6004/10000] Training loss: 1.6513356728616443e-05 / Validation loss: 0.00019064126832401868 / Long term Validation loss: 0.1591\n",
      "Epoch: [6005/10000] Training loss: 1.648687370626815e-05 / Validation loss: 0.0001907302957207017 / Long term Validation loss: 0.1591\n",
      "Epoch: [6006/10000] Training loss: 1.6481041353954464e-05 / Validation loss: 0.00019234128969995522 / Long term Validation loss: 0.1595\n",
      "Epoch: [6007/10000] Training loss: 1.6492942161675243e-05 / Validation loss: 0.00018997161882539665 / Long term Validation loss: 0.1589\n",
      "Epoch: [6008/10000] Training loss: 1.648941351551403e-05 / Validation loss: 0.00019164188064856055 / Long term Validation loss: 0.1594\n",
      "Epoch: [6009/10000] Training loss: 1.6470795163761448e-05 / Validation loss: 0.0001913385805883665 / Long term Validation loss: 0.1593\n",
      "Epoch: [6010/10000] Training loss: 1.6463818104784178e-05 / Validation loss: 0.0001901250577229537 / Long term Validation loss: 0.1590\n",
      "Epoch: [6011/10000] Training loss: 1.6470071269755834e-05 / Validation loss: 0.00019197327944698033 / Long term Validation loss: 0.1595\n",
      "Epoch: [6012/10000] Training loss: 1.6468114856205014e-05 / Validation loss: 0.00019041691611760742 / Long term Validation loss: 0.1591\n",
      "Epoch: [6013/10000] Training loss: 1.64549157210864e-05 / Validation loss: 0.0001907567362929645 / Long term Validation loss: 0.1592\n",
      "Epoch: [6014/10000] Training loss: 1.644747618740065e-05 / Validation loss: 0.00019156917395434783 / Long term Validation loss: 0.1594\n",
      "Epoch: [6015/10000] Training loss: 1.6449786925613066e-05 / Validation loss: 0.00019005141740421292 / Long term Validation loss: 0.1590\n",
      "Epoch: [6016/10000] Training loss: 1.6448522944652367e-05 / Validation loss: 0.00019132627417645353 / Long term Validation loss: 0.1593\n",
      "Epoch: [6017/10000] Training loss: 1.6439095646162444e-05 / Validation loss: 0.00019082286604878016 / Long term Validation loss: 0.1592\n",
      "Epoch: [6018/10000] Training loss: 1.6431738470071022e-05 / Validation loss: 0.00019027503118088869 / Long term Validation loss: 0.1591\n",
      "Epoch: [6019/10000] Training loss: 1.6431296507406975e-05 / Validation loss: 0.0001914124690887146 / Long term Validation loss: 0.1594\n",
      "Epoch: [6020/10000] Training loss: 1.6430090747894066e-05 / Validation loss: 0.00019024808965599377 / Long term Validation loss: 0.1591\n",
      "Epoch: [6021/10000] Training loss: 1.6423261214526282e-05 / Validation loss: 0.00019075133080998982 / Long term Validation loss: 0.1592\n",
      "Epoch: [6022/10000] Training loss: 1.6416361419437997e-05 / Validation loss: 0.00019100307725721384 / Long term Validation loss: 0.1593\n",
      "Epoch: [6023/10000] Training loss: 1.6414046818519374e-05 / Validation loss: 0.00019011298838877896 / Long term Validation loss: 0.1591\n",
      "Epoch: [6024/10000] Training loss: 1.6412489492932412e-05 / Validation loss: 0.0001910473766459248 / Long term Validation loss: 0.1593\n",
      "Epoch: [6025/10000] Training loss: 1.640737962893821e-05 / Validation loss: 0.00019045012386816884 / Long term Validation loss: 0.1592\n",
      "Epoch: [6026/10000] Training loss: 1.6401147633186046e-05 / Validation loss: 0.00019035257629805208 / Long term Validation loss: 0.1591\n",
      "Epoch: [6027/10000] Training loss: 1.6397640638284896e-05 / Validation loss: 0.00019094423074004488 / Long term Validation loss: 0.1593\n",
      "Epoch: [6028/10000] Training loss: 1.6395518084479646e-05 / Validation loss: 0.00019012326126192602 / Long term Validation loss: 0.1591\n",
      "Epoch: [6029/10000] Training loss: 1.639145518029612e-05 / Validation loss: 0.00019066436212583266 / Long term Validation loss: 0.1593\n",
      "Epoch: [6030/10000] Training loss: 1.63859573830502e-05 / Validation loss: 0.00019055547305065512 / Long term Validation loss: 0.1592\n",
      "Epoch: [6031/10000] Training loss: 1.6381789345920137e-05 / Validation loss: 0.00019013748641897644 / Long term Validation loss: 0.1591\n",
      "Epoch: [6032/10000] Training loss: 1.637904795385521e-05 / Validation loss: 0.00019074397892976722 / Long term Validation loss: 0.1593\n",
      "Epoch: [6033/10000] Training loss: 1.6375523343355643e-05 / Validation loss: 0.00019017554096736295 / Long term Validation loss: 0.1592\n",
      "Epoch: [6034/10000] Training loss: 1.637070744966969e-05 / Validation loss: 0.00019034233193153266 / Long term Validation loss: 0.1592\n",
      "Epoch: [6035/10000] Training loss: 1.6366277391253165e-05 / Validation loss: 0.00019052524000923442 / Long term Validation loss: 0.1593\n",
      "Epoch: [6036/10000] Training loss: 1.636298384317665e-05 / Validation loss: 0.00019002547146653622 / Long term Validation loss: 0.1592\n",
      "Epoch: [6037/10000] Training loss: 1.6359633120183132e-05 / Validation loss: 0.00019048270845177924 / Long term Validation loss: 0.1593\n",
      "Epoch: [6038/10000] Training loss: 1.6355362572745903e-05 / Validation loss: 0.00019019038964503173 / Long term Validation loss: 0.1592\n",
      "Epoch: [6039/10000] Training loss: 1.63509411770537e-05 / Validation loss: 0.00019010426935845445 / Long term Validation loss: 0.1592\n",
      "Epoch: [6040/10000] Training loss: 1.6347236968474683e-05 / Validation loss: 0.0001904013102864406 / Long term Validation loss: 0.1593\n",
      "Epoch: [6041/10000] Training loss: 1.6343831388378727e-05 / Validation loss: 0.0001899647715776262 / Long term Validation loss: 0.1592\n",
      "Epoch: [6042/10000] Training loss: 1.633992550626893e-05 / Validation loss: 0.00019023082579003523 / Long term Validation loss: 0.1593\n",
      "Epoch: [6043/10000] Training loss: 1.633566156961592e-05 / Validation loss: 0.00019014825745143862 / Long term Validation loss: 0.1593\n",
      "Epoch: [6044/10000] Training loss: 1.633171708568766e-05 / Validation loss: 0.0001899326962147964 / Long term Validation loss: 0.1592\n",
      "Epoch: [6045/10000] Training loss: 1.6328149376361368e-05 / Validation loss: 0.00019022319274388297 / Long term Validation loss: 0.1593\n",
      "Epoch: [6046/10000] Training loss: 1.63244275879041e-05 / Validation loss: 0.00018990456839007813 / Long term Validation loss: 0.1593\n",
      "Epoch: [6047/10000] Training loss: 1.6320364237265362e-05 / Validation loss: 0.00019000752164147032 / Long term Validation loss: 0.1593\n",
      "Epoch: [6048/10000] Training loss: 1.6316335555089236e-05 / Validation loss: 0.0001900518998175036 / Long term Validation loss: 0.1593\n",
      "Epoch: [6049/10000] Training loss: 1.6312595837650563e-05 / Validation loss: 0.00018980275555128862 / Long term Validation loss: 0.1593\n",
      "Epoch: [6050/10000] Training loss: 1.630891382812117e-05 / Validation loss: 0.00019003233118214564 / Long term Validation loss: 0.1593\n",
      "Epoch: [6051/10000] Training loss: 1.6305018216473537e-05 / Validation loss: 0.0001898323903496425 / Long term Validation loss: 0.1593\n",
      "Epoch: [6052/10000] Training loss: 1.630101512157429e-05 / Validation loss: 0.0001898241028965418 / Long term Validation loss: 0.1593\n",
      "Epoch: [6053/10000] Training loss: 1.6297154456040548e-05 / Validation loss: 0.00018992472607759833 / Long term Validation loss: 0.1594\n",
      "Epoch: [6054/10000] Training loss: 1.6293426604363815e-05 / Validation loss: 0.00018969430701833365 / Long term Validation loss: 0.1593\n",
      "Epoch: [6055/10000] Training loss: 1.628962737240336e-05 / Validation loss: 0.00018984792757062772 / Long term Validation loss: 0.1594\n",
      "Epoch: [6056/10000] Training loss: 1.6285699966844663e-05 / Validation loss: 0.00018973933210643363 / Long term Validation loss: 0.1593\n",
      "Epoch: [6057/10000] Training loss: 1.6281790083940382e-05 / Validation loss: 0.00018966821415206835 / Long term Validation loss: 0.1593\n",
      "Epoch: [6058/10000] Training loss: 1.627799101914102e-05 / Validation loss: 0.00018977964573830915 / Long term Validation loss: 0.1594\n",
      "Epoch: [6059/10000] Training loss: 1.62742164545551e-05 / Validation loss: 0.000189590604600182 / Long term Validation loss: 0.1593\n",
      "Epoch: [6060/10000] Training loss: 1.6270362009219004e-05 / Validation loss: 0.00018967817718237612 / Long term Validation loss: 0.1594\n",
      "Epoch: [6061/10000] Training loss: 1.6266461184811965e-05 / Validation loss: 0.00018963041755382212 / Long term Validation loss: 0.1594\n",
      "Epoch: [6062/10000] Training loss: 1.626260942239018e-05 / Validation loss: 0.00018953257032195743 / Long term Validation loss: 0.1593\n",
      "Epoch: [6063/10000] Training loss: 1.6258813694241915e-05 / Validation loss: 0.00018963049069431617 / Long term Validation loss: 0.1594\n",
      "Epoch: [6064/10000] Training loss: 1.625499969636304e-05 / Validation loss: 0.0001894844027710072 / Long term Validation loss: 0.1594\n",
      "Epoch: [6065/10000] Training loss: 1.6251134441862e-05 / Validation loss: 0.00018952168168305793 / Long term Validation loss: 0.1594\n",
      "Epoch: [6066/10000] Training loss: 1.624726540073425e-05 / Validation loss: 0.00018950742225519435 / Long term Validation loss: 0.1594\n",
      "Epoch: [6067/10000] Training loss: 1.624343690769148e-05 / Validation loss: 0.00018940413039688996 / Long term Validation loss: 0.1594\n",
      "Epoch: [6068/10000] Training loss: 1.6239627822187794e-05 / Validation loss: 0.00018947933865365858 / Long term Validation loss: 0.1594\n",
      "Epoch: [6069/10000] Training loss: 1.6235794234057323e-05 / Validation loss: 0.0001893696977080885 / Long term Validation loss: 0.1594\n",
      "Epoch: [6070/10000] Training loss: 1.623193566293473e-05 / Validation loss: 0.00018937387283272833 / Long term Validation loss: 0.1594\n",
      "Epoch: [6071/10000] Training loss: 1.622808723803077e-05 / Validation loss: 0.00018937516566980598 / Long term Validation loss: 0.1594\n",
      "Epoch: [6072/10000] Training loss: 1.622426355474195e-05 / Validation loss: 0.0001892779867135776 / Long term Validation loss: 0.1594\n",
      "Epoch: [6073/10000] Training loss: 1.6220442397813336e-05 / Validation loss: 0.00018933003388433645 / Long term Validation loss: 0.1594\n",
      "Epoch: [6074/10000] Training loss: 1.6216602729292534e-05 / Validation loss: 0.00018924729650056545 / Long term Validation loss: 0.1594\n",
      "Epoch: [6075/10000] Training loss: 1.621275338516719e-05 / Validation loss: 0.00018923125909505646 / Long term Validation loss: 0.1594\n",
      "Epoch: [6076/10000] Training loss: 1.6208915101657605e-05 / Validation loss: 0.00018923641204637658 / Long term Validation loss: 0.1594\n",
      "Epoch: [6077/10000] Training loss: 1.6205089765336665e-05 / Validation loss: 0.0001891493994554448 / Long term Validation loss: 0.1594\n",
      "Epoch: [6078/10000] Training loss: 1.6201261537405417e-05 / Validation loss: 0.0001891816043503902 / Long term Validation loss: 0.1595\n",
      "Epoch: [6079/10000] Training loss: 1.6197421813165232e-05 / Validation loss: 0.00018911744062739073 / Long term Validation loss: 0.1594\n",
      "Epoch: [6080/10000] Training loss: 1.6193579346356955e-05 / Validation loss: 0.0001890909775574111 / Long term Validation loss: 0.1595\n",
      "Epoch: [6081/10000] Training loss: 1.6189745098643446e-05 / Validation loss: 0.00018909455867749877 / Long term Validation loss: 0.1595\n",
      "Epoch: [6082/10000] Training loss: 1.6185917104030903e-05 / Validation loss: 0.00018901877289600696 / Long term Validation loss: 0.1595\n",
      "Epoch: [6083/10000] Training loss: 1.6182085617956713e-05 / Validation loss: 0.00018903585708811833 / Long term Validation loss: 0.1595\n",
      "Epoch: [6084/10000] Training loss: 1.6178247577816446e-05 / Validation loss: 0.0001889837153836502 / Long term Validation loss: 0.1595\n",
      "Epoch: [6085/10000] Training loss: 1.617440927887954e-05 / Validation loss: 0.00018895271105414272 / Long term Validation loss: 0.1595\n",
      "Epoch: [6086/10000] Training loss: 1.6170576281608817e-05 / Validation loss: 0.0001889519578929779 / Long term Validation loss: 0.1595\n",
      "Epoch: [6087/10000] Training loss: 1.616674625901181e-05 / Validation loss: 0.00018888615401752847 / Long term Validation loss: 0.1595\n",
      "Epoch: [6088/10000] Training loss: 1.6162913567467842e-05 / Validation loss: 0.00018889216262317504 / Long term Validation loss: 0.1595\n",
      "Epoch: [6089/10000] Training loss: 1.6159077224387426e-05 / Validation loss: 0.00018884746542836337 / Long term Validation loss: 0.1595\n",
      "Epoch: [6090/10000] Training loss: 1.6155241217651783e-05 / Validation loss: 0.000188815594238826 / Long term Validation loss: 0.1595\n",
      "Epoch: [6091/10000] Training loss: 1.6151408422907187e-05 / Validation loss: 0.00018881001854802895 / Long term Validation loss: 0.1595\n",
      "Epoch: [6092/10000] Training loss: 1.6147577130877962e-05 / Validation loss: 0.00018875279470468716 / Long term Validation loss: 0.1595\n",
      "Epoch: [6093/10000] Training loss: 1.6143744114714512e-05 / Validation loss: 0.00018875110127486585 / Long term Validation loss: 0.1595\n",
      "Epoch: [6094/10000] Training loss: 1.613990902327052e-05 / Validation loss: 0.00018871081533753116 / Long term Validation loss: 0.1595\n",
      "Epoch: [6095/10000] Training loss: 1.6136074225653804e-05 / Validation loss: 0.00018867977629431683 / Long term Validation loss: 0.1595\n",
      "Epoch: [6096/10000] Training loss: 1.6132241318256502e-05 / Validation loss: 0.00018866931520136577 / Long term Validation loss: 0.1595\n",
      "Epoch: [6097/10000] Training loss: 1.6128409272807812e-05 / Validation loss: 0.00018861883843041772 / Long term Validation loss: 0.1595\n",
      "Epoch: [6098/10000] Training loss: 1.6124576222875818e-05 / Validation loss: 0.00018861155483238963 / Long term Validation loss: 0.1595\n",
      "Epoch: [6099/10000] Training loss: 1.6120741943006132e-05 / Validation loss: 0.00018857369621635468 / Long term Validation loss: 0.1595\n",
      "Epoch: [6100/10000] Training loss: 1.611690777821327e-05 / Validation loss: 0.00018854424477784 / Long term Validation loss: 0.1595\n",
      "Epoch: [6101/10000] Training loss: 1.6113074699888544e-05 / Validation loss: 0.00018852940859904677 / Long term Validation loss: 0.1596\n",
      "Epoch: [6102/10000] Training loss: 1.610924218701366e-05 / Validation loss: 0.00018848438007126032 / Long term Validation loss: 0.1596\n",
      "Epoch: [6103/10000] Training loss: 1.610540914628929e-05 / Validation loss: 0.00018847304590402294 / Long term Validation loss: 0.1596\n",
      "Epoch: [6104/10000] Training loss: 1.6101575348345623e-05 / Validation loss: 0.00018843652595727594 / Long term Validation loss: 0.1596\n",
      "Epoch: [6105/10000] Training loss: 1.6097741513532253e-05 / Validation loss: 0.00018840876192761108 / Long term Validation loss: 0.1596\n",
      "Epoch: [6106/10000] Training loss: 1.6093908274359403e-05 / Validation loss: 0.0001883899800242599 / Long term Validation loss: 0.1596\n",
      "Epoch: [6107/10000] Training loss: 1.6090075433469814e-05 / Validation loss: 0.00018834920937109618 / Long term Validation loss: 0.1596\n",
      "Epoch: [6108/10000] Training loss: 1.608624235729724e-05 / Validation loss: 0.00018833462126186128 / Long term Validation loss: 0.1596\n",
      "Epoch: [6109/10000] Training loss: 1.6082408810091688e-05 / Validation loss: 0.00018829884442069146 / Long term Validation loss: 0.1596\n",
      "Epoch: [6110/10000] Training loss: 1.6078575136299152e-05 / Validation loss: 0.00018827261363448203 / Long term Validation loss: 0.1596\n",
      "Epoch: [6111/10000] Training loss: 1.6074741753163627e-05 / Validation loss: 0.00018825048671643814 / Long term Validation loss: 0.1596\n",
      "Epoch: [6112/10000] Training loss: 1.6070908646017007e-05 / Validation loss: 0.00018821328255557137 / Long term Validation loss: 0.1596\n",
      "Epoch: [6113/10000] Training loss: 1.606707546928258e-05 / Validation loss: 0.00018819604723445304 / Long term Validation loss: 0.1596\n",
      "Epoch: [6114/10000] Training loss: 1.6063242009140482e-05 / Validation loss: 0.00018816091651341132 / Long term Validation loss: 0.1596\n",
      "Epoch: [6115/10000] Training loss: 1.6059408389760296e-05 / Validation loss: 0.00018813593243433832 / Long term Validation loss: 0.1596\n",
      "Epoch: [6116/10000] Training loss: 1.6055574872053265e-05 / Validation loss: 0.00018811100330537144 / Long term Validation loss: 0.1596\n",
      "Epoch: [6117/10000] Training loss: 1.60517415256682e-05 / Validation loss: 0.00018807678294090324 / Long term Validation loss: 0.1596\n",
      "Epoch: [6118/10000] Training loss: 1.6047908189308372e-05 / Validation loss: 0.0001880571704337124 / Long term Validation loss: 0.1596\n",
      "Epoch: [6119/10000] Training loss: 1.6044074696412878e-05 / Validation loss: 0.0001880227621503166 / Long term Validation loss: 0.1596\n",
      "Epoch: [6120/10000] Training loss: 1.6040241052582733e-05 / Validation loss: 0.000187998647212117 / Long term Validation loss: 0.1596\n",
      "Epoch: [6121/10000] Training loss: 1.6036407399992597e-05 / Validation loss: 0.00018797152505010855 / Long term Validation loss: 0.1596\n",
      "Epoch: [6122/10000] Training loss: 1.6032573827975406e-05 / Validation loss: 0.00018793991841168747 / Long term Validation loss: 0.1596\n",
      "Epoch: [6123/10000] Training loss: 1.6028740287878347e-05 / Validation loss: 0.00018791817665193147 / Long term Validation loss: 0.1597\n",
      "Epoch: [6124/10000] Training loss: 1.602490667222998e-05 / Validation loss: 0.0001878847568016789 / Long term Validation loss: 0.1597\n",
      "Epoch: [6125/10000] Training loss: 1.6021072936678955e-05 / Validation loss: 0.00018786109127322278 / Long term Validation loss: 0.1597\n",
      "Epoch: [6126/10000] Training loss: 1.6017239138179845e-05 / Validation loss: 0.00018783236172803717 / Long term Validation loss: 0.1597\n",
      "Epoch: [6127/10000] Training loss: 1.6013405348276738e-05 / Validation loss: 0.00018780294608511323 / Long term Validation loss: 0.1597\n",
      "Epoch: [6128/10000] Training loss: 1.6009571576834164e-05 / Validation loss: 0.00018777920255218158 / Long term Validation loss: 0.1597\n",
      "Epoch: [6129/10000] Training loss: 1.6005737772090575e-05 / Validation loss: 0.00018774699123930526 / Long term Validation loss: 0.1597\n",
      "Epoch: [6130/10000] Training loss: 1.6001903883915227e-05 / Validation loss: 0.000187723294315726 / Long term Validation loss: 0.1597\n",
      "Epoch: [6131/10000] Training loss: 1.59980699174744e-05 / Validation loss: 0.0001876935635032816 / Long term Validation loss: 0.1597\n",
      "Epoch: [6132/10000] Training loss: 1.5994235911795273e-05 / Validation loss: 0.0001876659073211085 / Long term Validation loss: 0.1597\n",
      "Epoch: [6133/10000] Training loss: 1.599040189539095e-05 / Validation loss: 0.00018764037453052819 / Long term Validation loss: 0.1597\n",
      "Epoch: [6134/10000] Training loss: 1.5986567856759898e-05 / Validation loss: 0.00018760956724043348 / Long term Validation loss: 0.1597\n",
      "Epoch: [6135/10000] Training loss: 1.5982733762826765e-05 / Validation loss: 0.00018758539143349262 / Long term Validation loss: 0.1597\n",
      "Epoch: [6136/10000] Training loss: 1.597889959727102e-05 / Validation loss: 0.00018755524759977535 / Long term Validation loss: 0.1597\n",
      "Epoch: [6137/10000] Training loss: 1.5975065369603883e-05 / Validation loss: 0.00018752880782007183 / Long term Validation loss: 0.1597\n",
      "Epoch: [6138/10000] Training loss: 1.5971231102639436e-05 / Validation loss: 0.00018750174537608542 / Long term Validation loss: 0.1597\n",
      "Epoch: [6139/10000] Training loss: 1.596739680443553e-05 / Validation loss: 0.00018747236585083037 / Long term Validation loss: 0.1597\n",
      "Epoch: [6140/10000] Training loss: 1.5963562463500867e-05 / Validation loss: 0.00018744733048615337 / Long term Validation loss: 0.1597\n",
      "Epoch: [6141/10000] Training loss: 1.5959728063600687e-05 / Validation loss: 0.00018741732243823234 / Long term Validation loss: 0.1597\n",
      "Epoch: [6142/10000] Training loss: 1.5955893597883246e-05 / Validation loss: 0.00018739152466298037 / Long term Validation loss: 0.1597\n",
      "Epoch: [6143/10000] Training loss: 1.5952059075042966e-05 / Validation loss: 0.00018736334822785114 / Long term Validation loss: 0.1597\n",
      "Epoch: [6144/10000] Training loss: 1.5948224505097554e-05 / Validation loss: 0.00018733527687035982 / Long term Validation loss: 0.1597\n",
      "Epoch: [6145/10000] Training loss: 1.5944389890529023e-05 / Validation loss: 0.0001873091850383041 / Long term Validation loss: 0.1597\n",
      "Epoch: [6146/10000] Training loss: 1.594055522435763e-05 / Validation loss: 0.0001872797573024188 / Long term Validation loss: 0.1597\n",
      "Epoch: [6147/10000] Training loss: 1.5936720497369287e-05 / Validation loss: 0.00018725402189583598 / Long term Validation loss: 0.1597\n",
      "Epoch: [6148/10000] Training loss: 1.5932885707798325e-05 / Validation loss: 0.00018722522764571034 / Long term Validation loss: 0.1598\n",
      "Epoch: [6149/10000] Training loss: 1.5929050859151556e-05 / Validation loss: 0.00018719814098885917 / Long term Validation loss: 0.1598\n",
      "Epoch: [6150/10000] Training loss: 1.5925215957020567e-05 / Validation loss: 0.0001871710045622533 / Long term Validation loss: 0.1598\n",
      "Epoch: [6151/10000] Training loss: 1.592138100200869e-05 / Validation loss: 0.00018714241936426367 / Long term Validation loss: 0.1598\n",
      "Epoch: [6152/10000] Training loss: 1.591754599005802e-05 / Validation loss: 0.0001871162820911395 / Long term Validation loss: 0.1598\n",
      "Epoch: [6153/10000] Training loss: 1.5913710916924526e-05 / Validation loss: 0.0001870874139898032 / Long term Validation loss: 0.1598\n",
      "Epoch: [6154/10000] Training loss: 1.5909875780493447e-05 / Validation loss: 0.00018706087085970476 / Long term Validation loss: 0.1598\n",
      "Epoch: [6155/10000] Training loss: 1.590604058303384e-05 / Validation loss: 0.00018703294471343311 / Long term Validation loss: 0.1598\n",
      "Epoch: [6156/10000] Training loss: 1.5902205326695267e-05 / Validation loss: 0.00018700522622505365 / Long term Validation loss: 0.1598\n",
      "Epoch: [6157/10000] Training loss: 1.5898370012151116e-05 / Validation loss: 0.0001869784170095965 / Long term Validation loss: 0.1598\n",
      "Epoch: [6158/10000] Training loss: 1.5894534637600746e-05 / Validation loss: 0.00018694991571595898 / Long term Validation loss: 0.1598\n",
      "Epoch: [6159/10000] Training loss: 1.5890699200204305e-05 / Validation loss: 0.00018692342307979624 / Long term Validation loss: 0.1598\n",
      "Epoch: [6160/10000] Training loss: 1.5886863699006436e-05 / Validation loss: 0.00018689511685452044 / Long term Validation loss: 0.1598\n",
      "Epoch: [6161/10000] Training loss: 1.5883028134075012e-05 / Validation loss: 0.00018686804218482825 / Long term Validation loss: 0.1598\n",
      "Epoch: [6162/10000] Training loss: 1.5879192506858065e-05 / Validation loss: 0.00018684055821897822 / Long term Validation loss: 0.1598\n",
      "Epoch: [6163/10000] Training loss: 1.587535681767566e-05 / Validation loss: 0.00018681266510893363 / Long term Validation loss: 0.1598\n",
      "Epoch: [6164/10000] Training loss: 1.587152106577227e-05 / Validation loss: 0.00018678584255605484 / Long term Validation loss: 0.1598\n",
      "Epoch: [6165/10000] Training loss: 1.5867685249905175e-05 / Validation loss: 0.00018675760714085662 / Long term Validation loss: 0.1598\n",
      "Epoch: [6166/10000] Training loss: 1.586384936876827e-05 / Validation loss: 0.00018673079496883422 / Long term Validation loss: 0.1598\n",
      "Epoch: [6167/10000] Training loss: 1.5860013422428852e-05 / Validation loss: 0.00018670287542566391 / Long term Validation loss: 0.1598\n",
      "Epoch: [6168/10000] Training loss: 1.585617741103547e-05 / Validation loss: 0.00018667555163306784 / Long term Validation loss: 0.1598\n",
      "Epoch: [6169/10000] Training loss: 1.585234133508555e-05 / Validation loss: 0.00018664823511691704 / Long term Validation loss: 0.1598\n",
      "Epoch: [6170/10000] Training loss: 1.5848505194346244e-05 / Validation loss: 0.00018662038386179567 / Long term Validation loss: 0.1598\n",
      "Epoch: [6171/10000] Training loss: 1.5844668988100637e-05 / Validation loss: 0.00018659344568376735 / Long term Validation loss: 0.1598\n",
      "Epoch: [6172/10000] Training loss: 1.5840832715760337e-05 / Validation loss: 0.00018656546007709548 / Long term Validation loss: 0.1598\n",
      "Epoch: [6173/10000] Training loss: 1.5836996376709287e-05 / Validation loss: 0.00018653844309278025 / Long term Validation loss: 0.1598\n",
      "Epoch: [6174/10000] Training loss: 1.5833159971104494e-05 / Validation loss: 0.00018651074280063207 / Long term Validation loss: 0.1599\n",
      "Epoch: [6175/10000] Training loss: 1.582932349892374e-05 / Validation loss: 0.00018648334586859923 / Long term Validation loss: 0.1599\n",
      "Epoch: [6176/10000] Training loss: 1.5825486960279092e-05 / Validation loss: 0.00018645606367460624 / Long term Validation loss: 0.1599\n",
      "Epoch: [6177/10000] Training loss: 1.582165035492814e-05 / Validation loss: 0.00018642832673972733 / Long term Validation loss: 0.1599\n",
      "Epoch: [6178/10000] Training loss: 1.5817813682415853e-05 / Validation loss: 0.00018640127567371955 / Long term Validation loss: 0.1599\n",
      "Epoch: [6179/10000] Training loss: 1.5813976942479407e-05 / Validation loss: 0.00018637347735487377 / Long term Validation loss: 0.1599\n",
      "Epoch: [6180/10000] Training loss: 1.5810140134767285e-05 / Validation loss: 0.00018634635235075255 / Long term Validation loss: 0.1599\n",
      "Epoch: [6181/10000] Training loss: 1.5806303259369674e-05 / Validation loss: 0.0001863187632173246 / Long term Validation loss: 0.1599\n",
      "Epoch: [6182/10000] Training loss: 1.5802466316195424e-05 / Validation loss: 0.0001862913775229548 / Long term Validation loss: 0.1599\n",
      "Epoch: [6183/10000] Training loss: 1.5798629305266697e-05 / Validation loss: 0.0001862640764313787 / Long term Validation loss: 0.1599\n",
      "Epoch: [6184/10000] Training loss: 1.579479222643139e-05 / Validation loss: 0.00018623646176741418 / Long term Validation loss: 0.1599\n",
      "Epoch: [6185/10000] Training loss: 1.5790955079439136e-05 / Validation loss: 0.0001862093250110071 / Long term Validation loss: 0.1599\n",
      "Epoch: [6186/10000] Training loss: 1.5787117864170514e-05 / Validation loss: 0.000186181662412261 / Long term Validation loss: 0.1599\n",
      "Epoch: [6187/10000] Training loss: 1.5783280580403903e-05 / Validation loss: 0.00018615448969950776 / Long term Validation loss: 0.1599\n",
      "Epoch: [6188/10000] Training loss: 1.5779443228188834e-05 / Validation loss: 0.00018612695930015014 / Long term Validation loss: 0.1599\n",
      "Epoch: [6189/10000] Training loss: 1.577560580743547e-05 / Validation loss: 0.00018609961943908863 / Long term Validation loss: 0.1599\n",
      "Epoch: [6190/10000] Training loss: 1.5771768318172486e-05 / Validation loss: 0.00018607228652538322 / Long term Validation loss: 0.1599\n",
      "Epoch: [6191/10000] Training loss: 1.5767930760323866e-05 / Validation loss: 0.00018604478434283358 / Long term Validation loss: 0.1599\n",
      "Epoch: [6192/10000] Training loss: 1.5764093133780737e-05 / Validation loss: 0.0001860175843527031 / Long term Validation loss: 0.1599\n",
      "Epoch: [6193/10000] Training loss: 1.5760255438498337e-05 / Validation loss: 0.0001859900271579791 / Long term Validation loss: 0.1599\n",
      "Epoch: [6194/10000] Training loss: 1.57564176743495e-05 / Validation loss: 0.00018596283369228564 / Long term Validation loss: 0.1599\n",
      "Epoch: [6195/10000] Training loss: 1.575257984137654e-05 / Validation loss: 0.00018593534393383752 / Long term Validation loss: 0.1599\n",
      "Epoch: [6196/10000] Training loss: 1.5748741939518045e-05 / Validation loss: 0.00018590805743325526 / Long term Validation loss: 0.1599\n",
      "Epoch: [6197/10000] Training loss: 1.5744903968829398e-05 / Validation loss: 0.0001858806978502195 / Long term Validation loss: 0.1599\n",
      "Epoch: [6198/10000] Training loss: 1.5741065929286655e-05 / Validation loss: 0.0001858532971501116 / Long term Validation loss: 0.1599\n",
      "Epoch: [6199/10000] Training loss: 1.5737227820881245e-05 / Validation loss: 0.00018582604871338946 / Long term Validation loss: 0.1600\n",
      "Epoch: [6200/10000] Training loss: 1.5733389643617962e-05 / Validation loss: 0.0001857985852520707 / Long term Validation loss: 0.1600\n",
      "Epoch: [6201/10000] Training loss: 1.5729551397453977e-05 / Validation loss: 0.0001857713763734597 / Long term Validation loss: 0.1600\n",
      "Epoch: [6202/10000] Training loss: 1.5725713082442885e-05 / Validation loss: 0.00018574392934678165 / Long term Validation loss: 0.1600\n",
      "Epoch: [6203/10000] Training loss: 1.5721874698564163e-05 / Validation loss: 0.00018571668651938664 / Long term Validation loss: 0.1600\n",
      "Epoch: [6204/10000] Training loss: 1.571803624589808e-05 / Validation loss: 0.00018568931411519458 / Long term Validation loss: 0.1600\n",
      "Epoch: [6205/10000] Training loss: 1.5714197724457998e-05 / Validation loss: 0.00018566200097481765 / Long term Validation loss: 0.1600\n",
      "Epoch: [6206/10000] Training loss: 1.5710359134305597e-05 / Validation loss: 0.0001856347150025434 / Long term Validation loss: 0.1600\n",
      "Epoch: [6207/10000] Training loss: 1.570652047548236e-05 / Validation loss: 0.00018560734240095965 / Long term Validation loss: 0.1600\n",
      "Epoch: [6208/10000] Training loss: 1.5702681748022137e-05 / Validation loss: 0.0001855801132089279 / Long term Validation loss: 0.1600\n",
      "Epoch: [6209/10000] Training loss: 1.5698842951997922e-05 / Validation loss: 0.00018555272291614257 / Long term Validation loss: 0.1600\n",
      "Epoch: [6210/10000] Training loss: 1.5695004087440044e-05 / Validation loss: 0.00018552550368734321 / Long term Validation loss: 0.1600\n",
      "Epoch: [6211/10000] Training loss: 1.5691165154448656e-05 / Validation loss: 0.00018549814097962392 / Long term Validation loss: 0.1600\n",
      "Epoch: [6212/10000] Training loss: 1.568732615307126e-05 / Validation loss: 0.00018547089398645653 / Long term Validation loss: 0.1600\n",
      "Epoch: [6213/10000] Training loss: 1.5683487083414826e-05 / Validation loss: 0.0001854435855300674 / Long term Validation loss: 0.1600\n",
      "Epoch: [6214/10000] Training loss: 1.567964794554975e-05 / Validation loss: 0.00018541629727657073 / Long term Validation loss: 0.1600\n",
      "Epoch: [6215/10000] Training loss: 1.5675808739572698e-05 / Validation loss: 0.00018538904339245648 / Long term Validation loss: 0.1600\n",
      "Epoch: [6216/10000] Training loss: 1.567196946557784e-05 / Validation loss: 0.00018536172482768232 / Long term Validation loss: 0.1600\n",
      "Epoch: [6217/10000] Training loss: 1.566813012365185e-05 / Validation loss: 0.0001853345056931129 / Long term Validation loss: 0.1600\n",
      "Epoch: [6218/10000] Training loss: 1.5664290713911902e-05 / Validation loss: 0.00018530718165183646 / Long term Validation loss: 0.1600\n",
      "Epoch: [6219/10000] Training loss: 1.5660451236445896e-05 / Validation loss: 0.00018527997064143057 / Long term Validation loss: 0.1600\n",
      "Epoch: [6220/10000] Training loss: 1.565661169138742e-05 / Validation loss: 0.0001852526661483376 / Long term Validation loss: 0.1600\n",
      "Epoch: [6221/10000] Training loss: 1.5652772078835688e-05 / Validation loss: 0.0001852254424002268 / Long term Validation loss: 0.1600\n",
      "Epoch: [6222/10000] Training loss: 1.5648932398929932e-05 / Validation loss: 0.0001851981725080092 / Long term Validation loss: 0.1600\n",
      "Epoch: [6223/10000] Training loss: 1.564509265178564e-05 / Validation loss: 0.00018517092767832214 / Long term Validation loss: 0.1600\n",
      "Epoch: [6224/10000] Training loss: 1.5641252837539864e-05 / Validation loss: 0.0001851436940207097 / Long term Validation loss: 0.1600\n",
      "Epoch: [6225/10000] Training loss: 1.563741295632589e-05 / Validation loss: 0.0001851164322861398 / Long term Validation loss: 0.1601\n",
      "Epoch: [6226/10000] Training loss: 1.5633573008277824e-05 / Validation loss: 0.00018508922579184132 / Long term Validation loss: 0.1601\n",
      "Epoch: [6227/10000] Training loss: 1.5629732993545698e-05 / Validation loss: 0.0001850619591748474 / Long term Validation loss: 0.1601\n",
      "Epoch: [6228/10000] Training loss: 1.562589291226485e-05 / Validation loss: 0.00018503476604854173 / Long term Validation loss: 0.1601\n",
      "Epoch: [6229/10000] Training loss: 1.5622052764598304e-05 / Validation loss: 0.0001850075082235735 / Long term Validation loss: 0.1601\n",
      "Epoch: [6230/10000] Training loss: 1.561821255068781e-05 / Validation loss: 0.00018498031591503207 / Long term Validation loss: 0.1601\n",
      "Epoch: [6231/10000] Training loss: 1.5614372270704097e-05 / Validation loss: 0.00018495307716489285 / Long term Validation loss: 0.1601\n",
      "Epoch: [6232/10000] Training loss: 1.561053192479918e-05 / Validation loss: 0.00018492587814845617 / Long term Validation loss: 0.1601\n",
      "Epoch: [6233/10000] Training loss: 1.5606691513146884e-05 / Validation loss: 0.00018489866287862297 / Long term Validation loss: 0.1601\n",
      "Epoch: [6234/10000] Training loss: 1.5602851035911502e-05 / Validation loss: 0.0001848714556502644 / Long term Validation loss: 0.1601\n",
      "Epoch: [6235/10000] Training loss: 1.5599010493267843e-05 / Validation loss: 0.00018484426253866296 / Long term Validation loss: 0.1601\n",
      "Epoch: [6236/10000] Training loss: 1.5595169885392682e-05 / Validation loss: 0.00018481705043769397 / Long term Validation loss: 0.1601\n",
      "Epoch: [6237/10000] Training loss: 1.5591329212462233e-05 / Validation loss: 0.00018478987435516345 / Long term Validation loss: 0.1601\n",
      "Epoch: [6238/10000] Training loss: 1.558748847466447e-05 / Validation loss: 0.00018476266330455453 / Long term Validation loss: 0.1601\n",
      "Epoch: [6239/10000] Training loss: 1.558364767217887e-05 / Validation loss: 0.00018473549780263437 / Long term Validation loss: 0.1601\n",
      "Epoch: [6240/10000] Training loss: 1.557980680520237e-05 / Validation loss: 0.00018470829398773664 / Long term Validation loss: 0.1601\n",
      "Epoch: [6241/10000] Training loss: 1.557596587391987e-05 / Validation loss: 0.00018468113337180632 / Long term Validation loss: 0.1601\n",
      "Epoch: [6242/10000] Training loss: 1.5572124878534724e-05 / Validation loss: 0.00018465394154045168 / Long term Validation loss: 0.1601\n",
      "Epoch: [6243/10000] Training loss: 1.5568283819239016e-05 / Validation loss: 0.00018462678205724453 / Long term Validation loss: 0.1601\n",
      "Epoch: [6244/10000] Training loss: 1.556444269624051e-05 / Validation loss: 0.00018459960472822916 / Long term Validation loss: 0.1601\n",
      "Epoch: [6245/10000] Training loss: 1.5560601509739533e-05 / Validation loss: 0.00018457244486872986 / Long term Validation loss: 0.1601\n",
      "Epoch: [6246/10000] Training loss: 1.555676025994704e-05 / Validation loss: 0.00018454528238048606 / Long term Validation loss: 0.1601\n",
      "Epoch: [6247/10000] Training loss: 1.5552918947071883e-05 / Validation loss: 0.00018451812254972807 / Long term Validation loss: 0.1601\n",
      "Epoch: [6248/10000] Training loss: 1.5549077571327914e-05 / Validation loss: 0.00018449097364420589 / Long term Validation loss: 0.1601\n",
      "Epoch: [6249/10000] Training loss: 1.554523613293215e-05 / Validation loss: 0.00018446381549957805 / Long term Validation loss: 0.1602\n",
      "Epoch: [6250/10000] Training loss: 1.5541394632101647e-05 / Validation loss: 0.00018443667807725502 / Long term Validation loss: 0.1602\n",
      "Epoch: [6251/10000] Training loss: 1.5537553069060836e-05 / Validation loss: 0.0001844095237965799 / Long term Validation loss: 0.1602\n",
      "Epoch: [6252/10000] Training loss: 1.553371144403048e-05 / Validation loss: 0.00018438239557729577 / Long term Validation loss: 0.1602\n",
      "Epoch: [6253/10000] Training loss: 1.552986975724148e-05 / Validation loss: 0.00018435524725294658 / Long term Validation loss: 0.1602\n",
      "Epoch: [6254/10000] Training loss: 1.552602800891904e-05 / Validation loss: 0.00018432812623059045 / Long term Validation loss: 0.1602\n",
      "Epoch: [6255/10000] Training loss: 1.5522186199299514e-05 / Validation loss: 0.00018430098549886657 / Long term Validation loss: 0.1602\n",
      "Epoch: [6256/10000] Training loss: 1.5518344328613066e-05 / Validation loss: 0.00018427387018498667 / Long term Validation loss: 0.1602\n",
      "Epoch: [6257/10000] Training loss: 1.5514502397100746e-05 / Validation loss: 0.0001842467380963444 / Long term Validation loss: 0.1602\n",
      "Epoch: [6258/10000] Training loss: 1.551066040499803e-05 / Validation loss: 0.0001842196275857816 / Long term Validation loss: 0.1602\n",
      "Epoch: [6259/10000] Training loss: 1.5506818352550093e-05 / Validation loss: 0.00018419250464300315 / Long term Validation loss: 0.1602\n",
      "Epoch: [6260/10000] Training loss: 1.550297623999783e-05 / Validation loss: 0.0001841653985446011 / Long term Validation loss: 0.1602\n",
      "Epoch: [6261/10000] Training loss: 1.549913406759019e-05 / Validation loss: 0.0001841382848192446 / Long term Validation loss: 0.1602\n",
      "Epoch: [6262/10000] Training loss: 1.549529183557351e-05 / Validation loss: 0.00018411118310870717 / Long term Validation loss: 0.1602\n",
      "Epoch: [6263/10000] Training loss: 1.5491449544200315e-05 / Validation loss: 0.0001840840783782776 / Long term Validation loss: 0.1602\n",
      "Epoch: [6264/10000] Training loss: 1.5487607193722195e-05 / Validation loss: 0.00018405698123818992 / Long term Validation loss: 0.1602\n",
      "Epoch: [6265/10000] Training loss: 1.548376478439519e-05 / Validation loss: 0.00018402988512062588 / Long term Validation loss: 0.1602\n",
      "Epoch: [6266/10000] Training loss: 1.5479922316475904e-05 / Validation loss: 0.00018400279281398306 / Long term Validation loss: 0.1602\n",
      "Epoch: [6267/10000] Training loss: 1.547607979022388e-05 / Validation loss: 0.0001839757048854634 / Long term Validation loss: 0.1602\n",
      "Epoch: [6268/10000] Training loss: 1.5472237205900528e-05 / Validation loss: 0.0001839486176726789 / Long term Validation loss: 0.1602\n",
      "Epoch: [6269/10000] Training loss: 1.5468394563768832e-05 / Validation loss: 0.00018392153755244958 / Long term Validation loss: 0.1602\n",
      "Epoch: [6270/10000] Training loss: 1.5464551864094763e-05 / Validation loss: 0.00018389445563757892 / Long term Validation loss: 0.1603\n",
      "Epoch: [6271/10000] Training loss: 1.5460709107144793e-05 / Validation loss: 0.0001838673830324648 / Long term Validation loss: 0.1603\n",
      "Epoch: [6272/10000] Training loss: 1.545686629318918e-05 / Validation loss: 0.00018384030652578173 / Long term Validation loss: 0.1603\n",
      "Epoch: [6273/10000] Training loss: 1.545302342249785e-05 / Validation loss: 0.00018381324124596798 / Long term Validation loss: 0.1603\n",
      "Epoch: [6274/10000] Training loss: 1.5449180495345206e-05 / Validation loss: 0.0001837861701413327 / Long term Validation loss: 0.1603\n",
      "Epoch: [6275/10000] Training loss: 1.544533751200453e-05 / Validation loss: 0.00018375911210867854 / Long term Validation loss: 0.1603\n",
      "Epoch: [6276/10000] Training loss: 1.5441494472754226e-05 / Validation loss: 0.00018373204627446176 / Long term Validation loss: 0.1603\n",
      "Epoch: [6277/10000] Training loss: 1.5437651377870843e-05 / Validation loss: 0.00018370499553708436 / Long term Validation loss: 0.1603\n",
      "Epoch: [6278/10000] Training loss: 1.543380822763671e-05 / Validation loss: 0.00018367793470915056 / Long term Validation loss: 0.1603\n",
      "Epoch: [6279/10000] Training loss: 1.5429965022331462e-05 / Validation loss: 0.00018365089146435865 / Long term Validation loss: 0.1603\n",
      "Epoch: [6280/10000] Training loss: 1.542612176224131e-05 / Validation loss: 0.00018362383522390675 / Long term Validation loss: 0.1603\n",
      "Epoch: [6281/10000] Training loss: 1.5422278447648828e-05 / Validation loss: 0.00018359679985151223 / Long term Validation loss: 0.1603\n",
      "Epoch: [6282/10000] Training loss: 1.541843507884413e-05 / Validation loss: 0.0001835697475755388 / Long term Validation loss: 0.1603\n",
      "Epoch: [6283/10000] Training loss: 1.5414591656112424e-05 / Validation loss: 0.00018354272069411493 / Long term Validation loss: 0.1603\n",
      "Epoch: [6284/10000] Training loss: 1.541074817974785e-05 / Validation loss: 0.0001835156714716068 / Long term Validation loss: 0.1603\n",
      "Epoch: [6285/10000] Training loss: 1.5406904650037933e-05 / Validation loss: 0.00018348865403935776 / Long term Validation loss: 0.1603\n",
      "Epoch: [6286/10000] Training loss: 1.540306106728105e-05 / Validation loss: 0.00018346160653987017 / Long term Validation loss: 0.1603\n",
      "Epoch: [6287/10000] Training loss: 1.5399217431766557e-05 / Validation loss: 0.0001834346000256222 / Long term Validation loss: 0.1603\n",
      "Epoch: [6288/10000] Training loss: 1.539537374379749e-05 / Validation loss: 0.00018340755228846798 / Long term Validation loss: 0.1603\n",
      "Epoch: [6289/10000] Training loss: 1.5391530003664392e-05 / Validation loss: 0.00018338055894704343 / Long term Validation loss: 0.1603\n",
      "Epoch: [6290/10000] Training loss: 1.5387686211675563e-05 / Validation loss: 0.00018335350803607488 / Long term Validation loss: 0.1604\n",
      "Epoch: [6291/10000] Training loss: 1.5383842368121818e-05 / Validation loss: 0.00018332653134874556 / Long term Validation loss: 0.1604\n",
      "Epoch: [6292/10000] Training loss: 1.5379998473317806e-05 / Validation loss: 0.0001832994727873616 / Long term Validation loss: 0.1604\n",
      "Epoch: [6293/10000] Training loss: 1.5376154527553296e-05 / Validation loss: 0.00018327251817828693 / Long term Validation loss: 0.1604\n",
      "Epoch: [6294/10000] Training loss: 1.5372310531150928e-05 / Validation loss: 0.00018324544502541784 / Long term Validation loss: 0.1604\n",
      "Epoch: [6295/10000] Training loss: 1.5368466484397574e-05 / Validation loss: 0.00018321852104493655 / Long term Validation loss: 0.1604\n",
      "Epoch: [6296/10000] Training loss: 1.5364622387626694e-05 / Validation loss: 0.00018319142236984743 / Long term Validation loss: 0.1604\n",
      "Epoch: [6297/10000] Training loss: 1.5360778241119496e-05 / Validation loss: 0.00018316454266787007 / Long term Validation loss: 0.1604\n",
      "Epoch: [6298/10000] Training loss: 1.535693404522513e-05 / Validation loss: 0.0001831374009975118 / Long term Validation loss: 0.1604\n",
      "Epoch: [6299/10000] Training loss: 1.535308980021542e-05 / Validation loss: 0.00018311058764595807 / Long term Validation loss: 0.1604\n",
      "Epoch: [6300/10000] Training loss: 1.5349245506463888e-05 / Validation loss: 0.00018308337463713442 / Long term Validation loss: 0.1604\n",
      "Epoch: [6301/10000] Training loss: 1.5345401164228562e-05 / Validation loss: 0.0001830566637887122 / Long term Validation loss: 0.1604\n",
      "Epoch: [6302/10000] Training loss: 1.5341556773924146e-05 / Validation loss: 0.00018302933280603967 / Long term Validation loss: 0.1604\n",
      "Epoch: [6303/10000] Training loss: 1.5337712335791826e-05 / Validation loss: 0.0001830027844520209 / Long term Validation loss: 0.1604\n",
      "Epoch: [6304/10000] Training loss: 1.5333867850322388e-05 / Validation loss: 0.00018297525769413056 / Long term Validation loss: 0.1604\n",
      "Epoch: [6305/10000] Training loss: 1.5330023317747493e-05 / Validation loss: 0.00018294897269351704 / Long term Validation loss: 0.1604\n",
      "Epoch: [6306/10000] Training loss: 1.532617873871325e-05 / Validation loss: 0.00018292111859987128 / Long term Validation loss: 0.1604\n",
      "Epoch: [6307/10000] Training loss: 1.532233411349019e-05 / Validation loss: 0.0001828952687463065 / Long term Validation loss: 0.1604\n",
      "Epoch: [6308/10000] Training loss: 1.5318489443076344e-05 / Validation loss: 0.00018286686187671435 / Long term Validation loss: 0.1604\n",
      "Epoch: [6309/10000] Training loss: 1.5314644727972456e-05 / Validation loss: 0.00018284174360249479 / Long term Validation loss: 0.1605\n",
      "Epoch: [6310/10000] Training loss: 1.531079997005683e-05 / Validation loss: 0.0001828123925582831 / Long term Validation loss: 0.1605\n",
      "Epoch: [6311/10000] Training loss: 1.530695517072824e-05 / Validation loss: 0.00018278852397445088 / Long term Validation loss: 0.1605\n",
      "Epoch: [6312/10000] Training loss: 1.5303110334262823e-05 / Validation loss: 0.00018275754040769494 / Long term Validation loss: 0.1605\n",
      "Epoch: [6313/10000] Training loss: 1.529926546519702e-05 / Validation loss: 0.0001827358386822026 / Long term Validation loss: 0.1605\n",
      "Epoch: [6314/10000] Training loss: 1.5295420574803437e-05 / Validation loss: 0.00018270199651310845 / Long term Validation loss: 0.1605\n",
      "Epoch: [6315/10000] Training loss: 1.529157567821159e-05 / Validation loss: 0.0001826841058274947 / Long term Validation loss: 0.1605\n",
      "Epoch: [6316/10000] Training loss: 1.5287730808238725e-05 / Validation loss: 0.0001826451936115774 / Long term Validation loss: 0.1605\n",
      "Epoch: [6317/10000] Training loss: 1.52838860156135e-05 / Validation loss: 0.00018263409845666216 / Long term Validation loss: 0.1605\n",
      "Epoch: [6318/10000] Training loss: 1.528004140239166e-05 / Validation loss: 0.00018258607783821517 / Long term Validation loss: 0.1605\n",
      "Epoch: [6319/10000] Training loss: 1.527619714004948e-05 / Validation loss: 0.00018258726297754857 / Long term Validation loss: 0.1605\n",
      "Epoch: [6320/10000] Training loss: 1.5272353561123824e-05 / Validation loss: 0.00018252266904636696 / Long term Validation loss: 0.1605\n",
      "Epoch: [6321/10000] Training loss: 1.526851125335809e-05 / Validation loss: 0.00018254633844433767 / Long term Validation loss: 0.1605\n",
      "Epoch: [6322/10000] Training loss: 1.52646713402151e-05 / Validation loss: 0.00018245120605463189 / Long term Validation loss: 0.1605\n",
      "Epoch: [6323/10000] Training loss: 1.5260835873219053e-05 / Validation loss: 0.00018251657633174283 / Long term Validation loss: 0.1606\n",
      "Epoch: [6324/10000] Training loss: 1.5257008765673935e-05 / Validation loss: 0.00018236447203934922 / Long term Validation loss: 0.1605\n",
      "Epoch: [6325/10000] Training loss: 1.525319732895984e-05 / Validation loss: 0.00018250817769174726 / Long term Validation loss: 0.1606\n",
      "Epoch: [6326/10000] Training loss: 1.5249415573273692e-05 / Validation loss: 0.00018224849700951046 / Long term Validation loss: 0.1605\n",
      "Epoch: [6327/10000] Training loss: 1.5245690140018993e-05 / Validation loss: 0.00018254124861355952 / Long term Validation loss: 0.1606\n",
      "Epoch: [6328/10000] Training loss: 1.5242072490939662e-05 / Validation loss: 0.00018207606629539423 / Long term Validation loss: 0.1604\n",
      "Epoch: [6329/10000] Training loss: 1.5238661943491952e-05 / Validation loss: 0.00018265611523625273 / Long term Validation loss: 0.1607\n",
      "Epoch: [6330/10000] Training loss: 1.5235652223288098e-05 / Validation loss: 0.0001817940872081977 / Long term Validation loss: 0.1604\n",
      "Epoch: [6331/10000] Training loss: 1.5233422199286753e-05 / Validation loss: 0.00018293554160991437 / Long term Validation loss: 0.1609\n",
      "Epoch: [6332/10000] Training loss: 1.5232718699185283e-05 / Validation loss: 0.00018129989912600835 / Long term Validation loss: 0.1603\n",
      "Epoch: [6333/10000] Training loss: 1.5235020513290249e-05 / Validation loss: 0.00018355514419150787 / Long term Validation loss: 0.1611\n",
      "Epoch: [6334/10000] Training loss: 1.524327399247456e-05 / Validation loss: 0.00018040134966940417 / Long term Validation loss: 0.1600\n",
      "Epoch: [6335/10000] Training loss: 1.526338238072408e-05 / Validation loss: 0.00018490704052821048 / Long term Validation loss: 0.1613\n",
      "Epoch: [6336/10000] Training loss: 1.5307228560429017e-05 / Validation loss: 0.00017877165168225646 / Long term Validation loss: 0.1597\n",
      "Epoch: [6337/10000] Training loss: 1.5398881377320256e-05 / Validation loss: 0.00018793665963667505 / Long term Validation loss: 0.1605\n",
      "Epoch: [6338/10000] Training loss: 1.558719690632651e-05 / Validation loss: 0.00017601747786716082 / Long term Validation loss: 0.1594\n",
      "Epoch: [6339/10000] Training loss: 1.5971923328569767e-05 / Validation loss: 0.00019517173270166505 / Long term Validation loss: 0.1595\n",
      "Epoch: [6340/10000] Training loss: 1.675581104184381e-05 / Validation loss: 0.0001725161946557478 / Long term Validation loss: 0.1546\n",
      "Epoch: [6341/10000] Training loss: 1.835111063349899e-05 / Validation loss: 0.00021410376746804974 / Long term Validation loss: 0.1499\n",
      "Epoch: [6342/10000] Training loss: 2.1573817400733805e-05 / Validation loss: 0.00017383063476025747 / Long term Validation loss: 0.1404\n",
      "Epoch: [6343/10000] Training loss: 2.7989687308647258e-05 / Validation loss: 0.0002669596101517426 / Long term Validation loss: 0.1468\n",
      "Epoch: [6344/10000] Training loss: 4.024731352916423e-05 / Validation loss: 0.00020406602849490772 / Long term Validation loss: 0.1408\n",
      "Epoch: [6345/10000] Training loss: 6.183888575081536e-05 / Validation loss: 0.00039063256449988057 / Long term Validation loss: 0.1519\n",
      "Epoch: [6346/10000] Training loss: 9.321175940107594e-05 / Validation loss: 0.00027667692183412846 / Long term Validation loss: 0.1564\n",
      "Epoch: [6347/10000] Training loss: 0.00012249035242298476 / Validation loss: 0.00044864038247522195 / Long term Validation loss: 0.1553\n",
      "Epoch: [6348/10000] Training loss: 0.00011889986165918389 / Validation loss: 0.0002118557053672672 / Long term Validation loss: 0.1420\n",
      "Epoch: [6349/10000] Training loss: 6.940109812172599e-05 / Validation loss: 0.00021442662887778783 / Long term Validation loss: 0.1498\n",
      "Epoch: [6350/10000] Training loss: 2.0460188882971588e-05 / Validation loss: 0.00023096727674185876 / Long term Validation loss: 0.1481\n",
      "Epoch: [6351/10000] Training loss: 2.5065690864964637e-05 / Validation loss: 0.0002047064536087903 / Long term Validation loss: 0.1378\n",
      "Epoch: [6352/10000] Training loss: 6.126439371399576e-05 / Validation loss: 0.00034072630377983475 / Long term Validation loss: 0.1478\n",
      "Epoch: [6353/10000] Training loss: 6.692464998817544e-05 / Validation loss: 0.00018102715739614533 / Long term Validation loss: 0.1396\n",
      "Epoch: [6354/10000] Training loss: 3.3112955427310046e-05 / Validation loss: 0.0001848142059116827 / Long term Validation loss: 0.1607\n",
      "Epoch: [6355/10000] Training loss: 1.5549588524523013e-05 / Validation loss: 0.0002653476481784614 / Long term Validation loss: 0.1463\n",
      "Epoch: [6356/10000] Training loss: 3.58821256403251e-05 / Validation loss: 0.00019402626871444902 / Long term Validation loss: 0.1355\n",
      "Epoch: [6357/10000] Training loss: 4.887136455033572e-05 / Validation loss: 0.00024777374985045746 / Long term Validation loss: 0.1473\n",
      "Epoch: [6358/10000] Training loss: 2.9696614550833028e-05 / Validation loss: 0.00018980935057503698 / Long term Validation loss: 0.1606\n",
      "Epoch: [6359/10000] Training loss: 1.5288348522377435e-05 / Validation loss: 0.0001783340007541253 / Long term Validation loss: 0.1422\n",
      "Epoch: [6360/10000] Training loss: 2.8014806598303265e-05 / Validation loss: 0.0002658079014517945 / Long term Validation loss: 0.1460\n",
      "Epoch: [6361/10000] Training loss: 3.6795601358596995e-05 / Validation loss: 0.00017676563337165304 / Long term Validation loss: 0.1466\n",
      "Epoch: [6362/10000] Training loss: 2.369663687454434e-05 / Validation loss: 0.000184430401226765 / Long term Validation loss: 0.1610\n",
      "Epoch: [6363/10000] Training loss: 1.5379210400193202e-05 / Validation loss: 0.00022996841382099924 / Long term Validation loss: 0.1484\n",
      "Epoch: [6364/10000] Training loss: 2.4879494259932075e-05 / Validation loss: 0.00017850599095482351 / Long term Validation loss: 0.1393\n",
      "Epoch: [6365/10000] Training loss: 2.8985917205135547e-05 / Validation loss: 0.00020961802540900401 / Long term Validation loss: 0.1535\n",
      "Epoch: [6366/10000] Training loss: 1.9257498651135193e-05 / Validation loss: 0.0001923690724381913 / Long term Validation loss: 0.1606\n",
      "Epoch: [6367/10000] Training loss: 1.575494037831142e-05 / Validation loss: 0.00017490172662791044 / Long term Validation loss: 0.1440\n",
      "Epoch: [6368/10000] Training loss: 2.286497723743719e-05 / Validation loss: 0.00022211226663918117 / Long term Validation loss: 0.1492\n",
      "Epoch: [6369/10000] Training loss: 2.3611789072135094e-05 / Validation loss: 0.00017611925810316536 / Long term Validation loss: 0.1568\n",
      "Epoch: [6370/10000] Training loss: 1.672600279185796e-05 / Validation loss: 0.0001766967408848803 / Long term Validation loss: 0.1583\n",
      "Epoch: [6371/10000] Training loss: 1.6221811786254347e-05 / Validation loss: 0.0002122862568167922 / Long term Validation loss: 0.1514\n",
      "Epoch: [6372/10000] Training loss: 2.106961005480202e-05 / Validation loss: 0.00017326052456298745 / Long term Validation loss: 0.1481\n",
      "Epoch: [6373/10000] Training loss: 2.001721134364204e-05 / Validation loss: 0.00018811199291305034 / Long term Validation loss: 0.1622\n",
      "Epoch: [6374/10000] Training loss: 1.5575239032235058e-05 / Validation loss: 0.00019368730464204746 / Long term Validation loss: 0.1609\n",
      "Epoch: [6375/10000] Training loss: 1.6516361530124437e-05 / Validation loss: 0.00017297351841931853 / Long term Validation loss: 0.1490\n",
      "Epoch: [6376/10000] Training loss: 1.9438887883246532e-05 / Validation loss: 0.0001990001588889612 / Long term Validation loss: 0.1589\n",
      "Epoch: [6377/10000] Training loss: 1.7743180160085292e-05 / Validation loss: 0.00018022124548317295 / Long term Validation loss: 0.1616\n",
      "Epoch: [6378/10000] Training loss: 1.5190112299514184e-05 / Validation loss: 0.0001746570782107418 / Long term Validation loss: 0.1563\n",
      "Epoch: [6379/10000] Training loss: 1.6553190433096967e-05 / Validation loss: 0.00020016496231425773 / Long term Validation loss: 0.1579\n",
      "Epoch: [6380/10000] Training loss: 1.8063696165146502e-05 / Validation loss: 0.000174764564922003 / Long term Validation loss: 0.1568\n",
      "Epoch: [6381/10000] Training loss: 1.641432074807284e-05 / Validation loss: 0.00018097912436011832 / Long term Validation loss: 0.1617\n",
      "Epoch: [6382/10000] Training loss: 1.513639802200457e-05 / Validation loss: 0.0001929093832793592 / Long term Validation loss: 0.1610\n",
      "Epoch: [6383/10000] Training loss: 1.63952748449899e-05 / Validation loss: 0.00017398453708291858 / Long term Validation loss: 0.1551\n",
      "Epoch: [6384/10000] Training loss: 1.701946102242423e-05 / Validation loss: 0.00018907853069536005 / Long term Validation loss: 0.1616\n",
      "Epoch: [6385/10000] Training loss: 1.5698199478682038e-05 / Validation loss: 0.00018421976098521552 / Long term Validation loss: 0.1621\n",
      "Epoch: [6386/10000] Training loss: 1.517677569608628e-05 / Validation loss: 0.00017564264651344442 / Long term Validation loss: 0.1581\n",
      "Epoch: [6387/10000] Training loss: 1.6142629790698093e-05 / Validation loss: 0.00019305930370372963 / Long term Validation loss: 0.1608\n",
      "Epoch: [6388/10000] Training loss: 1.6285065503957667e-05 / Validation loss: 0.00017863917012956042 / Long term Validation loss: 0.1607\n",
      "Epoch: [6389/10000] Training loss: 1.5341855102167352e-05 / Validation loss: 0.00017969705745331233 / Long term Validation loss: 0.1610\n",
      "Epoch: [6390/10000] Training loss: 1.5210844688687648e-05 / Validation loss: 0.0001909287954923621 / Long term Validation loss: 0.1607\n",
      "Epoch: [6391/10000] Training loss: 1.5874958384777427e-05 / Validation loss: 0.00017660399093010937 / Long term Validation loss: 0.1593\n",
      "Epoch: [6392/10000] Training loss: 1.5801783242752838e-05 / Validation loss: 0.00018504859515896957 / Long term Validation loss: 0.1617\n",
      "Epoch: [6393/10000] Training loss: 1.5175420548168554e-05 / Validation loss: 0.0001856753523260113 / Long term Validation loss: 0.1617\n",
      "Epoch: [6394/10000] Training loss: 1.521339538674176e-05 / Validation loss: 0.00017718948922802386 / Long term Validation loss: 0.1598\n",
      "Epoch: [6395/10000] Training loss: 1.5639327354929185e-05 / Validation loss: 0.00018858808141317747 / Long term Validation loss: 0.1609\n",
      "Epoch: [6396/10000] Training loss: 1.5497085958041626e-05 / Validation loss: 0.0001810295072096271 / Long term Validation loss: 0.1607\n",
      "Epoch: [6397/10000] Training loss: 1.5099714295046145e-05 / Validation loss: 0.0001796900748036393 / Long term Validation loss: 0.1606\n",
      "Epoch: [6398/10000] Training loss: 1.518997673773621e-05 / Validation loss: 0.00018823081256089344 / Long term Validation loss: 0.1607\n",
      "Epoch: [6399/10000] Training loss: 1.545165916657269e-05 / Validation loss: 0.0001785485108752284 / Long term Validation loss: 0.1600\n",
      "Epoch: [6400/10000] Training loss: 1.5310256592074398e-05 / Validation loss: 0.00018307409691240418 / Long term Validation loss: 0.1608\n",
      "Epoch: [6401/10000] Training loss: 1.5063455591201714e-05 / Validation loss: 0.0001850122919091118 / Long term Validation loss: 0.1612\n",
      "Epoch: [6402/10000] Training loss: 1.5153732052777365e-05 / Validation loss: 0.00017824432003245985 / Long term Validation loss: 0.1598\n",
      "Epoch: [6403/10000] Training loss: 1.5311600718530688e-05 / Validation loss: 0.0001855298899208603 / Long term Validation loss: 0.1611\n",
      "Epoch: [6404/10000] Training loss: 1.5195928376434943e-05 / Validation loss: 0.00018150137263542215 / Long term Validation loss: 0.1602\n",
      "Epoch: [6405/10000] Training loss: 1.5042604147525475e-05 / Validation loss: 0.00017968345575750458 / Long term Validation loss: 0.1600\n",
      "Epoch: [6406/10000] Training loss: 1.5114720188013414e-05 / Validation loss: 0.00018562381556193715 / Long term Validation loss: 0.1610\n",
      "Epoch: [6407/10000] Training loss: 1.521009092701536e-05 / Validation loss: 0.00017934130270052322 / Long term Validation loss: 0.1599\n",
      "Epoch: [6408/10000] Training loss: 1.5124274469290799e-05 / Validation loss: 0.0001819288826301345 / Long term Validation loss: 0.1603\n",
      "Epoch: [6409/10000] Training loss: 1.5027058079045954e-05 / Validation loss: 0.00018364670587872503 / Long term Validation loss: 0.1610\n",
      "Epoch: [6410/10000] Training loss: 1.507828875905871e-05 / Validation loss: 0.0001788506092387586 / Long term Validation loss: 0.1599\n",
      "Epoch: [6411/10000] Training loss: 1.5137059211291665e-05 / Validation loss: 0.00018359324536834472 / Long term Validation loss: 0.1609\n",
      "Epoch: [6412/10000] Training loss: 1.5076930095867514e-05 / Validation loss: 0.00018118089239693973 / Long term Validation loss: 0.1601\n",
      "Epoch: [6413/10000] Training loss: 1.5013129870747359e-05 / Validation loss: 0.0001797549823340386 / Long term Validation loss: 0.1598\n",
      "Epoch: [6414/10000] Training loss: 1.5046541386487593e-05 / Validation loss: 0.00018370282790389694 / Long term Validation loss: 0.1609\n",
      "Epoch: [6415/10000] Training loss: 1.5083847547076144e-05 / Validation loss: 0.00017958188913434972 / Long term Validation loss: 0.1598\n",
      "Epoch: [6416/10000] Training loss: 1.504341783521881e-05 / Validation loss: 0.00018132315548376803 / Long term Validation loss: 0.1602\n",
      "Epoch: [6417/10000] Training loss: 1.4999810141955037e-05 / Validation loss: 0.00018242567607178227 / Long term Validation loss: 0.1608\n",
      "Epoch: [6418/10000] Training loss: 1.501971574041121e-05 / Validation loss: 0.0001792583059273973 / Long term Validation loss: 0.1598\n",
      "Epoch: [6419/10000] Training loss: 1.5044119782500622e-05 / Validation loss: 0.0001824638760871302 / Long term Validation loss: 0.1608\n",
      "Epoch: [6420/10000] Training loss: 1.5017841710054344e-05 / Validation loss: 0.0001807681407601662 / Long term Validation loss: 0.1601\n",
      "Epoch: [6421/10000] Training loss: 1.4986869113344422e-05 / Validation loss: 0.00017995642443876899 / Long term Validation loss: 0.1599\n",
      "Epoch: [6422/10000] Training loss: 1.4997158674377193e-05 / Validation loss: 0.00018246630713970473 / Long term Validation loss: 0.1608\n",
      "Epoch: [6423/10000] Training loss: 1.5013354970976287e-05 / Validation loss: 0.00017969082891368428 / Long term Validation loss: 0.1599\n",
      "Epoch: [6424/10000] Training loss: 1.4996831842831823e-05 / Validation loss: 0.00018105453793981906 / Long term Validation loss: 0.1603\n",
      "Epoch: [6425/10000] Training loss: 1.4974169688852489e-05 / Validation loss: 0.00018153055048169067 / Long term Validation loss: 0.1605\n",
      "Epoch: [6426/10000] Training loss: 1.4977906066950767e-05 / Validation loss: 0.00017956207529863378 / Long term Validation loss: 0.1599\n",
      "Epoch: [6427/10000] Training loss: 1.4988504965258851e-05 / Validation loss: 0.00018176562624981133 / Long term Validation loss: 0.1607\n",
      "Epoch: [6428/10000] Training loss: 1.497845882669533e-05 / Validation loss: 0.00018041238708813115 / Long term Validation loss: 0.1601\n",
      "Epoch: [6429/10000] Training loss: 1.4961579923639132e-05 / Validation loss: 0.00018015668459959343 / Long term Validation loss: 0.1600\n",
      "Epoch: [6430/10000] Training loss: 1.4961048639202156e-05 / Validation loss: 0.00018163662252135252 / Long term Validation loss: 0.1607\n",
      "Epoch: [6431/10000] Training loss: 1.4967569239213838e-05 / Validation loss: 0.00017975821538183384 / Long term Validation loss: 0.1600\n",
      "Epoch: [6432/10000] Training loss: 1.496165145741048e-05 / Validation loss: 0.00018091051161723913 / Long term Validation loss: 0.1603\n",
      "Epoch: [6433/10000] Training loss: 1.494900769697427e-05 / Validation loss: 0.0001808933089976956 / Long term Validation loss: 0.1603\n",
      "Epoch: [6434/10000] Training loss: 1.4945878904584928e-05 / Validation loss: 0.00017978749146397365 / Long term Validation loss: 0.1600\n",
      "Epoch: [6435/10000] Training loss: 1.4949290968244477e-05 / Validation loss: 0.0001812658953915833 / Long term Validation loss: 0.1606\n",
      "Epoch: [6436/10000] Training loss: 1.4945830083865304e-05 / Validation loss: 0.00018013653077889795 / Long term Validation loss: 0.1601\n",
      "Epoch: [6437/10000] Training loss: 1.4936385254006401e-05 / Validation loss: 0.00018028352493501735 / Long term Validation loss: 0.1602\n",
      "Epoch: [6438/10000] Training loss: 1.4931859795884756e-05 / Validation loss: 0.0001810230623682839 / Long term Validation loss: 0.1605\n",
      "Epoch: [6439/10000] Training loss: 1.4932862390398487e-05 / Validation loss: 0.0001798040334643865 / Long term Validation loss: 0.1600\n",
      "Epoch: [6440/10000] Training loss: 1.4930678938743912e-05 / Validation loss: 0.00018077019748940245 / Long term Validation loss: 0.1604\n",
      "Epoch: [6441/10000] Training loss: 1.4923654530907483e-05 / Validation loss: 0.00018043939908030438 / Long term Validation loss: 0.1602\n",
      "Epoch: [6442/10000] Training loss: 1.4918574158588192e-05 / Validation loss: 0.00017995320152234452 / Long term Validation loss: 0.1601\n",
      "Epoch: [6443/10000] Training loss: 1.4917739794020353e-05 / Validation loss: 0.00018086489906289768 / Long term Validation loss: 0.1605\n",
      "Epoch: [6444/10000] Training loss: 1.4916023743520839e-05 / Validation loss: 0.00017995872702117539 / Long term Validation loss: 0.1601\n",
      "Epoch: [6445/10000] Training loss: 1.4910776549450565e-05 / Validation loss: 0.00018033476718173444 / Long term Validation loss: 0.1602\n",
      "Epoch: [6446/10000] Training loss: 1.4905702304493227e-05 / Validation loss: 0.00018055193085282748 / Long term Validation loss: 0.1603\n",
      "Epoch: [6447/10000] Training loss: 1.4903541949943455e-05 / Validation loss: 0.00017984707005628257 / Long term Validation loss: 0.1601\n",
      "Epoch: [6448/10000] Training loss: 1.4901775791767012e-05 / Validation loss: 0.00018058677792715484 / Long term Validation loss: 0.1604\n",
      "Epoch: [6449/10000] Training loss: 1.4897751392943769e-05 / Validation loss: 0.00018011470141995583 / Long term Validation loss: 0.1602\n",
      "Epoch: [6450/10000] Training loss: 1.4893015388348567e-05 / Validation loss: 0.0001800476675363108 / Long term Validation loss: 0.1602\n",
      "Epoch: [6451/10000] Training loss: 1.4889990217169577e-05 / Validation loss: 0.00018050270879420405 / Long term Validation loss: 0.1604\n",
      "Epoch: [6452/10000] Training loss: 1.4887892598618218e-05 / Validation loss: 0.0001798537724490644 / Long term Validation loss: 0.1602\n",
      "Epoch: [6453/10000] Training loss: 1.4884615681017902e-05 / Validation loss: 0.0001802942176707134 / Long term Validation loss: 0.1603\n",
      "Epoch: [6454/10000] Training loss: 1.4880362329870957e-05 / Validation loss: 0.0001801793276254552 / Long term Validation loss: 0.1603\n",
      "Epoch: [6455/10000] Training loss: 1.4876869414973259e-05 / Validation loss: 0.0001798734423595867 / Long term Validation loss: 0.1602\n",
      "Epoch: [6456/10000] Training loss: 1.487434293732561e-05 / Validation loss: 0.00018034329525604622 / Long term Validation loss: 0.1604\n",
      "Epoch: [6457/10000] Training loss: 1.487142832244951e-05 / Validation loss: 0.00017987920775696303 / Long term Validation loss: 0.1602\n",
      "Epoch: [6458/10000] Training loss: 1.486765489484031e-05 / Validation loss: 0.00018004659182661742 / Long term Validation loss: 0.1603\n",
      "Epoch: [6459/10000] Training loss: 1.4864004393078065e-05 / Validation loss: 0.00018015329081587895 / Long term Validation loss: 0.1604\n",
      "Epoch: [6460/10000] Training loss: 1.4861085297546457e-05 / Validation loss: 0.00017977865821748308 / Long term Validation loss: 0.1602\n",
      "Epoch: [6461/10000] Training loss: 1.485825292879046e-05 / Validation loss: 0.00018014735847999858 / Long term Validation loss: 0.1604\n",
      "Epoch: [6462/10000] Training loss: 1.4854858316049954e-05 / Validation loss: 0.0001798800276182939 / Long term Validation loss: 0.1603\n",
      "Epoch: [6463/10000] Training loss: 1.4851256345754828e-05 / Validation loss: 0.00017985439835145812 / Long term Validation loss: 0.1603\n",
      "Epoch: [6464/10000] Training loss: 1.4848060127340304e-05 / Validation loss: 0.0001800546524175514 / Long term Validation loss: 0.1604\n",
      "Epoch: [6465/10000] Training loss: 1.4845144266896824e-05 / Validation loss: 0.00017971227858280597 / Long term Validation loss: 0.1603\n",
      "Epoch: [6466/10000] Training loss: 1.4841981288952055e-05 / Validation loss: 0.00017994888737302175 / Long term Validation loss: 0.1604\n",
      "Epoch: [6467/10000] Training loss: 1.4838527606464302e-05 / Validation loss: 0.0001798381972945002 / Long term Validation loss: 0.1604\n",
      "Epoch: [6468/10000] Training loss: 1.483519495504343e-05 / Validation loss: 0.0001797091779776466 / Long term Validation loss: 0.1603\n",
      "Epoch: [6469/10000] Training loss: 1.4832135329934533e-05 / Validation loss: 0.00017992133466539125 / Long term Validation loss: 0.1604\n",
      "Epoch: [6470/10000] Training loss: 1.4829061189555252e-05 / Validation loss: 0.00017965555057480802 / Long term Validation loss: 0.1603\n",
      "Epoch: [6471/10000] Training loss: 1.4825764926015606e-05 / Validation loss: 0.00017977569777082 / Long term Validation loss: 0.1604\n",
      "Epoch: [6472/10000] Training loss: 1.4822415249437413e-05 / Validation loss: 0.00017976505865305176 / Long term Validation loss: 0.1604\n",
      "Epoch: [6473/10000] Training loss: 1.4819230554016325e-05 / Validation loss: 0.00017959724694894053 / Long term Validation loss: 0.1603\n",
      "Epoch: [6474/10000] Training loss: 1.4816143193334906e-05 / Validation loss: 0.00017977681572185482 / Long term Validation loss: 0.1604\n",
      "Epoch: [6475/10000] Training loss: 1.4812955822372585e-05 / Validation loss: 0.00017959085571096843 / Long term Validation loss: 0.1604\n",
      "Epoch: [6476/10000] Training loss: 1.4809658764555177e-05 / Validation loss: 0.00017962638022404886 / Long term Validation loss: 0.1604\n",
      "Epoch: [6477/10000] Training loss: 1.4806407528686014e-05 / Validation loss: 0.00017966809123157412 / Long term Validation loss: 0.1604\n",
      "Epoch: [6478/10000] Training loss: 1.480326171129238e-05 / Validation loss: 0.00017950221357267282 / Long term Validation loss: 0.1604\n",
      "Epoch: [6479/10000] Training loss: 1.4800116431556942e-05 / Validation loss: 0.000179636154158123 / Long term Validation loss: 0.1604\n",
      "Epoch: [6480/10000] Training loss: 1.4796887410051398e-05 / Validation loss: 0.00017951669953945916 / Long term Validation loss: 0.1604\n",
      "Epoch: [6481/10000] Training loss: 1.479362916435378e-05 / Validation loss: 0.00017949985032922423 / Long term Validation loss: 0.1604\n",
      "Epoch: [6482/10000] Training loss: 1.4790428937558138e-05 / Validation loss: 0.00017956020607841504 / Long term Validation loss: 0.1605\n",
      "Epoch: [6483/10000] Training loss: 1.4787274416612864e-05 / Validation loss: 0.00017941392387230061 / Long term Validation loss: 0.1604\n",
      "Epoch: [6484/10000] Training loss: 1.4784090841804435e-05 / Validation loss: 0.00017950288212458603 / Long term Validation loss: 0.1605\n",
      "Epoch: [6485/10000] Training loss: 1.478086011750612e-05 / Validation loss: 0.0001794296250985005 / Long term Validation loss: 0.1605\n",
      "Epoch: [6486/10000] Training loss: 1.4777635331002888e-05 / Validation loss: 0.0001793844386593507 / Long term Validation loss: 0.1604\n",
      "Epoch: [6487/10000] Training loss: 1.47744515097788e-05 / Validation loss: 0.00017944392334817878 / Long term Validation loss: 0.1605\n",
      "Epoch: [6488/10000] Training loss: 1.4771279609713797e-05 / Validation loss: 0.0001793226599129211 / Long term Validation loss: 0.1605\n",
      "Epoch: [6489/10000] Training loss: 1.4768079879773059e-05 / Validation loss: 0.0001793757060633676 / Long term Validation loss: 0.1605\n",
      "Epoch: [6490/10000] Training loss: 1.4764859659961709e-05 / Validation loss: 0.000179331857965432 / Long term Validation loss: 0.1605\n",
      "Epoch: [6491/10000] Training loss: 1.4761653795397218e-05 / Validation loss: 0.00017927504746853738 / Long term Validation loss: 0.1605\n",
      "Epoch: [6492/10000] Training loss: 1.4758470830325579e-05 / Validation loss: 0.00017932452393670548 / Long term Validation loss: 0.1605\n",
      "Epoch: [6493/10000] Training loss: 1.4755286309174953e-05 / Validation loss: 0.00017922619856933623 / Long term Validation loss: 0.1605\n",
      "Epoch: [6494/10000] Training loss: 1.475208289917312e-05 / Validation loss: 0.00017925266984141335 / Long term Validation loss: 0.1605\n",
      "Epoch: [6495/10000] Training loss: 1.4748872786003934e-05 / Validation loss: 0.00017922479111928737 / Long term Validation loss: 0.1605\n",
      "Epoch: [6496/10000] Training loss: 1.4745675152526361e-05 / Validation loss: 0.00017916564761844044 / Long term Validation loss: 0.1605\n",
      "Epoch: [6497/10000] Training loss: 1.4742488715341683e-05 / Validation loss: 0.0001792022911511622 / Long term Validation loss: 0.1605\n",
      "Epoch: [6498/10000] Training loss: 1.4739297466074062e-05 / Validation loss: 0.00017912283990146772 / Long term Validation loss: 0.1605\n",
      "Epoch: [6499/10000] Training loss: 1.4736095111405237e-05 / Validation loss: 0.00017913185994931428 / Long term Validation loss: 0.1605\n",
      "Epoch: [6500/10000] Training loss: 1.4732891629613465e-05 / Validation loss: 0.00017911198978042938 / Long term Validation loss: 0.1605\n",
      "Epoch: [6501/10000] Training loss: 1.4729696678066957e-05 / Validation loss: 0.0001790559340505231 / Long term Validation loss: 0.1605\n",
      "Epoch: [6502/10000] Training loss: 1.472650680758649e-05 / Validation loss: 0.00017908052612795284 / Long term Validation loss: 0.1606\n",
      "Epoch: [6503/10000] Training loss: 1.4723312603186607e-05 / Validation loss: 0.00017901539662511373 / Long term Validation loss: 0.1606\n",
      "Epoch: [6504/10000] Training loss: 1.4720112242243545e-05 / Validation loss: 0.00017901343896070123 / Long term Validation loss: 0.1606\n",
      "Epoch: [6505/10000] Training loss: 1.4716912409015018e-05 / Validation loss: 0.00017899631746895055 / Long term Validation loss: 0.1606\n",
      "Epoch: [6506/10000] Training loss: 1.4713717843282168e-05 / Validation loss: 0.000178945284478868 / Long term Validation loss: 0.1606\n",
      "Epoch: [6507/10000] Training loss: 1.4710525558263296e-05 / Validation loss: 0.00017895961225293378 / Long term Validation loss: 0.1606\n",
      "Epoch: [6508/10000] Training loss: 1.4707330228089204e-05 / Validation loss: 0.0001789049936958847 / Long term Validation loss: 0.1606\n",
      "Epoch: [6509/10000] Training loss: 1.4704131487177942e-05 / Validation loss: 0.00017889685111785515 / Long term Validation loss: 0.1606\n",
      "Epoch: [6510/10000] Training loss: 1.4700933435294049e-05 / Validation loss: 0.0001788800078977119 / Long term Validation loss: 0.1606\n",
      "Epoch: [6511/10000] Training loss: 1.4697738491694242e-05 / Validation loss: 0.00017883473625944266 / Long term Validation loss: 0.1606\n",
      "Epoch: [6512/10000] Training loss: 1.4694544652534074e-05 / Validation loss: 0.00017884096656539715 / Long term Validation loss: 0.1606\n",
      "Epoch: [6513/10000] Training loss: 1.4691348897464438e-05 / Validation loss: 0.00017879384615916005 / Long term Validation loss: 0.1606\n",
      "Epoch: [6514/10000] Training loss: 1.4688151171540171e-05 / Validation loss: 0.00017878226148157985 / Long term Validation loss: 0.1606\n",
      "Epoch: [6515/10000] Training loss: 1.4684953880299554e-05 / Validation loss: 0.00017876411667822208 / Long term Validation loss: 0.1606\n",
      "Epoch: [6516/10000] Training loss: 1.4681758382023102e-05 / Validation loss: 0.0001787240649612952 / Long term Validation loss: 0.1606\n",
      "Epoch: [6517/10000] Training loss: 1.4678563510304263e-05 / Validation loss: 0.00017872375631283212 / Long term Validation loss: 0.1606\n",
      "Epoch: [6518/10000] Training loss: 1.4675367512545547e-05 / Validation loss: 0.0001786818902789547 / Long term Validation loss: 0.1606\n",
      "Epoch: [6519/10000] Training loss: 1.4672170302784491e-05 / Validation loss: 0.00017866851777611364 / Long term Validation loss: 0.1607\n",
      "Epoch: [6520/10000] Training loss: 1.4668973246890167e-05 / Validation loss: 0.00017864856165954412 / Long term Validation loss: 0.1607\n",
      "Epoch: [6521/10000] Training loss: 1.466577719643551e-05 / Validation loss: 0.00017861315879638817 / Long term Validation loss: 0.1607\n",
      "Epoch: [6522/10000] Training loss: 1.4662581561183613e-05 / Validation loss: 0.00017860765506795334 / Long term Validation loss: 0.1607\n",
      "Epoch: [6523/10000] Training loss: 1.4659385304120784e-05 / Validation loss: 0.0001785695560725473 / Long term Validation loss: 0.1607\n",
      "Epoch: [6524/10000] Training loss: 1.465618825992144e-05 / Validation loss: 0.00017855513064020726 / Long term Validation loss: 0.1607\n",
      "Epoch: [6525/10000] Training loss: 1.465299116702294e-05 / Validation loss: 0.0001785331025189483 / Long term Validation loss: 0.1607\n",
      "Epoch: [6526/10000] Training loss: 1.4649794598939086e-05 / Validation loss: 0.0001785014917198032 / Long term Validation loss: 0.1607\n",
      "Epoch: [6527/10000] Training loss: 1.4646598322245591e-05 / Validation loss: 0.0001784916140510769 / Long term Validation loss: 0.1607\n",
      "Epoch: [6528/10000] Training loss: 1.4643401728532315e-05 / Validation loss: 0.00017845631385241299 / Long term Validation loss: 0.1607\n",
      "Epoch: [6529/10000] Training loss: 1.4640204609808457e-05 / Validation loss: 0.00017844117247612309 / Long term Validation loss: 0.1607\n",
      "Epoch: [6530/10000] Training loss: 1.463700732641645e-05 / Validation loss: 0.00017841729090679016 / Long term Validation loss: 0.1607\n",
      "Epoch: [6531/10000] Training loss: 1.4633810266698494e-05 / Validation loss: 0.0001783889028585224 / Long term Validation loss: 0.1607\n",
      "Epoch: [6532/10000] Training loss: 1.4630613397635713e-05 / Validation loss: 0.00017837541063099843 / Long term Validation loss: 0.1607\n",
      "Epoch: [6533/10000] Training loss: 1.4627416383518404e-05 / Validation loss: 0.00017834245456704474 / Long term Validation loss: 0.1607\n",
      "Epoch: [6534/10000] Training loss: 1.462421902116474e-05 / Validation loss: 0.0001783266492380943 / Long term Validation loss: 0.1607\n",
      "Epoch: [6535/10000] Training loss: 1.4621021447417787e-05 / Validation loss: 0.0001783012355676784 / Long term Validation loss: 0.1607\n",
      "Epoch: [6536/10000] Training loss: 1.4617823908523604e-05 / Validation loss: 0.00017827545693225772 / Long term Validation loss: 0.1608\n",
      "Epoch: [6537/10000] Training loss: 1.4614626465791634e-05 / Validation loss: 0.00017825885138913184 / Long term Validation loss: 0.1608\n",
      "Epoch: [6538/10000] Training loss: 1.4611428960865868e-05 / Validation loss: 0.0001782280003179515 / Long term Validation loss: 0.1608\n",
      "Epoch: [6539/10000] Training loss: 1.460823123037198e-05 / Validation loss: 0.00017821144314361293 / Long term Validation loss: 0.1608\n",
      "Epoch: [6540/10000] Training loss: 1.4605033289135756e-05 / Validation loss: 0.0001781850176372509 / Long term Validation loss: 0.1608\n",
      "Epoch: [6541/10000] Training loss: 1.4601835271097335e-05 / Validation loss: 0.0001781613871671577 / Long term Validation loss: 0.1608\n",
      "Epoch: [6542/10000] Training loss: 1.4598637262772269e-05 / Validation loss: 0.00017814219499004525 / Long term Validation loss: 0.1608\n",
      "Epoch: [6543/10000] Training loss: 1.4595439215433614e-05 / Validation loss: 0.00017811339580775035 / Long term Validation loss: 0.1608\n",
      "Epoch: [6544/10000] Training loss: 1.45922410222096e-05 / Validation loss: 0.00017809590814142254 / Long term Validation loss: 0.1608\n",
      "Epoch: [6545/10000] Training loss: 1.4589042645043167e-05 / Validation loss: 0.00017806898722512408 / Long term Validation loss: 0.1608\n",
      "Epoch: [6546/10000] Training loss: 1.4585844135626918e-05 / Validation loss: 0.00017804694427329392 / Long term Validation loss: 0.1608\n",
      "Epoch: [6547/10000] Training loss: 1.4582645565432251e-05 / Validation loss: 0.00017802558301620595 / Long term Validation loss: 0.1608\n",
      "Epoch: [6548/10000] Training loss: 1.4579446942818828e-05 / Validation loss: 0.00017799873214053132 / Long term Validation loss: 0.1608\n",
      "Epoch: [6549/10000] Training loss: 1.4576248216215588e-05 / Validation loss: 0.00017798009191365467 / Long term Validation loss: 0.1608\n",
      "Epoch: [6550/10000] Training loss: 1.4573049339564672e-05 / Validation loss: 0.00017795321389981906 / Long term Validation loss: 0.1608\n",
      "Epoch: [6551/10000] Training loss: 1.4569850314512592e-05 / Validation loss: 0.00017793221139246422 / Long term Validation loss: 0.1608\n",
      "Epoch: [6552/10000] Training loss: 1.4566651181159485e-05 / Validation loss: 0.00017790917652758632 / Long term Validation loss: 0.1608\n",
      "Epoch: [6553/10000] Training loss: 1.4563451965725324e-05 / Validation loss: 0.0001778841303925153 / Long term Validation loss: 0.1609\n",
      "Epoch: [6554/10000] Training loss: 1.456025265715188e-05 / Validation loss: 0.0001778641646072843 / Long term Validation loss: 0.1609\n",
      "Epoch: [6555/10000] Training loss: 1.4557053224718115e-05 / Validation loss: 0.00017783778496273633 / Long term Validation loss: 0.1609\n",
      "Epoch: [6556/10000] Training loss: 1.4553853649999082e-05 / Validation loss: 0.00017781720529807155 / Long term Validation loss: 0.1609\n",
      "Epoch: [6557/10000] Training loss: 1.4550653943931957e-05 / Validation loss: 0.00017779299085461631 / Long term Validation loss: 0.1609\n",
      "Epoch: [6558/10000] Training loss: 1.4547454126734479e-05 / Validation loss: 0.00017776944359413404 / Long term Validation loss: 0.1609\n",
      "Epoch: [6559/10000] Training loss: 1.4544254207135158e-05 / Validation loss: 0.00017774808432545927 / Long term Validation loss: 0.1609\n",
      "Epoch: [6560/10000] Training loss: 1.4541054174536654e-05 / Validation loss: 0.0001777225594863824 / Long term Validation loss: 0.1609\n",
      "Epoch: [6561/10000] Training loss: 1.4537854011809694e-05 / Validation loss: 0.00017770184962654246 / Long term Validation loss: 0.1609\n",
      "Epoch: [6562/10000] Training loss: 1.4534653713490812e-05 / Validation loss: 0.0001776770316503522 / Long term Validation loss: 0.1609\n",
      "Epoch: [6563/10000] Training loss: 1.4531453285937292e-05 / Validation loss: 0.0001776545766801606 / Long term Validation loss: 0.1609\n",
      "Epoch: [6564/10000] Training loss: 1.452825273991143e-05 / Validation loss: 0.00017763194551462046 / Long term Validation loss: 0.1609\n",
      "Epoch: [6565/10000] Training loss: 1.4525052077569563e-05 / Validation loss: 0.00017760745594961173 / Long term Validation loss: 0.1609\n",
      "Epoch: [6566/10000] Training loss: 1.452185129187413e-05 / Validation loss: 0.00017758615508095736 / Long term Validation loss: 0.1609\n",
      "Epoch: [6567/10000] Training loss: 1.4518650374648082e-05 / Validation loss: 0.0001775612759024145 / Long term Validation loss: 0.1609\n",
      "Epoch: [6568/10000] Training loss: 1.4515449322517082e-05 / Validation loss: 0.0001775393907053353 / Long term Validation loss: 0.1609\n",
      "Epoch: [6569/10000] Training loss: 1.4512248139734046e-05 / Validation loss: 0.00017751579096444205 / Long term Validation loss: 0.1609\n",
      "Epoch: [6570/10000] Training loss: 1.450904683079925e-05 / Validation loss: 0.00017749230934128313 / Long term Validation loss: 0.1610\n",
      "Epoch: [6571/10000] Training loss: 1.4505845396660585e-05 / Validation loss: 0.00017747017123816332 / Long term Validation loss: 0.1610\n",
      "Epoch: [6572/10000] Training loss: 1.4502643833660675e-05 / Validation loss: 0.0001774456997846333 / Long term Validation loss: 0.1610\n",
      "Epoch: [6573/10000] Training loss: 1.4499442136805157e-05 / Validation loss: 0.00017742386940997274 / Long term Validation loss: 0.1610\n",
      "Epoch: [6574/10000] Training loss: 1.4496240304649919e-05 / Validation loss: 0.0001773997587418707 / Long term Validation loss: 0.1610\n",
      "Epoch: [6575/10000] Training loss: 1.449303833823704e-05 / Validation loss: 0.00017737703868224414 / Long term Validation loss: 0.1610\n",
      "Epoch: [6576/10000] Training loss: 1.4489836240172773e-05 / Validation loss: 0.0001773540551448782 / Long term Validation loss: 0.1610\n",
      "Epoch: [6577/10000] Training loss: 1.448663401082801e-05 / Validation loss: 0.00017733024705450837 / Long term Validation loss: 0.1610\n",
      "Epoch: [6578/10000] Training loss: 1.4483431648289007e-05 / Validation loss: 0.000177308042039927 / Long term Validation loss: 0.1610\n",
      "Epoch: [6579/10000] Training loss: 1.4480229150123403e-05 / Validation loss: 0.00017728390424218032 / Long term Validation loss: 0.1610\n",
      "Epoch: [6580/10000] Training loss: 1.4477026514520684e-05 / Validation loss: 0.0001772615439500601 / Long term Validation loss: 0.1610\n",
      "Epoch: [6581/10000] Training loss: 1.4473823741988151e-05 / Validation loss: 0.00017723795549463205 / Long term Validation loss: 0.1610\n",
      "Epoch: [6582/10000] Training loss: 1.4470620833251933e-05 / Validation loss: 0.0001772148183686665 / Long term Validation loss: 0.1610\n",
      "Epoch: [6583/10000] Training loss: 1.4467417788826942e-05 / Validation loss: 0.00017719203151479455 / Long term Validation loss: 0.1610\n",
      "Epoch: [6584/10000] Training loss: 1.4464214607920167e-05 / Validation loss: 0.00017716824850299437 / Long term Validation loss: 0.1610\n",
      "Epoch: [6585/10000] Training loss: 1.4461011288945111e-05 / Validation loss: 0.00017714581921549698 / Long term Validation loss: 0.1610\n",
      "Epoch: [6586/10000] Training loss: 1.4457807830895797e-05 / Validation loss: 0.00017712200921936444 / Long term Validation loss: 0.1610\n",
      "Epoch: [6587/10000] Training loss: 1.4454604233149844e-05 / Validation loss: 0.00017709929931365606 / Long term Validation loss: 0.1610\n",
      "Epoch: [6588/10000] Training loss: 1.4451400496127735e-05 / Validation loss: 0.00017707597808976213 / Long term Validation loss: 0.1610\n",
      "Epoch: [6589/10000] Training loss: 1.4448196619900916e-05 / Validation loss: 0.00017705269505043097 / Long term Validation loss: 0.1611\n",
      "Epoch: [6590/10000] Training loss: 1.4444992604246077e-05 / Validation loss: 0.00017702989813387084 / Long term Validation loss: 0.1611\n",
      "Epoch: [6591/10000] Training loss: 1.444178844845639e-05 / Validation loss: 0.00017700624475623398 / Long term Validation loss: 0.1611\n",
      "Epoch: [6592/10000] Training loss: 1.4438584151566587e-05 / Validation loss: 0.00017698360328497235 / Long term Validation loss: 0.1611\n",
      "Epoch: [6593/10000] Training loss: 1.4435379713158055e-05 / Validation loss: 0.00017696001799376044 / Long term Validation loss: 0.1611\n",
      "Epoch: [6594/10000] Training loss: 1.443217513287367e-05 / Validation loss: 0.00017693712109188265 / Long term Validation loss: 0.1611\n",
      "Epoch: [6595/10000] Training loss: 1.4428970410836817e-05 / Validation loss: 0.00017691390313939094 / Long term Validation loss: 0.1611\n",
      "Epoch: [6596/10000] Training loss: 1.4425765546867177e-05 / Validation loss: 0.0001768906082496847 / Long term Validation loss: 0.1611\n",
      "Epoch: [6597/10000] Training loss: 1.4422560540644986e-05 / Validation loss: 0.00017686773115159737 / Long term Validation loss: 0.1611\n",
      "Epoch: [6598/10000] Training loss: 1.4419355391703942e-05 / Validation loss: 0.00017684420848059767 / Long term Validation loss: 0.1611\n",
      "Epoch: [6599/10000] Training loss: 1.4416150099450685e-05 / Validation loss: 0.00017682140969273435 / Long term Validation loss: 0.1611\n",
      "Epoch: [6600/10000] Training loss: 1.4412944663644106e-05 / Validation loss: 0.00017679795402993022 / Long term Validation loss: 0.1611\n",
      "Epoch: [6601/10000] Training loss: 1.4409739083982069e-05 / Validation loss: 0.00017677496927549378 / Long term Validation loss: 0.1611\n",
      "Epoch: [6602/10000] Training loss: 1.4406533360436556e-05 / Validation loss: 0.00017675176886439725 / Long term Validation loss: 0.1611\n",
      "Epoch: [6603/10000] Training loss: 1.4403327492788914e-05 / Validation loss: 0.00017672851298181763 / Long term Validation loss: 0.1611\n",
      "Epoch: [6604/10000] Training loss: 1.4400121480786291e-05 / Validation loss: 0.000176705545459224 / Long term Validation loss: 0.1611\n",
      "Epoch: [6605/10000] Training loss: 1.4396915324126033e-05 / Validation loss: 0.0001766821302669101 / Long term Validation loss: 0.1611\n",
      "Epoch: [6606/10000] Training loss: 1.4393709022419597e-05 / Validation loss: 0.000176659224101343 / Long term Validation loss: 0.1611\n",
      "Epoch: [6607/10000] Training loss: 1.4390502575485353e-05 / Validation loss: 0.00017663584199577946 / Long term Validation loss: 0.1611\n",
      "Epoch: [6608/10000] Training loss: 1.4387295983058715e-05 / Validation loss: 0.0001766128209369062 / Long term Validation loss: 0.1611\n",
      "Epoch: [6609/10000] Training loss: 1.438408924506342e-05 / Validation loss: 0.0001765896037390116 / Long term Validation loss: 0.1612\n",
      "Epoch: [6610/10000] Training loss: 1.4380882361303742e-05 / Validation loss: 0.0001765663996664138 / Long term Validation loss: 0.1612\n",
      "Epoch: [6611/10000] Training loss: 1.4377675331607937e-05 / Validation loss: 0.00017654334921668417 / Long term Validation loss: 0.1612\n",
      "Epoch: [6612/10000] Training loss: 1.4374468155771067e-05 / Validation loss: 0.0001765200201684526 / Long term Validation loss: 0.1612\n",
      "Epoch: [6613/10000] Training loss: 1.4371260833535753e-05 / Validation loss: 0.00017649703607663464 / Long term Validation loss: 0.1612\n",
      "Epoch: [6614/10000] Training loss: 1.4368053364756938e-05 / Validation loss: 0.0001764737028635902 / Long term Validation loss: 0.1612\n",
      "Epoch: [6615/10000] Training loss: 1.4364845749217487e-05 / Validation loss: 0.00017645066568271517 / Long term Validation loss: 0.1612\n",
      "Epoch: [6616/10000] Training loss: 1.4361637986836787e-05 / Validation loss: 0.0001764274264736749 / Long term Validation loss: 0.1612\n",
      "Epoch: [6617/10000] Training loss: 1.4358430077451868e-05 / Validation loss: 0.0001764042720888983 / Long term Validation loss: 0.1612\n",
      "Epoch: [6618/10000] Training loss: 1.4355222020955525e-05 / Validation loss: 0.00017638115097203 / Long term Validation loss: 0.1612\n",
      "Epoch: [6619/10000] Training loss: 1.4352013817203914e-05 / Validation loss: 0.00017635789476594922 / Long term Validation loss: 0.1612\n",
      "Epoch: [6620/10000] Training loss: 1.4348805466036954e-05 / Validation loss: 0.00017633484494464184 / Long term Validation loss: 0.1612\n",
      "Epoch: [6621/10000] Training loss: 1.4345596967343765e-05 / Validation loss: 0.00017631155507509953 / Long term Validation loss: 0.1612\n",
      "Epoch: [6622/10000] Training loss: 1.4342388320963558e-05 / Validation loss: 0.0001762885006003697 / Long term Validation loss: 0.1612\n",
      "Epoch: [6623/10000] Training loss: 1.43391795268277e-05 / Validation loss: 0.00017626524890476797 / Long term Validation loss: 0.1612\n",
      "Epoch: [6624/10000] Training loss: 1.433597058480709e-05 / Validation loss: 0.0001762421319655012 / Long term Validation loss: 0.1612\n",
      "Epoch: [6625/10000] Training loss: 1.433276149483982e-05 / Validation loss: 0.00017621895526555108 / Long term Validation loss: 0.1612\n",
      "Epoch: [6626/10000] Training loss: 1.4329552256823122e-05 / Validation loss: 0.00017619576223378342 / Long term Validation loss: 0.1612\n",
      "Epoch: [6627/10000] Training loss: 1.4326342870671464e-05 / Validation loss: 0.00017617265166436087 / Long term Validation loss: 0.1612\n",
      "Epoch: [6628/10000] Training loss: 1.4323133336306302e-05 / Validation loss: 0.00017614940987341926 / Long term Validation loss: 0.1612\n",
      "Epoch: [6629/10000] Training loss: 1.4319923653627932e-05 / Validation loss: 0.00017612632598203968 / Long term Validation loss: 0.1612\n",
      "Epoch: [6630/10000] Training loss: 1.431671382258618e-05 / Validation loss: 0.00017610308071956167 / Long term Validation loss: 0.1613\n",
      "Epoch: [6631/10000] Training loss: 1.4313503843090635e-05 / Validation loss: 0.00017607997967589823 / Long term Validation loss: 0.1613\n",
      "Epoch: [6632/10000] Training loss: 1.431029371510945e-05 / Validation loss: 0.0001760567682657892 / Long term Validation loss: 0.1613\n",
      "Epoch: [6633/10000] Training loss: 1.4307083438571928e-05 / Validation loss: 0.00017603362343795024 / Long term Validation loss: 0.1613\n",
      "Epoch: [6634/10000] Training loss: 1.4303873013445482e-05 / Validation loss: 0.0001760104598260919 / Long term Validation loss: 0.1613\n",
      "Epoch: [6635/10000] Training loss: 1.4300662439679852e-05 / Validation loss: 0.00017598726987675558 / Long term Validation loss: 0.1613\n",
      "Epoch: [6636/10000] Training loss: 1.4297451717233242e-05 / Validation loss: 0.00017596414402094618 / Long term Validation loss: 0.1613\n",
      "Epoch: [6637/10000] Training loss: 1.4294240846076236e-05 / Validation loss: 0.00017594092754479143 / Long term Validation loss: 0.1613\n",
      "Epoch: [6638/10000] Training loss: 1.429102982616415e-05 / Validation loss: 0.00017591781560389274 / Long term Validation loss: 0.1613\n",
      "Epoch: [6639/10000] Training loss: 1.428781865748645e-05 / Validation loss: 0.0001758945984236388 / Long term Validation loss: 0.1613\n",
      "Epoch: [6640/10000] Training loss: 1.4284607340005659e-05 / Validation loss: 0.00017587147608689048 / Long term Validation loss: 0.1613\n",
      "Epoch: [6641/10000] Training loss: 1.4281395873722542e-05 / Validation loss: 0.0001758482788003516 / Long term Validation loss: 0.1613\n",
      "Epoch: [6642/10000] Training loss: 1.427818425861296e-05 / Validation loss: 0.00017582513118816404 / Long term Validation loss: 0.1613\n",
      "Epoch: [6643/10000] Training loss: 1.4274972494680079e-05 / Validation loss: 0.00017580196220210083 / Long term Validation loss: 0.1613\n",
      "Epoch: [6644/10000] Training loss: 1.427176058191535e-05 / Validation loss: 0.0001757787873339155 / Long term Validation loss: 0.1613\n",
      "Epoch: [6645/10000] Training loss: 1.4268548520320253e-05 / Validation loss: 0.00017575564267171282 / Long term Validation loss: 0.1613\n",
      "Epoch: [6646/10000] Training loss: 1.4265336309902075e-05 / Validation loss: 0.00017573244904240488 / Long term Validation loss: 0.1613\n",
      "Epoch: [6647/10000] Training loss: 1.4262123950662446e-05 / Validation loss: 0.00017570931695810177 / Long term Validation loss: 0.1613\n",
      "Epoch: [6648/10000] Training loss: 1.4258911442622616e-05 / Validation loss: 0.00017568611786927127 / Long term Validation loss: 0.1613\n",
      "Epoch: [6649/10000] Training loss: 1.4255698785788766e-05 / Validation loss: 0.00017566298499037376 / Long term Validation loss: 0.1613\n",
      "Epoch: [6650/10000] Training loss: 1.4252485980192205e-05 / Validation loss: 0.0001756397926897798 / Long term Validation loss: 0.1613\n",
      "Epoch: [6651/10000] Training loss: 1.4249273025847499e-05 / Validation loss: 0.00017561664894318813 / Long term Validation loss: 0.1613\n",
      "Epoch: [6652/10000] Training loss: 1.4246059922791527e-05 / Validation loss: 0.00017559347076770413 / Long term Validation loss: 0.1613\n",
      "Epoch: [6653/10000] Training loss: 1.424284667104961e-05 / Validation loss: 0.000175570311767079 / Long term Validation loss: 0.1614\n",
      "Epoch: [6654/10000] Training loss: 1.4239633270661184e-05 / Validation loss: 0.00017554714906342165 / Long term Validation loss: 0.1614\n",
      "Epoch: [6655/10000] Training loss: 1.4236419721663125e-05 / Validation loss: 0.00017552397598546415 / Long term Validation loss: 0.1614\n",
      "Epoch: [6656/10000] Training loss: 1.4233206024096755e-05 / Validation loss: 0.0001755008253058106 / Long term Validation loss: 0.1614\n",
      "Epoch: [6657/10000] Training loss: 1.4229992178010072e-05 / Validation loss: 0.00017547764308544422 / Long term Validation loss: 0.1614\n",
      "Epoch: [6658/10000] Training loss: 1.4226778183447332e-05 / Validation loss: 0.00017545449849935585 / Long term Validation loss: 0.1614\n",
      "Epoch: [6659/10000] Training loss: 1.4223564040466108e-05 / Validation loss: 0.00017543131341894329 / Long term Validation loss: 0.1614\n",
      "Epoch: [6660/10000] Training loss: 1.4220349749115421e-05 / Validation loss: 0.00017540816881989128 / Long term Validation loss: 0.1614\n",
      "Epoch: [6661/10000] Training loss: 1.4217135309460254e-05 / Validation loss: 0.0001753849864168793 / Long term Validation loss: 0.1614\n",
      "Epoch: [6662/10000] Training loss: 1.4213920721556098e-05 / Validation loss: 0.00017536183714492585 / Long term Validation loss: 0.1614\n",
      "Epoch: [6663/10000] Training loss: 1.421070598547336e-05 / Validation loss: 0.00017533866097643082 / Long term Validation loss: 0.1614\n",
      "Epoch: [6664/10000] Training loss: 1.4207491101275072e-05 / Validation loss: 0.00017531550455653416 / Long term Validation loss: 0.1614\n",
      "Epoch: [6665/10000] Training loss: 1.4204276069035684e-05 / Validation loss: 0.0001752923359050289 / Long term Validation loss: 0.1614\n",
      "Epoch: [6666/10000] Training loss: 1.4201060888826336e-05 / Validation loss: 0.00017526917201180113 / Long term Validation loss: 0.1614\n",
      "Epoch: [6667/10000] Training loss: 1.4197845560724831e-05 / Validation loss: 0.00017524601027465908 / Long term Validation loss: 0.1614\n",
      "Epoch: [6668/10000] Training loss: 1.4194630084810353e-05 / Validation loss: 0.00017522284018102487 / Long term Validation loss: 0.1614\n",
      "Epoch: [6669/10000] Training loss: 1.4191414461164133e-05 / Validation loss: 0.00017519968357073597 / Long term Validation loss: 0.1614\n",
      "Epoch: [6670/10000] Training loss: 1.4188198689872803e-05 / Validation loss: 0.00017517650938460143 / Long term Validation loss: 0.1614\n",
      "Epoch: [6671/10000] Training loss: 1.418498277102146e-05 / Validation loss: 0.00017515335564269264 / Long term Validation loss: 0.1614\n",
      "Epoch: [6672/10000] Training loss: 1.4181766704703384e-05 / Validation loss: 0.0001751301796046698 / Long term Validation loss: 0.1614\n",
      "Epoch: [6673/10000] Training loss: 1.4178550491008057e-05 / Validation loss: 0.00017510702657017225 / Long term Validation loss: 0.1614\n",
      "Epoch: [6674/10000] Training loss: 1.4175334130034568e-05 / Validation loss: 0.0001750838505788123 / Long term Validation loss: 0.1614\n",
      "Epoch: [6675/10000] Training loss: 1.41721176218773e-05 / Validation loss: 0.0001750606965506198 / Long term Validation loss: 0.1615\n",
      "Epoch: [6676/10000] Training loss: 1.4168900966640331e-05 / Validation loss: 0.0001750375219440081 / Long term Validation loss: 0.1615\n",
      "Epoch: [6677/10000] Training loss: 1.4165684164423336e-05 / Validation loss: 0.00017501436582854728 / Long term Validation loss: 0.1615\n",
      "Epoch: [6678/10000] Training loss: 1.4162467215334813e-05 / Validation loss: 0.0001749911933567614 / Long term Validation loss: 0.1615\n",
      "Epoch: [6679/10000] Training loss: 1.4159250119479862e-05 / Validation loss: 0.00017496803463481445 / Long term Validation loss: 0.1615\n",
      "Epoch: [6680/10000] Training loss: 1.4156032876971012e-05 / Validation loss: 0.00017494486454098956 / Long term Validation loss: 0.1615\n",
      "Epoch: [6681/10000] Training loss: 1.415281548791882e-05 / Validation loss: 0.00017492170312703321 / Long term Validation loss: 0.1615\n",
      "Epoch: [6682/10000] Training loss: 1.4149597952439629e-05 / Validation loss: 0.00017489853528250218 / Long term Validation loss: 0.1615\n",
      "Epoch: [6683/10000] Training loss: 1.4146380270649318e-05 / Validation loss: 0.0001748753713594333 / Long term Validation loss: 0.1615\n",
      "Epoch: [6684/10000] Training loss: 1.414316244266795e-05 / Validation loss: 0.00017485220541894742 / Long term Validation loss: 0.1615\n",
      "Epoch: [6685/10000] Training loss: 1.4139944468616535e-05 / Validation loss: 0.00017482903930004964 / Long term Validation loss: 0.1615\n",
      "Epoch: [6686/10000] Training loss: 1.4136726348618799e-05 / Validation loss: 0.0001748058748439018 / Long term Validation loss: 0.1615\n",
      "Epoch: [6687/10000] Training loss: 1.4133508082800652e-05 / Validation loss: 0.0001747827068714552 / Long term Validation loss: 0.1615\n",
      "Epoch: [6688/10000] Training loss: 1.4130289671289509e-05 / Validation loss: 0.00017475954350589627 / Long term Validation loss: 0.1615\n",
      "Epoch: [6689/10000] Training loss: 1.412707111421592e-05 / Validation loss: 0.00017473637397684514 / Long term Validation loss: 0.1615\n",
      "Epoch: [6690/10000] Training loss: 1.4123852411710951e-05 / Validation loss: 0.00017471321138651556 / Long term Validation loss: 0.1615\n",
      "Epoch: [6691/10000] Training loss: 1.4120633563909608e-05 / Validation loss: 0.00017469004050179937 / Long term Validation loss: 0.1615\n",
      "Epoch: [6692/10000] Training loss: 1.4117414570946583e-05 / Validation loss: 0.00017466687847179116 / Long term Validation loss: 0.1615\n",
      "Epoch: [6693/10000] Training loss: 1.4114195432961112e-05 / Validation loss: 0.00017464370631205717 / Long term Validation loss: 0.1615\n",
      "Epoch: [6694/10000] Training loss: 1.4110976150091528e-05 / Validation loss: 0.0001746205447413398 / Long term Validation loss: 0.1615\n",
      "Epoch: [6695/10000] Training loss: 1.4107756722481112e-05 / Validation loss: 0.0001745973712632602 / Long term Validation loss: 0.1615\n",
      "Epoch: [6696/10000] Training loss: 1.4104537150271676e-05 / Validation loss: 0.00017457421017817154 / Long term Validation loss: 0.1615\n",
      "Epoch: [6697/10000] Training loss: 1.4101317433610541e-05 / Validation loss: 0.00017455103521365136 / Long term Validation loss: 0.1616\n",
      "Epoch: [6698/10000] Training loss: 1.4098097572642861e-05 / Validation loss: 0.00017452787478095024 / Long term Validation loss: 0.1616\n",
      "Epoch: [6699/10000] Training loss: 1.4094877567519913e-05 / Validation loss: 0.00017450469802207893 / Long term Validation loss: 0.1616\n",
      "Epoch: [6700/10000] Training loss: 1.4091657418390035e-05 / Validation loss: 0.00017448153856641237 / Long term Validation loss: 0.1616\n",
      "Epoch: [6701/10000] Training loss: 1.408843712540844e-05 / Validation loss: 0.0001744583595295929 / Long term Validation loss: 0.1616\n",
      "Epoch: [6702/10000] Training loss: 1.4085216688726468e-05 / Validation loss: 0.0001744352015704728 / Long term Validation loss: 0.1616\n",
      "Epoch: [6703/10000] Training loss: 1.4081996108503307e-05 / Validation loss: 0.0001744120195377363 / Long term Validation loss: 0.1616\n",
      "Epoch: [6704/10000] Training loss: 1.4078775384893004e-05 / Validation loss: 0.00017438886386366372 / Long term Validation loss: 0.1616\n",
      "Epoch: [6705/10000] Training loss: 1.407555451805888e-05 / Validation loss: 0.00017436567779016132 / Long term Validation loss: 0.1616\n",
      "Epoch: [6706/10000] Training loss: 1.4072333508157352e-05 / Validation loss: 0.00017434252558495394 / Long term Validation loss: 0.1616\n",
      "Epoch: [6707/10000] Training loss: 1.4069112355356092e-05 / Validation loss: 0.00017431933394650804 / Long term Validation loss: 0.1616\n",
      "Epoch: [6708/10000] Training loss: 1.4065891059813422e-05 / Validation loss: 0.00017429618698872648 / Long term Validation loss: 0.1616\n",
      "Epoch: [6709/10000] Training loss: 1.4062669621701782e-05 / Validation loss: 0.00017427298753022125 / Long term Validation loss: 0.1616\n",
      "Epoch: [6710/10000] Training loss: 1.4059448041180747e-05 / Validation loss: 0.00017424984850848037 / Long term Validation loss: 0.1616\n",
      "Epoch: [6711/10000] Training loss: 1.4056226318428198e-05 / Validation loss: 0.0001742266378372658 / Long term Validation loss: 0.1616\n",
      "Epoch: [6712/10000] Training loss: 1.4053004453604041e-05 / Validation loss: 0.00017420351085988375 / Long term Validation loss: 0.1616\n",
      "Epoch: [6713/10000] Training loss: 1.4049782446892709e-05 / Validation loss: 0.0001741802837922812 / Long term Validation loss: 0.1616\n",
      "Epoch: [6714/10000] Training loss: 1.4046560298453151e-05 / Validation loss: 0.00017415717522045927 / Long term Validation loss: 0.1616\n",
      "Epoch: [6715/10000] Training loss: 1.4043338008478089e-05 / Validation loss: 0.0001741339237186325 / Long term Validation loss: 0.1616\n",
      "Epoch: [6716/10000] Training loss: 1.4040115577123705e-05 / Validation loss: 0.00017411084353579663 / Long term Validation loss: 0.1616\n",
      "Epoch: [6717/10000] Training loss: 1.4036893004594019e-05 / Validation loss: 0.00017408755495119143 / Long term Validation loss: 0.1616\n",
      "Epoch: [6718/10000] Training loss: 1.4033670291039882e-05 / Validation loss: 0.00017406451903329923 / Long term Validation loss: 0.1617\n",
      "Epoch: [6719/10000] Training loss: 1.4030447436681877e-05 / Validation loss: 0.00017404117316895302 / Long term Validation loss: 0.1617\n",
      "Epoch: [6720/10000] Training loss: 1.402722444166227e-05 / Validation loss: 0.000174018207095478 / Long term Validation loss: 0.1617\n",
      "Epoch: [6721/10000] Training loss: 1.4024001306227905e-05 / Validation loss: 0.00017399477124086378 / Long term Validation loss: 0.1617\n",
      "Epoch: [6722/10000] Training loss: 1.4020778030509413e-05 / Validation loss: 0.00017397191677400679 / Long term Validation loss: 0.1617\n",
      "Epoch: [6723/10000] Training loss: 1.4017554614799134e-05 / Validation loss: 0.00017394833721835572 / Long term Validation loss: 0.1617\n",
      "Epoch: [6724/10000] Training loss: 1.4014331059216884e-05 / Validation loss: 0.00017392566344950317 / Long term Validation loss: 0.1617\n",
      "Epoch: [6725/10000] Training loss: 1.401110736414203e-05 / Validation loss: 0.0001739018508023075 / Long term Validation loss: 0.1617\n",
      "Epoch: [6726/10000] Training loss: 1.4007883529701585e-05 / Validation loss: 0.00017387947354841144 / Long term Validation loss: 0.1617\n",
      "Epoch: [6727/10000] Training loss: 1.4004659556459477e-05 / Validation loss: 0.0001738552770454251 / Long term Validation loss: 0.1617\n",
      "Epoch: [6728/10000] Training loss: 1.4001435444628862e-05 / Validation loss: 0.0001738333929930055 / Long term Validation loss: 0.1617\n",
      "Epoch: [6729/10000] Training loss: 1.3998211195207087e-05 / Validation loss: 0.0001738085549991358 / Long term Validation loss: 0.1617\n",
      "Epoch: [6730/10000] Training loss: 1.3994986808773915e-05 / Validation loss: 0.00017378750251052915 / Long term Validation loss: 0.1617\n",
      "Epoch: [6731/10000] Training loss: 1.399176228744176e-05 / Validation loss: 0.0001737615770287641 / Long term Validation loss: 0.1617\n",
      "Epoch: [6732/10000] Training loss: 1.3988537633090405e-05 / Validation loss: 0.0001737419456857487 / Long term Validation loss: 0.1617\n",
      "Epoch: [6733/10000] Training loss: 1.3985312850926613e-05 / Validation loss: 0.0001737141507223948 / Long term Validation loss: 0.1617\n",
      "Epoch: [6734/10000] Training loss: 1.3982087947180303e-05 / Validation loss: 0.00017369698093677133 / Long term Validation loss: 0.1617\n",
      "Epoch: [6735/10000] Training loss: 1.3978862936176397e-05 / Validation loss: 0.00017366592798769107 / Long term Validation loss: 0.1617\n",
      "Epoch: [6736/10000] Training loss: 1.3975637838482124e-05 / Validation loss: 0.00017365307888030825 / Long term Validation loss: 0.1617\n",
      "Epoch: [6737/10000] Training loss: 1.397241269659492e-05 / Validation loss: 0.00017361627166972662 / Long term Validation loss: 0.1617\n",
      "Epoch: [6738/10000] Training loss: 1.3969187578520899e-05 / Validation loss: 0.00017361110676550575 / Long term Validation loss: 0.1617\n",
      "Epoch: [6739/10000] Training loss: 1.3965962617208174e-05 / Validation loss: 0.0001735640020561028 / Long term Validation loss: 0.1617\n",
      "Epoch: [6740/10000] Training loss: 1.3962738039744605e-05 / Validation loss: 0.00017357268184358722 / Long term Validation loss: 0.1618\n",
      "Epoch: [6741/10000] Training loss: 1.3959514279070853e-05 / Validation loss: 0.00017350691040620705 / Long term Validation loss: 0.1617\n",
      "Epoch: [6742/10000] Training loss: 1.3956292105588256e-05 / Validation loss: 0.0001735408565191281 / Long term Validation loss: 0.1618\n",
      "Epoch: [6743/10000] Training loss: 1.3953072976283526e-05 / Validation loss: 0.00017344081710084677 / Long term Validation loss: 0.1617\n",
      "Epoch: [6744/10000] Training loss: 1.3949859556186217e-05 / Validation loss: 0.00017352146402071077 / Long term Validation loss: 0.1618\n",
      "Epoch: [6745/10000] Training loss: 1.3946656891907788e-05 / Validation loss: 0.00017335773491491185 / Long term Validation loss: 0.1617\n",
      "Epoch: [6746/10000] Training loss: 1.3943474402858165e-05 / Validation loss: 0.00017352580075345495 / Long term Validation loss: 0.1618\n",
      "Epoch: [6747/10000] Training loss: 1.3940330038369218e-05 / Validation loss: 0.00017324227234983183 / Long term Validation loss: 0.1617\n",
      "Epoch: [6748/10000] Training loss: 1.3937257848495489e-05 / Validation loss: 0.00017357607247675175 / Long term Validation loss: 0.1619\n",
      "Epoch: [6749/10000] Training loss: 1.3934323295775399e-05 / Validation loss: 0.00017306460513972087 / Long term Validation loss: 0.1617\n",
      "Epoch: [6750/10000] Training loss: 1.393165232010475e-05 / Validation loss: 0.00017371673332681876 / Long term Validation loss: 0.1619\n",
      "Epoch: [6751/10000] Training loss: 1.392948951243651e-05 / Validation loss: 0.00017276694791241016 / Long term Validation loss: 0.1616\n",
      "Epoch: [6752/10000] Training loss: 1.39283115116353e-05 / Validation loss: 0.0001740389825558459 / Long term Validation loss: 0.1620\n",
      "Epoch: [6753/10000] Training loss: 1.3929053935784707e-05 / Validation loss: 0.00017223867891745019 / Long term Validation loss: 0.1614\n",
      "Epoch: [6754/10000] Training loss: 1.3933562306691578e-05 / Validation loss: 0.00017473674262029335 / Long term Validation loss: 0.1619\n",
      "Epoch: [6755/10000] Training loss: 1.3945498243738948e-05 / Validation loss: 0.00017127629194825584 / Long term Validation loss: 0.1610\n",
      "Epoch: [6756/10000] Training loss: 1.3972168423265315e-05 / Validation loss: 0.00017624555590144787 / Long term Validation loss: 0.1617\n",
      "Epoch: [6757/10000] Training loss: 1.4028216782662966e-05 / Validation loss: 0.00016954681475792198 / Long term Validation loss: 0.1609\n",
      "Epoch: [6758/10000] Training loss: 1.4143174700350093e-05 / Validation loss: 0.00017962632165069458 / Long term Validation loss: 0.1615\n",
      "Epoch: [6759/10000] Training loss: 1.4376693531384896e-05 / Validation loss: 0.0001667082956348678 / Long term Validation loss: 0.1612\n",
      "Epoch: [6760/10000] Training loss: 1.4849910916426168e-05 / Validation loss: 0.0001877502285327853 / Long term Validation loss: 0.1601\n",
      "Epoch: [6761/10000] Training loss: 1.5807390335294482e-05 / Validation loss: 0.0001634866081702483 / Long term Validation loss: 0.1561\n",
      "Epoch: [6762/10000] Training loss: 1.7741994552429665e-05 / Validation loss: 0.00020917398617638353 / Long term Validation loss: 0.1512\n",
      "Epoch: [6763/10000] Training loss: 2.161555462310313e-05 / Validation loss: 0.00016692300412840714 / Long term Validation loss: 0.1425\n",
      "Epoch: [6764/10000] Training loss: 2.922973357605903e-05 / Validation loss: 0.00026864129653718697 / Long term Validation loss: 0.1497\n",
      "Epoch: [6765/10000] Training loss: 4.34779927112074e-05 / Validation loss: 0.00020394600808167995 / Long term Validation loss: 0.1490\n",
      "Epoch: [6766/10000] Training loss: 6.766302405506552e-05 / Validation loss: 0.00039877906830403147 / Long term Validation loss: 0.1571\n",
      "Epoch: [6767/10000] Training loss: 0.00010029398614767326 / Validation loss: 0.0002740542278373308 / Long term Validation loss: 0.1624\n",
      "Epoch: [6768/10000] Training loss: 0.00012517259018585806 / Validation loss: 0.00042422528468845567 / Long term Validation loss: 0.1582\n",
      "Epoch: [6769/10000] Training loss: 0.00011073477589627204 / Validation loss: 0.00019052868494070293 / Long term Validation loss: 0.1426\n",
      "Epoch: [6770/10000] Training loss: 5.551166892747441e-05 / Validation loss: 0.00018904089456377074 / Long term Validation loss: 0.1594\n",
      "Epoch: [6771/10000] Training loss: 1.5221692891348649e-05 / Validation loss: 0.00024208262377305037 / Long term Validation loss: 0.1491\n",
      "Epoch: [6772/10000] Training loss: 3.0509960273518866e-05 / Validation loss: 0.00020259027045269558 / Long term Validation loss: 0.1450\n",
      "Epoch: [6773/10000] Training loss: 6.510794121610295e-05 / Validation loss: 0.0003211306718790881 / Long term Validation loss: 0.1504\n",
      "Epoch: [6774/10000] Training loss: 6.11414943953326e-05 / Validation loss: 0.00016991821189443957 / Long term Validation loss: 0.1490\n",
      "Epoch: [6775/10000] Training loss: 2.5653481054142243e-05 / Validation loss: 0.00017177708409917537 / Long term Validation loss: 0.1614\n",
      "Epoch: [6776/10000] Training loss: 1.587359793927867e-05 / Validation loss: 0.0002676883272404771 / Long term Validation loss: 0.1481\n",
      "Epoch: [6777/10000] Training loss: 3.889618425372289e-05 / Validation loss: 0.0001849366071390903 / Long term Validation loss: 0.1399\n",
      "Epoch: [6778/10000] Training loss: 4.614803620767886e-05 / Validation loss: 0.00022689091348955596 / Long term Validation loss: 0.1497\n",
      "Epoch: [6779/10000] Training loss: 2.444371425389443e-05 / Validation loss: 0.00018769514635539258 / Long term Validation loss: 0.1606\n",
      "Epoch: [6780/10000] Training loss: 1.4556674619407211e-05 / Validation loss: 0.00017159874721915793 / Long term Validation loss: 0.1439\n",
      "Epoch: [6781/10000] Training loss: 2.9300220638983223e-05 / Validation loss: 0.0002537991648543743 / Long term Validation loss: 0.1480\n",
      "Epoch: [6782/10000] Training loss: 3.4613140469377194e-05 / Validation loss: 0.00016825234084397563 / Long term Validation loss: 0.1542\n",
      "Epoch: [6783/10000] Training loss: 2.015918092415067e-05 / Validation loss: 0.00017276601582629723 / Long term Validation loss: 0.1620\n",
      "Epoch: [6784/10000] Training loss: 1.4624602506857247e-05 / Validation loss: 0.000224323466411136 / Long term Validation loss: 0.1501\n",
      "Epoch: [6785/10000] Training loss: 2.493724105966457e-05 / Validation loss: 0.00016982219073801772 / Long term Validation loss: 0.1434\n",
      "Epoch: [6786/10000] Training loss: 2.6919986144960702e-05 / Validation loss: 0.00019563085704171174 / Long term Validation loss: 0.1576\n",
      "Epoch: [6787/10000] Training loss: 1.6845515287277435e-05 / Validation loss: 0.0001861444749354614 / Long term Validation loss: 0.1618\n",
      "Epoch: [6788/10000] Training loss: 1.4972529231226881e-05 / Validation loss: 0.00016646554740418332 / Long term Validation loss: 0.1468\n",
      "Epoch: [6789/10000] Training loss: 2.215158677708983e-05 / Validation loss: 0.00021034394031391898 / Long term Validation loss: 0.1515\n",
      "Epoch: [6790/10000] Training loss: 2.1736533399904774e-05 / Validation loss: 0.00016797881639116742 / Long term Validation loss: 0.1600\n",
      "Epoch: [6791/10000] Training loss: 1.497957255253178e-05 / Validation loss: 0.00016704674837842213 / Long term Validation loss: 0.1590\n",
      "Epoch: [6792/10000] Training loss: 1.529269488370515e-05 / Validation loss: 0.00020301412733714517 / Long term Validation loss: 0.1531\n",
      "Epoch: [6793/10000] Training loss: 1.994327349484146e-05 / Validation loss: 0.0001646054494389788 / Long term Validation loss: 0.1521\n",
      "Epoch: [6794/10000] Training loss: 1.83743488989834e-05 / Validation loss: 0.0001776495423592154 / Long term Validation loss: 0.1636\n",
      "Epoch: [6795/10000] Training loss: 1.415731187455495e-05 / Validation loss: 0.0001851291183519568 / Long term Validation loss: 0.1625\n",
      "Epoch: [6796/10000] Training loss: 1.5409323184678707e-05 / Validation loss: 0.0001642632777875504 / Long term Validation loss: 0.1522\n",
      "Epoch: [6797/10000] Training loss: 1.8144227968804832e-05 / Validation loss: 0.00018887779906775782 / Long term Validation loss: 0.1610\n",
      "Epoch: [6798/10000] Training loss: 1.62981830405727e-05 / Validation loss: 0.00017152332595097571 / Long term Validation loss: 0.1627\n",
      "Epoch: [6799/10000] Training loss: 1.3891667194470578e-05 / Validation loss: 0.00016574742320596745 / Long term Validation loss: 0.1584\n",
      "Epoch: [6800/10000] Training loss: 1.5312810373431538e-05 / Validation loss: 0.00019066214389906183 / Long term Validation loss: 0.1594\n",
      "Epoch: [6801/10000] Training loss: 1.6737277002265946e-05 / Validation loss: 0.00016604521033707404 / Long term Validation loss: 0.1588\n",
      "Epoch: [6802/10000] Training loss: 1.5093991109308777e-05 / Validation loss: 0.00017204740400755545 / Long term Validation loss: 0.1624\n",
      "Epoch: [6803/10000] Training loss: 1.385681057598011e-05 / Validation loss: 0.00018367173624766225 / Long term Validation loss: 0.1625\n",
      "Epoch: [6804/10000] Training loss: 1.5084668410127424e-05 / Validation loss: 0.00016534522425352573 / Long term Validation loss: 0.1575\n",
      "Epoch: [6805/10000] Training loss: 1.571467221172077e-05 / Validation loss: 0.0001802413760285614 / Long term Validation loss: 0.1627\n",
      "Epoch: [6806/10000] Training loss: 1.4438613709763318e-05 / Validation loss: 0.00017505832285111888 / Long term Validation loss: 0.1632\n",
      "Epoch: [6807/10000] Training loss: 1.388329723477591e-05 / Validation loss: 0.00016705217274403498 / Long term Validation loss: 0.1602\n",
      "Epoch: [6808/10000] Training loss: 1.4809722758582077e-05 / Validation loss: 0.00018408116449303757 / Long term Validation loss: 0.1623\n",
      "Epoch: [6809/10000] Training loss: 1.5010972357394506e-05 / Validation loss: 0.0001696141973051696 / Long term Validation loss: 0.1617\n",
      "Epoch: [6810/10000] Training loss: 1.410170202381583e-05 / Validation loss: 0.00017123139103495218 / Long term Validation loss: 0.1619\n",
      "Epoch: [6811/10000] Training loss: 1.3902177075559362e-05 / Validation loss: 0.00018162029127560619 / Long term Validation loss: 0.1620\n",
      "Epoch: [6812/10000] Training loss: 1.4546030271722306e-05 / Validation loss: 0.0001677710019085563 / Long term Validation loss: 0.1609\n",
      "Epoch: [6813/10000] Training loss: 1.4549365286686574e-05 / Validation loss: 0.00017662827375568692 / Long term Validation loss: 0.1624\n",
      "Epoch: [6814/10000] Training loss: 1.3934626921693311e-05 / Validation loss: 0.00017619016945375157 / Long term Validation loss: 0.1624\n",
      "Epoch: [6815/10000] Training loss: 1.38978118678958e-05 / Validation loss: 0.00016854576174370433 / Long term Validation loss: 0.1612\n",
      "Epoch: [6816/10000] Training loss: 1.4323725595891126e-05 / Validation loss: 0.00017984063498424128 / Long term Validation loss: 0.1618\n",
      "Epoch: [6817/10000] Training loss: 1.4255164163805957e-05 / Validation loss: 0.00017164347765790308 / Long term Validation loss: 0.1614\n",
      "Epoch: [6818/10000] Training loss: 1.3852099937615675e-05 / Validation loss: 0.0001712226592550901 / Long term Validation loss: 0.1613\n",
      "Epoch: [6819/10000] Training loss: 1.387565906276748e-05 / Validation loss: 0.0001789286128563778 / Long term Validation loss: 0.1615\n",
      "Epoch: [6820/10000] Training loss: 1.415014116957271e-05 / Validation loss: 0.00016941542201129393 / Long term Validation loss: 0.1612\n",
      "Epoch: [6821/10000] Training loss: 1.407082017470928e-05 / Validation loss: 0.00017464161839694285 / Long term Validation loss: 0.1619\n",
      "Epoch: [6822/10000] Training loss: 1.3809417369982858e-05 / Validation loss: 0.00017538223427376103 / Long term Validation loss: 0.1619\n",
      "Epoch: [6823/10000] Training loss: 1.384574351434864e-05 / Validation loss: 0.00016942326499500997 / Long term Validation loss: 0.1611\n",
      "Epoch: [6824/10000] Training loss: 1.4021448452948343e-05 / Validation loss: 0.00017680439430621148 / Long term Validation loss: 0.1615\n",
      "Epoch: [6825/10000] Training loss: 1.39547781759221e-05 / Validation loss: 0.00017195095238166057 / Long term Validation loss: 0.1609\n",
      "Epoch: [6826/10000] Training loss: 1.3784600685019284e-05 / Validation loss: 0.00017114722239411942 / Long term Validation loss: 0.1608\n",
      "Epoch: [6827/10000] Training loss: 1.3815053631343525e-05 / Validation loss: 0.0001763971385950407 / Long term Validation loss: 0.1615\n",
      "Epoch: [6828/10000] Training loss: 1.3928066242933843e-05 / Validation loss: 0.00017010711416228446 / Long term Validation loss: 0.1608\n",
      "Epoch: [6829/10000] Training loss: 1.3879834868373306e-05 / Validation loss: 0.0001734718459055922 / Long term Validation loss: 0.1616\n",
      "Epoch: [6830/10000] Training loss: 1.37674668296078e-05 / Validation loss: 0.00017410104680826827 / Long term Validation loss: 0.1617\n",
      "Epoch: [6831/10000] Training loss: 1.3787095437059472e-05 / Validation loss: 0.00016999719981306637 / Long term Validation loss: 0.1607\n",
      "Epoch: [6832/10000] Training loss: 1.3860567851272558e-05 / Validation loss: 0.00017489324199871842 / Long term Validation loss: 0.1616\n",
      "Epoch: [6833/10000] Training loss: 1.3829069261708232e-05 / Validation loss: 0.00017170620684201098 / Long term Validation loss: 0.1609\n",
      "Epoch: [6834/10000] Training loss: 1.3753597358641404e-05 / Validation loss: 0.00017122813973466627 / Long term Validation loss: 0.1607\n",
      "Epoch: [6835/10000] Training loss: 1.3763130427213914e-05 / Validation loss: 0.0001745954516278093 / Long term Validation loss: 0.1616\n",
      "Epoch: [6836/10000] Training loss: 1.3811290261538126e-05 / Validation loss: 0.0001704395374601187 / Long term Validation loss: 0.1607\n",
      "Epoch: [6837/10000] Training loss: 1.3792726335203288e-05 / Validation loss: 0.0001728841007985116 / Long term Validation loss: 0.1615\n",
      "Epoch: [6838/10000] Training loss: 1.3741293643190668e-05 / Validation loss: 0.00017306634280647563 / Long term Validation loss: 0.1616\n",
      "Epoch: [6839/10000] Training loss: 1.3743152580640673e-05 / Validation loss: 0.00017049730132677124 / Long term Validation loss: 0.1606\n",
      "Epoch: [6840/10000] Training loss: 1.3774644901672275e-05 / Validation loss: 0.0001737933236794099 / Long term Validation loss: 0.1617\n",
      "Epoch: [6841/10000] Training loss: 1.3765110660705427e-05 / Validation loss: 0.00017147476286496225 / Long term Validation loss: 0.1610\n",
      "Epoch: [6842/10000] Training loss: 1.3729799278043321e-05 / Validation loss: 0.00017146897888081376 / Long term Validation loss: 0.1610\n",
      "Epoch: [6843/10000] Training loss: 1.3726511808576066e-05 / Validation loss: 0.0001734431512187952 / Long term Validation loss: 0.1616\n",
      "Epoch: [6844/10000] Training loss: 1.3746597609797265e-05 / Validation loss: 0.0001706925774697261 / Long term Validation loss: 0.1607\n",
      "Epoch: [6845/10000] Training loss: 1.3742828356402938e-05 / Validation loss: 0.00017259304214162888 / Long term Validation loss: 0.1615\n",
      "Epoch: [6846/10000] Training loss: 1.3718639999347221e-05 / Validation loss: 0.00017232189281954084 / Long term Validation loss: 0.1615\n",
      "Epoch: [6847/10000] Training loss: 1.371234108683394e-05 / Validation loss: 0.00017089299188520688 / Long term Validation loss: 0.1608\n",
      "Epoch: [6848/10000] Training loss: 1.3724366635490603e-05 / Validation loss: 0.0001730666200711764 / Long term Validation loss: 0.1616\n",
      "Epoch: [6849/10000] Training loss: 1.3723858031324621e-05 / Validation loss: 0.00017128660354736778 / Long term Validation loss: 0.1610\n",
      "Epoch: [6850/10000] Training loss: 1.3707514435321403e-05 / Validation loss: 0.00017167295703683852 / Long term Validation loss: 0.1613\n",
      "Epoch: [6851/10000] Training loss: 1.369985146158137e-05 / Validation loss: 0.00017264250302950383 / Long term Validation loss: 0.1616\n",
      "Epoch: [6852/10000] Training loss: 1.3706095361457836e-05 / Validation loss: 0.00017088650183722952 / Long term Validation loss: 0.1609\n",
      "Epoch: [6853/10000] Training loss: 1.3707054152353328e-05 / Validation loss: 0.00017237296608182035 / Long term Validation loss: 0.1615\n",
      "Epoch: [6854/10000] Training loss: 1.3696278473590738e-05 / Validation loss: 0.00017178682692911405 / Long term Validation loss: 0.1613\n",
      "Epoch: [6855/10000] Training loss: 1.368843799065371e-05 / Validation loss: 0.00017117165123169736 / Long term Validation loss: 0.1611\n",
      "Epoch: [6856/10000] Training loss: 1.3690571978627528e-05 / Validation loss: 0.00017249779407396834 / Long term Validation loss: 0.1616\n",
      "Epoch: [6857/10000] Training loss: 1.3691778388837152e-05 / Validation loss: 0.00017114891764703278 / Long term Validation loss: 0.1611\n",
      "Epoch: [6858/10000] Training loss: 1.3684882780403985e-05 / Validation loss: 0.00017176731512126098 / Long term Validation loss: 0.1614\n",
      "Epoch: [6859/10000] Training loss: 1.3677638513797446e-05 / Validation loss: 0.00017204572894870677 / Long term Validation loss: 0.1615\n",
      "Epoch: [6860/10000] Training loss: 1.3676961837101333e-05 / Validation loss: 0.00017104398591014772 / Long term Validation loss: 0.1611\n",
      "Epoch: [6861/10000] Training loss: 1.367765763809273e-05 / Validation loss: 0.00017214645865887775 / Long term Validation loss: 0.1615\n",
      "Epoch: [6862/10000] Training loss: 1.3673326036839632e-05 / Validation loss: 0.00017143474263514924 / Long term Validation loss: 0.1613\n",
      "Epoch: [6863/10000] Training loss: 1.366709858435182e-05 / Validation loss: 0.00017137064896919108 / Long term Validation loss: 0.1613\n",
      "Epoch: [6864/10000] Training loss: 1.3664654321076294e-05 / Validation loss: 0.000172041902823967 / Long term Validation loss: 0.1615\n",
      "Epoch: [6865/10000] Training loss: 1.366445565959684e-05 / Validation loss: 0.00017110260853708597 / Long term Validation loss: 0.1612\n",
      "Epoch: [6866/10000] Training loss: 1.366164833871255e-05 / Validation loss: 0.00017177053321162223 / Long term Validation loss: 0.1615\n",
      "Epoch: [6867/10000] Training loss: 1.3656569750485019e-05 / Validation loss: 0.0001716155369614451 / Long term Validation loss: 0.1614\n",
      "Epoch: [6868/10000] Training loss: 1.3653189785638657e-05 / Validation loss: 0.00017118275865269812 / Long term Validation loss: 0.1612\n",
      "Epoch: [6869/10000] Training loss: 1.3652010437227674e-05 / Validation loss: 0.0001718923316163062 / Long term Validation loss: 0.1615\n",
      "Epoch: [6870/10000] Training loss: 1.3649933172055148e-05 / Validation loss: 0.000171228050183623 / Long term Validation loss: 0.1613\n",
      "Epoch: [6871/10000] Training loss: 1.3645912218957065e-05 / Validation loss: 0.00017147834994840478 / Long term Validation loss: 0.1614\n",
      "Epoch: [6872/10000] Training loss: 1.3642219470029172e-05 / Validation loss: 0.0001716638871868205 / Long term Validation loss: 0.1615\n",
      "Epoch: [6873/10000] Training loss: 1.3640187714022962e-05 / Validation loss: 0.0001711195699264482 / Long term Validation loss: 0.1613\n",
      "Epoch: [6874/10000] Training loss: 1.3638282519729657e-05 / Validation loss: 0.0001716740601689687 / Long term Validation loss: 0.1615\n",
      "Epoch: [6875/10000] Training loss: 1.3635080059559162e-05 / Validation loss: 0.00017131261053929882 / Long term Validation loss: 0.1614\n",
      "Epoch: [6876/10000] Training loss: 1.3631482110112142e-05 / Validation loss: 0.00017126957469440794 / Long term Validation loss: 0.1614\n",
      "Epoch: [6877/10000] Training loss: 1.362884892040087e-05 / Validation loss: 0.00017159925062964086 / Long term Validation loss: 0.1615\n",
      "Epoch: [6878/10000] Training loss: 1.362678498359585e-05 / Validation loss: 0.00017110916643829513 / Long term Validation loss: 0.1613\n",
      "Epoch: [6879/10000] Training loss: 1.3624095672173926e-05 / Validation loss: 0.0001714580925368919 / Long term Validation loss: 0.1615\n",
      "Epoch: [6880/10000] Training loss: 1.3620794240856745e-05 / Validation loss: 0.0001713362711811637 / Long term Validation loss: 0.1615\n",
      "Epoch: [6881/10000] Training loss: 1.3617844557137656e-05 / Validation loss: 0.0001711349728441741 / Long term Validation loss: 0.1614\n",
      "Epoch: [6882/10000] Training loss: 1.3615491354832148e-05 / Validation loss: 0.0001714723272067446 / Long term Validation loss: 0.1615\n",
      "Epoch: [6883/10000] Training loss: 1.3613024508661434e-05 / Validation loss: 0.00017110155240555007 / Long term Validation loss: 0.1614\n",
      "Epoch: [6884/10000] Training loss: 1.3610049941550246e-05 / Validation loss: 0.0001712661921883367 / Long term Validation loss: 0.1615\n",
      "Epoch: [6885/10000] Training loss: 1.360702758028338e-05 / Validation loss: 0.00017129345639295927 / Long term Validation loss: 0.1615\n",
      "Epoch: [6886/10000] Training loss: 1.3604406104098987e-05 / Validation loss: 0.00017103865359346872 / Long term Validation loss: 0.1614\n",
      "Epoch: [6887/10000] Training loss: 1.3601946554428645e-05 / Validation loss: 0.00017131837850351876 / Long term Validation loss: 0.1616\n",
      "Epoch: [6888/10000] Training loss: 1.3599216214756988e-05 / Validation loss: 0.00017107563086922359 / Long term Validation loss: 0.1615\n",
      "Epoch: [6889/10000] Training loss: 1.3596275275482901e-05 / Validation loss: 0.00017111140502975894 / Long term Validation loss: 0.1615\n",
      "Epoch: [6890/10000] Training loss: 1.3593490328088167e-05 / Validation loss: 0.00017121311156918732 / Long term Validation loss: 0.1616\n",
      "Epoch: [6891/10000] Training loss: 1.3590928084747895e-05 / Validation loss: 0.0001709682646495551 / Long term Validation loss: 0.1615\n",
      "Epoch: [6892/10000] Training loss: 1.3588314801774608e-05 / Validation loss: 0.00017116997802343482 / Long term Validation loss: 0.1616\n",
      "Epoch: [6893/10000] Training loss: 1.3585507330222493e-05 / Validation loss: 0.00017103000518033483 / Long term Validation loss: 0.1615\n",
      "Epoch: [6894/10000] Training loss: 1.3582678008469304e-05 / Validation loss: 0.00017098792070406173 / Long term Validation loss: 0.1615\n",
      "Epoch: [6895/10000] Training loss: 1.3580000033620801e-05 / Validation loss: 0.0001711111027949952 / Long term Validation loss: 0.1616\n",
      "Epoch: [6896/10000] Training loss: 1.3577393970415674e-05 / Validation loss: 0.00017090427231686908 / Long term Validation loss: 0.1615\n",
      "Epoch: [6897/10000] Training loss: 1.357469265904789e-05 / Validation loss: 0.0001710344466921319 / Long term Validation loss: 0.1616\n",
      "Epoch: [6898/10000] Training loss: 1.3571900699664293e-05 / Validation loss: 0.00017096653850145313 / Long term Validation loss: 0.1616\n",
      "Epoch: [6899/10000] Training loss: 1.3569153925451376e-05 / Validation loss: 0.00017088783323135158 / Long term Validation loss: 0.1615\n",
      "Epoch: [6900/10000] Training loss: 1.356649822764845e-05 / Validation loss: 0.00017100442591317264 / Long term Validation loss: 0.1616\n",
      "Epoch: [6901/10000] Training loss: 1.3563841289261956e-05 / Validation loss: 0.00017084217578734718 / Long term Validation loss: 0.1615\n",
      "Epoch: [6902/10000] Training loss: 1.3561111304779003e-05 / Validation loss: 0.0001709162192098745 / Long term Validation loss: 0.1616\n",
      "Epoch: [6903/10000] Training loss: 1.3558355022544523e-05 / Validation loss: 0.0001708909433119523 / Long term Validation loss: 0.1616\n",
      "Epoch: [6904/10000] Training loss: 1.3555648144499026e-05 / Validation loss: 0.00017079980232764588 / Long term Validation loss: 0.1616\n",
      "Epoch: [6905/10000] Training loss: 1.3552982865618639e-05 / Validation loss: 0.00017089589891535132 / Long term Validation loss: 0.1616\n",
      "Epoch: [6906/10000] Training loss: 1.3550295154766214e-05 / Validation loss: 0.0001707725120356192 / Long term Validation loss: 0.1616\n",
      "Epoch: [6907/10000] Training loss: 1.354756525095146e-05 / Validation loss: 0.00017080712855135143 / Long term Validation loss: 0.1616\n",
      "Epoch: [6908/10000] Training loss: 1.3544836508782782e-05 / Validation loss: 0.00017080385015341397 / Long term Validation loss: 0.1616\n",
      "Epoch: [6909/10000] Training loss: 1.3542142753381065e-05 / Validation loss: 0.00017071474397047285 / Long term Validation loss: 0.1616\n",
      "Epoch: [6910/10000] Training loss: 1.3539463401970739e-05 / Validation loss: 0.0001707879655642298 / Long term Validation loss: 0.1616\n",
      "Epoch: [6911/10000] Training loss: 1.3536762438025086e-05 / Validation loss: 0.00017069493479636685 / Long term Validation loss: 0.1616\n",
      "Epoch: [6912/10000] Training loss: 1.3534040706139785e-05 / Validation loss: 0.00017070418810689986 / Long term Validation loss: 0.1616\n",
      "Epoch: [6913/10000] Training loss: 1.3531327529552543e-05 / Validation loss: 0.00017070905581701289 / Long term Validation loss: 0.1616\n",
      "Epoch: [6914/10000] Training loss: 1.352863520558146e-05 / Validation loss: 0.00017062767892834986 / Long term Validation loss: 0.1616\n",
      "Epoch: [6915/10000] Training loss: 1.3525945704037155e-05 / Validation loss: 0.00017067949057964472 / Long term Validation loss: 0.1617\n",
      "Epoch: [6916/10000] Training loss: 1.3523240903283923e-05 / Validation loss: 0.00017060778271926218 / Long term Validation loss: 0.1616\n",
      "Epoch: [6917/10000] Training loss: 1.3520526847207065e-05 / Validation loss: 0.00017060208999214585 / Long term Validation loss: 0.1617\n",
      "Epoch: [6918/10000] Training loss: 1.3517820936485272e-05 / Validation loss: 0.00017060782699761376 / Long term Validation loss: 0.1617\n",
      "Epoch: [6919/10000] Training loss: 1.3515126513827889e-05 / Validation loss: 0.00017053667472093203 / Long term Validation loss: 0.1617\n",
      "Epoch: [6920/10000] Training loss: 1.3512431195482142e-05 / Validation loss: 0.0001705715857079877 / Long term Validation loss: 0.1617\n",
      "Epoch: [6921/10000] Training loss: 1.3509726282828479e-05 / Validation loss: 0.00017051471645385688 / Long term Validation loss: 0.1617\n",
      "Epoch: [6922/10000] Training loss: 1.3507017399402234e-05 / Validation loss: 0.00017050142089603798 / Long term Validation loss: 0.1617\n",
      "Epoch: [6923/10000] Training loss: 1.350431434071736e-05 / Validation loss: 0.00017050438135920713 / Long term Validation loss: 0.1617\n",
      "Epoch: [6924/10000] Training loss: 1.3501617485193277e-05 / Validation loss: 0.000170443123360323 / Long term Validation loss: 0.1617\n",
      "Epoch: [6925/10000] Training loss: 1.3498919150940174e-05 / Validation loss: 0.00017046498986651504 / Long term Validation loss: 0.1617\n",
      "Epoch: [6926/10000] Training loss: 1.3496215081870966e-05 / Validation loss: 0.00017041778153202542 / Long term Validation loss: 0.1617\n",
      "Epoch: [6927/10000] Training loss: 1.3493509192527267e-05 / Validation loss: 0.0001704012941783522 / Long term Validation loss: 0.1617\n",
      "Epoch: [6928/10000] Training loss: 1.3490806972926305e-05 / Validation loss: 0.00017040021667473064 / Long term Validation loss: 0.1617\n",
      "Epoch: [6929/10000] Training loss: 1.348810817941744e-05 / Validation loss: 0.00017034803533851742 / Long term Validation loss: 0.1617\n",
      "Epoch: [6930/10000] Training loss: 1.3485408266633525e-05 / Validation loss: 0.0001703605717181971 / Long term Validation loss: 0.1617\n",
      "Epoch: [6931/10000] Training loss: 1.3482704959222928e-05 / Validation loss: 0.00017031988005913602 / Long term Validation loss: 0.1617\n",
      "Epoch: [6932/10000] Training loss: 1.3480000624919461e-05 / Validation loss: 0.00017030283137198446 / Long term Validation loss: 0.1617\n",
      "Epoch: [6933/10000] Training loss: 1.3477298416771374e-05 / Validation loss: 0.00017029740278269966 / Long term Validation loss: 0.1617\n",
      "Epoch: [6934/10000] Training loss: 1.3474598188556706e-05 / Validation loss: 0.00017025276510232928 / Long term Validation loss: 0.1617\n",
      "Epoch: [6935/10000] Training loss: 1.3471897321916134e-05 / Validation loss: 0.00017025821707587418 / Long term Validation loss: 0.1617\n",
      "Epoch: [6936/10000] Training loss: 1.3469194434047838e-05 / Validation loss: 0.00017022158874066375 / Long term Validation loss: 0.1617\n",
      "Epoch: [6937/10000] Training loss: 1.3466490802076182e-05 / Validation loss: 0.00017020501416166583 / Long term Validation loss: 0.1618\n",
      "Epoch: [6938/10000] Training loss: 1.3463788292428683e-05 / Validation loss: 0.00017019537229271008 / Long term Validation loss: 0.1618\n",
      "Epoch: [6939/10000] Training loss: 1.3461086970859634e-05 / Validation loss: 0.00017015698067059196 / Long term Validation loss: 0.1618\n",
      "Epoch: [6940/10000] Training loss: 1.3458385376365379e-05 / Validation loss: 0.00017015708226648187 / Long term Validation loss: 0.1618\n",
      "Epoch: [6941/10000] Training loss: 1.3455682575697142e-05 / Validation loss: 0.00017012322015650072 / Long term Validation loss: 0.1618\n",
      "Epoch: [6942/10000] Training loss: 1.3452979154905318e-05 / Validation loss: 0.00017010754346832312 / Long term Validation loss: 0.1618\n",
      "Epoch: [6943/10000] Training loss: 1.3450276230625166e-05 / Validation loss: 0.00017009404023808706 / Long term Validation loss: 0.1618\n",
      "Epoch: [6944/10000] Training loss: 1.3447574015335478e-05 / Validation loss: 0.0001700606414322566 / Long term Validation loss: 0.1618\n",
      "Epoch: [6945/10000] Training loss: 1.3444871745577817e-05 / Validation loss: 0.00017005627494501654 / Long term Validation loss: 0.1618\n",
      "Epoch: [6946/10000] Training loss: 1.3442168767417075e-05 / Validation loss: 0.00017002426008400483 / Long term Validation loss: 0.1618\n",
      "Epoch: [6947/10000] Training loss: 1.3439465261873605e-05 / Validation loss: 0.00017000931810507195 / Long term Validation loss: 0.1618\n",
      "Epoch: [6948/10000] Training loss: 1.3436761875918761e-05 / Validation loss: 0.00016999241570060913 / Long term Validation loss: 0.1618\n",
      "Epoch: [6949/10000] Training loss: 1.3434058876066705e-05 / Validation loss: 0.00016996313549256142 / Long term Validation loss: 0.1618\n",
      "Epoch: [6950/10000] Training loss: 1.3431355919074121e-05 / Validation loss: 0.00016995503488248805 / Long term Validation loss: 0.1618\n",
      "Epoch: [6951/10000] Training loss: 1.3428652562549806e-05 / Validation loss: 0.0001699246403979689 / Long term Validation loss: 0.1618\n",
      "Epoch: [6952/10000] Training loss: 1.3425948775834962e-05 / Validation loss: 0.00016991023325152286 / Long term Validation loss: 0.1618\n",
      "Epoch: [6953/10000] Training loss: 1.3423244897135369e-05 / Validation loss: 0.0001698905608517142 / Long term Validation loss: 0.1618\n",
      "Epoch: [6954/10000] Training loss: 1.3420541172999605e-05 / Validation loss: 0.00016986469034144824 / Long term Validation loss: 0.1618\n",
      "Epoch: [6955/10000] Training loss: 1.3417837503264087e-05 / Validation loss: 0.00016985327069546683 / Long term Validation loss: 0.1618\n",
      "Epoch: [6956/10000] Training loss: 1.3415133618997092e-05 / Validation loss: 0.0001698244265395136 / Long term Validation loss: 0.1618\n",
      "Epoch: [6957/10000] Training loss: 1.3412429405596641e-05 / Validation loss: 0.00016981014178892 / Long term Validation loss: 0.1618\n",
      "Epoch: [6958/10000] Training loss: 1.3409725000579159e-05 / Validation loss: 0.00016978836828066777 / Long term Validation loss: 0.1618\n",
      "Epoch: [6959/10000] Training loss: 1.3407020586234163e-05 / Validation loss: 0.00016976535650248614 / Long term Validation loss: 0.1618\n",
      "Epoch: [6960/10000] Training loss: 1.3404316185259077e-05 / Validation loss: 0.00016975104871912947 / Long term Validation loss: 0.1619\n",
      "Epoch: [6961/10000] Training loss: 1.340161166546883e-05 / Validation loss: 0.00016972395267966396 / Long term Validation loss: 0.1619\n",
      "Epoch: [6962/10000] Training loss: 1.3398906907984755e-05 / Validation loss: 0.00016970941932142856 / Long term Validation loss: 0.1619\n",
      "Epoch: [6963/10000] Training loss: 1.3396201932768648e-05 / Validation loss: 0.00016968629416468865 / Long term Validation loss: 0.1619\n",
      "Epoch: [6964/10000] Training loss: 1.339349684355688e-05 / Validation loss: 0.00016966557429973775 / Long term Validation loss: 0.1619\n",
      "Epoch: [6965/10000] Training loss: 1.3390791704138973e-05 / Validation loss: 0.00016964871400603027 / Long term Validation loss: 0.1619\n",
      "Epoch: [6966/10000] Training loss: 1.3388086476460747e-05 / Validation loss: 0.00016962346589407138 / Long term Validation loss: 0.1619\n",
      "Epoch: [6967/10000] Training loss: 1.3385381078372954e-05 / Validation loss: 0.00016960820773534484 / Long term Validation loss: 0.1619\n",
      "Epoch: [6968/10000] Training loss: 1.3382675477808375e-05 / Validation loss: 0.0001695844294635393 / Long term Validation loss: 0.1619\n",
      "Epoch: [6969/10000] Training loss: 1.3379969710734385e-05 / Validation loss: 0.000169565380492658 / Long term Validation loss: 0.1619\n",
      "Epoch: [6970/10000] Training loss: 1.337726383270708e-05 / Validation loss: 0.00016954639102075112 / Long term Validation loss: 0.1619\n",
      "Epoch: [6971/10000] Training loss: 1.3374557855160374e-05 / Validation loss: 0.00016952304971129144 / Long term Validation loss: 0.1619\n",
      "Epoch: [6972/10000] Training loss: 1.3371851742099901e-05 / Validation loss: 0.00016950669468681893 / Long term Validation loss: 0.1619\n",
      "Epoch: [6973/10000] Training loss: 1.3369145455161923e-05 / Validation loss: 0.00016948294182484244 / Long term Validation loss: 0.1619\n",
      "Epoch: [6974/10000] Training loss: 1.336643898872151e-05 / Validation loss: 0.00016946487084363432 / Long term Validation loss: 0.1619\n",
      "Epoch: [6975/10000] Training loss: 1.3363732370215468e-05 / Validation loss: 0.00016944422480050044 / Long term Validation loss: 0.1619\n",
      "Epoch: [6976/10000] Training loss: 1.3361025623153838e-05 / Validation loss: 0.0001694226208541913 / Long term Validation loss: 0.1619\n",
      "Epoch: [6977/10000] Training loss: 1.3358318745296934e-05 / Validation loss: 0.0001694048774140679 / Long term Validation loss: 0.1619\n",
      "Epoch: [6978/10000] Training loss: 1.3355611714717266e-05 / Validation loss: 0.00016938169069317428 / Long term Validation loss: 0.1619\n",
      "Epoch: [6979/10000] Training loss: 1.3352904512741438e-05 / Validation loss: 0.0001693638902998979 / Long term Validation loss: 0.1619\n",
      "Epoch: [6980/10000] Training loss: 1.3350197141782355e-05 / Validation loss: 0.00016934218617820185 / Long term Validation loss: 0.1619\n",
      "Epoch: [6981/10000] Training loss: 1.3347489615812914e-05 / Validation loss: 0.0001693219964090191 / Long term Validation loss: 0.1619\n",
      "Epoch: [6982/10000] Training loss: 1.334478194557611e-05 / Validation loss: 0.00016930282811770562 / Long term Validation loss: 0.1619\n",
      "Epoch: [6983/10000] Training loss: 1.3342074127354068e-05 / Validation loss: 0.00016928059721426457 / Long term Validation loss: 0.1619\n",
      "Epoch: [6984/10000] Training loss: 1.3339366148420747e-05 / Validation loss: 0.00016926244523782645 / Long term Validation loss: 0.1619\n",
      "Epoch: [6985/10000] Training loss: 1.33366580005036e-05 / Validation loss: 0.00016924032093554187 / Long term Validation loss: 0.1619\n",
      "Epoch: [6986/10000] Training loss: 1.3333949684218704e-05 / Validation loss: 0.00016922102598183948 / Long term Validation loss: 0.1620\n",
      "Epoch: [6987/10000] Training loss: 1.3331241207367138e-05 / Validation loss: 0.00016920062320423268 / Long term Validation loss: 0.1620\n",
      "Epoch: [6988/10000] Training loss: 1.332853257442408e-05 / Validation loss: 0.00016917947317765544 / Long term Validation loss: 0.1620\n",
      "Epoch: [6989/10000] Training loss: 1.3325823783107577e-05 / Validation loss: 0.0001691605355332574 / Long term Validation loss: 0.1620\n",
      "Epoch: [6990/10000] Training loss: 1.3323114827013935e-05 / Validation loss: 0.00016913859575026105 / Long term Validation loss: 0.1620\n",
      "Epoch: [6991/10000] Training loss: 1.3320405700752825e-05 / Validation loss: 0.00016911961504513658 / Long term Validation loss: 0.1620\n",
      "Epoch: [6992/10000] Training loss: 1.3317696404747596e-05 / Validation loss: 0.00016909841912781374 / Long term Validation loss: 0.1620\n",
      "Epoch: [6993/10000] Training loss: 1.3314986941876245e-05 / Validation loss: 0.00016907821380125686 / Long term Validation loss: 0.1620\n",
      "Epoch: [6994/10000] Training loss: 1.3312277314780803e-05 / Validation loss: 0.00016905834021181207 / Long term Validation loss: 0.1620\n",
      "Epoch: [6995/10000] Training loss: 1.330956752245079e-05 / Validation loss: 0.00016903700204845082 / Long term Validation loss: 0.1620\n",
      "Epoch: [6996/10000] Training loss: 1.3306857561277414e-05 / Validation loss: 0.00016901778523287626 / Long term Validation loss: 0.1620\n",
      "Epoch: [6997/10000] Training loss: 1.3304147428392899e-05 / Validation loss: 0.000168996334168179 / Long term Validation loss: 0.1620\n",
      "Epoch: [6998/10000] Training loss: 1.3301437122616294e-05 / Validation loss: 0.00016897668678368327 / Long term Validation loss: 0.1620\n",
      "Epoch: [6999/10000] Training loss: 1.329872664535562e-05 / Validation loss: 0.00016895602673176172 / Long term Validation loss: 0.1620\n",
      "Epoch: [7000/10000] Training loss: 1.3296015997615277e-05 / Validation loss: 0.0001689354258550715 / Long term Validation loss: 0.1620\n",
      "Epoch: [7001/10000] Training loss: 1.3293305179243822e-05 / Validation loss: 0.00016891562983812326 / Long term Validation loss: 0.1620\n",
      "Epoch: [7002/10000] Training loss: 1.329059418860552e-05 / Validation loss: 0.00016889442107783528 / Long term Validation loss: 0.1620\n",
      "Epoch: [7003/10000] Training loss: 1.3287883023519773e-05 / Validation loss: 0.00016887484975300584 / Long term Validation loss: 0.1620\n",
      "Epoch: [7004/10000] Training loss: 1.3285171683071826e-05 / Validation loss: 0.00016885378703152375 / Long term Validation loss: 0.1620\n",
      "Epoch: [7005/10000] Training loss: 1.3282460167004515e-05 / Validation loss: 0.00016883374746493618 / Long term Validation loss: 0.1620\n",
      "Epoch: [7006/10000] Training loss: 1.327974847593634e-05 / Validation loss: 0.00016881331543336763 / Long term Validation loss: 0.1620\n",
      "Epoch: [7007/10000] Training loss: 1.3277036609791909e-05 / Validation loss: 0.00016879261202271988 / Long term Validation loss: 0.1620\n",
      "Epoch: [7008/10000] Training loss: 1.3274324567834017e-05 / Validation loss: 0.00016877271058553066 / Long term Validation loss: 0.1620\n",
      "Epoch: [7009/10000] Training loss: 1.3271612348950921e-05 / Validation loss: 0.000168751687609036 / Long term Validation loss: 0.1620\n",
      "Epoch: [7010/10000] Training loss: 1.3268899951943575e-05 / Validation loss: 0.00016873183296492258 / Long term Validation loss: 0.1620\n",
      "Epoch: [7011/10000] Training loss: 1.3266187376446378e-05 / Validation loss: 0.00016871099952968805 / Long term Validation loss: 0.1620\n",
      "Epoch: [7012/10000] Training loss: 1.3263474622199837e-05 / Validation loss: 0.0001686907661315205 / Long term Validation loss: 0.1620\n",
      "Epoch: [7013/10000] Training loss: 1.3260761689279802e-05 / Validation loss: 0.0001686703854173641 / Long term Validation loss: 0.1620\n",
      "Epoch: [7014/10000] Training loss: 1.3258048577326619e-05 / Validation loss: 0.0001686497040809101 / Long term Validation loss: 0.1620\n",
      "Epoch: [7015/10000] Training loss: 1.3255335285691507e-05 / Validation loss: 0.0001686296567366939 / Long term Validation loss: 0.1621\n",
      "Epoch: [7016/10000] Training loss: 1.3252621813681838e-05 / Validation loss: 0.00016860878738822298 / Long term Validation loss: 0.1621\n",
      "Epoch: [7017/10000] Training loss: 1.3249908160545817e-05 / Validation loss: 0.00016858873966079806 / Long term Validation loss: 0.1621\n",
      "Epoch: [7018/10000] Training loss: 1.3247194326007118e-05 / Validation loss: 0.00016856801738073324 / Long term Validation loss: 0.1621\n",
      "Epoch: [7019/10000] Training loss: 1.3244480309733757e-05 / Validation loss: 0.00016854769984686908 / Long term Validation loss: 0.1621\n",
      "Epoch: [7020/10000] Training loss: 1.3241766111592237e-05 / Validation loss: 0.0001685272874990949 / Long term Validation loss: 0.1621\n",
      "Epoch: [7021/10000] Training loss: 1.3239051731216031e-05 / Validation loss: 0.00016850666429764195 / Long term Validation loss: 0.1621\n",
      "Epoch: [7022/10000] Training loss: 1.3236337168127312e-05 / Validation loss: 0.00016848648013007076 / Long term Validation loss: 0.1621\n",
      "Epoch: [7023/10000] Training loss: 1.3233622421858849e-05 / Validation loss: 0.0001684657216181453 / Long term Validation loss: 0.1621\n",
      "Epoch: [7024/10000] Training loss: 1.323090749187248e-05 / Validation loss: 0.00016844554723699208 / Long term Validation loss: 0.1621\n",
      "Epoch: [7025/10000] Training loss: 1.3228192377900156e-05 / Validation loss: 0.00016842487356706257 / Long term Validation loss: 0.1621\n",
      "Epoch: [7026/10000] Training loss: 1.3225477079593688e-05 / Validation loss: 0.0001684045257865849 / Long term Validation loss: 0.1621\n",
      "Epoch: [7027/10000] Training loss: 1.3222761596763426e-05 / Validation loss: 0.0001683840554068995 / Long term Validation loss: 0.1621\n",
      "Epoch: [7028/10000] Training loss: 1.3220045929089003e-05 / Validation loss: 0.00016836349461223733 / Long term Validation loss: 0.1621\n",
      "Epoch: [7029/10000] Training loss: 1.3217330076227341e-05 / Validation loss: 0.00016834319273631578 / Long term Validation loss: 0.1621\n",
      "Epoch: [7030/10000] Training loss: 1.321461403783351e-05 / Validation loss: 0.00016832251536485807 / Long term Validation loss: 0.1621\n",
      "Epoch: [7031/10000] Training loss: 1.3211897813502462e-05 / Validation loss: 0.0001683022492308048 / Long term Validation loss: 0.1621\n",
      "Epoch: [7032/10000] Training loss: 1.3209181402979774e-05 / Validation loss: 0.00016828159892744125 / Long term Validation loss: 0.1621\n",
      "Epoch: [7033/10000] Training loss: 1.3206464805939653e-05 / Validation loss: 0.0001682612390829929 / Long term Validation loss: 0.1621\n",
      "Epoch: [7034/10000] Training loss: 1.3203748022188311e-05 / Validation loss: 0.0001682407113745329 / Long term Validation loss: 0.1621\n",
      "Epoch: [7035/10000] Training loss: 1.3201031051447969e-05 / Validation loss: 0.00016822020674814214 / Long term Validation loss: 0.1621\n",
      "Epoch: [7036/10000] Training loss: 1.3198313893469647e-05 / Validation loss: 0.00016819980507752053 / Long term Validation loss: 0.1621\n",
      "Epoch: [7037/10000] Training loss: 1.3195596547983342e-05 / Validation loss: 0.0001681791946126387 / Long term Validation loss: 0.1621\n",
      "Epoch: [7038/10000] Training loss: 1.319287901468738e-05 / Validation loss: 0.00016815884938756586 / Long term Validation loss: 0.1621\n",
      "Epoch: [7039/10000] Training loss: 1.3190161293353613e-05 / Validation loss: 0.0001681382199882647 / Long term Validation loss: 0.1621\n",
      "Epoch: [7040/10000] Training loss: 1.3187443383699995e-05 / Validation loss: 0.0001681178432755224 / Long term Validation loss: 0.1621\n",
      "Epoch: [7041/10000] Training loss: 1.3184725285544274e-05 / Validation loss: 0.00016809727168384087 / Long term Validation loss: 0.1621\n",
      "Epoch: [7042/10000] Training loss: 1.3182006998643478e-05 / Validation loss: 0.00016807680827522574 / Long term Validation loss: 0.1621\n",
      "Epoch: [7043/10000] Training loss: 1.317928852281068e-05 / Validation loss: 0.00016805632291852995 / Long term Validation loss: 0.1621\n",
      "Epoch: [7044/10000] Training loss: 1.3176569857825487e-05 / Validation loss: 0.0001680357715845657 / Long term Validation loss: 0.1621\n",
      "Epoch: [7045/10000] Training loss: 1.3173851003470528e-05 / Validation loss: 0.00016801534929036333 / Long term Validation loss: 0.1621\n",
      "Epoch: [7046/10000] Training loss: 1.3171131959550095e-05 / Validation loss: 0.0001679947510965756 / Long term Validation loss: 0.1621\n",
      "Epoch: [7047/10000] Training loss: 1.3168412725839013e-05 / Validation loss: 0.00016797434078289926 / Long term Validation loss: 0.1621\n",
      "Epoch: [7048/10000] Training loss: 1.3165693302172882e-05 / Validation loss: 0.00016795374915256036 / Long term Validation loss: 0.1621\n",
      "Epoch: [7049/10000] Training loss: 1.3162973688344271e-05 / Validation loss: 0.00016793330310255978 / Long term Validation loss: 0.1622\n",
      "Epoch: [7050/10000] Training loss: 1.3160253884204786e-05 / Validation loss: 0.00016791275537138143 / Long term Validation loss: 0.1622\n",
      "Epoch: [7051/10000] Training loss: 1.3157533889569396e-05 / Validation loss: 0.0001678922507942035 / Long term Validation loss: 0.1622\n",
      "Epoch: [7052/10000] Training loss: 1.3154813704283822e-05 / Validation loss: 0.00016787175465857914 / Long term Validation loss: 0.1622\n",
      "Epoch: [7053/10000] Training loss: 1.3152093328183615e-05 / Validation loss: 0.0001678511981675625 / Long term Validation loss: 0.1622\n",
      "Epoch: [7054/10000] Training loss: 1.3149372761104262e-05 / Validation loss: 0.00016783073533038726 / Long term Validation loss: 0.1622\n",
      "Epoch: [7055/10000] Training loss: 1.3146652002903113e-05 / Validation loss: 0.00016781015311844222 / Long term Validation loss: 0.1622\n",
      "Epoch: [7056/10000] Training loss: 1.3143931053416389e-05 / Validation loss: 0.00016778969358758594 / Long term Validation loss: 0.1622\n",
      "Epoch: [7057/10000] Training loss: 1.3141209912520714e-05 / Validation loss: 0.00016776911564302514 / Long term Validation loss: 0.1622\n",
      "Epoch: [7058/10000] Training loss: 1.3138488580063736e-05 / Validation loss: 0.0001677486332157882 / Long term Validation loss: 0.1622\n",
      "Epoch: [7059/10000] Training loss: 1.3135767055931774e-05 / Validation loss: 0.0001677280800318625 / Long term Validation loss: 0.1622\n",
      "Epoch: [7060/10000] Training loss: 1.3133045339988204e-05 / Validation loss: 0.00016770756196612702 / Long term Validation loss: 0.1622\n",
      "Epoch: [7061/10000] Training loss: 1.313032343211971e-05 / Validation loss: 0.0001676870386412716 / Long term Validation loss: 0.1622\n",
      "Epoch: [7062/10000] Training loss: 1.3127601332206387e-05 / Validation loss: 0.0001676664872899553 / Long term Validation loss: 0.1622\n",
      "Epoch: [7063/10000] Training loss: 1.3124879040132949e-05 / Validation loss: 0.00016764598535779076 / Long term Validation loss: 0.1622\n",
      "Epoch: [7064/10000] Training loss: 1.3122156555796061e-05 / Validation loss: 0.00016762541356601997 / Long term Validation loss: 0.1622\n",
      "Epoch: [7065/10000] Training loss: 1.311943387908223e-05 / Validation loss: 0.0001676049175710521 / Long term Validation loss: 0.1622\n",
      "Epoch: [7066/10000] Training loss: 1.311671100990231e-05 / Validation loss: 0.00016758434145961612 / Long term Validation loss: 0.1622\n",
      "Epoch: [7067/10000] Training loss: 1.3113987948149689e-05 / Validation loss: 0.00016756383627600463 / Long term Validation loss: 0.1622\n",
      "Epoch: [7068/10000] Training loss: 1.311126469374476e-05 / Validation loss: 0.0001675432688236913 / Long term Validation loss: 0.1622\n",
      "Epoch: [7069/10000] Training loss: 1.3108541246591395e-05 / Validation loss: 0.0001675227447295308 / Long term Validation loss: 0.1622\n",
      "Epoch: [7070/10000] Training loss: 1.3105817606614692e-05 / Validation loss: 0.0001675021922066835 / Long term Validation loss: 0.1622\n",
      "Epoch: [7071/10000] Training loss: 1.3103093773730785e-05 / Validation loss: 0.00016748164662491836 / Long term Validation loss: 0.1622\n",
      "Epoch: [7072/10000] Training loss: 1.3100369747866971e-05 / Validation loss: 0.0001674611082922634 / Long term Validation loss: 0.1622\n",
      "Epoch: [7073/10000] Training loss: 1.3097645528951933e-05 / Validation loss: 0.00016744054471448433 / Long term Validation loss: 0.1622\n",
      "Epoch: [7074/10000] Training loss: 1.3094921116915366e-05 / Validation loss: 0.00016742001490974886 / Long term Validation loss: 0.1622\n",
      "Epoch: [7075/10000] Training loss: 1.3092196511697603e-05 / Validation loss: 0.00016739944027665602 / Long term Validation loss: 0.1622\n",
      "Epoch: [7076/10000] Training loss: 1.3089471713232475e-05 / Validation loss: 0.00016737891142836042 / Long term Validation loss: 0.1622\n",
      "Epoch: [7077/10000] Training loss: 1.3086746721470002e-05 / Validation loss: 0.00016735833325652504 / Long term Validation loss: 0.1622\n",
      "Epoch: [7078/10000] Training loss: 1.3084021536350217e-05 / Validation loss: 0.00016733779852693968 / Long term Validation loss: 0.1622\n",
      "Epoch: [7079/10000] Training loss: 1.3081296157830431e-05 / Validation loss: 0.00016731722270542775 / Long term Validation loss: 0.1622\n",
      "Epoch: [7080/10000] Training loss: 1.3078570585858542e-05 / Validation loss: 0.00016729667757486765 / Long term Validation loss: 0.1622\n",
      "Epoch: [7081/10000] Training loss: 1.307584482039711e-05 / Validation loss: 0.00016727610727251712 / Long term Validation loss: 0.1622\n",
      "Epoch: [7082/10000] Training loss: 1.3073118861402784e-05 / Validation loss: 0.00016725554999917367 / Long term Validation loss: 0.1622\n",
      "Epoch: [7083/10000] Training loss: 1.307039270884219e-05 / Validation loss: 0.0001672349856573381 / Long term Validation loss: 0.1622\n",
      "Epoch: [7084/10000] Training loss: 1.3067666362681018e-05 / Validation loss: 0.0001672144169031324 / Long term Validation loss: 0.1622\n",
      "Epoch: [7085/10000] Training loss: 1.3064939822889634e-05 / Validation loss: 0.00016719385695123954 / Long term Validation loss: 0.1622\n",
      "Epoch: [7086/10000] Training loss: 1.3062213089442418e-05 / Validation loss: 0.00016717327895347537 / Long term Validation loss: 0.1622\n",
      "Epoch: [7087/10000] Training loss: 1.305948616231381e-05 / Validation loss: 0.0001671527207752779 / Long term Validation loss: 0.1622\n",
      "Epoch: [7088/10000] Training loss: 1.30567590414861e-05 / Validation loss: 0.00016713213640545793 / Long term Validation loss: 0.1623\n",
      "Epoch: [7089/10000] Training loss: 1.3054031726938406e-05 / Validation loss: 0.00016711157719593537 / Long term Validation loss: 0.1623\n",
      "Epoch: [7090/10000] Training loss: 1.3051304218659954e-05 / Validation loss: 0.00016709098917126004 / Long term Validation loss: 0.1623\n",
      "Epoch: [7091/10000] Training loss: 1.304857651663521e-05 / Validation loss: 0.00016707042652540813 / Long term Validation loss: 0.1623\n",
      "Epoch: [7092/10000] Training loss: 1.3045848620859373e-05 / Validation loss: 0.0001670498369277327 / Long term Validation loss: 0.1623\n",
      "Epoch: [7093/10000] Training loss: 1.3043120531322736e-05 / Validation loss: 0.00016702926914967292 / Long term Validation loss: 0.1623\n",
      "Epoch: [7094/10000] Training loss: 1.3040392248025726e-05 / Validation loss: 0.00016700867926655708 / Long term Validation loss: 0.1623\n",
      "Epoch: [7095/10000] Training loss: 1.3037663770964768e-05 / Validation loss: 0.00016698810544152904 / Long term Validation loss: 0.1623\n",
      "Epoch: [7096/10000] Training loss: 1.3034935100144946e-05 / Validation loss: 0.00016696751583314642 / Long term Validation loss: 0.1623\n",
      "Epoch: [7097/10000] Training loss: 1.3032206235568956e-05 / Validation loss: 0.00016694693571966447 / Long term Validation loss: 0.1623\n",
      "Epoch: [7098/10000] Training loss: 1.3029477177246196e-05 / Validation loss: 0.00016692634638895045 / Long term Validation loss: 0.1623\n",
      "Epoch: [7099/10000] Training loss: 1.3026747925185549e-05 / Validation loss: 0.00016690576020878016 / Long term Validation loss: 0.1623\n",
      "Epoch: [7100/10000] Training loss: 1.3024018479400626e-05 / Validation loss: 0.00016688517079523966 / Long term Validation loss: 0.1623\n",
      "Epoch: [7101/10000] Training loss: 1.3021288839906267e-05 / Validation loss: 0.00016686457901080138 / Long term Validation loss: 0.1623\n",
      "Epoch: [7102/10000] Training loss: 1.301855900672023e-05 / Validation loss: 0.00016684398897574872 / Long term Validation loss: 0.1623\n",
      "Epoch: [7103/10000] Training loss: 1.301582897986308e-05 / Validation loss: 0.00016682339211869876 / Long term Validation loss: 0.1623\n",
      "Epoch: [7104/10000] Training loss: 1.301309875935678e-05 / Validation loss: 0.00016680280090295997 / Long term Validation loss: 0.1623\n",
      "Epoch: [7105/10000] Training loss: 1.301036834522725e-05 / Validation loss: 0.00016678219946664766 / Long term Validation loss: 0.1623\n",
      "Epoch: [7106/10000] Training loss: 1.3007637737500676e-05 / Validation loss: 0.00016676160659889874 / Long term Validation loss: 0.1623\n",
      "Epoch: [7107/10000] Training loss: 1.3004906936208154e-05 / Validation loss: 0.00016674100097375358 / Long term Validation loss: 0.1623\n",
      "Epoch: [7108/10000] Training loss: 1.300217594138005e-05 / Validation loss: 0.0001667204061206457 / Long term Validation loss: 0.1623\n",
      "Epoch: [7109/10000] Training loss: 1.2999444753052334e-05 / Validation loss: 0.0001666997965542326 / Long term Validation loss: 0.1623\n",
      "Epoch: [7110/10000] Training loss: 1.2996713371259599e-05 / Validation loss: 0.00016667919952981477 / Long term Validation loss: 0.1623\n",
      "Epoch: [7111/10000] Training loss: 1.2993981796042488e-05 / Validation loss: 0.00016665858610908564 / Long term Validation loss: 0.1623\n",
      "Epoch: [7112/10000] Training loss: 1.299125002743971e-05 / Validation loss: 0.00016663798687336205 / Long term Validation loss: 0.1623\n",
      "Epoch: [7113/10000] Training loss: 1.2988518065496496e-05 / Validation loss: 0.00016661736952706935 / Long term Validation loss: 0.1623\n",
      "Epoch: [7114/10000] Training loss: 1.298578591025554e-05 / Validation loss: 0.00016659676819132539 / Long term Validation loss: 0.1623\n",
      "Epoch: [7115/10000] Training loss: 1.2983053561766526e-05 / Validation loss: 0.00016657614669697665 / Long term Validation loss: 0.1623\n",
      "Epoch: [7116/10000] Training loss: 1.2980321020076027e-05 / Validation loss: 0.00016655554353663365 / Long term Validation loss: 0.1623\n",
      "Epoch: [7117/10000] Training loss: 1.2977588285238136e-05 / Validation loss: 0.00016653491751043444 / Long term Validation loss: 0.1623\n",
      "Epoch: [7118/10000] Training loss: 1.2974855357303107e-05 / Validation loss: 0.0001665143129857054 / Long term Validation loss: 0.1623\n",
      "Epoch: [7119/10000] Training loss: 1.2972122236329444e-05 / Validation loss: 0.0001664936818434881 / Long term Validation loss: 0.1623\n",
      "Epoch: [7120/10000] Training loss: 1.2969388922370857e-05 / Validation loss: 0.0001664730766408501 / Long term Validation loss: 0.1623\n",
      "Epoch: [7121/10000] Training loss: 1.2966655415490317e-05 / Validation loss: 0.00016645243952764758 / Long term Validation loss: 0.1623\n",
      "Epoch: [7122/10000] Training loss: 1.2963921715744677e-05 / Validation loss: 0.00016643183464355618 / Long term Validation loss: 0.1623\n",
      "Epoch: [7123/10000] Training loss: 1.2961187823201574e-05 / Validation loss: 0.0001664111903235788 / Long term Validation loss: 0.1623\n",
      "Epoch: [7124/10000] Training loss: 1.2958453737920595e-05 / Validation loss: 0.00016639058721083682 / Long term Validation loss: 0.1623\n",
      "Epoch: [7125/10000] Training loss: 1.2955719459974314e-05 / Validation loss: 0.00016636993389115766 / Long term Validation loss: 0.1623\n",
      "Epoch: [7126/10000] Training loss: 1.2952984989424531e-05 / Validation loss: 0.00016634933469151028 / Long term Validation loss: 0.1623\n",
      "Epoch: [7127/10000] Training loss: 1.2950250326349235e-05 / Validation loss: 0.0001663286697338023 / Long term Validation loss: 0.1623\n",
      "Epoch: [7128/10000] Training loss: 1.2947515470811755e-05 / Validation loss: 0.00016630807764178918 / Long term Validation loss: 0.1623\n",
      "Epoch: [7129/10000] Training loss: 1.2944780422896195e-05 / Validation loss: 0.00016628739709685387 / Long term Validation loss: 0.1623\n",
      "Epoch: [7130/10000] Training loss: 1.294204518266648e-05 / Validation loss: 0.0001662668169409628 / Long term Validation loss: 0.1623\n",
      "Epoch: [7131/10000] Training loss: 1.2939309750214035e-05 / Validation loss: 0.00016624611480358577 / Long term Validation loss: 0.1623\n",
      "Epoch: [7132/10000] Training loss: 1.293657412560199e-05 / Validation loss: 0.0001662255539899292 / Long term Validation loss: 0.1623\n",
      "Epoch: [7133/10000] Training loss: 1.2933838308931105e-05 / Validation loss: 0.00016620482099569318 / Long term Validation loss: 0.1624\n",
      "Epoch: [7134/10000] Training loss: 1.2931102300261832e-05 / Validation loss: 0.00016618429105155037 / Long term Validation loss: 0.1624\n",
      "Epoch: [7135/10000] Training loss: 1.2928366099707582e-05 / Validation loss: 0.00016616351270345325 / Long term Validation loss: 0.1624\n",
      "Epoch: [7136/10000] Training loss: 1.2925629707323692e-05 / Validation loss: 0.00016614303182193828 / Long term Validation loss: 0.1624\n",
      "Epoch: [7137/10000] Training loss: 1.2922893123242119e-05 / Validation loss: 0.00016612218511034767 / Long term Validation loss: 0.1624\n",
      "Epoch: [7138/10000] Training loss: 1.292015634751022e-05 / Validation loss: 0.00016610178239446032 / Long term Validation loss: 0.1624\n",
      "Epoch: [7139/10000] Training loss: 1.2917419380289629e-05 / Validation loss: 0.00016608083028565727 / Long term Validation loss: 0.1624\n",
      "Epoch: [7140/10000] Training loss: 1.2914682221617974e-05 / Validation loss: 0.0001660605529175572 / Long term Validation loss: 0.1624\n",
      "Epoch: [7141/10000] Training loss: 1.291194487170904e-05 / Validation loss: 0.00016603943499080554 / Long term Validation loss: 0.1624\n",
      "Epoch: [7142/10000] Training loss: 1.2909207330595374e-05 / Validation loss: 0.00016601936048900293 / Long term Validation loss: 0.1624\n",
      "Epoch: [7143/10000] Training loss: 1.2906469598592312e-05 / Validation loss: 0.00016599797684357508 / Long term Validation loss: 0.1624\n",
      "Epoch: [7144/10000] Training loss: 1.2903731675756285e-05 / Validation loss: 0.00016597823425978494 / Long term Validation loss: 0.1624\n",
      "Epoch: [7145/10000] Training loss: 1.2900993562621845e-05 / Validation loss: 0.00016595641752553501 / Long term Validation loss: 0.1624\n",
      "Epoch: [7146/10000] Training loss: 1.289825525937892e-05 / Validation loss: 0.00016593722452257616 / Long term Validation loss: 0.1624\n",
      "Epoch: [7147/10000] Training loss: 1.2895516767084056e-05 / Validation loss: 0.00016591469061288068 / Long term Validation loss: 0.1624\n",
      "Epoch: [7148/10000] Training loss: 1.2892778086427564e-05 / Validation loss: 0.0001658964190736104 / Long term Validation loss: 0.1624\n",
      "Epoch: [7149/10000] Training loss: 1.2890039219818496e-05 / Validation loss: 0.00016587267954218838 / Long term Validation loss: 0.1624\n",
      "Epoch: [7150/10000] Training loss: 1.2887300169627709e-05 / Validation loss: 0.00016585597300418335 / Long term Validation loss: 0.1624\n",
      "Epoch: [7151/10000] Training loss: 1.2884560942018796e-05 / Validation loss: 0.00016583017729140115 / Long term Validation loss: 0.1624\n",
      "Epoch: [7152/10000] Training loss: 1.2881821544808781e-05 / Validation loss: 0.00016581616355008068 / Long term Validation loss: 0.1624\n",
      "Epoch: [7153/10000] Training loss: 1.287908199516876e-05 / Validation loss: 0.00016578681181406518 / Long term Validation loss: 0.1624\n",
      "Epoch: [7154/10000] Training loss: 1.2876342318460511e-05 / Validation loss: 0.00016577749219003817 / Long term Validation loss: 0.1624\n",
      "Epoch: [7155/10000] Training loss: 1.2873602565566544e-05 / Validation loss: 0.00016574190668386334 / Long term Validation loss: 0.1624\n",
      "Epoch: [7156/10000] Training loss: 1.2870862818835339e-05 / Validation loss: 0.00016574087681828356 / Long term Validation loss: 0.1624\n",
      "Epoch: [7157/10000] Training loss: 1.2868123236183114e-05 / Validation loss: 0.000165694218004427 / Long term Validation loss: 0.1624\n",
      "Epoch: [7158/10000] Training loss: 1.2865384087951017e-05 / Validation loss: 0.00016570801761229414 / Long term Validation loss: 0.1624\n",
      "Epoch: [7159/10000] Training loss: 1.286264588262703e-05 / Validation loss: 0.00016564143294981178 / Long term Validation loss: 0.1624\n",
      "Epoch: [7160/10000] Training loss: 1.2859909522842806e-05 / Validation loss: 0.000165682101987011 / Long term Validation loss: 0.1624\n",
      "Epoch: [7161/10000] Training loss: 1.2857176697949176e-05 / Validation loss: 0.00016557920570019552 / Long term Validation loss: 0.1624\n",
      "Epoch: [7162/10000] Training loss: 1.2854450483106461e-05 / Validation loss: 0.00016566918074987805 / Long term Validation loss: 0.1624\n",
      "Epoch: [7163/10000] Training loss: 1.2851736650353682e-05 / Validation loss: 0.00016549929124197883 / Long term Validation loss: 0.1624\n",
      "Epoch: [7164/10000] Training loss: 1.2849045908117741e-05 / Validation loss: 0.0001656808954271424 / Long term Validation loss: 0.1624\n",
      "Epoch: [7165/10000] Training loss: 1.2846398498861092e-05 / Validation loss: 0.00016538592009050134 / Long term Validation loss: 0.1624\n",
      "Epoch: [7166/10000] Training loss: 1.2843832567487878e-05 / Validation loss: 0.0001657399869355798 / Long term Validation loss: 0.1624\n",
      "Epoch: [7167/10000] Training loss: 1.284142089121277e-05 / Validation loss: 0.00016520877792313134 / Long term Validation loss: 0.1624\n",
      "Epoch: [7168/10000] Training loss: 1.2839302507269023e-05 / Validation loss: 0.00016589171532658612 / Long term Validation loss: 0.1625\n",
      "Epoch: [7169/10000] Training loss: 1.2837745438480766e-05 / Validation loss: 0.0001649096433669198 / Long term Validation loss: 0.1623\n",
      "Epoch: [7170/10000] Training loss: 1.283726825931462e-05 / Validation loss: 0.00016622845710903493 / Long term Validation loss: 0.1624\n",
      "Epoch: [7171/10000] Training loss: 1.283888142624259e-05 / Validation loss: 0.00016437820770388183 / Long term Validation loss: 0.1622\n",
      "Epoch: [7172/10000] Training loss: 1.2844563644014101e-05 / Validation loss: 0.00016694583377424707 / Long term Validation loss: 0.1624\n",
      "Epoch: [7173/10000] Training loss: 1.285821201069851e-05 / Validation loss: 0.0001634143177791245 / Long term Validation loss: 0.1621\n",
      "Epoch: [7174/10000] Training loss: 1.2887545976521882e-05 / Validation loss: 0.00016848194583086128 / Long term Validation loss: 0.1623\n",
      "Epoch: [7175/10000] Training loss: 1.2947923009886595e-05 / Validation loss: 0.0001616981265642791 / Long term Validation loss: 0.1620\n",
      "Epoch: [7176/10000] Training loss: 1.3070080912058392e-05 / Validation loss: 0.00017190173973345528 / Long term Validation loss: 0.1624\n",
      "Epoch: [7177/10000] Training loss: 1.3315644643185683e-05 / Validation loss: 0.00015892800784949763 / Long term Validation loss: 0.1622\n",
      "Epoch: [7178/10000] Training loss: 1.3808774609611868e-05 / Validation loss: 0.0001800791819022711 / Long term Validation loss: 0.1608\n",
      "Epoch: [7179/10000] Training loss: 1.4798181821310023e-05 / Validation loss: 0.00015592933207370902 / Long term Validation loss: 0.1588\n",
      "Epoch: [7180/10000] Training loss: 1.67806839814496e-05 / Validation loss: 0.00020152559896592607 / Long term Validation loss: 0.1535\n",
      "Epoch: [7181/10000] Training loss: 2.0717354086375507e-05 / Validation loss: 0.00015986755371593037 / Long term Validation loss: 0.1457\n",
      "Epoch: [7182/10000] Training loss: 2.8388363021271833e-05 / Validation loss: 0.0002605522710033397 / Long term Validation loss: 0.1527\n",
      "Epoch: [7183/10000] Training loss: 4.261991683381859e-05 / Validation loss: 0.00019720144463644288 / Long term Validation loss: 0.1538\n",
      "Epoch: [7184/10000] Training loss: 6.656509676963591e-05 / Validation loss: 0.0003884541418275028 / Long term Validation loss: 0.1606\n",
      "Epoch: [7185/10000] Training loss: 9.868043917706937e-05 / Validation loss: 0.0002666046728224709 / Long term Validation loss: 0.1662\n",
      "Epoch: [7186/10000] Training loss: 0.00012319136323079208 / Validation loss: 0.0004151206918764876 / Long term Validation loss: 0.1616\n",
      "Epoch: [7187/10000] Training loss: 0.00010961729921530339 / Validation loss: 0.00018502581081990204 / Long term Validation loss: 0.1475\n",
      "Epoch: [7188/10000] Training loss: 5.5653140663517256e-05 / Validation loss: 0.00018382248430179332 / Long term Validation loss: 0.1586\n",
      "Epoch: [7189/10000] Training loss: 1.4616353960998556e-05 / Validation loss: 0.0002293289656317114 / Long term Validation loss: 0.1512\n",
      "Epoch: [7190/10000] Training loss: 2.7734155357673275e-05 / Validation loss: 0.00019415980619065363 / Long term Validation loss: 0.1490\n",
      "Epoch: [7191/10000] Training loss: 6.224932622901812e-05 / Validation loss: 0.00031495910542747075 / Long term Validation loss: 0.1538\n",
      "Epoch: [7192/10000] Training loss: 6.10004067366018e-05 / Validation loss: 0.00016387961380804686 / Long term Validation loss: 0.1505\n",
      "Epoch: [7193/10000] Training loss: 2.650689349011328e-05 / Validation loss: 0.00016618452917891872 / Long term Validation loss: 0.1626\n",
      "Epoch: [7194/10000] Training loss: 1.3964849638478146e-05 / Validation loss: 0.00025343177122899856 / Long term Validation loss: 0.1502\n",
      "Epoch: [7195/10000] Training loss: 3.550136121682306e-05 / Validation loss: 0.00017857561298932325 / Long term Validation loss: 0.1441\n",
      "Epoch: [7196/10000] Training loss: 4.540420036187712e-05 / Validation loss: 0.0002250971938579414 / Long term Validation loss: 0.1508\n",
      "Epoch: [7197/10000] Training loss: 2.5482834291662174e-05 / Validation loss: 0.00017545457033131803 / Long term Validation loss: 0.1616\n",
      "Epoch: [7198/10000] Training loss: 1.3029546327229812e-05 / Validation loss: 0.00016336306727883469 / Long term Validation loss: 0.1489\n",
      "Epoch: [7199/10000] Training loss: 2.604910704881234e-05 / Validation loss: 0.0002460551471636845 / Long term Validation loss: 0.1498\n",
      "Epoch: [7200/10000] Training loss: 3.3869704877039126e-05 / Validation loss: 0.00016119149180564242 / Long term Validation loss: 0.1537\n",
      "Epoch: [7201/10000] Training loss: 2.0989999160395105e-05 / Validation loss: 0.00016796958050935664 / Long term Validation loss: 0.1622\n",
      "Epoch: [7202/10000] Training loss: 1.30044550336874e-05 / Validation loss: 0.0002102375982076621 / Long term Validation loss: 0.1526\n",
      "Epoch: [7203/10000] Training loss: 2.204165686879743e-05 / Validation loss: 0.00016297656411698515 / Long term Validation loss: 0.1453\n",
      "Epoch: [7204/10000] Training loss: 2.6412992911761935e-05 / Validation loss: 0.00019300203581664274 / Long term Validation loss: 0.1558\n",
      "Epoch: [7205/10000] Training loss: 1.7310543730080202e-05 / Validation loss: 0.00017279108990697604 / Long term Validation loss: 0.1632\n",
      "Epoch: [7206/10000] Training loss: 1.314666365024808e-05 / Validation loss: 0.00015860977378490463 / Long term Validation loss: 0.1516\n",
      "Epoch: [7207/10000] Training loss: 1.9700060590272072e-05 / Validation loss: 0.00020393278329337717 / Long term Validation loss: 0.1530\n",
      "Epoch: [7208/10000] Training loss: 2.144351742365227e-05 / Validation loss: 0.00015859063752019282 / Long term Validation loss: 0.1583\n",
      "Epoch: [7209/10000] Training loss: 1.5005331870968697e-05 / Validation loss: 0.00016118924011925605 / Long term Validation loss: 0.1622\n",
      "Epoch: [7210/10000] Training loss: 1.3345334266879098e-05 / Validation loss: 0.0001913212815444222 / Long term Validation loss: 0.1558\n",
      "Epoch: [7211/10000] Training loss: 1.7959902294774235e-05 / Validation loss: 0.00015697827059583364 / Long term Validation loss: 0.1524\n",
      "Epoch: [7212/10000] Training loss: 1.815850303444544e-05 / Validation loss: 0.00017397558407114758 / Long term Validation loss: 0.1640\n",
      "Epoch: [7213/10000] Training loss: 1.3759693575190608e-05 / Validation loss: 0.00017216176792373962 / Long term Validation loss: 0.1640\n",
      "Epoch: [7214/10000] Training loss: 1.3472240129052124e-05 / Validation loss: 0.00015672250357992243 / Long term Validation loss: 0.1548\n",
      "Epoch: [7215/10000] Training loss: 1.655781932387358e-05 / Validation loss: 0.00018368273080694958 / Long term Validation loss: 0.1600\n",
      "Epoch: [7216/10000] Training loss: 1.6025825958020325e-05 / Validation loss: 0.00016075273284082089 / Long term Validation loss: 0.1625\n",
      "Epoch: [7217/10000] Training loss: 1.3155703057217543e-05 / Validation loss: 0.00015950305597160484 / Long term Validation loss: 0.1618\n",
      "Epoch: [7218/10000] Training loss: 1.348587790004474e-05 / Validation loss: 0.00018153518795467434 / Long term Validation loss: 0.1612\n",
      "Epoch: [7219/10000] Training loss: 1.5443100243097268e-05 / Validation loss: 0.0001574172992781756 / Long term Validation loss: 0.1586\n",
      "Epoch: [7220/10000] Training loss: 1.4689073604755667e-05 / Validation loss: 0.00016761487306733098 / Long term Validation loss: 0.1641\n",
      "Epoch: [7221/10000] Training loss: 1.2892074731083565e-05 / Validation loss: 0.00017221525852991149 / Long term Validation loss: 0.1638\n",
      "Epoch: [7222/10000] Training loss: 1.34134121456719e-05 / Validation loss: 0.00015777450004314037 / Long term Validation loss: 0.1591\n",
      "Epoch: [7223/10000] Training loss: 1.4602451647375188e-05 / Validation loss: 0.00017535173503807587 / Long term Validation loss: 0.1639\n",
      "Epoch: [7224/10000] Training loss: 1.387966446384557e-05 / Validation loss: 0.00016412946327729148 / Long term Validation loss: 0.1628\n",
      "Epoch: [7225/10000] Training loss: 1.2789670704393068e-05 / Validation loss: 0.00016065022746513893 / Long term Validation loss: 0.1622\n",
      "Epoch: [7226/10000] Training loss: 1.3293492588040735e-05 / Validation loss: 0.0001763927934800017 / Long term Validation loss: 0.1637\n",
      "Epoch: [7227/10000] Training loss: 1.3993101312573317e-05 / Validation loss: 0.00016030109726558652 / Long term Validation loss: 0.1619\n",
      "Epoch: [7228/10000] Training loss: 1.3403231789304872e-05 / Validation loss: 0.00016610442515679102 / Long term Validation loss: 0.1631\n",
      "Epoch: [7229/10000] Training loss: 1.275216938118051e-05 / Validation loss: 0.00017146284897233077 / Long term Validation loss: 0.1630\n",
      "Epoch: [7230/10000] Training loss: 1.3160416139352177e-05 / Validation loss: 0.0001598105680186018 / Long term Validation loss: 0.1615\n",
      "Epoch: [7231/10000] Training loss: 1.3567232001300697e-05 / Validation loss: 0.0001713065797346921 / Long term Validation loss: 0.1628\n",
      "Epoch: [7232/10000] Training loss: 1.3125904010067644e-05 / Validation loss: 0.00016563122807328263 / Long term Validation loss: 0.1627\n",
      "Epoch: [7233/10000] Training loss: 1.2735362588088202e-05 / Validation loss: 0.00016178399364953102 / Long term Validation loss: 0.1623\n",
      "Epoch: [7234/10000] Training loss: 1.3036174634602606e-05 / Validation loss: 0.0001725102033999061 / Long term Validation loss: 0.1625\n",
      "Epoch: [7235/10000] Training loss: 1.3275765571714486e-05 / Validation loss: 0.00016209714507423617 / Long term Validation loss: 0.1621\n",
      "Epoch: [7236/10000] Training loss: 1.2964313631727868e-05 / Validation loss: 0.0001654128418372936 / Long term Validation loss: 0.1625\n",
      "Epoch: [7237/10000] Training loss: 1.2723557594909002e-05 / Validation loss: 0.0001695003022924624 / Long term Validation loss: 0.1623\n",
      "Epoch: [7238/10000] Training loss: 1.2931863850194023e-05 / Validation loss: 0.0001611410840440826 / Long term Validation loss: 0.1620\n",
      "Epoch: [7239/10000] Training loss: 1.3079198097418363e-05 / Validation loss: 0.00016866956640829186 / Long term Validation loss: 0.1622\n",
      "Epoch: [7240/10000] Training loss: 1.2868089063601913e-05 / Validation loss: 0.00016531701435328836 / Long term Validation loss: 0.1622\n",
      "Epoch: [7241/10000] Training loss: 1.2712752038687886e-05 / Validation loss: 0.00016233334736476577 / Long term Validation loss: 0.1614\n",
      "Epoch: [7242/10000] Training loss: 1.2850345991283515e-05 / Validation loss: 0.00016937386968346208 / Long term Validation loss: 0.1621\n",
      "Epoch: [7243/10000] Training loss: 1.2946550181956515e-05 / Validation loss: 0.00016256796323649343 / Long term Validation loss: 0.1613\n",
      "Epoch: [7244/10000] Training loss: 1.2808145666963753e-05 / Validation loss: 0.0001648571046719758 / Long term Validation loss: 0.1621\n",
      "Epoch: [7245/10000] Training loss: 1.2702322163741759e-05 / Validation loss: 0.0001673938280546718 / Long term Validation loss: 0.1622\n",
      "Epoch: [7246/10000] Training loss: 1.2788930735394097e-05 / Validation loss: 0.00016185188552311539 / Long term Validation loss: 0.1614\n",
      "Epoch: [7247/10000] Training loss: 1.2855723017168081e-05 / Validation loss: 0.00016700594650173996 / Long term Validation loss: 0.1621\n",
      "Epoch: [7248/10000] Training loss: 1.2767972691655162e-05 / Validation loss: 0.00016457364823447058 / Long term Validation loss: 0.1620\n",
      "Epoch: [7249/10000] Training loss: 1.2692360832054035e-05 / Validation loss: 0.00016281288472044423 / Long term Validation loss: 0.1613\n",
      "Epoch: [7250/10000] Training loss: 1.2743535278193894e-05 / Validation loss: 0.00016730754492151454 / Long term Validation loss: 0.1621\n",
      "Epoch: [7251/10000] Training loss: 1.2791868162154557e-05 / Validation loss: 0.00016271838279988192 / Long term Validation loss: 0.1613\n",
      "Epoch: [7252/10000] Training loss: 1.2738652807323474e-05 / Validation loss: 0.0001646624657407285 / Long term Validation loss: 0.1620\n",
      "Epoch: [7253/10000] Training loss: 1.2683054028131067e-05 / Validation loss: 0.00016588268799258343 / Long term Validation loss: 0.1621\n",
      "Epoch: [7254/10000] Training loss: 1.2710314919921436e-05 / Validation loss: 0.00016242406605882084 / Long term Validation loss: 0.1613\n",
      "Epoch: [7255/10000] Training loss: 1.2745792965025815e-05 / Validation loss: 0.00016607026991569915 / Long term Validation loss: 0.1621\n",
      "Epoch: [7256/10000] Training loss: 1.2715599773820428e-05 / Validation loss: 0.00016403488268414417 / Long term Validation loss: 0.1619\n",
      "Epoch: [7257/10000] Training loss: 1.2674404451398789e-05 / Validation loss: 0.00016335461224294785 / Long term Validation loss: 0.1617\n",
      "Epoch: [7258/10000] Training loss: 1.268599254343463e-05 / Validation loss: 0.00016605940812871304 / Long term Validation loss: 0.1621\n",
      "Epoch: [7259/10000] Training loss: 1.2711593889608443e-05 / Validation loss: 0.0001629216810390297 / Long term Validation loss: 0.1616\n",
      "Epoch: [7260/10000] Training loss: 1.2696296475163443e-05 / Validation loss: 0.0001646895254815124 / Long term Validation loss: 0.1620\n",
      "Epoch: [7261/10000] Training loss: 1.266612456157172e-05 / Validation loss: 0.00016491485658798072 / Long term Validation loss: 0.1621\n",
      "Epoch: [7262/10000] Training loss: 1.2667878901541595e-05 / Validation loss: 0.00016293776611261187 / Long term Validation loss: 0.1616\n",
      "Epoch: [7263/10000] Training loss: 1.2685433546743854e-05 / Validation loss: 0.00016545467033251193 / Long term Validation loss: 0.1621\n",
      "Epoch: [7264/10000] Training loss: 1.2679269707949273e-05 / Validation loss: 0.00016367620599035815 / Long term Validation loss: 0.1618\n",
      "Epoch: [7265/10000] Training loss: 1.2657811763253678e-05 / Validation loss: 0.00016376889143777286 / Long term Validation loss: 0.1618\n",
      "Epoch: [7266/10000] Training loss: 1.2653886781026805e-05 / Validation loss: 0.0001651725532598351 / Long term Validation loss: 0.1621\n",
      "Epoch: [7267/10000] Training loss: 1.2664798567879452e-05 / Validation loss: 0.00016310164020780484 / Long term Validation loss: 0.1617\n",
      "Epoch: [7268/10000] Training loss: 1.2663723679574076e-05 / Validation loss: 0.00016465626127991643 / Long term Validation loss: 0.1621\n",
      "Epoch: [7269/10000] Training loss: 1.2649169446302498e-05 / Validation loss: 0.00016424042802177553 / Long term Validation loss: 0.1620\n",
      "Epoch: [7270/10000] Training loss: 1.264251904313495e-05 / Validation loss: 0.00016332777523505581 / Long term Validation loss: 0.1617\n",
      "Epoch: [7271/10000] Training loss: 1.2648090955021531e-05 / Validation loss: 0.00016492141275426434 / Long term Validation loss: 0.1621\n",
      "Epoch: [7272/10000] Training loss: 1.2649301375670122e-05 / Validation loss: 0.00016344648159222617 / Long term Validation loss: 0.1618\n",
      "Epoch: [7273/10000] Training loss: 1.2640055300661534e-05 / Validation loss: 0.00016399675282252474 / Long term Validation loss: 0.1619\n",
      "Epoch: [7274/10000] Training loss: 1.2632710572701584e-05 / Validation loss: 0.00016447686874357628 / Long term Validation loss: 0.1620\n",
      "Epoch: [7275/10000] Training loss: 1.2634237374371429e-05 / Validation loss: 0.00016326009185049735 / Long term Validation loss: 0.1617\n",
      "Epoch: [7276/10000] Training loss: 1.2635878227929446e-05 / Validation loss: 0.00016450138468865086 / Long term Validation loss: 0.1621\n",
      "Epoch: [7277/10000] Training loss: 1.2630450180707487e-05 / Validation loss: 0.0001637854703582119 / Long term Validation loss: 0.1619\n",
      "Epoch: [7278/10000] Training loss: 1.262367967988627e-05 / Validation loss: 0.0001636135220558002 / Long term Validation loss: 0.1619\n",
      "Epoch: [7279/10000] Training loss: 1.2622426228141635e-05 / Validation loss: 0.00016444994380487003 / Long term Validation loss: 0.1620\n",
      "Epoch: [7280/10000] Training loss: 1.2623403844844892e-05 / Validation loss: 0.00016337479591874816 / Long term Validation loss: 0.1618\n",
      "Epoch: [7281/10000] Training loss: 1.2620427197384006e-05 / Validation loss: 0.00016409342550627955 / Long term Validation loss: 0.1620\n",
      "Epoch: [7282/10000] Training loss: 1.26148705040348e-05 / Validation loss: 0.00016398293574171723 / Long term Validation loss: 0.1620\n",
      "Epoch: [7283/10000] Training loss: 1.2611998465119886e-05 / Validation loss: 0.00016344608760068187 / Long term Validation loss: 0.1618\n",
      "Epoch: [7284/10000] Training loss: 1.2611835340365144e-05 / Validation loss: 0.0001642760343696046 / Long term Validation loss: 0.1620\n",
      "Epoch: [7285/10000] Training loss: 1.261014387342431e-05 / Validation loss: 0.0001635403443881079 / Long term Validation loss: 0.1619\n",
      "Epoch: [7286/10000] Training loss: 1.2605942747255218e-05 / Validation loss: 0.00016380211156973026 / Long term Validation loss: 0.1620\n",
      "Epoch: [7287/10000] Training loss: 1.2602413854105044e-05 / Validation loss: 0.00016404988807155988 / Long term Validation loss: 0.1620\n",
      "Epoch: [7288/10000] Training loss: 1.2601098307313734e-05 / Validation loss: 0.00016342277025956618 / Long term Validation loss: 0.1619\n",
      "Epoch: [7289/10000] Training loss: 1.259980571367007e-05 / Validation loss: 0.00016406016747429268 / Long term Validation loss: 0.1620\n",
      "Epoch: [7290/10000] Training loss: 1.2596751119511187e-05 / Validation loss: 0.00016366324162832302 / Long term Validation loss: 0.1619\n",
      "Epoch: [7291/10000] Training loss: 1.2593244043253975e-05 / Validation loss: 0.00016360953152424597 / Long term Validation loss: 0.1619\n",
      "Epoch: [7292/10000] Training loss: 1.2591058301805493e-05 / Validation loss: 0.00016399708016060782 / Long term Validation loss: 0.1620\n",
      "Epoch: [7293/10000] Training loss: 1.2589606096937122e-05 / Validation loss: 0.00016344138624448305 / Long term Validation loss: 0.1619\n",
      "Epoch: [7294/10000] Training loss: 1.2587300070739276e-05 / Validation loss: 0.00016384682657242898 / Long term Validation loss: 0.1620\n",
      "Epoch: [7295/10000] Training loss: 1.2584176312386308e-05 / Validation loss: 0.0001637111236384464 / Long term Validation loss: 0.1620\n",
      "Epoch: [7296/10000] Training loss: 1.2581520988974227e-05 / Validation loss: 0.00016349239464064931 / Long term Validation loss: 0.1619\n",
      "Epoch: [7297/10000] Training loss: 1.2579671335014299e-05 / Validation loss: 0.00016388260356111197 / Long term Validation loss: 0.1620\n",
      "Epoch: [7298/10000] Training loss: 1.2577689494992195e-05 / Validation loss: 0.00016346174390609235 / Long term Validation loss: 0.1620\n",
      "Epoch: [7299/10000] Training loss: 1.25750252080466e-05 / Validation loss: 0.00016366543485415758 / Long term Validation loss: 0.1620\n",
      "Epoch: [7300/10000] Training loss: 1.2572266410974596e-05 / Validation loss: 0.00016368916595504496 / Long term Validation loss: 0.1620\n",
      "Epoch: [7301/10000] Training loss: 1.2570035626063656e-05 / Validation loss: 0.0001634118847934867 / Long term Validation loss: 0.1620\n",
      "Epoch: [7302/10000] Training loss: 1.2568054062977174e-05 / Validation loss: 0.00016373582869216722 / Long term Validation loss: 0.1620\n",
      "Epoch: [7303/10000] Training loss: 1.2565731676960346e-05 / Validation loss: 0.0001634523204780219 / Long term Validation loss: 0.1620\n",
      "Epoch: [7304/10000] Training loss: 1.2563101063030342e-05 / Validation loss: 0.00016351464289790242 / Long term Validation loss: 0.1620\n",
      "Epoch: [7305/10000] Training loss: 1.2560647308604643e-05 / Validation loss: 0.00016361964978729428 / Long term Validation loss: 0.1620\n",
      "Epoch: [7306/10000] Training loss: 1.255850612073276e-05 / Validation loss: 0.00016335212478624716 / Long term Validation loss: 0.1620\n",
      "Epoch: [7307/10000] Training loss: 1.2556333224238426e-05 / Validation loss: 0.00016359329862039042 / Long term Validation loss: 0.1620\n",
      "Epoch: [7308/10000] Training loss: 1.255390012640966e-05 / Validation loss: 0.0001634219169998971 / Long term Validation loss: 0.1620\n",
      "Epoch: [7309/10000] Training loss: 1.2551402964509387e-05 / Validation loss: 0.00016339868679701177 / Long term Validation loss: 0.1620\n",
      "Epoch: [7310/10000] Training loss: 1.2549095384248719e-05 / Validation loss: 0.00016352946113234277 / Long term Validation loss: 0.1621\n",
      "Epoch: [7311/10000] Training loss: 1.254691134556073e-05 / Validation loss: 0.00016329995668519764 / Long term Validation loss: 0.1620\n",
      "Epoch: [7312/10000] Training loss: 1.2544622348920525e-05 / Validation loss: 0.00016346403240686687 / Long term Validation loss: 0.1620\n",
      "Epoch: [7313/10000] Training loss: 1.2542195947863747e-05 / Validation loss: 0.0001633710822717534 / Long term Validation loss: 0.1620\n",
      "Epoch: [7314/10000] Training loss: 1.2539803255590453e-05 / Validation loss: 0.0001633052440616883 / Long term Validation loss: 0.1620\n",
      "Epoch: [7315/10000] Training loss: 1.2537536172940335e-05 / Validation loss: 0.00016343247343443817 / Long term Validation loss: 0.1621\n",
      "Epoch: [7316/10000] Training loss: 1.2535292863730496e-05 / Validation loss: 0.000163249637063817 / Long term Validation loss: 0.1620\n",
      "Epoch: [7317/10000] Training loss: 1.2532958929960753e-05 / Validation loss: 0.00016335414742922925 / Long term Validation loss: 0.1621\n",
      "Epoch: [7318/10000] Training loss: 1.2530570203184931e-05 / Validation loss: 0.00016331038707332482 / Long term Validation loss: 0.1621\n",
      "Epoch: [7319/10000] Training loss: 1.2528233047621907e-05 / Validation loss: 0.00016322864411039487 / Long term Validation loss: 0.1620\n",
      "Epoch: [7320/10000] Training loss: 1.2525962583491183e-05 / Validation loss: 0.00016333712611739672 / Long term Validation loss: 0.1621\n",
      "Epoch: [7321/10000] Training loss: 1.2523679565315558e-05 / Validation loss: 0.0001631950867026066 / Long term Validation loss: 0.1621\n",
      "Epoch: [7322/10000] Training loss: 1.2521338079875758e-05 / Validation loss: 0.00016325593575670764 / Long term Validation loss: 0.1621\n",
      "Epoch: [7323/10000] Training loss: 1.2518982047798793e-05 / Validation loss: 0.00016323834704941317 / Long term Validation loss: 0.1621\n",
      "Epoch: [7324/10000] Training loss: 1.2516667381355884e-05 / Validation loss: 0.00016315557849521948 / Long term Validation loss: 0.1621\n",
      "Epoch: [7325/10000] Training loss: 1.251438346342779e-05 / Validation loss: 0.00016324147769605234 / Long term Validation loss: 0.1621\n",
      "Epoch: [7326/10000] Training loss: 1.2512080570450686e-05 / Validation loss: 0.0001631322083019816 / Long term Validation loss: 0.1621\n",
      "Epoch: [7327/10000] Training loss: 1.2509744616893515e-05 / Validation loss: 0.00016316438229605867 / Long term Validation loss: 0.1621\n",
      "Epoch: [7328/10000] Training loss: 1.2507409092881602e-05 / Validation loss: 0.00016315863892351953 / Long term Validation loss: 0.1621\n",
      "Epoch: [7329/10000] Training loss: 1.2505100780683887e-05 / Validation loss: 0.0001630819963294721 / Long term Validation loss: 0.1621\n",
      "Epoch: [7330/10000] Training loss: 1.2502805359925403e-05 / Validation loss: 0.00016314594002540903 / Long term Validation loss: 0.1621\n",
      "Epoch: [7331/10000] Training loss: 1.250049447539002e-05 / Validation loss: 0.00016306010462316083 / Long term Validation loss: 0.1621\n",
      "Epoch: [7332/10000] Training loss: 1.2498165901195335e-05 / Validation loss: 0.00016307378475065908 / Long term Validation loss: 0.1621\n",
      "Epoch: [7333/10000] Training loss: 1.249584133627702e-05 / Validation loss: 0.00016307092452903368 / Long term Validation loss: 0.1621\n",
      "Epoch: [7334/10000] Training loss: 1.2493533211045188e-05 / Validation loss: 0.00016300317877896784 / Long term Validation loss: 0.1621\n",
      "Epoch: [7335/10000] Training loss: 1.2491230297170194e-05 / Validation loss: 0.00016304885328269692 / Long term Validation loss: 0.1621\n",
      "Epoch: [7336/10000] Training loss: 1.2488916931779512e-05 / Validation loss: 0.00016298002781998605 / Long term Validation loss: 0.1621\n",
      "Epoch: [7337/10000] Training loss: 1.2486594081135574e-05 / Validation loss: 0.00016298341117736574 / Long term Validation loss: 0.1621\n",
      "Epoch: [7338/10000] Training loss: 1.2484274832978386e-05 / Validation loss: 0.00016297953010729904 / Long term Validation loss: 0.1621\n",
      "Epoch: [7339/10000] Training loss: 1.2481965205046745e-05 / Validation loss: 0.0001629214502226893 / Long term Validation loss: 0.1621\n",
      "Epoch: [7340/10000] Training loss: 1.2479657865218017e-05 / Validation loss: 0.00016295265353779374 / Long term Validation loss: 0.1621\n",
      "Epoch: [7341/10000] Training loss: 1.2477344061371215e-05 / Validation loss: 0.0001628955894514395 / Long term Validation loss: 0.1621\n",
      "Epoch: [7342/10000] Training loss: 1.2475024915449315e-05 / Validation loss: 0.00016289338095418595 / Long term Validation loss: 0.1621\n",
      "Epoch: [7343/10000] Training loss: 1.2472708076582517e-05 / Validation loss: 0.00016288642105466688 / Long term Validation loss: 0.1621\n",
      "Epoch: [7344/10000] Training loss: 1.2470396831957386e-05 / Validation loss: 0.00016283744705473575 / Long term Validation loss: 0.1621\n",
      "Epoch: [7345/10000] Training loss: 1.2468086873328143e-05 / Validation loss: 0.00016285769027346396 / Long term Validation loss: 0.1621\n",
      "Epoch: [7346/10000] Training loss: 1.246577313190641e-05 / Validation loss: 0.00016280923754850637 / Long term Validation loss: 0.1621\n",
      "Epoch: [7347/10000] Training loss: 1.2463456149851431e-05 / Validation loss: 0.00016280471540474462 / Long term Validation loss: 0.1621\n",
      "Epoch: [7348/10000] Training loss: 1.2461140316450124e-05 / Validation loss: 0.0001627943502436042 / Long term Validation loss: 0.1621\n",
      "Epoch: [7349/10000] Training loss: 1.2458827723004861e-05 / Validation loss: 0.00016275355647004584 / Long term Validation loss: 0.1621\n",
      "Epoch: [7350/10000] Training loss: 1.2456516075478419e-05 / Validation loss: 0.0001627652735114956 / Long term Validation loss: 0.1622\n",
      "Epoch: [7351/10000] Training loss: 1.2454202333186722e-05 / Validation loss: 0.00016272291866326137 / Long term Validation loss: 0.1621\n",
      "Epoch: [7352/10000] Training loss: 1.2451886493478436e-05 / Validation loss: 0.00016271730238925293 / Long term Validation loss: 0.1622\n",
      "Epoch: [7353/10000] Training loss: 1.244957100255008e-05 / Validation loss: 0.00016270330232022622 / Long term Validation loss: 0.1622\n",
      "Epoch: [7354/10000] Training loss: 1.2447257337561153e-05 / Validation loss: 0.00016266933634850265 / Long term Validation loss: 0.1622\n",
      "Epoch: [7355/10000] Training loss: 1.2444944449155584e-05 / Validation loss: 0.00016267418173106555 / Long term Validation loss: 0.1622\n",
      "Epoch: [7356/10000] Training loss: 1.2442630496562795e-05 / Validation loss: 0.00016263648017823611 / Long term Validation loss: 0.1622\n",
      "Epoch: [7357/10000] Training loss: 1.2440315148081637e-05 / Validation loss: 0.00016263039825740845 / Long term Validation loss: 0.1622\n",
      "Epoch: [7358/10000] Training loss: 1.2437999676188568e-05 / Validation loss: 0.0001626131558736626 / Long term Validation loss: 0.1622\n",
      "Epoch: [7359/10000] Training loss: 1.2435685146490714e-05 / Validation loss: 0.0001625848623989805 / Long term Validation loss: 0.1622\n",
      "Epoch: [7360/10000] Training loss: 1.2433371230454564e-05 / Validation loss: 0.000162583879455869 / Long term Validation loss: 0.1622\n",
      "Epoch: [7361/10000] Training loss: 1.2431056856053311e-05 / Validation loss: 0.0001625497878847457 / Long term Validation loss: 0.1622\n",
      "Epoch: [7362/10000] Training loss: 1.2428741569228543e-05 / Validation loss: 0.00016254303922571333 / Long term Validation loss: 0.1622\n",
      "Epoch: [7363/10000] Training loss: 1.2426425932486764e-05 / Validation loss: 0.0001625229079661823 / Long term Validation loss: 0.1622\n",
      "Epoch: [7364/10000] Training loss: 1.2424110674044705e-05 / Validation loss: 0.00016249912994418758 / Long term Validation loss: 0.1622\n",
      "Epoch: [7365/10000] Training loss: 1.242179583911237e-05 / Validation loss: 0.00016249308315661388 / Long term Validation loss: 0.1622\n",
      "Epoch: [7366/10000] Training loss: 1.2419480870960131e-05 / Validation loss: 0.0001624621919413759 / Long term Validation loss: 0.1622\n",
      "Epoch: [7367/10000] Training loss: 1.2417165342408652e-05 / Validation loss: 0.00016245457665087798 / Long term Validation loss: 0.1622\n",
      "Epoch: [7368/10000] Training loss: 1.2414849405945094e-05 / Validation loss: 0.00016243234835499527 / Long term Validation loss: 0.1622\n",
      "Epoch: [7369/10000] Training loss: 1.2412533499982797e-05 / Validation loss: 0.0001624122231609802 / Long term Validation loss: 0.1622\n",
      "Epoch: [7370/10000] Training loss: 1.2410217815784494e-05 / Validation loss: 0.00016240179871393617 / Long term Validation loss: 0.1622\n",
      "Epoch: [7371/10000] Training loss: 1.2407902133987882e-05 / Validation loss: 0.0001623739047449984 / Long term Validation loss: 0.1622\n",
      "Epoch: [7372/10000] Training loss: 1.2405586139356919e-05 / Validation loss: 0.00016236497865536453 / Long term Validation loss: 0.1622\n",
      "Epoch: [7373/10000] Training loss: 1.2403269780601079e-05 / Validation loss: 0.00016234143020599389 / Long term Validation loss: 0.1622\n",
      "Epoch: [7374/10000] Training loss: 1.2400953264068457e-05 / Validation loss: 0.00016232406367632768 / Long term Validation loss: 0.1622\n",
      "Epoch: [7375/10000] Training loss: 1.2398636786241176e-05 / Validation loss: 0.00016230993875308684 / Long term Validation loss: 0.1622\n",
      "Epoch: [7376/10000] Training loss: 1.2396320323067631e-05 / Validation loss: 0.0001622850176721112 / Long term Validation loss: 0.1622\n",
      "Epoch: [7377/10000] Training loss: 1.2394003697382194e-05 / Validation loss: 0.00016227446220136885 / Long term Validation loss: 0.1622\n",
      "Epoch: [7378/10000] Training loss: 1.2391686794736654e-05 / Validation loss: 0.000162250553948375 / Long term Validation loss: 0.1622\n",
      "Epoch: [7379/10000] Training loss: 1.2389369663227086e-05 / Validation loss: 0.00016223513083003663 / Long term Validation loss: 0.1622\n",
      "Epoch: [7380/10000] Training loss: 1.238705243399696e-05 / Validation loss: 0.0001622180557561231 / Long term Validation loss: 0.1622\n",
      "Epoch: [7381/10000] Training loss: 1.2384735165065237e-05 / Validation loss: 0.00016219595745103633 / Long term Validation loss: 0.1622\n",
      "Epoch: [7382/10000] Training loss: 1.2382417796656108e-05 / Validation loss: 0.00016218341858829818 / Long term Validation loss: 0.1622\n",
      "Epoch: [7383/10000] Training loss: 1.2380100234315175e-05 / Validation loss: 0.00016215994146311902 / Long term Validation loss: 0.1622\n",
      "Epoch: [7384/10000] Training loss: 1.2377782446540778e-05 / Validation loss: 0.00016214552248236414 / Long term Validation loss: 0.1622\n",
      "Epoch: [7385/10000] Training loss: 1.2375464485433898e-05 / Validation loss: 0.0001621262936878691 / Long term Validation loss: 0.1622\n",
      "Epoch: [7386/10000] Training loss: 1.237314641340349e-05 / Validation loss: 0.00016210670107514645 / Long term Validation loss: 0.1622\n",
      "Epoch: [7387/10000] Training loss: 1.2370828240287335e-05 / Validation loss: 0.00016209203969188017 / Long term Validation loss: 0.1622\n",
      "Epoch: [7388/10000] Training loss: 1.2368509922272683e-05 / Validation loss: 0.00016206969195930773 / Long term Validation loss: 0.1622\n",
      "Epoch: [7389/10000] Training loss: 1.2366191412879997e-05 / Validation loss: 0.0001620553961101716 / Long term Validation loss: 0.1622\n",
      "Epoch: [7390/10000] Training loss: 1.2363872709268822e-05 / Validation loss: 0.0001620348922829512 / Long term Validation loss: 0.1622\n",
      "Epoch: [7391/10000] Training loss: 1.2361553842195092e-05 / Validation loss: 0.00016201722877775724 / Long term Validation loss: 0.1622\n",
      "Epoch: [7392/10000] Training loss: 1.2359234841007666e-05 / Validation loss: 0.00016200049202115508 / Long term Validation loss: 0.1623\n",
      "Epoch: [7393/10000] Training loss: 1.2356915703489699e-05 / Validation loss: 0.0001619796487696068 / Long term Validation loss: 0.1623\n",
      "Epoch: [7394/10000] Training loss: 1.2354596403334753e-05 / Validation loss: 0.00016196466473528825 / Long term Validation loss: 0.1623\n",
      "Epoch: [7395/10000] Training loss: 1.235227691939142e-05 / Validation loss: 0.00016194375476630447 / Long term Validation loss: 0.1623\n",
      "Epoch: [7396/10000] Training loss: 1.2349957251366074e-05 / Validation loss: 0.00016192726577026484 / Long term Validation loss: 0.1623\n",
      "Epoch: [7397/10000] Training loss: 1.2347637416820873e-05 / Validation loss: 0.00016190883284032516 / Long term Validation loss: 0.1623\n",
      "Epoch: [7398/10000] Training loss: 1.234531742872664e-05 / Validation loss: 0.0001618895871889757 / Long term Validation loss: 0.1623\n",
      "Epoch: [7399/10000] Training loss: 1.234299728462056e-05 / Validation loss: 0.0001618734021788473 / Long term Validation loss: 0.1623\n",
      "Epoch: [7400/10000] Training loss: 1.234067697086416e-05 / Validation loss: 0.00016185286732908868 / Long term Validation loss: 0.1623\n",
      "Epoch: [7401/10000] Training loss: 1.233835647541915e-05 / Validation loss: 0.00016183671581425748 / Long term Validation loss: 0.1623\n",
      "Epoch: [7402/10000] Training loss: 1.2336035798623718e-05 / Validation loss: 0.0001618171997095012 / Long term Validation loss: 0.1623\n",
      "Epoch: [7403/10000] Training loss: 1.2333714947967469e-05 / Validation loss: 0.00016179926224985374 / Long term Validation loss: 0.1623\n",
      "Epoch: [7404/10000] Training loss: 1.2331393930537434e-05 / Validation loss: 0.0001617817075578382 / Long term Validation loss: 0.1623\n",
      "Epoch: [7405/10000] Training loss: 1.2329072745297442e-05 / Validation loss: 0.00016176206488734514 / Long term Validation loss: 0.1623\n",
      "Epoch: [7406/10000] Training loss: 1.2326751384966032e-05 / Validation loss: 0.00016174551915168183 / Long term Validation loss: 0.1623\n",
      "Epoch: [7407/10000] Training loss: 1.2324429843218185e-05 / Validation loss: 0.00016172568161294226 / Long term Validation loss: 0.1623\n",
      "Epoch: [7408/10000] Training loss: 1.2322108118147093e-05 / Validation loss: 0.0001617085095729884 / Long term Validation loss: 0.1623\n",
      "Epoch: [7409/10000] Training loss: 1.2319786213363642e-05 / Validation loss: 0.00016168983706300898 / Long term Validation loss: 0.1623\n",
      "Epoch: [7410/10000] Training loss: 1.2317464132098356e-05 / Validation loss: 0.0001616712400437634 / Long term Validation loss: 0.1623\n",
      "Epoch: [7411/10000] Training loss: 1.2315141874605315e-05 / Validation loss: 0.0001616538426622636 / Long term Validation loss: 0.1623\n",
      "Epoch: [7412/10000] Training loss: 1.2312819437650616e-05 / Validation loss: 0.00016163434384842536 / Long term Validation loss: 0.1623\n",
      "Epoch: [7413/10000] Training loss: 1.2310496816963566e-05 / Validation loss: 0.00016161725892150188 / Long term Validation loss: 0.1623\n",
      "Epoch: [7414/10000] Training loss: 1.2308174010888049e-05 / Validation loss: 0.0001615979984752408 / Long term Validation loss: 0.1623\n",
      "Epoch: [7415/10000] Training loss: 1.2305851019736755e-05 / Validation loss: 0.0001615801960763679 / Long term Validation loss: 0.1623\n",
      "Epoch: [7416/10000] Training loss: 1.230352784535888e-05 / Validation loss: 0.00016156188759654138 / Long term Validation loss: 0.1623\n",
      "Epoch: [7417/10000] Training loss: 1.2301204488153713e-05 / Validation loss: 0.00016154310165406407 / Long term Validation loss: 0.1623\n",
      "Epoch: [7418/10000] Training loss: 1.2298880946816768e-05 / Validation loss: 0.0001615255616106541 / Long term Validation loss: 0.1623\n",
      "Epoch: [7419/10000] Training loss: 1.2296557219183007e-05 / Validation loss: 0.00016150633423136858 / Long term Validation loss: 0.1623\n",
      "Epoch: [7420/10000] Training loss: 1.2294233303198612e-05 / Validation loss: 0.00016148881651812527 / Long term Validation loss: 0.1623\n",
      "Epoch: [7421/10000] Training loss: 1.2291909198499293e-05 / Validation loss: 0.00016146991025646972 / Long term Validation loss: 0.1623\n",
      "Epoch: [7422/10000] Training loss: 1.2289584905216927e-05 / Validation loss: 0.0001614517949288982 / Long term Validation loss: 0.1623\n",
      "Epoch: [7423/10000] Training loss: 1.2287260423853109e-05 / Validation loss: 0.00016143357214522087 / Long term Validation loss: 0.1623\n",
      "Epoch: [7424/10000] Training loss: 1.2284935753982025e-05 / Validation loss: 0.00016141480137409487 / Long term Validation loss: 0.1623\n",
      "Epoch: [7425/10000] Training loss: 1.2282610894443357e-05 / Validation loss: 0.00016139704248801145 / Long term Validation loss: 0.1623\n",
      "Epoch: [7426/10000] Training loss: 1.2280285844002505e-05 / Validation loss: 0.00016137804111115726 / Long term Validation loss: 0.1623\n",
      "Epoch: [7427/10000] Training loss: 1.2277960601524687e-05 / Validation loss: 0.00016136022910804706 / Long term Validation loss: 0.1623\n",
      "Epoch: [7428/10000] Training loss: 1.2275635166778216e-05 / Validation loss: 0.00016134149404777118 / Long term Validation loss: 0.1623\n",
      "Epoch: [7429/10000] Training loss: 1.2273309539558843e-05 / Validation loss: 0.0001613232469477043 / Long term Validation loss: 0.1623\n",
      "Epoch: [7430/10000] Training loss: 1.227098371982404e-05 / Validation loss: 0.0001613049811318021 / Long term Validation loss: 0.1623\n",
      "Epoch: [7431/10000] Training loss: 1.2268657707062947e-05 / Validation loss: 0.00016128628995656311 / Long term Validation loss: 0.1623\n",
      "Epoch: [7432/10000] Training loss: 1.226633150045875e-05 / Validation loss: 0.00016126832663624448 / Long term Validation loss: 0.1623\n",
      "Epoch: [7433/10000] Training loss: 1.2264005099246361e-05 / Validation loss: 0.00016124948075049361 / Long term Validation loss: 0.1623\n",
      "Epoch: [7434/10000] Training loss: 1.2261678502645839e-05 / Validation loss: 0.00016123147772533806 / Long term Validation loss: 0.1623\n",
      "Epoch: [7435/10000] Training loss: 1.2259351710363784e-05 / Validation loss: 0.00016121280531551576 / Long term Validation loss: 0.1623\n",
      "Epoch: [7436/10000] Training loss: 1.2257024722037646e-05 / Validation loss: 0.0001611945105119022 / Long term Validation loss: 0.1623\n",
      "Epoch: [7437/10000] Training loss: 1.2254697537457536e-05 / Validation loss: 0.00016117615394023544 / Long term Validation loss: 0.1623\n",
      "Epoch: [7438/10000] Training loss: 1.225237015617898e-05 / Validation loss: 0.00016115754927190329 / Long term Validation loss: 0.1623\n",
      "Epoch: [7439/10000] Training loss: 1.2250042577637316e-05 / Validation loss: 0.000161139414376599 / Long term Validation loss: 0.1624\n",
      "Epoch: [7440/10000] Training loss: 1.2247714801294032e-05 / Validation loss: 0.00016112067578232954 / Long term Validation loss: 0.1624\n",
      "Epoch: [7441/10000] Training loss: 1.2245386826548417e-05 / Validation loss: 0.00016110254401173046 / Long term Validation loss: 0.1624\n",
      "Epoch: [7442/10000] Training loss: 1.2243058653069199e-05 / Validation loss: 0.0001610838893965594 / Long term Validation loss: 0.1624\n",
      "Epoch: [7443/10000] Training loss: 1.2240730280450002e-05 / Validation loss: 0.00016106558058281043 / Long term Validation loss: 0.1624\n",
      "Epoch: [7444/10000] Training loss: 1.2238401708439191e-05 / Validation loss: 0.00016104712810478233 / Long term Validation loss: 0.1624\n",
      "Epoch: [7445/10000] Training loss: 1.2236072936655518e-05 / Validation loss: 0.00016102860045126593 / Long term Validation loss: 0.1624\n",
      "Epoch: [7446/10000] Training loss: 1.2233743964697152e-05 / Validation loss: 0.00016101032042247897 / Long term Validation loss: 0.1624\n",
      "Epoch: [7447/10000] Training loss: 1.2231414792151087e-05 / Validation loss: 0.00016099166385900485 / Long term Validation loss: 0.1624\n",
      "Epoch: [7448/10000] Training loss: 1.2229085418547572e-05 / Validation loss: 0.00016097342904960736 / Long term Validation loss: 0.1624\n",
      "Epoch: [7449/10000] Training loss: 1.2226755843557617e-05 / Validation loss: 0.00016095478390356282 / Long term Validation loss: 0.1624\n",
      "Epoch: [7450/10000] Training loss: 1.2224426066786976e-05 / Validation loss: 0.00016093646385226902 / Long term Validation loss: 0.1624\n",
      "Epoch: [7451/10000] Training loss: 1.2222096087977536e-05 / Validation loss: 0.00016091793113597738 / Long term Validation loss: 0.1624\n",
      "Epoch: [7452/10000] Training loss: 1.2219765906788681e-05 / Validation loss: 0.00016089946537938454 / Long term Validation loss: 0.1624\n",
      "Epoch: [7453/10000] Training loss: 1.2217435522919737e-05 / Validation loss: 0.00016088106087642918 / Long term Validation loss: 0.1624\n",
      "Epoch: [7454/10000] Training loss: 1.2215104936035985e-05 / Validation loss: 0.00016086247575214805 / Long term Validation loss: 0.1624\n",
      "Epoch: [7455/10000] Training loss: 1.2212774145779122e-05 / Validation loss: 0.00016084414112501157 / Long term Validation loss: 0.1624\n",
      "Epoch: [7456/10000] Training loss: 1.221044315184775e-05 / Validation loss: 0.00016082551593717923 / Long term Validation loss: 0.1624\n",
      "Epoch: [7457/10000] Training loss: 1.2208111953892761e-05 / Validation loss: 0.00016080716581070415 / Long term Validation loss: 0.1624\n",
      "Epoch: [7458/10000] Training loss: 1.220578055166227e-05 / Validation loss: 0.00016078857997237054 / Long term Validation loss: 0.1624\n",
      "Epoch: [7459/10000] Training loss: 1.2203448944846037e-05 / Validation loss: 0.00016077015115280047 / Long term Validation loss: 0.1624\n",
      "Epoch: [7460/10000] Training loss: 1.2201117133200917e-05 / Validation loss: 0.00016075164480173655 / Long term Validation loss: 0.1624\n",
      "Epoch: [7461/10000] Training loss: 1.21987851164413e-05 / Validation loss: 0.00016073312224117208 / Long term Validation loss: 0.1624\n",
      "Epoch: [7462/10000] Training loss: 1.2196452894298137e-05 / Validation loss: 0.00016071468626628643 / Long term Validation loss: 0.1624\n",
      "Epoch: [7463/10000] Training loss: 1.2194120466506615e-05 / Validation loss: 0.00016069609914729494 / Long term Validation loss: 0.1624\n",
      "Epoch: [7464/10000] Training loss: 1.2191787832782667e-05 / Validation loss: 0.00016067769114779643 / Long term Validation loss: 0.1624\n",
      "Epoch: [7465/10000] Training loss: 1.2189454992890952e-05 / Validation loss: 0.00016065908902122164 / Long term Validation loss: 0.1624\n",
      "Epoch: [7466/10000] Training loss: 1.218712194655855e-05 / Validation loss: 0.00016064066038271496 / Long term Validation loss: 0.1624\n",
      "Epoch: [7467/10000] Training loss: 1.2184788693573092e-05 / Validation loss: 0.00016062208609252172 / Long term Validation loss: 0.1624\n",
      "Epoch: [7468/10000] Training loss: 1.2182455233682894e-05 / Validation loss: 0.00016060360479284942 / Long term Validation loss: 0.1624\n",
      "Epoch: [7469/10000] Training loss: 1.2180121566678453e-05 / Validation loss: 0.000160585077537684 / Long term Validation loss: 0.1624\n",
      "Epoch: [7470/10000] Training loss: 1.2177787692328032e-05 / Validation loss: 0.00016056653787882452 / Long term Validation loss: 0.1624\n",
      "Epoch: [7471/10000] Training loss: 1.2175453610413537e-05 / Validation loss: 0.00016054805102141842 / Long term Validation loss: 0.1624\n",
      "Epoch: [7472/10000] Training loss: 1.2173119320723315e-05 / Validation loss: 0.000160529469768434 / Long term Validation loss: 0.1624\n",
      "Epoch: [7473/10000] Training loss: 1.2170784823035454e-05 / Validation loss: 0.0001605109999477136 / Long term Validation loss: 0.1624\n",
      "Epoch: [7474/10000] Training loss: 1.2168450117158592e-05 / Validation loss: 0.00016049240426727776 / Long term Validation loss: 0.1624\n",
      "Epoch: [7475/10000] Training loss: 1.2166115202877403e-05 / Validation loss: 0.00016047392463742414 / Long term Validation loss: 0.1624\n",
      "Epoch: [7476/10000] Training loss: 1.2163780080015558e-05 / Validation loss: 0.00016045533908886766 / Long term Validation loss: 0.1624\n",
      "Epoch: [7477/10000] Training loss: 1.2161444748370794e-05 / Validation loss: 0.00016043683022125076 / Long term Validation loss: 0.1624\n",
      "Epoch: [7478/10000] Training loss: 1.2159109207772953e-05 / Validation loss: 0.00016041826830217636 / Long term Validation loss: 0.1624\n",
      "Epoch: [7479/10000] Training loss: 1.2156773458035031e-05 / Validation loss: 0.000160399723359581 / Long term Validation loss: 0.1624\n",
      "Epoch: [7480/10000] Training loss: 1.2154437498987088e-05 / Validation loss: 0.0001603811855503281 / Long term Validation loss: 0.1624\n",
      "Epoch: [7481/10000] Training loss: 1.2152101330457991e-05 / Validation loss: 0.00016036260956455605 / Long term Validation loss: 0.1624\n",
      "Epoch: [7482/10000] Training loss: 1.214976495227787e-05 / Validation loss: 0.00016034408657589002 / Long term Validation loss: 0.1624\n",
      "Epoch: [7483/10000] Training loss: 1.2147428364290857e-05 / Validation loss: 0.00016032549180827808 / Long term Validation loss: 0.1624\n",
      "Epoch: [7484/10000] Training loss: 1.2145091566330696e-05 / Validation loss: 0.00016030697017348843 / Long term Validation loss: 0.1624\n",
      "Epoch: [7485/10000] Training loss: 1.2142754558254254e-05 / Validation loss: 0.0001602883703166167 / Long term Validation loss: 0.1624\n",
      "Epoch: [7486/10000] Training loss: 1.2140417339902704e-05 / Validation loss: 0.00016026983765622853 / Long term Validation loss: 0.1624\n",
      "Epoch: [7487/10000] Training loss: 1.213807991114175e-05 / Validation loss: 0.0001602512432244834 / Long term Validation loss: 0.1624\n",
      "Epoch: [7488/10000] Training loss: 1.2135742271822623e-05 / Validation loss: 0.00016023269162268505 / Long term Validation loss: 0.1624\n",
      "Epoch: [7489/10000] Training loss: 1.2133404421816234e-05 / Validation loss: 0.00016021410772698417 / Long term Validation loss: 0.1625\n",
      "Epoch: [7490/10000] Training loss: 1.2131066360985202e-05 / Validation loss: 0.00016019553481578524 / Long term Validation loss: 0.1625\n",
      "Epoch: [7491/10000] Training loss: 1.2128728089203704e-05 / Validation loss: 0.00016017696125959397 / Long term Validation loss: 0.1625\n",
      "Epoch: [7492/10000] Training loss: 1.2126389606346061e-05 / Validation loss: 0.0001601583694035363 / Long term Validation loss: 0.1625\n",
      "Epoch: [7493/10000] Training loss: 1.212405091228966e-05 / Validation loss: 0.00016013980224557754 / Long term Validation loss: 0.1625\n",
      "Epoch: [7494/10000] Training loss: 1.212171200691987e-05 / Validation loss: 0.00016012119663667647 / Long term Validation loss: 0.1625\n",
      "Epoch: [7495/10000] Training loss: 1.2119372890118345e-05 / Validation loss: 0.00016010263022885298 / Long term Validation loss: 0.1625\n",
      "Epoch: [7496/10000] Training loss: 1.211703356178011e-05 / Validation loss: 0.00016008401679025135 / Long term Validation loss: 0.1625\n",
      "Epoch: [7497/10000] Training loss: 1.2114694021792543e-05 / Validation loss: 0.00016006544558440044 / Long term Validation loss: 0.1625\n",
      "Epoch: [7498/10000] Training loss: 1.211235427005855e-05 / Validation loss: 0.00016004682936819592 / Long term Validation loss: 0.1625\n",
      "Epoch: [7499/10000] Training loss: 1.2110014306472491e-05 / Validation loss: 0.00016002824913915244 / Long term Validation loss: 0.1625\n",
      "Epoch: [7500/10000] Training loss: 1.2107674130943523e-05 / Validation loss: 0.00016000963350054317 / Long term Validation loss: 0.1625\n",
      "Epoch: [7501/10000] Training loss: 1.2105333743373815e-05 / Validation loss: 0.00015999104188504516 / Long term Validation loss: 0.1625\n",
      "Epoch: [7502/10000] Training loss: 1.210299314367763e-05 / Validation loss: 0.00015997242834317298 / Long term Validation loss: 0.1625\n",
      "Epoch: [7503/10000] Training loss: 1.2100652331765368e-05 / Validation loss: 0.00015995382476983918 / Long term Validation loss: 0.1625\n",
      "Epoch: [7504/10000] Training loss: 1.2098311307555792e-05 / Validation loss: 0.00015993521329869825 / Long term Validation loss: 0.1625\n",
      "Epoch: [7505/10000] Training loss: 1.2095970070967477e-05 / Validation loss: 0.0001599165985226681 / Long term Validation loss: 0.1625\n",
      "Epoch: [7506/10000] Training loss: 1.2093628621923564e-05 / Validation loss: 0.00015989798803965738 / Long term Validation loss: 0.1625\n",
      "Epoch: [7507/10000] Training loss: 1.2091286960350483e-05 / Validation loss: 0.00015987936355351187 / Long term Validation loss: 0.1625\n",
      "Epoch: [7508/10000] Training loss: 1.208894508617587e-05 / Validation loss: 0.00015986075245159382 / Long term Validation loss: 0.1625\n",
      "Epoch: [7509/10000] Training loss: 1.2086602999333512e-05 / Validation loss: 0.00015984211998064127 / Long term Validation loss: 0.1625\n",
      "Epoch: [7510/10000] Training loss: 1.2084260699755835e-05 / Validation loss: 0.00015982350659231553 / Long term Validation loss: 0.1625\n",
      "Epoch: [7511/10000] Training loss: 1.208191818738331e-05 / Validation loss: 0.00015980486775084646 / Long term Validation loss: 0.1625\n",
      "Epoch: [7512/10000] Training loss: 1.2079575462153467e-05 / Validation loss: 0.00015978625065970497 / Long term Validation loss: 0.1625\n",
      "Epoch: [7513/10000] Training loss: 1.2077232524012885e-05 / Validation loss: 0.00015976760675022935 / Long term Validation loss: 0.1625\n",
      "Epoch: [7514/10000] Training loss: 1.2074889372904427e-05 / Validation loss: 0.00015974898492463477 / Long term Validation loss: 0.1625\n",
      "Epoch: [7515/10000] Training loss: 1.2072546008780297e-05 / Validation loss: 0.0001597303368470177 / Long term Validation loss: 0.1625\n",
      "Epoch: [7516/10000] Training loss: 1.2070202431588785e-05 / Validation loss: 0.00015971170964215801 / Long term Validation loss: 0.1625\n",
      "Epoch: [7517/10000] Training loss: 1.206785864128734e-05 / Validation loss: 0.0001596930578968151 / Long term Validation loss: 0.1625\n",
      "Epoch: [7518/10000] Training loss: 1.2065514637829734e-05 / Validation loss: 0.00015967442499943853 / Long term Validation loss: 0.1625\n",
      "Epoch: [7519/10000] Training loss: 1.2063170421178373e-05 / Validation loss: 0.00015965576976280035 / Long term Validation loss: 0.1625\n",
      "Epoch: [7520/10000] Training loss: 1.206082599129245e-05 / Validation loss: 0.00015963713112455917 / Long term Validation loss: 0.1625\n",
      "Epoch: [7521/10000] Training loss: 1.205848134813916e-05 / Validation loss: 0.00015961847235140033 / Long term Validation loss: 0.1625\n",
      "Epoch: [7522/10000] Training loss: 1.2056136491682997e-05 / Validation loss: 0.00015959982811827522 / Long term Validation loss: 0.1625\n",
      "Epoch: [7523/10000] Training loss: 1.20537914218958e-05 / Validation loss: 0.00015958116562479147 / Long term Validation loss: 0.1625\n",
      "Epoch: [7524/10000] Training loss: 1.2051446138747224e-05 / Validation loss: 0.0001595625160638067 / Long term Validation loss: 0.1625\n",
      "Epoch: [7525/10000] Training loss: 1.2049100642213684e-05 / Validation loss: 0.0001595438495761022 / Long term Validation loss: 0.1625\n",
      "Epoch: [7526/10000] Training loss: 1.2046754932269792e-05 / Validation loss: 0.000159525195016323 / Long term Validation loss: 0.1625\n",
      "Epoch: [7527/10000] Training loss: 1.2044409008896501e-05 / Validation loss: 0.0001595065241973498 / Long term Validation loss: 0.1625\n",
      "Epoch: [7528/10000] Training loss: 1.2042062872073154e-05 / Validation loss: 0.0001594878650043711 / Long term Validation loss: 0.1625\n",
      "Epoch: [7529/10000] Training loss: 1.2039716521785223e-05 / Validation loss: 0.00015946918947080832 / Long term Validation loss: 0.1625\n",
      "Epoch: [7530/10000] Training loss: 1.2037369958016589e-05 / Validation loss: 0.00015945052605479667 / Long term Validation loss: 0.1625\n",
      "Epoch: [7531/10000] Training loss: 1.203502318075722e-05 / Validation loss: 0.00015943184537700318 / Long term Validation loss: 0.1625\n",
      "Epoch: [7532/10000] Training loss: 1.2032676189995308e-05 / Validation loss: 0.00015941317821925054 / Long term Validation loss: 0.1625\n",
      "Epoch: [7533/10000] Training loss: 1.203032898572533e-05 / Validation loss: 0.00015939449189128583 / Long term Validation loss: 0.1625\n",
      "Epoch: [7534/10000] Training loss: 1.2027981567939557e-05 / Validation loss: 0.0001593758215822317 / Long term Validation loss: 0.1625\n",
      "Epoch: [7535/10000] Training loss: 1.2025633936637018e-05 / Validation loss: 0.00015935712895951407 / Long term Validation loss: 0.1625\n",
      "Epoch: [7536/10000] Training loss: 1.2023286091813768e-05 / Validation loss: 0.00015933845626038535 / Long term Validation loss: 0.1625\n",
      "Epoch: [7537/10000] Training loss: 1.2020938033473467e-05 / Validation loss: 0.00015931975646865234 / Long term Validation loss: 0.1625\n",
      "Epoch: [7538/10000] Training loss: 1.2018589761615696e-05 / Validation loss: 0.0001593010824176937 / Long term Validation loss: 0.1625\n",
      "Epoch: [7539/10000] Training loss: 1.2016241276248871e-05 / Validation loss: 0.00015928237422402334 / Long term Validation loss: 0.1626\n",
      "Epoch: [7540/10000] Training loss: 1.2013892577375721e-05 / Validation loss: 0.0001592637003062146 / Long term Validation loss: 0.1626\n",
      "Epoch: [7541/10000] Training loss: 1.2011543665009676e-05 / Validation loss: 0.00015924498191974393 / Long term Validation loss: 0.1626\n",
      "Epoch: [7542/10000] Training loss: 1.200919453915607e-05 / Validation loss: 0.00015922631032538697 / Long term Validation loss: 0.1626\n",
      "Epoch: [7543/10000] Training loss: 1.2006845199833837e-05 / Validation loss: 0.0001592075790758554 / Long term Validation loss: 0.1626\n",
      "Epoch: [7544/10000] Training loss: 1.2004495647050272e-05 / Validation loss: 0.0001591889131019543 / Long term Validation loss: 0.1626\n",
      "Epoch: [7545/10000] Training loss: 1.2002145880830466e-05 / Validation loss: 0.0001591701649228962 / Long term Validation loss: 0.1626\n",
      "Epoch: [7546/10000] Training loss: 1.199979590118276e-05 / Validation loss: 0.00015915150961950646 / Long term Validation loss: 0.1626\n",
      "Epoch: [7547/10000] Training loss: 1.19974457081396e-05 / Validation loss: 0.000159132738217087 / Long term Validation loss: 0.1626\n",
      "Epoch: [7548/10000] Training loss: 1.1995095301709091e-05 / Validation loss: 0.00015911410144832424 / Long term Validation loss: 0.1626\n",
      "Epoch: [7549/10000] Training loss: 1.1992744681932995e-05 / Validation loss: 0.0001590952969436227 / Long term Validation loss: 0.1626\n",
      "Epoch: [7550/10000] Training loss: 1.1990393848817416e-05 / Validation loss: 0.00015907669114258077 / Long term Validation loss: 0.1626\n",
      "Epoch: [7551/10000] Training loss: 1.1988042802416934e-05 / Validation loss: 0.00015905783781389838 / Long term Validation loss: 0.1626\n",
      "Epoch: [7552/10000] Training loss: 1.198569154273339e-05 / Validation loss: 0.00015903928291186975 / Long term Validation loss: 0.1626\n",
      "Epoch: [7553/10000] Training loss: 1.1983340069840601e-05 / Validation loss: 0.0001590203553915427 / Long term Validation loss: 0.1626\n",
      "Epoch: [7554/10000] Training loss: 1.1980988383733928e-05 / Validation loss: 0.00015900188376948907 / Long term Validation loss: 0.1626\n",
      "Epoch: [7555/10000] Training loss: 1.197863648451909e-05 / Validation loss: 0.00015898284056956675 / Long term Validation loss: 0.1626\n",
      "Epoch: [7556/10000] Training loss: 1.197628437218508e-05 / Validation loss: 0.00015896450553600706 / Long term Validation loss: 0.1626\n",
      "Epoch: [7557/10000] Training loss: 1.1973932046896543e-05 / Validation loss: 0.0001589452779045764 / Long term Validation loss: 0.1626\n",
      "Epoch: [7558/10000] Training loss: 1.1971579508646896e-05 / Validation loss: 0.0001589271683821581 / Long term Validation loss: 0.1626\n",
      "Epoch: [7559/10000] Training loss: 1.1969226757722324e-05 / Validation loss: 0.00015890764089647263 / Long term Validation loss: 0.1626\n",
      "Epoch: [7560/10000] Training loss: 1.1966873794169226e-05 / Validation loss: 0.00015888990715092995 / Long term Validation loss: 0.1626\n",
      "Epoch: [7561/10000] Training loss: 1.1964520618553135e-05 / Validation loss: 0.00015886988352375658 / Long term Validation loss: 0.1626\n",
      "Epoch: [7562/10000] Training loss: 1.1962167231146238e-05 / Validation loss: 0.00015885278275031014 / Long term Validation loss: 0.1626\n",
      "Epoch: [7563/10000] Training loss: 1.195981363321991e-05 / Validation loss: 0.00015883192489459105 / Long term Validation loss: 0.1626\n",
      "Epoch: [7564/10000] Training loss: 1.1957459825844821e-05 / Validation loss: 0.00015881590291257438 / Long term Validation loss: 0.1626\n",
      "Epoch: [7565/10000] Training loss: 1.195510581222093e-05 / Validation loss: 0.0001587936211259477 / Long term Validation loss: 0.1626\n",
      "Epoch: [7566/10000] Training loss: 1.1952751596074825e-05 / Validation loss: 0.00015877946044671174 / Long term Validation loss: 0.1626\n",
      "Epoch: [7567/10000] Training loss: 1.1950397186210683e-05 / Validation loss: 0.0001587547132838429 / Long term Validation loss: 0.1626\n",
      "Epoch: [7568/10000] Training loss: 1.1948042595038321e-05 / Validation loss: 0.00015874380449814996 / Long term Validation loss: 0.1626\n",
      "Epoch: [7569/10000] Training loss: 1.1945687848452518e-05 / Validation loss: 0.0001587147299906771 / Long term Validation loss: 0.1626\n",
      "Epoch: [7570/10000] Training loss: 1.194333298732351e-05 / Validation loss: 0.0001587095747482644 / Long term Validation loss: 0.1626\n",
      "Epoch: [7571/10000] Training loss: 1.1940978091731355e-05 / Validation loss: 0.00015867280335050377 / Long term Validation loss: 0.1626\n",
      "Epoch: [7572/10000] Training loss: 1.193862329699386e-05 / Validation loss: 0.00015867795696465246 / Long term Validation loss: 0.1626\n",
      "Epoch: [7573/10000] Training loss: 1.1936268860694754e-05 / Validation loss: 0.0001586273176273906 / Long term Validation loss: 0.1626\n",
      "Epoch: [7574/10000] Training loss: 1.1933915237140344e-05 / Validation loss: 0.00015865117527066313 / Long term Validation loss: 0.1626\n",
      "Epoch: [7575/10000] Training loss: 1.193156328239776e-05 / Validation loss: 0.00015857523262543524 / Long term Validation loss: 0.1626\n",
      "Epoch: [7576/10000] Training loss: 1.1929214550201712e-05 / Validation loss: 0.0001586334521325122 / Long term Validation loss: 0.1626\n",
      "Epoch: [7577/10000] Training loss: 1.1926871968549376e-05 / Validation loss: 0.00015851077039782455 / Long term Validation loss: 0.1626\n",
      "Epoch: [7578/10000] Training loss: 1.1924540963375098e-05 / Validation loss: 0.00015863290642688277 / Long term Validation loss: 0.1626\n",
      "Epoch: [7579/10000] Training loss: 1.1922231816747727e-05 / Validation loss: 0.00015842285249463185 / Long term Validation loss: 0.1626\n",
      "Epoch: [7580/10000] Training loss: 1.1919963899557479e-05 / Validation loss: 0.00015866536920644783 / Long term Validation loss: 0.1626\n",
      "Epoch: [7581/10000] Training loss: 1.1917774229613348e-05 / Validation loss: 0.00015829009587450108 / Long term Validation loss: 0.1626\n",
      "Epoch: [7582/10000] Training loss: 1.1915733539204391e-05 / Validation loss: 0.00015876222500516213 / Long term Validation loss: 0.1626\n",
      "Epoch: [7583/10000] Training loss: 1.1913978336159478e-05 / Validation loss: 0.0001580711306289321 / Long term Validation loss: 0.1626\n",
      "Epoch: [7584/10000] Training loss: 1.1912772819806226e-05 / Validation loss: 0.00015898704188103093 / Long term Validation loss: 0.1627\n",
      "Epoch: [7585/10000] Training loss: 1.1912632258076476e-05 / Validation loss: 0.000157686435703776 / Long term Validation loss: 0.1626\n",
      "Epoch: [7586/10000] Training loss: 1.1914566004421102e-05 / Validation loss: 0.00015947256255400942 / Long term Validation loss: 0.1626\n",
      "Epoch: [7587/10000] Training loss: 1.1920563738797267e-05 / Validation loss: 0.00015698710485898573 / Long term Validation loss: 0.1625\n",
      "Epoch: [7588/10000] Training loss: 1.193456819701033e-05 / Validation loss: 0.0001605090815356632 / Long term Validation loss: 0.1628\n",
      "Epoch: [7589/10000] Training loss: 1.1964432837622814e-05 / Validation loss: 0.00015571479880835501 / Long term Validation loss: 0.1626\n",
      "Epoch: [7590/10000] Training loss: 1.2025889411745234e-05 / Validation loss: 0.00016277748120972874 / Long term Validation loss: 0.1629\n",
      "Epoch: [7591/10000] Training loss: 1.2150564014731087e-05 / Validation loss: 0.00015352062667933633 / Long term Validation loss: 0.1624\n",
      "Epoch: [7592/10000] Training loss: 1.2402372431079658e-05 / Validation loss: 0.00016803818288356595 / Long term Validation loss: 0.1636\n",
      "Epoch: [7593/10000] Training loss: 1.291043680640632e-05 / Validation loss: 0.00015043437017778934 / Long term Validation loss: 0.1623\n",
      "Epoch: [7594/10000] Training loss: 1.3936046585800432e-05 / Validation loss: 0.00018136539825992155 / Long term Validation loss: 0.1577\n",
      "Epoch: [7595/10000] Training loss: 1.6000852670027676e-05 / Validation loss: 0.00014955708578179408 / Long term Validation loss: 0.1549\n",
      "Epoch: [7596/10000] Training loss: 2.0128620882823793e-05 / Validation loss: 0.0002182017810087681 / Long term Validation loss: 0.1550\n",
      "Epoch: [7597/10000] Training loss: 2.8198875660512237e-05 / Validation loss: 0.00016721826083953106 / Long term Validation loss: 0.1486\n",
      "Epoch: [7598/10000] Training loss: 4.3268132486741446e-05 / Validation loss: 0.00031544503190961154 / Long term Validation loss: 0.1601\n",
      "Epoch: [7599/10000] Training loss: 6.854983778093878e-05 / Validation loss: 0.00023608491012290096 / Long term Validation loss: 0.1671\n",
      "Epoch: [7600/10000] Training loss: 0.00010242813769159058 / Validation loss: 0.0004434470663663923 / Long term Validation loss: 0.1661\n",
      "Epoch: [7601/10000] Training loss: 0.00012694660973152356 / Validation loss: 0.0002457205608899561 / Long term Validation loss: 0.1675\n",
      "Epoch: [7602/10000] Training loss: 0.00011068137111064316 / Validation loss: 0.00028556500704746623 / Long term Validation loss: 0.1564\n",
      "Epoch: [7603/10000] Training loss: 5.2993894403407245e-05 / Validation loss: 0.0001560162041240897 / Long term Validation loss: 0.1614\n",
      "Epoch: [7604/10000] Training loss: 1.2922968020562801e-05 / Validation loss: 0.00015833248892183825 / Long term Validation loss: 0.1492\n",
      "Epoch: [7605/10000] Training loss: 3.0113675010727467e-05 / Validation loss: 0.0003186116713482883 / Long term Validation loss: 0.1581\n",
      "Epoch: [7606/10000] Training loss: 6.519437462843187e-05 / Validation loss: 0.00018766023397062204 / Long term Validation loss: 0.1516\n",
      "Epoch: [7607/10000] Training loss: 5.968022442404887e-05 / Validation loss: 0.00021417337019633608 / Long term Validation loss: 0.1531\n",
      "Epoch: [7608/10000] Training loss: 2.3060963716520038e-05 / Validation loss: 0.0001843104020924371 / Long term Validation loss: 0.1585\n",
      "Epoch: [7609/10000] Training loss: 1.4401513921053276e-05 / Validation loss: 0.00016755617152464634 / Long term Validation loss: 0.1477\n",
      "Epoch: [7610/10000] Training loss: 3.8416597482701626e-05 / Validation loss: 0.0002714349156105055 / Long term Validation loss: 0.1532\n",
      "Epoch: [7611/10000] Training loss: 4.465011139027233e-05 / Validation loss: 0.0001562717140051761 / Long term Validation loss: 0.1554\n",
      "Epoch: [7612/10000] Training loss: 2.1985968746782053e-05 / Validation loss: 0.0001596469482062476 / Long term Validation loss: 0.1619\n",
      "Epoch: [7613/10000] Training loss: 1.2835697817021392e-05 / Validation loss: 0.00022704663474243698 / Long term Validation loss: 0.1517\n",
      "Epoch: [7614/10000] Training loss: 2.832948038359087e-05 / Validation loss: 0.00016266340769989694 / Long term Validation loss: 0.1465\n",
      "Epoch: [7615/10000] Training loss: 3.297298766379389e-05 / Validation loss: 0.00019398778876757628 / Long term Validation loss: 0.1552\n",
      "Epoch: [7616/10000] Training loss: 1.784740768428871e-05 / Validation loss: 0.0001727721594610088 / Long term Validation loss: 0.1641\n",
      "Epoch: [7617/10000] Training loss: 1.2865788195031622e-05 / Validation loss: 0.00015599590968805579 / Long term Validation loss: 0.1499\n",
      "Epoch: [7618/10000] Training loss: 2.3625941145200386e-05 / Validation loss: 0.00021284679923187636 / Long term Validation loss: 0.1531\n",
      "Epoch: [7619/10000] Training loss: 2.5093543478261765e-05 / Validation loss: 0.0001535108749600239 / Long term Validation loss: 0.1596\n",
      "Epoch: [7620/10000] Training loss: 1.4683156983098121e-05 / Validation loss: 0.00015441152438117422 / Long term Validation loss: 0.1619\n",
      "Epoch: [7621/10000] Training loss: 1.318271754289224e-05 / Validation loss: 0.0001967718069537486 / Long term Validation loss: 0.1548\n",
      "Epoch: [7622/10000] Training loss: 2.0562446717268574e-05 / Validation loss: 0.00015185755101065546 / Long term Validation loss: 0.1506\n",
      "Epoch: [7623/10000] Training loss: 1.984529279529716e-05 / Validation loss: 0.00016794204605617282 / Long term Validation loss: 0.1645\n",
      "Epoch: [7624/10000] Training loss: 1.2931750270427354e-05 / Validation loss: 0.0001701168233925284 / Long term Validation loss: 0.1648\n",
      "Epoch: [7625/10000] Training loss: 1.3454694735932337e-05 / Validation loss: 0.00015055191151678595 / Long term Validation loss: 0.1522\n",
      "Epoch: [7626/10000] Training loss: 1.8192711942175275e-05 / Validation loss: 0.00018114010341543037 / Long term Validation loss: 0.1581\n",
      "Epoch: [7627/10000] Training loss: 1.6456357096239002e-05 / Validation loss: 0.00015457970424115024 / Long term Validation loss: 0.1633\n",
      "Epoch: [7628/10000] Training loss: 1.2165154931138182e-05 / Validation loss: 0.00015123540646343986 / Long term Validation loss: 0.1598\n",
      "Epoch: [7629/10000] Training loss: 1.3512140963703657e-05 / Validation loss: 0.0001802315102940872 / Long term Validation loss: 0.1587\n",
      "Epoch: [7630/10000] Training loss: 1.6292446849799807e-05 / Validation loss: 0.00015024709132703464 / Long term Validation loss: 0.1586\n",
      "Epoch: [7631/10000] Training loss: 1.4376930829458482e-05 / Validation loss: 0.0001586033467684409 / Long term Validation loss: 0.1645\n",
      "Epoch: [7632/10000] Training loss: 1.1914513277754207e-05 / Validation loss: 0.00016892585169444547 / Long term Validation loss: 0.1655\n",
      "Epoch: [7633/10000] Training loss: 1.3371015360784015e-05 / Validation loss: 0.00014979674820157112 / Long term Validation loss: 0.1583\n",
      "Epoch: [7634/10000] Training loss: 1.4840508047570271e-05 / Validation loss: 0.00016824295201860786 / Long term Validation loss: 0.1654\n",
      "Epoch: [7635/10000] Training loss: 1.3166174607144824e-05 / Validation loss: 0.00015831230028673883 / Long term Validation loss: 0.1641\n",
      "Epoch: [7636/10000] Training loss: 1.1877121744764194e-05 / Validation loss: 0.00015164118958077184 / Long term Validation loss: 0.1614\n",
      "Epoch: [7637/10000] Training loss: 1.3119610188123425e-05 / Validation loss: 0.00017189176978908864 / Long term Validation loss: 0.1639\n",
      "Epoch: [7638/10000] Training loss: 1.3794543932170888e-05 / Validation loss: 0.00015326142207957604 / Long term Validation loss: 0.1624\n",
      "Epoch: [7639/10000] Training loss: 1.2503468811726948e-05 / Validation loss: 0.00015688841538612227 / Long term Validation loss: 0.1630\n",
      "Epoch: [7640/10000] Training loss: 1.1898787766474295e-05 / Validation loss: 0.00016775846946092565 / Long term Validation loss: 0.1650\n",
      "Epoch: [7641/10000] Training loss: 1.2835025873706638e-05 / Validation loss: 0.00015207072940115913 / Long term Validation loss: 0.1614\n",
      "Epoch: [7642/10000] Training loss: 1.3080898753202508e-05 / Validation loss: 0.00016344155964720432 / Long term Validation loss: 0.1637\n",
      "Epoch: [7643/10000] Training loss: 1.2158762786030126e-05 / Validation loss: 0.0001608090441312134 / Long term Validation loss: 0.1637\n",
      "Epoch: [7644/10000] Training loss: 1.1914985605198765e-05 / Validation loss: 0.00015319792133162491 / Long term Validation loss: 0.1622\n",
      "Epoch: [7645/10000] Training loss: 1.2569714809609059e-05 / Validation loss: 0.00016685700579264175 / Long term Validation loss: 0.1641\n",
      "Epoch: [7646/10000] Training loss: 1.2612241398211613e-05 / Validation loss: 0.00015587133513401394 / Long term Validation loss: 0.1623\n",
      "Epoch: [7647/10000] Training loss: 1.1984842078852358e-05 / Validation loss: 0.0001565834208717034 / Long term Validation loss: 0.1623\n",
      "Epoch: [7648/10000] Training loss: 1.1910431327696517e-05 / Validation loss: 0.00016519027833504737 / Long term Validation loss: 0.1635\n",
      "Epoch: [7649/10000] Training loss: 1.2348226643951807e-05 / Validation loss: 0.0001539344991060229 / Long term Validation loss: 0.1625\n",
      "Epoch: [7650/10000] Training loss: 1.2313438795093834e-05 / Validation loss: 0.0001608131926551279 / Long term Validation loss: 0.1631\n",
      "Epoch: [7651/10000] Training loss: 1.1897943984534377e-05 / Validation loss: 0.00016065140795981414 / Long term Validation loss: 0.1630\n",
      "Epoch: [7652/10000] Training loss: 1.1890718253557423e-05 / Validation loss: 0.00015422807146113932 / Long term Validation loss: 0.1621\n",
      "Epoch: [7653/10000] Training loss: 1.2177125298048659e-05 / Validation loss: 0.0001631527665834846 / Long term Validation loss: 0.1628\n",
      "Epoch: [7654/10000] Training loss: 1.212553314487864e-05 / Validation loss: 0.00015662228579277704 / Long term Validation loss: 0.1621\n",
      "Epoch: [7655/10000] Training loss: 1.1853216434649984e-05 / Validation loss: 0.00015630309761524902 / Long term Validation loss: 0.1620\n",
      "Epoch: [7656/10000] Training loss: 1.186507778927908e-05 / Validation loss: 0.00016227142519846996 / Long term Validation loss: 0.1627\n",
      "Epoch: [7657/10000] Training loss: 1.2051317803020996e-05 / Validation loss: 0.00015472067440185295 / Long term Validation loss: 0.1618\n",
      "Epoch: [7658/10000] Training loss: 1.2007084433875173e-05 / Validation loss: 0.00015911633446349797 / Long term Validation loss: 0.1624\n",
      "Epoch: [7659/10000] Training loss: 1.1828125811521677e-05 / Validation loss: 0.00015937985562398615 / Long term Validation loss: 0.1625\n",
      "Epoch: [7660/10000] Training loss: 1.1839413199015945e-05 / Validation loss: 0.00015483794913586206 / Long term Validation loss: 0.1618\n",
      "Epoch: [7661/10000] Training loss: 1.1960848619481267e-05 / Validation loss: 0.0001607568380241631 / Long term Validation loss: 0.1627\n",
      "Epoch: [7662/10000] Training loss: 1.1930391061225351e-05 / Validation loss: 0.0001566081952211448 / Long term Validation loss: 0.1621\n",
      "Epoch: [7663/10000] Training loss: 1.1811752193223709e-05 / Validation loss: 0.00015636908908408593 / Long term Validation loss: 0.1621\n",
      "Epoch: [7664/10000] Training loss: 1.1816553850562818e-05 / Validation loss: 0.00016022772782751764 / Long term Validation loss: 0.1626\n",
      "Epoch: [7665/10000] Training loss: 1.1896043426369457e-05 / Validation loss: 0.00015524486618575303 / Long term Validation loss: 0.1618\n",
      "Epoch: [7666/10000] Training loss: 1.1878565449665585e-05 / Validation loss: 0.00015834718813454674 / Long term Validation loss: 0.1623\n",
      "Epoch: [7667/10000] Training loss: 1.1799299578285873e-05 / Validation loss: 0.00015834326997002078 / Long term Validation loss: 0.1623\n",
      "Epoch: [7668/10000] Training loss: 1.1797561218828052e-05 / Validation loss: 0.00015546358046844403 / Long term Validation loss: 0.1620\n",
      "Epoch: [7669/10000] Training loss: 1.1849535991268828e-05 / Validation loss: 0.00015944987709568906 / Long term Validation loss: 0.1625\n",
      "Epoch: [7670/10000] Training loss: 1.1841900612704299e-05 / Validation loss: 0.00015656249576408328 / Long term Validation loss: 0.1621\n",
      "Epoch: [7671/10000] Training loss: 1.1788840791045132e-05 / Validation loss: 0.00015672663284397296 / Long term Validation loss: 0.1622\n",
      "Epoch: [7672/10000] Training loss: 1.1782331833838217e-05 / Validation loss: 0.0001590256562624356 / Long term Validation loss: 0.1625\n",
      "Epoch: [7673/10000] Training loss: 1.181580973155217e-05 / Validation loss: 0.00015575401411691345 / Long term Validation loss: 0.1621\n",
      "Epoch: [7674/10000] Training loss: 1.1814676502749045e-05 / Validation loss: 0.00015809544106668786 / Long term Validation loss: 0.1624\n",
      "Epoch: [7675/10000] Training loss: 1.1779406274422228e-05 / Validation loss: 0.00015768503534958626 / Long term Validation loss: 0.1623\n",
      "Epoch: [7676/10000] Training loss: 1.1770123148787345e-05 / Validation loss: 0.000156051827638318 / Long term Validation loss: 0.1621\n",
      "Epoch: [7677/10000] Training loss: 1.179083698703829e-05 / Validation loss: 0.00015866458284180066 / Long term Validation loss: 0.1624\n",
      "Epoch: [7678/10000] Training loss: 1.1793356792916855e-05 / Validation loss: 0.0001565081164285894 / Long term Validation loss: 0.1622\n",
      "Epoch: [7679/10000] Training loss: 1.177036361134024e-05 / Validation loss: 0.00015702711467333657 / Long term Validation loss: 0.1623\n",
      "Epoch: [7680/10000] Training loss: 1.1760014857203103e-05 / Validation loss: 0.0001581813762952504 / Long term Validation loss: 0.1624\n",
      "Epoch: [7681/10000] Training loss: 1.177177977295878e-05 / Validation loss: 0.0001560999811874347 / Long term Validation loss: 0.1621\n",
      "Epoch: [7682/10000] Training loss: 1.1775821560272905e-05 / Validation loss: 0.00015789385850374066 / Long term Validation loss: 0.1624\n",
      "Epoch: [7683/10000] Training loss: 1.1761364667615401e-05 / Validation loss: 0.00015718342291722433 / Long term Validation loss: 0.1623\n",
      "Epoch: [7684/10000] Training loss: 1.1751251183056058e-05 / Validation loss: 0.00015645515808825373 / Long term Validation loss: 0.1622\n",
      "Epoch: [7685/10000] Training loss: 1.1756772947523217e-05 / Validation loss: 0.00015804802877714784 / Long term Validation loss: 0.1623\n",
      "Epoch: [7686/10000] Training loss: 1.1760865133790372e-05 / Validation loss: 0.00015643355109858163 / Long term Validation loss: 0.1622\n",
      "Epoch: [7687/10000] Training loss: 1.1752276171956908e-05 / Validation loss: 0.00015716412645605806 / Long term Validation loss: 0.1623\n",
      "Epoch: [7688/10000] Training loss: 1.1743274303986683e-05 / Validation loss: 0.00015752130446579466 / Long term Validation loss: 0.1623\n",
      "Epoch: [7689/10000] Training loss: 1.1744577716184187e-05 / Validation loss: 0.0001563203347612866 / Long term Validation loss: 0.1622\n",
      "Epoch: [7690/10000] Training loss: 1.1747801870098067e-05 / Validation loss: 0.00015764385592276583 / Long term Validation loss: 0.1623\n",
      "Epoch: [7691/10000] Training loss: 1.1743076847952036e-05 / Validation loss: 0.00015682064367014033 / Long term Validation loss: 0.1622\n",
      "Epoch: [7692/10000] Training loss: 1.1735653990867193e-05 / Validation loss: 0.00015672184429634423 / Long term Validation loss: 0.1622\n",
      "Epoch: [7693/10000] Training loss: 1.1734298616563143e-05 / Validation loss: 0.00015755628505677142 / Long term Validation loss: 0.1623\n",
      "Epoch: [7694/10000] Training loss: 1.1736190984608298e-05 / Validation loss: 0.00015643638225801707 / Long term Validation loss: 0.1622\n",
      "Epoch: [7695/10000] Training loss: 1.1733792387286619e-05 / Validation loss: 0.00015721520803049416 / Long term Validation loss: 0.1623\n",
      "Epoch: [7696/10000] Training loss: 1.1728069874322532e-05 / Validation loss: 0.00015707466543917565 / Long term Validation loss: 0.1622\n",
      "Epoch: [7697/10000] Training loss: 1.1725261965813029e-05 / Validation loss: 0.00015653163348002225 / Long term Validation loss: 0.1622\n",
      "Epoch: [7698/10000] Training loss: 1.1725725062204431e-05 / Validation loss: 0.00015740036977577874 / Long term Validation loss: 0.1623\n",
      "Epoch: [7699/10000] Training loss: 1.1724497065675141e-05 / Validation loss: 0.00015663540063899683 / Long term Validation loss: 0.1622\n",
      "Epoch: [7700/10000] Training loss: 1.1720328475509979e-05 / Validation loss: 0.00015690557778846707 / Long term Validation loss: 0.1622\n",
      "Epoch: [7701/10000] Training loss: 1.1716964975629677e-05 / Validation loss: 0.00015718259775438463 / Long term Validation loss: 0.1623\n",
      "Epoch: [7702/10000] Training loss: 1.171617262514634e-05 / Validation loss: 0.00015652228968342626 / Long term Validation loss: 0.1622\n",
      "Epoch: [7703/10000] Training loss: 1.171530136039592e-05 / Validation loss: 0.00015718578927683704 / Long term Validation loss: 0.1623\n",
      "Epoch: [7704/10000] Training loss: 1.171235914395022e-05 / Validation loss: 0.0001568005064197731 / Long term Validation loss: 0.1622\n",
      "Epoch: [7705/10000] Training loss: 1.1709040421149755e-05 / Validation loss: 0.00015671272236937744 / Long term Validation loss: 0.1622\n",
      "Epoch: [7706/10000] Training loss: 1.17073283657072e-05 / Validation loss: 0.0001571485132332219 / Long term Validation loss: 0.1623\n",
      "Epoch: [7707/10000] Training loss: 1.170631286156809e-05 / Validation loss: 0.0001565691382631763 / Long term Validation loss: 0.1622\n",
      "Epoch: [7708/10000] Training loss: 1.1704181336532187e-05 / Validation loss: 0.00015696730770669936 / Long term Validation loss: 0.1622\n",
      "Epoch: [7709/10000] Training loss: 1.1701229128093008e-05 / Validation loss: 0.00015687701666433236 / Long term Validation loss: 0.1622\n",
      "Epoch: [7710/10000] Training loss: 1.1698986471810915e-05 / Validation loss: 0.00015660741113346764 / Long term Validation loss: 0.1622\n",
      "Epoch: [7711/10000] Training loss: 1.1697598961407534e-05 / Validation loss: 0.00015703658750803847 / Long term Validation loss: 0.1623\n",
      "Epoch: [7712/10000] Training loss: 1.1695871020407245e-05 / Validation loss: 0.00015661870254880765 / Long term Validation loss: 0.1622\n",
      "Epoch: [7713/10000] Training loss: 1.1693371204769615e-05 / Validation loss: 0.00015678571902521892 / Long term Validation loss: 0.1623\n",
      "Epoch: [7714/10000] Training loss: 1.1690945790765958e-05 / Validation loss: 0.00015686978286315572 / Long term Validation loss: 0.1623\n",
      "Epoch: [7715/10000] Training loss: 1.1689171782027777e-05 / Validation loss: 0.0001565459534063245 / Long term Validation loss: 0.1622\n",
      "Epoch: [7716/10000] Training loss: 1.1687527874975382e-05 / Validation loss: 0.0001568858389484804 / Long term Validation loss: 0.1623\n",
      "Epoch: [7717/10000] Training loss: 1.1685401397001857e-05 / Validation loss: 0.00015663057709138992 / Long term Validation loss: 0.1623\n",
      "Epoch: [7718/10000] Training loss: 1.1683034201076888e-05 / Validation loss: 0.00015663958791211946 / Long term Validation loss: 0.1623\n",
      "Epoch: [7719/10000] Training loss: 1.1680990113543858e-05 / Validation loss: 0.00015680324044197217 / Long term Validation loss: 0.1623\n",
      "Epoch: [7720/10000] Training loss: 1.1679242945752386e-05 / Validation loss: 0.00015650577318940834 / Long term Validation loss: 0.1623\n",
      "Epoch: [7721/10000] Training loss: 1.1677330645766836e-05 / Validation loss: 0.00015674029375583297 / Long term Validation loss: 0.1623\n",
      "Epoch: [7722/10000] Training loss: 1.1675130500510703e-05 / Validation loss: 0.00015661450781610502 / Long term Validation loss: 0.1623\n",
      "Epoch: [7723/10000] Training loss: 1.1672975000218023e-05 / Validation loss: 0.00015653504008759342 / Long term Validation loss: 0.1623\n",
      "Epoch: [7724/10000] Training loss: 1.167106956553281e-05 / Validation loss: 0.0001567133293455166 / Long term Validation loss: 0.1623\n",
      "Epoch: [7725/10000] Training loss: 1.1669213568106728e-05 / Validation loss: 0.0001564730119393636 / Long term Validation loss: 0.1623\n",
      "Epoch: [7726/10000] Training loss: 1.166717476382943e-05 / Validation loss: 0.00015661354084890494 / Long term Validation loss: 0.1623\n",
      "Epoch: [7727/10000] Training loss: 1.166503589515105e-05 / Validation loss: 0.0001565733522201743 / Long term Validation loss: 0.1623\n",
      "Epoch: [7728/10000] Training loss: 1.1663011872505676e-05 / Validation loss: 0.00015645692032449014 / Long term Validation loss: 0.1623\n",
      "Epoch: [7729/10000] Training loss: 1.1661111024376452e-05 / Validation loss: 0.00015661735195408665 / Long term Validation loss: 0.1623\n",
      "Epoch: [7730/10000] Training loss: 1.1659163278620582e-05 / Validation loss: 0.00015643995395743813 / Long term Validation loss: 0.1623\n",
      "Epoch: [7731/10000] Training loss: 1.1657100496675033e-05 / Validation loss: 0.00015651165869299763 / Long term Validation loss: 0.1623\n",
      "Epoch: [7732/10000] Training loss: 1.1655034479604758e-05 / Validation loss: 0.00015652055433827694 / Long term Validation loss: 0.1623\n",
      "Epoch: [7733/10000] Training loss: 1.1653061315170198e-05 / Validation loss: 0.0001563980399036316 / Long term Validation loss: 0.1623\n",
      "Epoch: [7734/10000] Training loss: 1.1651127224522625e-05 / Validation loss: 0.0001565264958817989 / Long term Validation loss: 0.1623\n",
      "Epoch: [7735/10000] Training loss: 1.1649134306943455e-05 / Validation loss: 0.00015640083242126582 / Long term Validation loss: 0.1623\n",
      "Epoch: [7736/10000] Training loss: 1.164708769201694e-05 / Validation loss: 0.0001564254506695384 / Long term Validation loss: 0.1623\n",
      "Epoch: [7737/10000] Training loss: 1.1645068090956973e-05 / Validation loss: 0.00015645565591382212 / Long term Validation loss: 0.1623\n",
      "Epoch: [7738/10000] Training loss: 1.164310345569096e-05 / Validation loss: 0.00015634210088759202 / Long term Validation loss: 0.1623\n",
      "Epoch: [7739/10000] Training loss: 1.1641140160117586e-05 / Validation loss: 0.0001564375784478075 / Long term Validation loss: 0.1623\n",
      "Epoch: [7740/10000] Training loss: 1.1639133702323106e-05 / Validation loss: 0.00015635054406266485 / Long term Validation loss: 0.1623\n",
      "Epoch: [7741/10000] Training loss: 1.1637108577575113e-05 / Validation loss: 0.0001563472165522137 / Long term Validation loss: 0.1623\n",
      "Epoch: [7742/10000] Training loss: 1.1635110871518751e-05 / Validation loss: 0.00015638272800167315 / Long term Validation loss: 0.1623\n",
      "Epoch: [7743/10000] Training loss: 1.1633140493948822e-05 / Validation loss: 0.00015628389454714432 / Long term Validation loss: 0.1623\n",
      "Epoch: [7744/10000] Training loss: 1.1631160328108475e-05 / Validation loss: 0.0001563504658307556 / Long term Validation loss: 0.1624\n",
      "Epoch: [7745/10000] Training loss: 1.1629154163366885e-05 / Validation loss: 0.00015628879518653026 / Long term Validation loss: 0.1623\n",
      "Epoch: [7746/10000] Training loss: 1.1627144548734343e-05 / Validation loss: 0.00015626987706277896 / Long term Validation loss: 0.1624\n",
      "Epoch: [7747/10000] Training loss: 1.1625154649671071e-05 / Validation loss: 0.00015630160476103405 / Long term Validation loss: 0.1624\n",
      "Epoch: [7748/10000] Training loss: 1.1623176844209443e-05 / Validation loss: 0.00015621804160241017 / Long term Validation loss: 0.1624\n",
      "Epoch: [7749/10000] Training loss: 1.1621188977055791e-05 / Validation loss: 0.00015626241547749643 / Long term Validation loss: 0.1624\n",
      "Epoch: [7750/10000] Training loss: 1.1619186830567903e-05 / Validation loss: 0.00015621718886636837 / Long term Validation loss: 0.1624\n",
      "Epoch: [7751/10000] Training loss: 1.1617186152231005e-05 / Validation loss: 0.00015619196137353657 / Long term Validation loss: 0.1624\n",
      "Epoch: [7752/10000] Training loss: 1.1615197863770044e-05 / Validation loss: 0.0001562170290896884 / Long term Validation loss: 0.1624\n",
      "Epoch: [7753/10000] Training loss: 1.1613214568313413e-05 / Validation loss: 0.0001561475512899725 / Long term Validation loss: 0.1624\n",
      "Epoch: [7754/10000] Training loss: 1.1611223849615435e-05 / Validation loss: 0.00015617596373078454 / Long term Validation loss: 0.1624\n",
      "Epoch: [7755/10000] Training loss: 1.1609225501933472e-05 / Validation loss: 0.00015614051571041695 / Long term Validation loss: 0.1624\n",
      "Epoch: [7756/10000] Training loss: 1.1607229349779437e-05 / Validation loss: 0.00015611387565431706 / Long term Validation loss: 0.1624\n",
      "Epoch: [7757/10000] Training loss: 1.1605240542540059e-05 / Validation loss: 0.00015613137966473072 / Long term Validation loss: 0.1624\n",
      "Epoch: [7758/10000] Training loss: 1.1603253808936474e-05 / Validation loss: 0.00015607370604804786 / Long term Validation loss: 0.1624\n",
      "Epoch: [7759/10000] Training loss: 1.1601262263323718e-05 / Validation loss: 0.0001560913011031521 / Long term Validation loss: 0.1624\n",
      "Epoch: [7760/10000] Training loss: 1.1599266533014203e-05 / Validation loss: 0.00015606178283527865 / Long term Validation loss: 0.1624\n",
      "Epoch: [7761/10000] Training loss: 1.1597272445059444e-05 / Validation loss: 0.00015603685317975018 / Long term Validation loss: 0.1624\n",
      "Epoch: [7762/10000] Training loss: 1.1595282622355586e-05 / Validation loss: 0.00015604759344923437 / Long term Validation loss: 0.1624\n",
      "Epoch: [7763/10000] Training loss: 1.159329379233847e-05 / Validation loss: 0.00015599958169657587 / Long term Validation loss: 0.1624\n",
      "Epoch: [7764/10000] Training loss: 1.159130204402012e-05 / Validation loss: 0.00015600981770148498 / Long term Validation loss: 0.1624\n",
      "Epoch: [7765/10000] Training loss: 1.1589307844788349e-05 / Validation loss: 0.00015598338890269678 / Long term Validation loss: 0.1624\n",
      "Epoch: [7766/10000] Training loss: 1.1587314569591574e-05 / Validation loss: 0.00015596100658121897 / Long term Validation loss: 0.1624\n",
      "Epoch: [7767/10000] Training loss: 1.1585323754627836e-05 / Validation loss: 0.00015596549989475394 / Long term Validation loss: 0.1624\n",
      "Epoch: [7768/10000] Training loss: 1.1583333557030311e-05 / Validation loss: 0.0001559249111801922 / Long term Validation loss: 0.1624\n",
      "Epoch: [7769/10000] Training loss: 1.1581341671102506e-05 / Validation loss: 0.00015592994155328197 / Long term Validation loss: 0.1624\n",
      "Epoch: [7770/10000] Training loss: 1.1579348236258694e-05 / Validation loss: 0.00015590504445029608 / Long term Validation loss: 0.1624\n",
      "Epoch: [7771/10000] Training loss: 1.1577355165128559e-05 / Validation loss: 0.00015588550671615333 / Long term Validation loss: 0.1624\n",
      "Epoch: [7772/10000] Training loss: 1.157536348694269e-05 / Validation loss: 0.0001558846378601576 / Long term Validation loss: 0.1624\n",
      "Epoch: [7773/10000] Training loss: 1.157337227584003e-05 / Validation loss: 0.00015584985403962275 / Long term Validation loss: 0.1624\n",
      "Epoch: [7774/10000] Training loss: 1.1571380137642469e-05 / Validation loss: 0.00015585091197570217 / Long term Validation loss: 0.1624\n",
      "Epoch: [7775/10000] Training loss: 1.156938696436497e-05 / Validation loss: 0.0001558265136173914 / Long term Validation loss: 0.1624\n",
      "Epoch: [7776/10000] Training loss: 1.1567393792378803e-05 / Validation loss: 0.00015580935675546284 / Long term Validation loss: 0.1624\n",
      "Epoch: [7777/10000] Training loss: 1.156540135872253e-05 / Validation loss: 0.0001558036699445019 / Long term Validation loss: 0.1624\n",
      "Epoch: [7778/10000] Training loss: 1.1563409288372526e-05 / Validation loss: 0.00015577337047514674 / Long term Validation loss: 0.1624\n",
      "Epoch: [7779/10000] Training loss: 1.1561416746897098e-05 / Validation loss: 0.00015577116335044027 / Long term Validation loss: 0.1624\n",
      "Epoch: [7780/10000] Training loss: 1.1559423500943997e-05 / Validation loss: 0.000155746979684205 / Long term Validation loss: 0.1624\n",
      "Epoch: [7781/10000] Training loss: 1.1557430056666912e-05 / Validation loss: 0.0001557318760408983 / Long term Validation loss: 0.1624\n",
      "Epoch: [7782/10000] Training loss: 1.1555436935399073e-05 / Validation loss: 0.00015572219265727572 / Long term Validation loss: 0.1624\n",
      "Epoch: [7783/10000] Training loss: 1.1553444068001283e-05 / Validation loss: 0.00015569565860818475 / Long term Validation loss: 0.1624\n",
      "Epoch: [7784/10000] Training loss: 1.1551450987639166e-05 / Validation loss: 0.00015569064518879269 / Long term Validation loss: 0.1624\n",
      "Epoch: [7785/10000] Training loss: 1.154945743651398e-05 / Validation loss: 0.00015566670661436893 / Long term Validation loss: 0.1624\n",
      "Epoch: [7786/10000] Training loss: 1.1547463605150981e-05 / Validation loss: 0.00015565312385530054 / Long term Validation loss: 0.1624\n",
      "Epoch: [7787/10000] Training loss: 1.154546983325999e-05 / Validation loss: 0.00015564009918481812 / Long term Validation loss: 0.1624\n",
      "Epoch: [7788/10000] Training loss: 1.1543476195833495e-05 / Validation loss: 0.00015561676612433603 / Long term Validation loss: 0.1624\n",
      "Epoch: [7789/10000] Training loss: 1.1541482471510653e-05 / Validation loss: 0.00015560921621894262 / Long term Validation loss: 0.1625\n",
      "Epoch: [7790/10000] Training loss: 1.1539488444161832e-05 / Validation loss: 0.00015558586135978308 / Long term Validation loss: 0.1625\n",
      "Epoch: [7791/10000] Training loss: 1.1537494131535014e-05 / Validation loss: 0.00015557337633099243 / Long term Validation loss: 0.1625\n",
      "Epoch: [7792/10000] Training loss: 1.153549972159639e-05 / Validation loss: 0.00015555781292242958 / Long term Validation loss: 0.1625\n",
      "Epoch: [7793/10000] Training loss: 1.1533505329432071e-05 / Validation loss: 0.00015553732584531367 / Long term Validation loss: 0.1625\n",
      "Epoch: [7794/10000] Training loss: 1.153151088782336e-05 / Validation loss: 0.00015552743094113808 / Long term Validation loss: 0.1625\n",
      "Epoch: [7795/10000] Training loss: 1.1529516253793535e-05 / Validation loss: 0.00015550501135600515 / Long term Validation loss: 0.1625\n",
      "Epoch: [7796/10000] Training loss: 1.152752137100052e-05 / Validation loss: 0.00015549303510487068 / Long term Validation loss: 0.1625\n",
      "Epoch: [7797/10000] Training loss: 1.15255263133822e-05 / Validation loss: 0.00015547559464689734 / Long term Validation loss: 0.1625\n",
      "Epoch: [7798/10000] Training loss: 1.1523531176230269e-05 / Validation loss: 0.00015545748256834315 / Long term Validation loss: 0.1625\n",
      "Epoch: [7799/10000] Training loss: 1.1521535972914453e-05 / Validation loss: 0.0001554453674703067 / Long term Validation loss: 0.1625\n",
      "Epoch: [7800/10000] Training loss: 1.1519540635077111e-05 / Validation loss: 0.00015542421927993897 / Long term Validation loss: 0.1625\n",
      "Epoch: [7801/10000] Training loss: 1.1517545096544176e-05 / Validation loss: 0.0001554122004343241 / Long term Validation loss: 0.1625\n",
      "Epoch: [7802/10000] Training loss: 1.1515549361327153e-05 / Validation loss: 0.00015539360898834733 / Long term Validation loss: 0.1625\n",
      "Epoch: [7803/10000] Training loss: 1.1513553481090278e-05 / Validation loss: 0.00015537737261008753 / Long term Validation loss: 0.1625\n",
      "Epoch: [7804/10000] Training loss: 1.1511557494057488e-05 / Validation loss: 0.00015536322867513302 / Long term Validation loss: 0.1625\n",
      "Epoch: [7805/10000] Training loss: 1.1509561386517814e-05 / Validation loss: 0.00015534354054002144 / Long term Validation loss: 0.1625\n",
      "Epoch: [7806/10000] Training loss: 1.1507565115378546e-05 / Validation loss: 0.00015533089961623358 / Long term Validation loss: 0.1625\n",
      "Epoch: [7807/10000] Training loss: 1.1505568656668536e-05 / Validation loss: 0.00015531179140690436 / Long term Validation loss: 0.1625\n",
      "Epoch: [7808/10000] Training loss: 1.150357202197206e-05 / Validation loss: 0.00015529678765195646 / Long term Validation loss: 0.1625\n",
      "Epoch: [7809/10000] Training loss: 1.1501575240631635e-05 / Validation loss: 0.00015528091054462556 / Long term Validation loss: 0.1625\n",
      "Epoch: [7810/10000] Training loss: 1.1499578324737811e-05 / Validation loss: 0.00015526272289585462 / Long term Validation loss: 0.1625\n",
      "Epoch: [7811/10000] Training loss: 1.1497581260310187e-05 / Validation loss: 0.000155249053924033 / Long term Validation loss: 0.1625\n",
      "Epoch: [7812/10000] Training loss: 1.149558402506079e-05 / Validation loss: 0.0001552300633934762 / Long term Validation loss: 0.1625\n",
      "Epoch: [7813/10000] Training loss: 1.1493586609019799e-05 / Validation loss: 0.00015521564693964308 / Long term Validation loss: 0.1625\n",
      "Epoch: [7814/10000] Training loss: 1.149158902189251e-05 / Validation loss: 0.00015519850386183678 / Long term Validation loss: 0.1625\n",
      "Epoch: [7815/10000] Training loss: 1.1489591277301902e-05 / Validation loss: 0.00015518165005721135 / Long term Validation loss: 0.1625\n",
      "Epoch: [7816/10000] Training loss: 1.1487593379247956e-05 / Validation loss: 0.0001551667297144811 / Long term Validation loss: 0.1625\n",
      "Epoch: [7817/10000] Training loss: 1.1485595318871215e-05 / Validation loss: 0.0001551483288930395 / Long term Validation loss: 0.1625\n",
      "Epoch: [7818/10000] Training loss: 1.148359708386726e-05 / Validation loss: 0.0001551338591425621 / Long term Validation loss: 0.1625\n",
      "Epoch: [7819/10000] Training loss: 1.148159867034752e-05 / Validation loss: 0.00015511602456440022 / Long term Validation loss: 0.1625\n",
      "Epoch: [7820/10000] Training loss: 1.1479600082516717e-05 / Validation loss: 0.0001551001535448425 / Long term Validation loss: 0.1625\n",
      "Epoch: [7821/10000] Training loss: 1.14776013274292e-05 / Validation loss: 0.00015508404824614235 / Long term Validation loss: 0.1625\n",
      "Epoch: [7822/10000] Training loss: 1.1475602406364705e-05 / Validation loss: 0.00015506652933896834 / Long term Validation loss: 0.1625\n",
      "Epoch: [7823/10000] Training loss: 1.1473603314264922e-05 / Validation loss: 0.0001550515260211238 / Long term Validation loss: 0.1625\n",
      "Epoch: [7824/10000] Training loss: 1.1471604044856552e-05 / Validation loss: 0.00015503360208122386 / Long term Validation loss: 0.1625\n",
      "Epoch: [7825/10000] Training loss: 1.1469604594895422e-05 / Validation loss: 0.00015501821429972427 / Long term Validation loss: 0.1625\n",
      "Epoch: [7826/10000] Training loss: 1.1467604966438343e-05 / Validation loss: 0.00015500122017764664 / Long term Validation loss: 0.1625\n",
      "Epoch: [7827/10000] Training loss: 1.1465605162326969e-05 / Validation loss: 0.00015498457603687292 / Long term Validation loss: 0.1625\n",
      "Epoch: [7828/10000] Training loss: 1.1463605183456412e-05 / Validation loss: 0.00015496877043842647 / Long term Validation loss: 0.1625\n",
      "Epoch: [7829/10000] Training loss: 1.1461605027378787e-05 / Validation loss: 0.00015495122941153064 / Long term Validation loss: 0.1625\n",
      "Epoch: [7830/10000] Training loss: 1.1459604690204689e-05 / Validation loss: 0.0001549358024022516 / Long term Validation loss: 0.1625\n",
      "Epoch: [7831/10000] Training loss: 1.1457604169889052e-05 / Validation loss: 0.00015491839223910075 / Long term Validation loss: 0.1625\n",
      "Epoch: [7832/10000] Training loss: 1.1455603466226019e-05 / Validation loss: 0.00015490236775850732 / Long term Validation loss: 0.1625\n",
      "Epoch: [7833/10000] Training loss: 1.145360258063555e-05 / Validation loss: 0.0001548857912556375 / Long term Validation loss: 0.1625\n",
      "Epoch: [7834/10000] Training loss: 1.1451601513483575e-05 / Validation loss: 0.0001548688656958265 / Long term Validation loss: 0.1625\n",
      "Epoch: [7835/10000] Training loss: 1.1449600263657765e-05 / Validation loss: 0.0001548529973646355 / Long term Validation loss: 0.1625\n",
      "Epoch: [7836/10000] Training loss: 1.1447598829141605e-05 / Validation loss: 0.00015483564351626258 / Long term Validation loss: 0.1625\n",
      "Epoch: [7837/10000] Training loss: 1.1445597207955221e-05 / Validation loss: 0.00015481980733240278 / Long term Validation loss: 0.1625\n",
      "Epoch: [7838/10000] Training loss: 1.144359539956435e-05 / Validation loss: 0.00015480273314953533 / Long term Validation loss: 0.1625\n",
      "Epoch: [7839/10000] Training loss: 1.1441593403937204e-05 / Validation loss: 0.00015478635333147719 / Long term Validation loss: 0.1625\n",
      "Epoch: [7840/10000] Training loss: 1.1439591221363636e-05 / Validation loss: 0.00015476989708364055 / Long term Validation loss: 0.1625\n",
      "Epoch: [7841/10000] Training loss: 1.1437588851324195e-05 / Validation loss: 0.0001547529227203154 / Long term Validation loss: 0.1625\n",
      "Epoch: [7842/10000] Training loss: 1.1435586292642136e-05 / Validation loss: 0.0001547368713222878 / Long term Validation loss: 0.1625\n",
      "Epoch: [7843/10000] Training loss: 1.1433583544096013e-05 / Validation loss: 0.00015471970254748993 / Long term Validation loss: 0.1625\n",
      "Epoch: [7844/10000] Training loss: 1.1431580604599324e-05 / Validation loss: 0.0001547035712448512 / Long term Validation loss: 0.1625\n",
      "Epoch: [7845/10000] Training loss: 1.1429577473851954e-05 / Validation loss: 0.00015468666317039814 / Long term Validation loss: 0.1625\n",
      "Epoch: [7846/10000] Training loss: 1.1427574151572564e-05 / Validation loss: 0.00015467011471406077 / Long term Validation loss: 0.1625\n",
      "Epoch: [7847/10000] Training loss: 1.1425570637546613e-05 / Validation loss: 0.0001546536313818891 / Long term Validation loss: 0.1625\n",
      "Epoch: [7848/10000] Training loss: 1.1423566931148319e-05 / Validation loss: 0.000154636690451521 / Long term Validation loss: 0.1625\n",
      "Epoch: [7849/10000] Training loss: 1.1421563031484131e-05 / Validation loss: 0.0001546204492972639 / Long term Validation loss: 0.1625\n",
      "Epoch: [7850/10000] Training loss: 1.141955893776937e-05 / Validation loss: 0.0001546034063056864 / Long term Validation loss: 0.1625\n",
      "Epoch: [7851/10000] Training loss: 1.1417554649248967e-05 / Validation loss: 0.0001545870824345428 / Long term Validation loss: 0.1625\n",
      "Epoch: [7852/10000] Training loss: 1.1415550165586067e-05 / Validation loss: 0.00015457023181186702 / Long term Validation loss: 0.1625\n",
      "Epoch: [7853/10000] Training loss: 1.141354548636775e-05 / Validation loss: 0.0001545536149631593 / Long term Validation loss: 0.1625\n",
      "Epoch: [7854/10000] Training loss: 1.1411540611241179e-05 / Validation loss: 0.00015453705199009176 / Long term Validation loss: 0.1625\n",
      "Epoch: [7855/10000] Training loss: 1.1409535539657927e-05 / Validation loss: 0.00015452016549011784 / Long term Validation loss: 0.1625\n",
      "Epoch: [7856/10000] Training loss: 1.140753027095458e-05 / Validation loss: 0.00015450376620381186 / Long term Validation loss: 0.1625\n",
      "Epoch: [7857/10000] Training loss: 1.1405524804551092e-05 / Validation loss: 0.00015448680082527522 / Long term Validation loss: 0.1625\n",
      "Epoch: [7858/10000] Training loss: 1.140351913983785e-05 / Validation loss: 0.00015447035241091062 / Long term Validation loss: 0.1625\n",
      "Epoch: [7859/10000] Training loss: 1.1401513276443722e-05 / Validation loss: 0.0001544535048307044 / Long term Validation loss: 0.1625\n",
      "Epoch: [7860/10000] Training loss: 1.1399507213925505e-05 / Validation loss: 0.000154436862873405 / Long term Validation loss: 0.1625\n",
      "Epoch: [7861/10000] Training loss: 1.139750095191959e-05 / Validation loss: 0.00015442020817477134 / Long term Validation loss: 0.1625\n",
      "Epoch: [7862/10000] Training loss: 1.1395494489960479e-05 / Validation loss: 0.00015440337296989431 / Long term Validation loss: 0.1625\n",
      "Epoch: [7863/10000] Training loss: 1.1393487827539055e-05 / Validation loss: 0.00015438684452230954 / Long term Validation loss: 0.1625\n",
      "Epoch: [7864/10000] Training loss: 1.139148096418434e-05 / Validation loss: 0.00015436992902754742 / Long term Validation loss: 0.1625\n",
      "Epoch: [7865/10000] Training loss: 1.1389473899382959e-05 / Validation loss: 0.00015435339118880577 / Long term Validation loss: 0.1625\n",
      "Epoch: [7866/10000] Training loss: 1.1387466632762129e-05 / Validation loss: 0.000154336527828826 / Long term Validation loss: 0.1625\n",
      "Epoch: [7867/10000] Training loss: 1.1385459163892777e-05 / Validation loss: 0.00015431987294299093 / Long term Validation loss: 0.1625\n",
      "Epoch: [7868/10000] Training loss: 1.1383451492432126e-05 / Validation loss: 0.00015430313093221554 / Long term Validation loss: 0.1625\n",
      "Epoch: [7869/10000] Training loss: 1.138144361797468e-05 / Validation loss: 0.00015428633606933987 / Long term Validation loss: 0.1625\n",
      "Epoch: [7870/10000] Training loss: 1.1379435540116373e-05 / Validation loss: 0.0001542696954120569 / Long term Validation loss: 0.1625\n",
      "Epoch: [7871/10000] Training loss: 1.1377427258457909e-05 / Validation loss: 0.00015425281623257116 / Long term Validation loss: 0.1625\n",
      "Epoch: [7872/10000] Training loss: 1.1375418772569348e-05 / Validation loss: 0.00015423619903832505 / Long term Validation loss: 0.1625\n",
      "Epoch: [7873/10000] Training loss: 1.1373410082097763e-05 / Validation loss: 0.00015421932098818336 / Long term Validation loss: 0.1625\n",
      "Epoch: [7874/10000] Training loss: 1.1371401186647744e-05 / Validation loss: 0.000154202647658774 / Long term Validation loss: 0.1625\n",
      "Epoch: [7875/10000] Training loss: 1.1369392085900826e-05 / Validation loss: 0.00015418583297286626 / Long term Validation loss: 0.1625\n",
      "Epoch: [7876/10000] Training loss: 1.1367382779493852e-05 / Validation loss: 0.00015416906559779427 / Long term Validation loss: 0.1625\n",
      "Epoch: [7877/10000] Training loss: 1.1365373267094544e-05 / Validation loss: 0.00015415232583959064 / Long term Validation loss: 0.1625\n",
      "Epoch: [7878/10000] Training loss: 1.1363363548355878e-05 / Validation loss: 0.00015413547858942776 / Long term Validation loss: 0.1626\n",
      "Epoch: [7879/10000] Training loss: 1.1361353622923436e-05 / Validation loss: 0.00015411878034150066 / Long term Validation loss: 0.1626\n",
      "Epoch: [7880/10000] Training loss: 1.135934349047463e-05 / Validation loss: 0.0001541019002051516 / Long term Validation loss: 0.1626\n",
      "Epoch: [7881/10000] Training loss: 1.1357333150660179e-05 / Validation loss: 0.00015408519217556635 / Long term Validation loss: 0.1626\n",
      "Epoch: [7882/10000] Training loss: 1.1355322603185377e-05 / Validation loss: 0.00015406832808639092 / Long term Validation loss: 0.1626\n",
      "Epoch: [7883/10000] Training loss: 1.1353311847722463e-05 / Validation loss: 0.00015405157036262916 / Long term Validation loss: 0.1626\n",
      "Epoch: [7884/10000] Training loss: 1.1351300883986407e-05 / Validation loss: 0.0001540347493692328 / Long term Validation loss: 0.1626\n",
      "Epoch: [7885/10000] Training loss: 1.1349289711669098e-05 / Validation loss: 0.00015401792987880555 / Long term Validation loss: 0.1626\n",
      "Epoch: [7886/10000] Training loss: 1.1347278330478683e-05 / Validation loss: 0.00015400114986503395 / Long term Validation loss: 0.1626\n",
      "Epoch: [7887/10000] Training loss: 1.1345266740125158e-05 / Validation loss: 0.00015398428353583432 / Long term Validation loss: 0.1626\n",
      "Epoch: [7888/10000] Training loss: 1.1343254940312216e-05 / Validation loss: 0.0001539675211631233 / Long term Validation loss: 0.1626\n",
      "Epoch: [7889/10000] Training loss: 1.134124293076985e-05 / Validation loss: 0.0001539506368839339 / Long term Validation loss: 0.1626\n",
      "Epoch: [7890/10000] Training loss: 1.133923071120873e-05 / Validation loss: 0.000153933862853043 / Long term Validation loss: 0.1626\n",
      "Epoch: [7891/10000] Training loss: 1.1337218281375017e-05 / Validation loss: 0.00015391698763152318 / Long term Validation loss: 0.1626\n",
      "Epoch: [7892/10000] Training loss: 1.1335205640993286e-05 / Validation loss: 0.00015390018054784748 / Long term Validation loss: 0.1626\n",
      "Epoch: [7893/10000] Training loss: 1.13331927898165e-05 / Validation loss: 0.0001538833287325172 / Long term Validation loss: 0.1626\n",
      "Epoch: [7894/10000] Training loss: 1.1331179727584503e-05 / Validation loss: 0.00015386648207328573 / Long term Validation loss: 0.1626\n",
      "Epoch: [7895/10000] Training loss: 1.132916645405059e-05 / Validation loss: 0.0001538496527625722 / Long term Validation loss: 0.1626\n",
      "Epoch: [7896/10000] Training loss: 1.1327152968970019e-05 / Validation loss: 0.0001538327739528856 / Long term Validation loss: 0.1626\n",
      "Epoch: [7897/10000] Training loss: 1.1325139272097031e-05 / Validation loss: 0.00015381595516763264 / Long term Validation loss: 0.1626\n",
      "Epoch: [7898/10000] Training loss: 1.1323125363201822e-05 / Validation loss: 0.0001537990593415461 / Long term Validation loss: 0.1626\n",
      "Epoch: [7899/10000] Training loss: 1.1321111242043979e-05 / Validation loss: 0.00015378223525046265 / Long term Validation loss: 0.1626\n",
      "Epoch: [7900/10000] Training loss: 1.1319096908405914e-05 / Validation loss: 0.00015376533772286816 / Long term Validation loss: 0.1626\n",
      "Epoch: [7901/10000] Training loss: 1.131708236205648e-05 / Validation loss: 0.0001537484952884755 / Long term Validation loss: 0.1626\n",
      "Epoch: [7902/10000] Training loss: 1.1315067602785897e-05 / Validation loss: 0.00015373160607287623 / Long term Validation loss: 0.1626\n",
      "Epoch: [7903/10000] Training loss: 1.1313052630374303e-05 / Validation loss: 0.00015371473889835293 / Long term Validation loss: 0.1626\n",
      "Epoch: [7904/10000] Training loss: 1.131103744461618e-05 / Validation loss: 0.00015369786070555842 / Long term Validation loss: 0.1626\n",
      "Epoch: [7905/10000] Training loss: 1.1309022045303698e-05 / Validation loss: 0.0001536809695448755 / Long term Validation loss: 0.1626\n",
      "Epoch: [7906/10000] Training loss: 1.1307006432234525e-05 / Validation loss: 0.00015366409884822187 / Long term Validation loss: 0.1626\n",
      "Epoch: [7907/10000] Training loss: 1.1304990605212605e-05 / Validation loss: 0.00015364718956332316 / Long term Validation loss: 0.1626\n",
      "Epoch: [7908/10000] Training loss: 1.1302974564039855e-05 / Validation loss: 0.00015363031934427604 / Long term Validation loss: 0.1626\n",
      "Epoch: [7909/10000] Training loss: 1.1300958308530761e-05 / Validation loss: 0.00015361339976589634 / Long term Validation loss: 0.1626\n",
      "Epoch: [7910/10000] Training loss: 1.1298941838493261e-05 / Validation loss: 0.00015359652251207678 / Long term Validation loss: 0.1626\n",
      "Epoch: [7911/10000] Training loss: 1.1296925153750416e-05 / Validation loss: 0.00015357959961458673 / Long term Validation loss: 0.1626\n",
      "Epoch: [7912/10000] Training loss: 1.1294908254117798e-05 / Validation loss: 0.0001535627095954576 / Long term Validation loss: 0.1626\n",
      "Epoch: [7913/10000] Training loss: 1.1292891139424952e-05 / Validation loss: 0.0001535457878148859 / Long term Validation loss: 0.1626\n",
      "Epoch: [7914/10000] Training loss: 1.1290873809496113e-05 / Validation loss: 0.00015352888219763588 / Long term Validation loss: 0.1626\n",
      "Epoch: [7915/10000] Training loss: 1.1288856264165882e-05 / Validation loss: 0.00015351196301002966 / Long term Validation loss: 0.1626\n",
      "Epoch: [7916/10000] Training loss: 1.1286838503267508e-05 / Validation loss: 0.0001534950418349558 / Long term Validation loss: 0.1626\n",
      "Epoch: [7917/10000] Training loss: 1.1284820526640093e-05 / Validation loss: 0.000153478124246504 / Long term Validation loss: 0.1626\n",
      "Epoch: [7918/10000] Training loss: 1.1282802334125767e-05 / Validation loss: 0.0001534611896156062 / Long term Validation loss: 0.1626\n",
      "Epoch: [7919/10000] Training loss: 1.1280783925568235e-05 / Validation loss: 0.00015344427110067102 / Long term Validation loss: 0.1626\n",
      "Epoch: [7920/10000] Training loss: 1.1278765300817905e-05 / Validation loss: 0.0001534273260821029 / Long term Validation loss: 0.1626\n",
      "Epoch: [7921/10000] Training loss: 1.1276746459723602e-05 / Validation loss: 0.00015341040359315524 / Long term Validation loss: 0.1626\n",
      "Epoch: [7922/10000] Training loss: 1.1274727402143111e-05 / Validation loss: 0.00015339345126614117 / Long term Validation loss: 0.1626\n",
      "Epoch: [7923/10000] Training loss: 1.1272708127930987e-05 / Validation loss: 0.00015337652205927097 / Long term Validation loss: 0.1626\n",
      "Epoch: [7924/10000] Training loss: 1.1270688636951459e-05 / Validation loss: 0.0001533595649012687 / Long term Validation loss: 0.1626\n",
      "Epoch: [7925/10000] Training loss: 1.1268668929065313e-05 / Validation loss: 0.0001533426270286668 / Long term Validation loss: 0.1626\n",
      "Epoch: [7926/10000] Training loss: 1.1266649004142425e-05 / Validation loss: 0.00015332566664391918 / Long term Validation loss: 0.1626\n",
      "Epoch: [7927/10000] Training loss: 1.1264628862050103e-05 / Validation loss: 0.00015330871908834044 / Long term Validation loss: 0.1626\n",
      "Epoch: [7928/10000] Training loss: 1.1262608502663325e-05 / Validation loss: 0.0001532917561932629 / Long term Validation loss: 0.1626\n",
      "Epoch: [7929/10000] Training loss: 1.1260587925855981e-05 / Validation loss: 0.0001532747987405124 / Long term Validation loss: 0.1626\n",
      "Epoch: [7930/10000] Training loss: 1.1258567131507836e-05 / Validation loss: 0.00015325783332263118 / Long term Validation loss: 0.1626\n",
      "Epoch: [7931/10000] Training loss: 1.1256546119499312e-05 / Validation loss: 0.00015324086632083445 / Long term Validation loss: 0.1626\n",
      "Epoch: [7932/10000] Training loss: 1.1254524889714767e-05 / Validation loss: 0.00015322389789402654 / Long term Validation loss: 0.1626\n",
      "Epoch: [7933/10000] Training loss: 1.1252503442040938e-05 / Validation loss: 0.00015320692201086576 / Long term Validation loss: 0.1626\n",
      "Epoch: [7934/10000] Training loss: 1.1250481776366782e-05 / Validation loss: 0.0001531899498804459 / Long term Validation loss: 0.1627\n",
      "Epoch: [7935/10000] Training loss: 1.1248459892585041e-05 / Validation loss: 0.00015317296589962488 / Long term Validation loss: 0.1627\n",
      "Epoch: [7936/10000] Training loss: 1.1246437790589297e-05 / Validation loss: 0.00015315598936347605 / Long term Validation loss: 0.1627\n",
      "Epoch: [7937/10000] Training loss: 1.124441547027796e-05 / Validation loss: 0.00015313899802622886 / Long term Validation loss: 0.1627\n",
      "Epoch: [7938/10000] Training loss: 1.1242392931549345e-05 / Validation loss: 0.0001531220164875941 / Long term Validation loss: 0.1627\n",
      "Epoch: [7939/10000] Training loss: 1.1240370174307159e-05 / Validation loss: 0.00015310501838843336 / Long term Validation loss: 0.1627\n",
      "Epoch: [7940/10000] Training loss: 1.1238347198454453e-05 / Validation loss: 0.00015308803140460324 / Long term Validation loss: 0.1627\n",
      "Epoch: [7941/10000] Training loss: 1.1236324003900031e-05 / Validation loss: 0.00015307102695082093 / Long term Validation loss: 0.1627\n",
      "Epoch: [7942/10000] Training loss: 1.1234300590551606e-05 / Validation loss: 0.00015305403425095276 / Long term Validation loss: 0.1627\n",
      "Epoch: [7943/10000] Training loss: 1.1232276958322855e-05 / Validation loss: 0.00015303702367326196 / Long term Validation loss: 0.1627\n",
      "Epoch: [7944/10000] Training loss: 1.1230253107126142e-05 / Validation loss: 0.00015302002515754293 / Long term Validation loss: 0.1627\n",
      "Epoch: [7945/10000] Training loss: 1.12282290368798e-05 / Validation loss: 0.0001530030085388447 / Long term Validation loss: 0.1627\n",
      "Epoch: [7946/10000] Training loss: 1.1226204747500754e-05 / Validation loss: 0.0001529860042584151 / Long term Validation loss: 0.1627\n",
      "Epoch: [7947/10000] Training loss: 1.1224180238911893e-05 / Validation loss: 0.00015296898155428088 / Long term Validation loss: 0.1627\n",
      "Epoch: [7948/10000] Training loss: 1.1222155511034546e-05 / Validation loss: 0.00015295197168111536 / Long term Validation loss: 0.1627\n",
      "Epoch: [7949/10000] Training loss: 1.122013056379609e-05 / Validation loss: 0.00015293494272842944 / Long term Validation loss: 0.1627\n",
      "Epoch: [7950/10000] Training loss: 1.12181053971221e-05 / Validation loss: 0.00015291792753584067 / Long term Validation loss: 0.1627\n",
      "Epoch: [7951/10000] Training loss: 1.121608001094433e-05 / Validation loss: 0.00015290089205607165 / Long term Validation loss: 0.1627\n",
      "Epoch: [7952/10000] Training loss: 1.1214054405192484e-05 / Validation loss: 0.00015288387192390982 / Long term Validation loss: 0.1627\n",
      "Epoch: [7953/10000] Training loss: 1.1212028579802657e-05 / Validation loss: 0.00015286682951756185 / Long term Validation loss: 0.1627\n",
      "Epoch: [7954/10000] Training loss: 1.1210002534708457e-05 / Validation loss: 0.00015284980496072282 / Long term Validation loss: 0.1627\n",
      "Epoch: [7955/10000] Training loss: 1.1207976269850344e-05 / Validation loss: 0.00015283275507813922 / Long term Validation loss: 0.1627\n",
      "Epoch: [7956/10000] Training loss: 1.1205949785165596e-05 / Validation loss: 0.00015281572679418806 / Long term Validation loss: 0.1627\n",
      "Epoch: [7957/10000] Training loss: 1.1203923080599084e-05 / Validation loss: 0.00015279866866986494 / Long term Validation loss: 0.1627\n",
      "Epoch: [7958/10000] Training loss: 1.1201896156091478e-05 / Validation loss: 0.00015278163761483325 / Long term Validation loss: 0.1627\n",
      "Epoch: [7959/10000] Training loss: 1.1199869011592155e-05 / Validation loss: 0.0001527645701588913 / Long term Validation loss: 0.1627\n",
      "Epoch: [7960/10000] Training loss: 1.1197841647044896e-05 / Validation loss: 0.0001527475376746378 / Long term Validation loss: 0.1627\n",
      "Epoch: [7961/10000] Training loss: 1.119581406240374e-05 / Validation loss: 0.00015273045930880994 / Long term Validation loss: 0.1627\n",
      "Epoch: [7962/10000] Training loss: 1.1193786257615145e-05 / Validation loss: 0.00015271342733171202 / Long term Validation loss: 0.1627\n",
      "Epoch: [7963/10000] Training loss: 1.1191758232638179e-05 / Validation loss: 0.0001526963357353799 / Long term Validation loss: 0.1627\n",
      "Epoch: [7964/10000] Training loss: 1.118972998742139e-05 / Validation loss: 0.00015267930712511694 / Long term Validation loss: 0.1627\n",
      "Epoch: [7965/10000] Training loss: 1.1187701521929441e-05 / Validation loss: 0.0001526621988286801 / Long term Validation loss: 0.1627\n",
      "Epoch: [7966/10000] Training loss: 1.1185672836112243e-05 / Validation loss: 0.0001526451778844301 / Long term Validation loss: 0.1627\n",
      "Epoch: [7967/10000] Training loss: 1.1183643929940893e-05 / Validation loss: 0.00015262804761342476 / Long term Validation loss: 0.1627\n",
      "Epoch: [7968/10000] Training loss: 1.1181614803365653e-05 / Validation loss: 0.0001526110409008111 / Long term Validation loss: 0.1627\n",
      "Epoch: [7969/10000] Training loss: 1.1179585456365561e-05 / Validation loss: 0.0001525938805162406 / Long term Validation loss: 0.1627\n",
      "Epoch: [7970/10000] Training loss: 1.1177555888889837e-05 / Validation loss: 0.000152576898216387 / Long term Validation loss: 0.1628\n",
      "Epoch: [7971/10000] Training loss: 1.1175526100927971e-05 / Validation loss: 0.00015255969498590119 / Long term Validation loss: 0.1628\n",
      "Epoch: [7972/10000] Training loss: 1.1173496092426363e-05 / Validation loss: 0.0001525427531200569 / Long term Validation loss: 0.1628\n",
      "Epoch: [7973/10000] Training loss: 1.1171465863389525e-05 / Validation loss: 0.00015252548685679016 / Long term Validation loss: 0.1628\n",
      "Epoch: [7974/10000] Training loss: 1.1169435413759109e-05 / Validation loss: 0.00015250861098993154 / Long term Validation loss: 0.1628\n",
      "Epoch: [7975/10000] Training loss: 1.1167404743563318e-05 / Validation loss: 0.00015249124925537506 / Long term Validation loss: 0.1628\n",
      "Epoch: [7976/10000] Training loss: 1.1165373852738286e-05 / Validation loss: 0.0001524744807329008 / Long term Validation loss: 0.1628\n",
      "Epoch: [7977/10000] Training loss: 1.1163342741353629e-05 / Validation loss: 0.00015245697070310669 / Long term Validation loss: 0.1628\n",
      "Epoch: [7978/10000] Training loss: 1.116131140934504e-05 / Validation loss: 0.00015244037728200793 / Long term Validation loss: 0.1628\n",
      "Epoch: [7979/10000] Training loss: 1.1159279856862782e-05 / Validation loss: 0.0001524226318032033 / Long term Validation loss: 0.1628\n",
      "Epoch: [7980/10000] Training loss: 1.1157248083867909e-05 / Validation loss: 0.00015240632598820594 / Long term Validation loss: 0.1628\n",
      "Epoch: [7981/10000] Training loss: 1.115521609068571e-05 / Validation loss: 0.00015238819939590993 / Long term Validation loss: 0.1628\n",
      "Epoch: [7982/10000] Training loss: 1.115318387739558e-05 / Validation loss: 0.00015237237042479374 / Long term Validation loss: 0.1628\n",
      "Epoch: [7983/10000] Training loss: 1.1151151444741612e-05 / Validation loss: 0.00015235361612654462 / Long term Validation loss: 0.1628\n",
      "Epoch: [7984/10000] Training loss: 1.1149118793225913e-05 / Validation loss: 0.0001523385863967269 / Long term Validation loss: 0.1628\n",
      "Epoch: [7985/10000] Training loss: 1.1147085924680618e-05 / Validation loss: 0.00015231878161679822 / Long term Validation loss: 0.1628\n",
      "Epoch: [7986/10000] Training loss: 1.1145052840996362e-05 / Validation loss: 0.00015230510735973407 / Long term Validation loss: 0.1628\n",
      "Epoch: [7987/10000] Training loss: 1.114301954702518e-05 / Validation loss: 0.0001522835181214631 / Long term Validation loss: 0.1628\n",
      "Epoch: [7988/10000] Training loss: 1.1140986049096387e-05 / Validation loss: 0.00015227217105300372 / Long term Validation loss: 0.1628\n",
      "Epoch: [7989/10000] Training loss: 1.1138952360893092e-05 / Validation loss: 0.00015224750723419323 / Long term Validation loss: 0.1628\n",
      "Epoch: [7990/10000] Training loss: 1.1136918502898874e-05 / Validation loss: 0.00015224020599011453 / Long term Validation loss: 0.1628\n",
      "Epoch: [7991/10000] Training loss: 1.113488451572401e-05 / Validation loss: 0.00015221017202857607 / Long term Validation loss: 0.1628\n",
      "Epoch: [7992/10000] Training loss: 1.1132850465429005e-05 / Validation loss: 0.0001522099936426774 / Long term Validation loss: 0.1628\n",
      "Epoch: [7993/10000] Training loss: 1.1130816477633662e-05 / Validation loss: 0.00015217045538107928 / Long term Validation loss: 0.1628\n",
      "Epoch: [7994/10000] Training loss: 1.112878276754177e-05 / Validation loss: 0.00015218297598641628 / Long term Validation loss: 0.1628\n",
      "Epoch: [7995/10000] Training loss: 1.1126749737386843e-05 / Validation loss: 0.00015212639904148994 / Long term Validation loss: 0.1628\n",
      "Epoch: [7996/10000] Training loss: 1.1124718100040749e-05 / Validation loss: 0.00015216184558153336 / Long term Validation loss: 0.1628\n",
      "Epoch: [7997/10000] Training loss: 1.1122689182798805e-05 / Validation loss: 0.00015207433743994638 / Long term Validation loss: 0.1628\n",
      "Epoch: [7998/10000] Training loss: 1.1120665394904247e-05 / Validation loss: 0.00015215169225450939 / Long term Validation loss: 0.1628\n",
      "Epoch: [7999/10000] Training loss: 1.1118651237472055e-05 / Validation loss: 0.00015200734275488454 / Long term Validation loss: 0.1628\n",
      "Epoch: [8000/10000] Training loss: 1.1116655031551777e-05 / Validation loss: 0.0001521622642279399 / Long term Validation loss: 0.1629\n",
      "Epoch: [8001/10000] Training loss: 1.111469243687962e-05 / Validation loss: 0.00015191221397008203 / Long term Validation loss: 0.1628\n",
      "Epoch: [8002/10000] Training loss: 1.1112792845708211e-05 / Validation loss: 0.00015221250854888585 / Long term Validation loss: 0.1629\n",
      "Epoch: [8003/10000] Training loss: 1.1111012089531488e-05 / Validation loss: 0.00015176365863308683 / Long term Validation loss: 0.1628\n",
      "Epoch: [8004/10000] Training loss: 1.1109456377255157e-05 / Validation loss: 0.00015233991587136457 / Long term Validation loss: 0.1629\n",
      "Epoch: [8005/10000] Training loss: 1.1108329542591663e-05 / Validation loss: 0.0001515132140034201 / Long term Validation loss: 0.1629\n",
      "Epoch: [8006/10000] Training loss: 1.110802419248593e-05 / Validation loss: 0.000152620457607486 / Long term Validation loss: 0.1629\n",
      "Epoch: [8007/10000] Training loss: 1.1109301916380537e-05 / Validation loss: 0.0001510690547382236 / Long term Validation loss: 0.1630\n",
      "Epoch: [8008/10000] Training loss: 1.1113647058275159e-05 / Validation loss: 0.0001532134910847804 / Long term Validation loss: 0.1629\n",
      "Epoch: [8009/10000] Training loss: 1.1123970057268313e-05 / Validation loss: 0.0001502635077457947 / Long term Validation loss: 0.1630\n",
      "Epoch: [8010/10000] Training loss: 1.1146007803530144e-05 / Validation loss: 0.00015447123583104606 / Long term Validation loss: 0.1631\n",
      "Epoch: [8011/10000] Training loss: 1.1191125360658054e-05 / Validation loss: 0.0001488214154356055 / Long term Validation loss: 0.1629\n",
      "Epoch: [8012/10000] Training loss: 1.1281958682846273e-05 / Validation loss: 0.00015723489386193406 / Long term Validation loss: 0.1645\n",
      "Epoch: [8013/10000] Training loss: 1.1463739893674388e-05 / Validation loss: 0.00014643944782609383 / Long term Validation loss: 0.1626\n",
      "Epoch: [8014/10000] Training loss: 1.182723754847774e-05 / Validation loss: 0.00016372964936703268 / Long term Validation loss: 0.1640\n",
      "Epoch: [8015/10000] Training loss: 1.2554303055751931e-05 / Validation loss: 0.00014354638985274165 / Long term Validation loss: 0.1611\n",
      "Epoch: [8016/10000] Training loss: 1.4008563139404927e-05 / Validation loss: 0.00018047135863802195 / Long term Validation loss: 0.1584\n",
      "Epoch: [8017/10000] Training loss: 1.6903389983275566e-05 / Validation loss: 0.0001451079180345692 / Long term Validation loss: 0.1537\n",
      "Epoch: [8018/10000] Training loss: 2.259524340706269e-05 / Validation loss: 0.0002268169862586799 / Long term Validation loss: 0.1579\n",
      "Epoch: [8019/10000] Training loss: 3.342879763330702e-05 / Validation loss: 0.00017249571841329575 / Long term Validation loss: 0.1558\n",
      "Epoch: [8020/10000] Training loss: 5.2701731113888705e-05 / Validation loss: 0.00033996781068398375 / Long term Validation loss: 0.1649\n",
      "Epoch: [8021/10000] Training loss: 8.218092121778402e-05 / Validation loss: 0.0002476422652260266 / Long term Validation loss: 0.1708\n",
      "Epoch: [8022/10000] Training loss: 0.00011439602860543796 / Validation loss: 0.00043095792207827575 / Long term Validation loss: 0.1681\n",
      "Epoch: [8023/10000] Training loss: 0.00012345067397431895 / Validation loss: 0.00021024707277585286 / Long term Validation loss: 0.1661\n",
      "Epoch: [8024/10000] Training loss: 8.55106011549037e-05 / Validation loss: 0.0002196309107061704 / Long term Validation loss: 0.1554\n",
      "Epoch: [8025/10000] Training loss: 2.8295653187992507e-05 / Validation loss: 0.00017205101294877058 / Long term Validation loss: 0.1599\n",
      "Epoch: [8026/10000] Training loss: 1.3081578479378659e-05 / Validation loss: 0.00016698824553513475 / Long term Validation loss: 0.1513\n",
      "Epoch: [8027/10000] Training loss: 4.515196890624988e-05 / Validation loss: 0.00031849595535129397 / Long term Validation loss: 0.1615\n",
      "Epoch: [8028/10000] Training loss: 6.732235641238522e-05 / Validation loss: 0.00016742970187741106 / Long term Validation loss: 0.1513\n",
      "Epoch: [8029/10000] Training loss: 4.382926543510218e-05 / Validation loss: 0.00017596479586443978 / Long term Validation loss: 0.1600\n",
      "Epoch: [8030/10000] Training loss: 1.3233584763921081e-05 / Validation loss: 0.00020433378981733264 / Long term Validation loss: 0.1547\n",
      "Epoch: [8031/10000] Training loss: 2.11855883079834e-05 / Validation loss: 0.0001675063940861346 / Long term Validation loss: 0.1507\n",
      "Epoch: [8032/10000] Training loss: 4.3498546696593936e-05 / Validation loss: 0.00024524194743992414 / Long term Validation loss: 0.1543\n",
      "Epoch: [8033/10000] Training loss: 3.64279439737981e-05 / Validation loss: 0.00015030169555543596 / Long term Validation loss: 0.1614\n",
      "Epoch: [8034/10000] Training loss: 1.4255226932924527e-05 / Validation loss: 0.00014921901058986386 / Long term Validation loss: 0.1601\n",
      "Epoch: [8035/10000] Training loss: 1.5961872244233267e-05 / Validation loss: 0.0002293506758457381 / Long term Validation loss: 0.1535\n",
      "Epoch: [8036/10000] Training loss: 3.137314552657992e-05 / Validation loss: 0.00015361476277281192 / Long term Validation loss: 0.1499\n",
      "Epoch: [8037/10000] Training loss: 2.7700492622196232e-05 / Validation loss: 0.00017116444153874875 / Long term Validation loss: 0.1619\n",
      "Epoch: [8038/10000] Training loss: 1.3049579008022156e-05 / Validation loss: 0.00017674146525581858 / Long term Validation loss: 0.1590\n",
      "Epoch: [8039/10000] Training loss: 1.4609853231287515e-05 / Validation loss: 0.0001510564608480769 / Long term Validation loss: 0.1497\n",
      "Epoch: [8040/10000] Training loss: 2.4705397528311282e-05 / Validation loss: 0.0001962533043280806 / Long term Validation loss: 0.1557\n",
      "Epoch: [8041/10000] Training loss: 2.1342648157339727e-05 / Validation loss: 0.00014902369421156809 / Long term Validation loss: 0.1633\n",
      "Epoch: [8042/10000] Training loss: 1.193573658937593e-05 / Validation loss: 0.00014591184949057704 / Long term Validation loss: 0.1595\n",
      "Epoch: [8043/10000] Training loss: 1.407472120466912e-05 / Validation loss: 0.00019057433826725627 / Long term Validation loss: 0.1563\n",
      "Epoch: [8044/10000] Training loss: 2.0414622959472803e-05 / Validation loss: 0.00014478023753274714 / Long term Validation loss: 0.1540\n",
      "Epoch: [8045/10000] Training loss: 1.7159052526583238e-05 / Validation loss: 0.0001554373533752458 / Long term Validation loss: 0.1652\n",
      "Epoch: [8046/10000] Training loss: 1.1356429937129016e-05 / Validation loss: 0.00016700550756301545 / Long term Validation loss: 0.1632\n",
      "Epoch: [8047/10000] Training loss: 1.3669989029095829e-05 / Validation loss: 0.0001442439578802235 / Long term Validation loss: 0.1528\n",
      "Epoch: [8048/10000] Training loss: 1.7435980392308065e-05 / Validation loss: 0.00016992983496525937 / Long term Validation loss: 0.1615\n",
      "Epoch: [8049/10000] Training loss: 1.4561281429396123e-05 / Validation loss: 0.0001498243662130435 / Long term Validation loss: 0.1636\n",
      "Epoch: [8050/10000] Training loss: 1.1143899934046698e-05 / Validation loss: 0.0001442319160387544 / Long term Validation loss: 0.1595\n",
      "Epoch: [8051/10000] Training loss: 1.3223751119262676e-05 / Validation loss: 0.00017249431870338696 / Long term Validation loss: 0.1602\n",
      "Epoch: [8052/10000] Training loss: 1.5324354017221924e-05 / Validation loss: 0.00014418867083623607 / Long term Validation loss: 0.1600\n",
      "Epoch: [8053/10000] Training loss: 1.3012202246201369e-05 / Validation loss: 0.00015050855811790038 / Long term Validation loss: 0.1639\n",
      "Epoch: [8054/10000] Training loss: 1.1095714602077845e-05 / Validation loss: 0.0001630278153269686 / Long term Validation loss: 0.1658\n",
      "Epoch: [8055/10000] Training loss: 1.2764906729174623e-05 / Validation loss: 0.0001436074508566862 / Long term Validation loss: 0.1592\n",
      "Epoch: [8056/10000] Training loss: 1.3857298393310208e-05 / Validation loss: 0.00016036798989915707 / Long term Validation loss: 0.1663\n",
      "Epoch: [8057/10000] Training loss: 1.2121247994524415e-05 / Validation loss: 0.00015255090342772555 / Long term Validation loss: 0.1647\n",
      "Epoch: [8058/10000] Training loss: 1.1100801241604015e-05 / Validation loss: 0.00014537256448489573 / Long term Validation loss: 0.1617\n",
      "Epoch: [8059/10000] Training loss: 1.2344285745015413e-05 / Validation loss: 0.00016475954282431127 / Long term Validation loss: 0.1647\n",
      "Epoch: [8060/10000] Training loss: 1.2868507097134025e-05 / Validation loss: 0.00014720922956889222 / Long term Validation loss: 0.1631\n",
      "Epoch: [8061/10000] Training loss: 1.1629969249103512e-05 / Validation loss: 0.00015041608818753538 / Long term Validation loss: 0.1634\n",
      "Epoch: [8062/10000] Training loss: 1.1108881618642073e-05 / Validation loss: 0.0001609031550662279 / Long term Validation loss: 0.1657\n",
      "Epoch: [8063/10000] Training loss: 1.198715427796104e-05 / Validation loss: 0.00014586026234149886 / Long term Validation loss: 0.1619\n",
      "Epoch: [8064/10000] Training loss: 1.2217788938127974e-05 / Validation loss: 0.00015689193151834685 / Long term Validation loss: 0.1644\n",
      "Epoch: [8065/10000] Training loss: 1.1364583619401348e-05 / Validation loss: 0.00015397291655088816 / Long term Validation loss: 0.1640\n",
      "Epoch: [8066/10000] Training loss: 1.1103038425581338e-05 / Validation loss: 0.00014702603120068084 / Long term Validation loss: 0.1626\n",
      "Epoch: [8067/10000] Training loss: 1.170196910722915e-05 / Validation loss: 0.00016007962760528532 / Long term Validation loss: 0.1651\n",
      "Epoch: [8068/10000] Training loss: 1.1795960858276643e-05 / Validation loss: 0.0001491595735697967 / Long term Validation loss: 0.1625\n",
      "Epoch: [8069/10000] Training loss: 1.1221282552698591e-05 / Validation loss: 0.00015051246624605955 / Long term Validation loss: 0.1630\n",
      "Epoch: [8070/10000] Training loss: 1.1084639524523998e-05 / Validation loss: 0.0001579629773969232 / Long term Validation loss: 0.1644\n",
      "Epoch: [8071/10000] Training loss: 1.1484372924284602e-05 / Validation loss: 0.00014739210715635265 / Long term Validation loss: 0.1621\n",
      "Epoch: [8072/10000] Training loss: 1.1524778550942123e-05 / Validation loss: 0.00015463368368897611 / Long term Validation loss: 0.1633\n",
      "Epoch: [8073/10000] Training loss: 1.1142988974858942e-05 / Validation loss: 0.00015321358217196295 / Long term Validation loss: 0.1632\n",
      "Epoch: [8074/10000] Training loss: 1.1061786422340324e-05 / Validation loss: 0.00014792969109613137 / Long term Validation loss: 0.1621\n",
      "Epoch: [8075/10000] Training loss: 1.132516621254189e-05 / Validation loss: 0.00015649001464340038 / Long term Validation loss: 0.1636\n",
      "Epoch: [8076/10000] Training loss: 1.1349782884895264e-05 / Validation loss: 0.00014944680309807303 / Long term Validation loss: 0.1624\n",
      "Epoch: [8077/10000] Training loss: 1.1098335020692116e-05 / Validation loss: 0.00015030779645267925 / Long term Validation loss: 0.1625\n",
      "Epoch: [8078/10000] Training loss: 1.1040441433351154e-05 / Validation loss: 0.00015502250226707788 / Long term Validation loss: 0.1632\n",
      "Epoch: [8079/10000] Training loss: 1.121200667419131e-05 / Validation loss: 0.00014801853724836467 / Long term Validation loss: 0.1621\n",
      "Epoch: [8080/10000] Training loss: 1.1234824812565066e-05 / Validation loss: 0.0001531058364096479 / Long term Validation loss: 0.1630\n",
      "Epoch: [8081/10000] Training loss: 1.1070514552700611e-05 / Validation loss: 0.00015191989687876855 / Long term Validation loss: 0.1628\n",
      "Epoch: [8082/10000] Training loss: 1.1022589870760542e-05 / Validation loss: 0.00014860162689988988 / Long term Validation loss: 0.1622\n",
      "Epoch: [8083/10000] Training loss: 1.1132345373751436e-05 / Validation loss: 0.0001542383003712316 / Long term Validation loss: 0.1630\n",
      "Epoch: [8084/10000] Training loss: 1.1156580189052681e-05 / Validation loss: 0.00014944045642348742 / Long term Validation loss: 0.1623\n",
      "Epoch: [8085/10000] Training loss: 1.1050755339353631e-05 / Validation loss: 0.00015048508123549916 / Long term Validation loss: 0.1625\n",
      "Epoch: [8086/10000] Training loss: 1.100853024389245e-05 / Validation loss: 0.0001531337813484979 / Long term Validation loss: 0.1628\n",
      "Epoch: [8087/10000] Training loss: 1.1076510694824005e-05 / Validation loss: 0.000148634530437629 / Long term Validation loss: 0.1622\n",
      "Epoch: [8088/10000] Training loss: 1.1101289592613293e-05 / Validation loss: 0.00015242060247362796 / Long term Validation loss: 0.1627\n",
      "Epoch: [8089/10000] Training loss: 1.1035039093156435e-05 / Validation loss: 0.00015109509539584432 / Long term Validation loss: 0.1626\n",
      "Epoch: [8090/10000] Training loss: 1.0997937515666285e-05 / Validation loss: 0.00014938655507483413 / Long term Validation loss: 0.1624\n",
      "Epoch: [8091/10000] Training loss: 1.1037615055514214e-05 / Validation loss: 0.00015301326183251714 / Long term Validation loss: 0.1627\n",
      "Epoch: [8092/10000] Training loss: 1.1060989457583147e-05 / Validation loss: 0.00014960939280663863 / Long term Validation loss: 0.1626\n",
      "Epoch: [8093/10000] Training loss: 1.102154728096401e-05 / Validation loss: 0.00015091142119878025 / Long term Validation loss: 0.1626\n",
      "Epoch: [8094/10000] Training loss: 1.0989991926568733e-05 / Validation loss: 0.0001520608242899127 / Long term Validation loss: 0.1626\n",
      "Epoch: [8095/10000] Training loss: 1.1010576320662193e-05 / Validation loss: 0.00014930389402820405 / Long term Validation loss: 0.1625\n",
      "Epoch: [8096/10000] Training loss: 1.1030746172117085e-05 / Validation loss: 0.00015209533474684747 / Long term Validation loss: 0.1626\n",
      "Epoch: [8097/10000] Training loss: 1.1009181893829965e-05 / Validation loss: 0.00015062102226915032 / Long term Validation loss: 0.1625\n",
      "Epoch: [8098/10000] Training loss: 1.0983580815207817e-05 / Validation loss: 0.00015004940960807936 / Long term Validation loss: 0.1626\n",
      "Epoch: [8099/10000] Training loss: 1.0991595404091145e-05 / Validation loss: 0.0001521583181847848 / Long term Validation loss: 0.1626\n",
      "Epoch: [8100/10000] Training loss: 1.100739278671146e-05 / Validation loss: 0.0001497524997843757 / Long term Validation loss: 0.1626\n",
      "Epoch: [8101/10000] Training loss: 1.0997323455979548e-05 / Validation loss: 0.00015114467823115418 / Long term Validation loss: 0.1626\n",
      "Epoch: [8102/10000] Training loss: 1.0977732634750235e-05 / Validation loss: 0.00015127192722435762 / Long term Validation loss: 0.1626\n",
      "Epoch: [8103/10000] Training loss: 1.0977949161883618e-05 / Validation loss: 0.00014978296271732545 / Long term Validation loss: 0.1626\n",
      "Epoch: [8104/10000] Training loss: 1.0988955078130773e-05 / Validation loss: 0.00015171454354959228 / Long term Validation loss: 0.1626\n",
      "Epoch: [8105/10000] Training loss: 1.0985770964183527e-05 / Validation loss: 0.00015027415537327478 / Long term Validation loss: 0.1625\n",
      "Epoch: [8106/10000] Training loss: 1.0971829324080079e-05 / Validation loss: 0.00015044642740965746 / Long term Validation loss: 0.1625\n",
      "Epoch: [8107/10000] Training loss: 1.0967781381308373e-05 / Validation loss: 0.00015142285184335894 / Long term Validation loss: 0.1626\n",
      "Epoch: [8108/10000] Training loss: 1.0974216322020267e-05 / Validation loss: 0.00014985396395349456 / Long term Validation loss: 0.1626\n",
      "Epoch: [8109/10000] Training loss: 1.0974583874985738e-05 / Validation loss: 0.00015113520027750684 / Long term Validation loss: 0.1626\n",
      "Epoch: [8110/10000] Training loss: 1.0965524042664583e-05 / Validation loss: 0.00015067473126216126 / Long term Validation loss: 0.1625\n",
      "Epoch: [8111/10000] Training loss: 1.095975550178752e-05 / Validation loss: 0.00015011562707593524 / Long term Validation loss: 0.1625\n",
      "Epoch: [8112/10000] Training loss: 1.096231635391168e-05 / Validation loss: 0.0001512900487236686 / Long term Validation loss: 0.1626\n",
      "Epoch: [8113/10000] Training loss: 1.0963906983623818e-05 / Validation loss: 0.00015010007015467953 / Long term Validation loss: 0.1625\n",
      "Epoch: [8114/10000] Training loss: 1.0958650769191804e-05 / Validation loss: 0.0001506779661464548 / Long term Validation loss: 0.1625\n",
      "Epoch: [8115/10000] Training loss: 1.0952855438981693e-05 / Validation loss: 0.00015087764938334283 / Long term Validation loss: 0.1625\n",
      "Epoch: [8116/10000] Training loss: 1.095254034066863e-05 / Validation loss: 0.00015003929460789594 / Long term Validation loss: 0.1625\n",
      "Epoch: [8117/10000] Training loss: 1.0953886020888964e-05 / Validation loss: 0.00015102861766192014 / Long term Validation loss: 0.1626\n",
      "Epoch: [8118/10000] Training loss: 1.0951216412948272e-05 / Validation loss: 0.00015034583640603322 / Long term Validation loss: 0.1625\n",
      "Epoch: [8119/10000] Training loss: 1.0946348652327291e-05 / Validation loss: 0.00015039879460873197 / Long term Validation loss: 0.1625\n",
      "Epoch: [8120/10000] Training loss: 1.0944260224880636e-05 / Validation loss: 0.0001509202688879741 / Long term Validation loss: 0.1626\n",
      "Epoch: [8121/10000] Training loss: 1.094463535031911e-05 / Validation loss: 0.00015011795315884784 / Long term Validation loss: 0.1625\n",
      "Epoch: [8122/10000] Training loss: 1.0943388470850568e-05 / Validation loss: 0.00015078344383356277 / Long term Validation loss: 0.1625\n",
      "Epoch: [8123/10000] Training loss: 1.0939783263348107e-05 / Validation loss: 0.0001505286486585059 / Long term Validation loss: 0.1625\n",
      "Epoch: [8124/10000] Training loss: 1.0936924069468783e-05 / Validation loss: 0.00015026952824405664 / Long term Validation loss: 0.1625\n",
      "Epoch: [8125/10000] Training loss: 1.093618483377945e-05 / Validation loss: 0.0001508489224994604 / Long term Validation loss: 0.1626\n",
      "Epoch: [8126/10000] Training loss: 1.0935418162054924e-05 / Validation loss: 0.00015022294782509606 / Long term Validation loss: 0.1625\n",
      "Epoch: [8127/10000] Training loss: 1.0932951399770074e-05 / Validation loss: 0.00015057108913202205 / Long term Validation loss: 0.1625\n",
      "Epoch: [8128/10000] Training loss: 1.0930062910171947e-05 / Validation loss: 0.00015059819811531828 / Long term Validation loss: 0.1625\n",
      "Epoch: [8129/10000] Training loss: 1.0928446692853313e-05 / Validation loss: 0.00015021398115744422 / Long term Validation loss: 0.1625\n",
      "Epoch: [8130/10000] Training loss: 1.0927544693203743e-05 / Validation loss: 0.000150715521248841 / Long term Validation loss: 0.1626\n",
      "Epoch: [8131/10000] Training loss: 1.0925833564037983e-05 / Validation loss: 0.0001503010220569231 / Long term Validation loss: 0.1625\n",
      "Epoch: [8132/10000] Training loss: 1.092331997693925e-05 / Validation loss: 0.00015041114793962337 / Long term Validation loss: 0.1625\n",
      "Epoch: [8133/10000] Training loss: 1.0921230883039442e-05 / Validation loss: 0.00015058295381892186 / Long term Validation loss: 0.1626\n",
      "Epoch: [8134/10000] Training loss: 1.091992814394224e-05 / Validation loss: 0.0001501909681239702 / Long term Validation loss: 0.1625\n",
      "Epoch: [8135/10000] Training loss: 1.0918534214746278e-05 / Validation loss: 0.00015056077970784285 / Long term Validation loss: 0.1626\n",
      "Epoch: [8136/10000] Training loss: 1.0916484234692483e-05 / Validation loss: 0.0001503242366680501 / Long term Validation loss: 0.1625\n",
      "Epoch: [8137/10000] Training loss: 1.0914304626363428e-05 / Validation loss: 0.00015028446781062494 / Long term Validation loss: 0.1625\n",
      "Epoch: [8138/10000] Training loss: 1.0912618860141384e-05 / Validation loss: 0.00015050532763946785 / Long term Validation loss: 0.1626\n",
      "Epoch: [8139/10000] Training loss: 1.0911206904608134e-05 / Validation loss: 0.00015016924794102445 / Long term Validation loss: 0.1625\n",
      "Epoch: [8140/10000] Training loss: 1.0909493445898883e-05 / Validation loss: 0.00015041443670957817 / Long term Validation loss: 0.1626\n",
      "Epoch: [8141/10000] Training loss: 1.0907460489367713e-05 / Validation loss: 0.00015030886820631076 / Long term Validation loss: 0.1625\n",
      "Epoch: [8142/10000] Training loss: 1.0905564064796829e-05 / Validation loss: 0.00015019551097644675 / Long term Validation loss: 0.1625\n",
      "Epoch: [8143/10000] Training loss: 1.0903977661172592e-05 / Validation loss: 0.00015041094651531436 / Long term Validation loss: 0.1626\n",
      "Epoch: [8144/10000] Training loss: 1.0902395675087946e-05 / Validation loss: 0.00015014928467487094 / Long term Validation loss: 0.1625\n",
      "Epoch: [8145/10000] Training loss: 1.0900568099590025e-05 / Validation loss: 0.0001502941064892159 / Long term Validation loss: 0.1626\n",
      "Epoch: [8146/10000] Training loss: 1.0898649047620628e-05 / Validation loss: 0.0001502690416040387 / Long term Validation loss: 0.1626\n",
      "Epoch: [8147/10000] Training loss: 1.0896896419213692e-05 / Validation loss: 0.0001501302393042126 / Long term Validation loss: 0.1625\n",
      "Epoch: [8148/10000] Training loss: 1.0895283308100761e-05 / Validation loss: 0.000150314879798644 / Long term Validation loss: 0.1626\n",
      "Epoch: [8149/10000] Training loss: 1.0893591839416805e-05 / Validation loss: 0.00015012412935469892 / Long term Validation loss: 0.1625\n",
      "Epoch: [8150/10000] Training loss: 1.0891758870011104e-05 / Validation loss: 0.00015020003419680524 / Long term Validation loss: 0.1626\n",
      "Epoch: [8151/10000] Training loss: 1.0889934632494659e-05 / Validation loss: 0.00015021937831978817 / Long term Validation loss: 0.1626\n",
      "Epoch: [8152/10000] Training loss: 1.0888229608307912e-05 / Validation loss: 0.00015008360311058168 / Long term Validation loss: 0.1626\n",
      "Epoch: [8153/10000] Training loss: 1.0886567627031303e-05 / Validation loss: 0.0001502302439089069 / Long term Validation loss: 0.1626\n",
      "Epoch: [8154/10000] Training loss: 1.0884828532862989e-05 / Validation loss: 0.00015009499977511844 / Long term Validation loss: 0.1626\n",
      "Epoch: [8155/10000] Training loss: 1.088302343129964e-05 / Validation loss: 0.0001501254658158834 / Long term Validation loss: 0.1626\n",
      "Epoch: [8156/10000] Training loss: 1.0881253546928513e-05 / Validation loss: 0.00015016188916931712 / Long term Validation loss: 0.1626\n",
      "Epoch: [8157/10000] Training loss: 1.0879553026523874e-05 / Validation loss: 0.00015003987664249108 / Long term Validation loss: 0.1626\n",
      "Epoch: [8158/10000] Training loss: 1.0877855223795717e-05 / Validation loss: 0.00015014981498247883 / Long term Validation loss: 0.1626\n",
      "Epoch: [8159/10000] Training loss: 1.0876103448883117e-05 / Validation loss: 0.00015005408943329781 / Long term Validation loss: 0.1626\n",
      "Epoch: [8160/10000] Training loss: 1.087432598141032e-05 / Validation loss: 0.0001500580510295061 / Long term Validation loss: 0.1626\n",
      "Epoch: [8161/10000] Training loss: 1.0872581067553869e-05 / Validation loss: 0.00015009711014871496 / Long term Validation loss: 0.1626\n",
      "Epoch: [8162/10000] Training loss: 1.087087239110012e-05 / Validation loss: 0.00014999287958260353 / Long term Validation loss: 0.1626\n",
      "Epoch: [8163/10000] Training loss: 1.086915480363485e-05 / Validation loss: 0.00015007222057822624 / Long term Validation loss: 0.1626\n",
      "Epoch: [8164/10000] Training loss: 1.0867404268939062e-05 / Validation loss: 0.00015000224058897712 / Long term Validation loss: 0.1626\n",
      "Epoch: [8165/10000] Training loss: 1.0865645623436547e-05 / Validation loss: 0.00014999143711349339 / Long term Validation loss: 0.1626\n",
      "Epoch: [8166/10000] Training loss: 1.0863910033738205e-05 / Validation loss: 0.00015002468167227436 / Long term Validation loss: 0.1626\n",
      "Epoch: [8167/10000] Training loss: 1.086219255134406e-05 / Validation loss: 0.00014993704192359364 / Long term Validation loss: 0.1626\n",
      "Epoch: [8168/10000] Training loss: 1.0860465690658996e-05 / Validation loss: 0.00014999259243083504 / Long term Validation loss: 0.1627\n",
      "Epoch: [8169/10000] Training loss: 1.0858719821147004e-05 / Validation loss: 0.00014993903244445636 / Long term Validation loss: 0.1626\n",
      "Epoch: [8170/10000] Training loss: 1.0856972134025596e-05 / Validation loss: 0.00014992218886000324 / Long term Validation loss: 0.1626\n",
      "Epoch: [8171/10000] Training loss: 1.0855239067409139e-05 / Validation loss: 0.00014994759929053082 / Long term Validation loss: 0.1627\n",
      "Epoch: [8172/10000] Training loss: 1.0853515191437655e-05 / Validation loss: 0.00014987525129523752 / Long term Validation loss: 0.1627\n",
      "Epoch: [8173/10000] Training loss: 1.0851784474261705e-05 / Validation loss: 0.0001499138430127904 / Long term Validation loss: 0.1627\n",
      "Epoch: [8174/10000] Training loss: 1.085004288442577e-05 / Validation loss: 0.00014987054450407578 / Long term Validation loss: 0.1627\n",
      "Epoch: [8175/10000] Training loss: 1.084830111324947e-05 / Validation loss: 0.00014985238576680393 / Long term Validation loss: 0.1627\n",
      "Epoch: [8176/10000] Training loss: 1.0846568120641095e-05 / Validation loss: 0.0001498693786770128 / Long term Validation loss: 0.1627\n",
      "Epoch: [8177/10000] Training loss: 1.0844840140550694e-05 / Validation loss: 0.0001498097643246279 / Long term Validation loss: 0.1627\n",
      "Epoch: [8178/10000] Training loss: 1.084310790609173e-05 / Validation loss: 0.00014983635469518686 / Long term Validation loss: 0.1627\n",
      "Epoch: [8179/10000] Training loss: 1.0841369323217647e-05 / Validation loss: 0.000149799588028474 / Long term Validation loss: 0.1627\n",
      "Epoch: [8180/10000] Training loss: 1.0839630625175468e-05 / Validation loss: 0.00014978307936715083 / Long term Validation loss: 0.1627\n",
      "Epoch: [8181/10000] Training loss: 1.083789701223168e-05 / Validation loss: 0.00014979278031115604 / Long term Validation loss: 0.1627\n",
      "Epoch: [8182/10000] Training loss: 1.0836166424003411e-05 / Validation loss: 0.00014974401901243032 / Long term Validation loss: 0.1627\n",
      "Epoch: [8183/10000] Training loss: 1.0834433489984571e-05 / Validation loss: 0.00014976223054685456 / Long term Validation loss: 0.1627\n",
      "Epoch: [8184/10000] Training loss: 1.083269674272934e-05 / Validation loss: 0.00014972952486162733 / Long term Validation loss: 0.1627\n",
      "Epoch: [8185/10000] Training loss: 1.083095960285729e-05 / Validation loss: 0.0001497154493663116 / Long term Validation loss: 0.1627\n",
      "Epoch: [8186/10000] Training loss: 1.0829225281861365e-05 / Validation loss: 0.0001497184089145595 / Long term Validation loss: 0.1627\n",
      "Epoch: [8187/10000] Training loss: 1.0827492943346752e-05 / Validation loss: 0.00014967817116919428 / Long term Validation loss: 0.1627\n",
      "Epoch: [8188/10000] Training loss: 1.082575949787978e-05 / Validation loss: 0.00014968989380769352 / Long term Validation loss: 0.1627\n",
      "Epoch: [8189/10000] Training loss: 1.0824023731142878e-05 / Validation loss: 0.00014965969982896665 / Long term Validation loss: 0.1627\n",
      "Epoch: [8190/10000] Training loss: 1.0822287352484381e-05 / Validation loss: 0.00014964815866308197 / Long term Validation loss: 0.1627\n",
      "Epoch: [8191/10000] Training loss: 1.0820552405126702e-05 / Validation loss: 0.00014964533105332997 / Long term Validation loss: 0.1627\n",
      "Epoch: [8192/10000] Training loss: 1.0818818789255554e-05 / Validation loss: 0.00014961205548833471 / Long term Validation loss: 0.1627\n",
      "Epoch: [8193/10000] Training loss: 1.0817084798672707e-05 / Validation loss: 0.00014961850919883204 / Long term Validation loss: 0.1627\n",
      "Epoch: [8194/10000] Training loss: 1.0815349421447364e-05 / Validation loss: 0.0001495899613945519 / Long term Validation loss: 0.1627\n",
      "Epoch: [8195/10000] Training loss: 1.0813613356673944e-05 / Validation loss: 0.00014958033665622743 / Long term Validation loss: 0.1627\n",
      "Epoch: [8196/10000] Training loss: 1.0811877875990879e-05 / Validation loss: 0.00014957231436943621 / Long term Validation loss: 0.1627\n",
      "Epoch: [8197/10000] Training loss: 1.0810143244564974e-05 / Validation loss: 0.00014954451868271003 / Long term Validation loss: 0.1627\n",
      "Epoch: [8198/10000] Training loss: 1.0808408616896686e-05 / Validation loss: 0.00014954626187364697 / Long term Validation loss: 0.1627\n",
      "Epoch: [8199/10000] Training loss: 1.0806673213190955e-05 / Validation loss: 0.00014951909386206607 / Long term Validation loss: 0.1627\n",
      "Epoch: [8200/10000] Training loss: 1.080493717291722e-05 / Validation loss: 0.0001495108317856025 / Long term Validation loss: 0.1627\n",
      "Epoch: [8201/10000] Training loss: 1.0803201219779362e-05 / Validation loss: 0.00014949861425284344 / Long term Validation loss: 0.1627\n",
      "Epoch: [8202/10000] Training loss: 1.0801465730167069e-05 / Validation loss: 0.00014947548103978742 / Long term Validation loss: 0.1627\n",
      "Epoch: [8203/10000] Training loss: 1.0799730384895244e-05 / Validation loss: 0.00014947305958313125 / Long term Validation loss: 0.1627\n",
      "Epoch: [8204/10000] Training loss: 1.0797994657162844e-05 / Validation loss: 0.00014944743671733944 / Long term Validation loss: 0.1627\n",
      "Epoch: [8205/10000] Training loss: 1.0796258423012639e-05 / Validation loss: 0.00014943983628770883 / Long term Validation loss: 0.1628\n",
      "Epoch: [8206/10000] Training loss: 1.0794522018921208e-05 / Validation loss: 0.00014942432433545243 / Long term Validation loss: 0.1628\n",
      "Epoch: [8207/10000] Training loss: 1.0792785777416041e-05 / Validation loss: 0.00014940504451137071 / Long term Validation loss: 0.1628\n",
      "Epoch: [8208/10000] Training loss: 1.079104967040963e-05 / Validation loss: 0.00014939881791313842 / Long term Validation loss: 0.1628\n",
      "Epoch: [8209/10000] Training loss: 1.0789313408310507e-05 / Validation loss: 0.00014937509130934858 / Long term Validation loss: 0.1628\n",
      "Epoch: [8210/10000] Training loss: 1.0787576795301297e-05 / Validation loss: 0.00014936754082211514 / Long term Validation loss: 0.1628\n",
      "Epoch: [8211/10000] Training loss: 1.0785839919543405e-05 / Validation loss: 0.00014934985207309362 / Long term Validation loss: 0.1628\n",
      "Epoch: [8212/10000] Training loss: 1.0784102996744585e-05 / Validation loss: 0.00014933380372791465 / Long term Validation loss: 0.1628\n",
      "Epoch: [8213/10000] Training loss: 1.0782366120247605e-05 / Validation loss: 0.00014932422956633534 / Long term Validation loss: 0.1628\n",
      "Epoch: [8214/10000] Training loss: 1.07806291848702e-05 / Validation loss: 0.0001493027347570537 / Long term Validation loss: 0.1628\n",
      "Epoch: [8215/10000] Training loss: 1.07788920322816e-05 / Validation loss: 0.00014929452944993773 / Long term Validation loss: 0.1628\n",
      "Epoch: [8216/10000] Training loss: 1.077715462482526e-05 / Validation loss: 0.00014927562826743814 / Long term Validation loss: 0.1628\n",
      "Epoch: [8217/10000] Training loss: 1.0775417054261925e-05 / Validation loss: 0.00014926196009521623 / Long term Validation loss: 0.1628\n",
      "Epoch: [8218/10000] Training loss: 1.077367942405015e-05 / Validation loss: 0.00014924946940944586 / Long term Validation loss: 0.1628\n",
      "Epoch: [8219/10000] Training loss: 1.0771941738762458e-05 / Validation loss: 0.00014923035220533467 / Long term Validation loss: 0.1628\n",
      "Epoch: [8220/10000] Training loss: 1.0770203917934757e-05 / Validation loss: 0.00014922089292708078 / Long term Validation loss: 0.1628\n",
      "Epoch: [8221/10000] Training loss: 1.0768465892729563e-05 / Validation loss: 0.00014920175511446676 / Long term Validation loss: 0.1628\n",
      "Epoch: [8222/10000] Training loss: 1.0766727667664807e-05 / Validation loss: 0.00014918959208188361 / Long term Validation loss: 0.1628\n",
      "Epoch: [8223/10000] Training loss: 1.0764989302491331e-05 / Validation loss: 0.00014917479686656527 / Long term Validation loss: 0.1628\n",
      "Epoch: [8224/10000] Training loss: 1.0763250838875304e-05 / Validation loss: 0.000149157950268352 / Long term Validation loss: 0.1628\n",
      "Epoch: [8225/10000] Training loss: 1.0761512263295841e-05 / Validation loss: 0.00014914676315010657 / Long term Validation loss: 0.1628\n",
      "Epoch: [8226/10000] Training loss: 1.0759773529736283e-05 / Validation loss: 0.00014912814644953265 / Long term Validation loss: 0.1628\n",
      "Epoch: [8227/10000] Training loss: 1.0758034607671168e-05 / Validation loss: 0.00014911649736038568 / Long term Validation loss: 0.1628\n",
      "Epoch: [8228/10000] Training loss: 1.0756295508477243e-05 / Validation loss: 0.0001491001149638971 / Long term Validation loss: 0.1628\n",
      "Epoch: [8229/10000] Training loss: 1.0754556262842902e-05 / Validation loss: 0.00014908515023652916 / Long term Validation loss: 0.1628\n",
      "Epoch: [8230/10000] Training loss: 1.07528168883448e-05 / Validation loss: 0.00014907208977145488 / Long term Validation loss: 0.1628\n",
      "Epoch: [8231/10000] Training loss: 1.0751077373635815e-05 / Validation loss: 0.00014905458664030602 / Long term Validation loss: 0.1628\n",
      "Epoch: [8232/10000] Training loss: 1.0749337693388765e-05 / Validation loss: 0.00014904261151938716 / Long term Validation loss: 0.1628\n",
      "Epoch: [8233/10000] Training loss: 1.0747597834174408e-05 / Validation loss: 0.0001490255024503084 / Long term Validation loss: 0.1628\n",
      "Epoch: [8234/10000] Training loss: 1.0745857801685101e-05 / Validation loss: 0.00014901179843246587 / Long term Validation loss: 0.1628\n",
      "Epoch: [8235/10000] Training loss: 1.0744117612084848e-05 / Validation loss: 0.0001489970521829512 / Long term Validation loss: 0.1628\n",
      "Epoch: [8236/10000] Training loss: 1.074237727304532e-05 / Validation loss: 0.0001489808964051121 / Long term Validation loss: 0.1628\n",
      "Epoch: [8237/10000] Training loss: 1.0740636778109778e-05 / Validation loss: 0.00014896795353243981 / Long term Validation loss: 0.1628\n",
      "Epoch: [8238/10000] Training loss: 1.0738896114357958e-05 / Validation loss: 0.00014895092223248758 / Long term Validation loss: 0.1628\n",
      "Epoch: [8239/10000] Training loss: 1.073715527336299e-05 / Validation loss: 0.0001489377205348705 / Long term Validation loss: 0.1628\n",
      "Epoch: [8240/10000] Training loss: 1.0735414257745245e-05 / Validation loss: 0.0001489218212164668 / Long term Validation loss: 0.1628\n",
      "Epoch: [8241/10000] Training loss: 1.0733673074546344e-05 / Validation loss: 0.00014890691017642118 / Long term Validation loss: 0.1628\n",
      "Epoch: [8242/10000] Training loss: 1.0731931728251075e-05 / Validation loss: 0.00014889275334056316 / Long term Validation loss: 0.1628\n",
      "Epoch: [8243/10000] Training loss: 1.0730190216068105e-05 / Validation loss: 0.00014887641361067903 / Long term Validation loss: 0.1628\n",
      "Epoch: [8244/10000] Training loss: 1.0728448530879999e-05 / Validation loss: 0.00014886298911412956 / Long term Validation loss: 0.1628\n",
      "Epoch: [8245/10000] Training loss: 1.0726706667726194e-05 / Validation loss: 0.00014884663310009994 / Long term Validation loss: 0.1628\n",
      "Epoch: [8246/10000] Training loss: 1.0724964625898018e-05 / Validation loss: 0.0001488325037718978 / Long term Validation loss: 0.1628\n",
      "Epoch: [8247/10000] Training loss: 1.0723222408707728e-05 / Validation loss: 0.00014881726161261285 / Long term Validation loss: 0.1628\n",
      "Epoch: [8248/10000] Training loss: 1.072148001840462e-05 / Validation loss: 0.0001488018428919798 / Long term Validation loss: 0.1628\n",
      "Epoch: [8249/10000] Training loss: 1.071973745424964e-05 / Validation loss: 0.0001487876951864036 / Long term Validation loss: 0.1628\n",
      "Epoch: [8250/10000] Training loss: 1.0717994712868371e-05 / Validation loss: 0.00014877154925449843 / Long term Validation loss: 0.1628\n",
      "Epoch: [8251/10000] Training loss: 1.071625179052086e-05 / Validation loss: 0.00014875758606990483 / Long term Validation loss: 0.1628\n",
      "Epoch: [8252/10000] Training loss: 1.0714508685950248e-05 / Validation loss: 0.00014874172771934676 / Long term Validation loss: 0.1628\n",
      "Epoch: [8253/10000] Training loss: 1.0712765399540282e-05 / Validation loss: 0.00014872706101412474 / Long term Validation loss: 0.1628\n",
      "Epoch: [8254/10000] Training loss: 1.0711021932607655e-05 / Validation loss: 0.00014871206316837318 / Long term Validation loss: 0.1628\n",
      "Epoch: [8255/10000] Training loss: 1.0709278285084946e-05 / Validation loss: 0.00014869651678101203 / Long term Validation loss: 0.1628\n",
      "Epoch: [8256/10000] Training loss: 1.0707534455422313e-05 / Validation loss: 0.00014868216248361318 / Long term Validation loss: 0.1628\n",
      "Epoch: [8257/10000] Training loss: 1.0705790441508281e-05 / Validation loss: 0.00014866625567290352 / Long term Validation loss: 0.1629\n",
      "Epoch: [8258/10000] Training loss: 1.070404624149254e-05 / Validation loss: 0.00014865187229053624 / Long term Validation loss: 0.1629\n",
      "Epoch: [8259/10000] Training loss: 1.0702301854955345e-05 / Validation loss: 0.00014863627825161931 / Long term Validation loss: 0.1629\n",
      "Epoch: [8260/10000] Training loss: 1.0700557281880191e-05 / Validation loss: 0.00014862133727505002 / Long term Validation loss: 0.1629\n",
      "Epoch: [8261/10000] Training loss: 1.0698812522430877e-05 / Validation loss: 0.0001486063503893161 / Long term Validation loss: 0.1629\n",
      "Epoch: [8262/10000] Training loss: 1.0697067575978233e-05 / Validation loss: 0.00014859082410125134 / Long term Validation loss: 0.1629\n",
      "Epoch: [8263/10000] Training loss: 1.069532244127655e-05 / Validation loss: 0.00014857622767962397 / Long term Validation loss: 0.1629\n",
      "Epoch: [8264/10000] Training loss: 1.0693577117057276e-05 / Validation loss: 0.00014856050174107325 / Long term Validation loss: 0.1629\n",
      "Epoch: [8265/10000] Training loss: 1.0691831602190787e-05 / Validation loss: 0.00014854583438902622 / Long term Validation loss: 0.1629\n",
      "Epoch: [8266/10000] Training loss: 1.0690085896284054e-05 / Validation loss: 0.00014853034641561217 / Long term Validation loss: 0.1629\n",
      "Epoch: [8267/10000] Training loss: 1.0688339998984807e-05 / Validation loss: 0.0001485152791338988 / Long term Validation loss: 0.1629\n",
      "Epoch: [8268/10000] Training loss: 1.0686593910034885e-05 / Validation loss: 0.000148500203887871 / Long term Validation loss: 0.1629\n",
      "Epoch: [8269/10000] Training loss: 1.0684847628790428e-05 / Validation loss: 0.00014848473723152019 / Long term Validation loss: 0.1629\n",
      "Epoch: [8270/10000] Training loss: 1.068310115435004e-05 / Validation loss: 0.0001484699246742068 / Long term Validation loss: 0.1629\n",
      "Epoch: [8271/10000] Training loss: 1.0681354485852205e-05 / Validation loss: 0.00014845431620264253 / Long term Validation loss: 0.1629\n",
      "Epoch: [8272/10000] Training loss: 1.0679607622445888e-05 / Validation loss: 0.00014843946272625363 / Long term Validation loss: 0.1629\n",
      "Epoch: [8273/10000] Training loss: 1.0677860563669807e-05 / Validation loss: 0.00014842400109893564 / Long term Validation loss: 0.1629\n",
      "Epoch: [8274/10000] Training loss: 1.067611330902836e-05 / Validation loss: 0.00014840888241600953 / Long term Validation loss: 0.1629\n",
      "Epoch: [8275/10000] Training loss: 1.067436585814065e-05 / Validation loss: 0.00014839369474699754 / Long term Validation loss: 0.1629\n",
      "Epoch: [8276/10000] Training loss: 1.0672618210437732e-05 / Validation loss: 0.00014837829269426543 / Long term Validation loss: 0.1629\n",
      "Epoch: [8277/10000] Training loss: 1.0670870365249354e-05 / Validation loss: 0.00014836330019548464 / Long term Validation loss: 0.1629\n",
      "Epoch: [8278/10000] Training loss: 1.0669122321915301e-05 / Validation loss: 0.00014834776864706322 / Long term Validation loss: 0.1629\n",
      "Epoch: [8279/10000] Training loss: 1.066737407973504e-05 / Validation loss: 0.00014833278089617757 / Long term Validation loss: 0.1629\n",
      "Epoch: [8280/10000] Training loss: 1.0665625638218407e-05 / Validation loss: 0.0001483173130265314 / Long term Validation loss: 0.1629\n",
      "Epoch: [8281/10000] Training loss: 1.066387699683141e-05 / Validation loss: 0.00014830216793896873 / Long term Validation loss: 0.1629\n",
      "Epoch: [8282/10000] Training loss: 1.0662128155163978e-05 / Validation loss: 0.00014828687174615643 / Long term Validation loss: 0.1629\n",
      "Epoch: [8283/10000] Training loss: 1.0660379112706578e-05 / Validation loss: 0.00014827152542886524 / Long term Validation loss: 0.1629\n",
      "Epoch: [8284/10000] Training loss: 1.0658629868933698e-05 / Validation loss: 0.00014825637999928983 / Long term Validation loss: 0.1629\n",
      "Epoch: [8285/10000] Training loss: 1.0656880423301605e-05 / Validation loss: 0.00014824090606344723 / Long term Validation loss: 0.1629\n",
      "Epoch: [8286/10000] Training loss: 1.0655130775226164e-05 / Validation loss: 0.00014822580286364401 / Long term Validation loss: 0.1629\n",
      "Epoch: [8287/10000] Training loss: 1.0653380924227516e-05 / Validation loss: 0.00014821032454379618 / Long term Validation loss: 0.1629\n",
      "Epoch: [8288/10000] Training loss: 1.065163086978421e-05 / Validation loss: 0.0001481951476863269 / Long term Validation loss: 0.1629\n",
      "Epoch: [8289/10000] Training loss: 1.0649880611482996e-05 / Validation loss: 0.00014817975852735032 / Long term Validation loss: 0.1629\n",
      "Epoch: [8290/10000] Training loss: 1.0648130148850907e-05 / Validation loss: 0.00014816444922136803 / Long term Validation loss: 0.1629\n",
      "Epoch: [8291/10000] Training loss: 1.0646379481449744e-05 / Validation loss: 0.00014814916945181175 / Long term Validation loss: 0.1629\n",
      "Epoch: [8292/10000] Training loss: 1.0644628608809867e-05 / Validation loss: 0.00014813374413678645 / Long term Validation loss: 0.1629\n",
      "Epoch: [8293/10000] Training loss: 1.064287753044704e-05 / Validation loss: 0.00014811852704545947 / Long term Validation loss: 0.1629\n",
      "Epoch: [8294/10000] Training loss: 1.0641126245914207e-05 / Validation loss: 0.00014810305195455114 / Long term Validation loss: 0.1629\n",
      "Epoch: [8295/10000] Training loss: 1.063937475473309e-05 / Validation loss: 0.0001480878234134944 / Long term Validation loss: 0.1629\n",
      "Epoch: [8296/10000] Training loss: 1.063762305649908e-05 / Validation loss: 0.00014807237009700297 / Long term Validation loss: 0.1629\n",
      "Epoch: [8297/10000] Training loss: 1.0635871150767186e-05 / Validation loss: 0.00014805707176767472 / Long term Validation loss: 0.1629\n",
      "Epoch: [8298/10000] Training loss: 1.063411903714736e-05 / Validation loss: 0.0001480416809398515 / Long term Validation loss: 0.1629\n",
      "Epoch: [8299/10000] Training loss: 1.0632366715216135e-05 / Validation loss: 0.00014802629460990477 / Long term Validation loss: 0.1629\n",
      "Epoch: [8300/10000] Training loss: 1.0630614184567174e-05 / Validation loss: 0.00014801096396146728 / Long term Validation loss: 0.1629\n",
      "Epoch: [8301/10000] Training loss: 1.062886144479244e-05 / Validation loss: 0.00014799551064336805 / Long term Validation loss: 0.1629\n",
      "Epoch: [8302/10000] Training loss: 1.0627108495473793e-05 / Validation loss: 0.00014798020611611734 / Long term Validation loss: 0.1629\n",
      "Epoch: [8303/10000] Training loss: 1.062535533622632e-05 / Validation loss: 0.00014796472746965726 / Long term Validation loss: 0.1629\n",
      "Epoch: [8304/10000] Training loss: 1.0623601966640605e-05 / Validation loss: 0.0001479494062431703 / Long term Validation loss: 0.1629\n",
      "Epoch: [8305/10000] Training loss: 1.062184838635332e-05 / Validation loss: 0.00014793394162639827 / Long term Validation loss: 0.1629\n",
      "Epoch: [8306/10000] Training loss: 1.0620094594973032e-05 / Validation loss: 0.00014791857292607236 / Long term Validation loss: 0.1629\n",
      "Epoch: [8307/10000] Training loss: 1.0618340592143445e-05 / Validation loss: 0.000147903143305269 / Long term Validation loss: 0.1629\n",
      "Epoch: [8308/10000] Training loss: 1.0616586377489736e-05 / Validation loss: 0.00014788771839690918 / Long term Validation loss: 0.1629\n",
      "Epoch: [8309/10000] Training loss: 1.0614831950651758e-05 / Validation loss: 0.0001478723221139148 / Long term Validation loss: 0.1629\n",
      "Epoch: [8310/10000] Training loss: 1.0613077311270631e-05 / Validation loss: 0.00014785685244793033 / Long term Validation loss: 0.1629\n",
      "Epoch: [8311/10000] Training loss: 1.0611322458984021e-05 / Validation loss: 0.00014784147138244342 / Long term Validation loss: 0.1629\n",
      "Epoch: [8312/10000] Training loss: 1.0609567393449903e-05 / Validation loss: 0.00014782597925170585 / Long term Validation loss: 0.1630\n",
      "Epoch: [8313/10000] Training loss: 1.060781211431139e-05 / Validation loss: 0.0001478105899391196 / Long term Validation loss: 0.1630\n",
      "Epoch: [8314/10000] Training loss: 1.06060566212407e-05 / Validation loss: 0.00014779509756381878 / Long term Validation loss: 0.1630\n",
      "Epoch: [8315/10000] Training loss: 1.060430091389178e-05 / Validation loss: 0.00014777968133411956 / Long term Validation loss: 0.1630\n",
      "Epoch: [8316/10000] Training loss: 1.0602544991944751e-05 / Validation loss: 0.00014776420291691481 / Long term Validation loss: 0.1630\n",
      "Epoch: [8317/10000] Training loss: 1.0600788855066224e-05 / Validation loss: 0.0001477487513632322 / Long term Validation loss: 0.1630\n",
      "Epoch: [8318/10000] Training loss: 1.0599032502938945e-05 / Validation loss: 0.00014773329013281043 / Long term Validation loss: 0.1630\n",
      "Epoch: [8319/10000] Training loss: 1.0597275935242507e-05 / Validation loss: 0.00014771780533397007 / Long term Validation loss: 0.1630\n",
      "Epoch: [8320/10000] Training loss: 1.0595519151661153e-05 / Validation loss: 0.00014770235521900272 / Long term Validation loss: 0.1630\n",
      "Epoch: [8321/10000] Training loss: 1.0593762151887298e-05 / Validation loss: 0.00014768684636558296 / Long term Validation loss: 0.1630\n",
      "Epoch: [8322/10000] Training loss: 1.0592004935608704e-05 / Validation loss: 0.00014767139638591155 / Long term Validation loss: 0.1630\n",
      "Epoch: [8323/10000] Training loss: 1.0590247502529273e-05 / Validation loss: 0.00014765587512977768 / Long term Validation loss: 0.1630\n",
      "Epoch: [8324/10000] Training loss: 1.0588489852343083e-05 / Validation loss: 0.0001476404141510531 / Long term Validation loss: 0.1630\n",
      "Epoch: [8325/10000] Training loss: 1.0586731984762923e-05 / Validation loss: 0.00014762489054383348 / Long term Validation loss: 0.1630\n",
      "Epoch: [8326/10000] Training loss: 1.0584973899491223e-05 / Validation loss: 0.00014760941064086725 / Long term Validation loss: 0.1630\n",
      "Epoch: [8327/10000] Training loss: 1.058321559624681e-05 / Validation loss: 0.00014759389069347797 / Long term Validation loss: 0.1630\n",
      "Epoch: [8328/10000] Training loss: 1.0581457074741558e-05 / Validation loss: 0.00014757838849057428 / Long term Validation loss: 0.1630\n",
      "Epoch: [8329/10000] Training loss: 1.0579698334698504e-05 / Validation loss: 0.00014756287360183728 / Long term Validation loss: 0.1630\n",
      "Epoch: [8330/10000] Training loss: 1.0577939375839246e-05 / Validation loss: 0.000147547349914301 / Long term Validation loss: 0.1630\n",
      "Epoch: [8331/10000] Training loss: 1.0576180197890523e-05 / Validation loss: 0.00014753183779431283 / Long term Validation loss: 0.1630\n",
      "Epoch: [8332/10000] Training loss: 1.057442080058342e-05 / Validation loss: 0.00014751629629752998 / Long term Validation loss: 0.1630\n",
      "Epoch: [8333/10000] Training loss: 1.057266118364883e-05 / Validation loss: 0.00014750078263571064 / Long term Validation loss: 0.1630\n",
      "Epoch: [8334/10000] Training loss: 1.0570901346826504e-05 / Validation loss: 0.00014748522823761754 / Long term Validation loss: 0.1630\n",
      "Epoch: [8335/10000] Training loss: 1.056914128985235e-05 / Validation loss: 0.00014746970834818808 / Long term Validation loss: 0.1630\n",
      "Epoch: [8336/10000] Training loss: 1.0567381012473607e-05 / Validation loss: 0.00014745414574542692 / Long term Validation loss: 0.1630\n",
      "Epoch: [8337/10000] Training loss: 1.0565620514432109e-05 / Validation loss: 0.00014743861572238197 / Long term Validation loss: 0.1630\n",
      "Epoch: [8338/10000] Training loss: 1.0563859795481287e-05 / Validation loss: 0.00014742304844029538 / Long term Validation loss: 0.1630\n",
      "Epoch: [8339/10000] Training loss: 1.0562098855369523e-05 / Validation loss: 0.00014740750572663535 / Long term Validation loss: 0.1630\n",
      "Epoch: [8340/10000] Training loss: 1.0560337693855476e-05 / Validation loss: 0.00014739193575377145 / Long term Validation loss: 0.1630\n",
      "Epoch: [8341/10000] Training loss: 1.0558576310694416e-05 / Validation loss: 0.00014737637924353204 / Long term Validation loss: 0.1630\n",
      "Epoch: [8342/10000] Training loss: 1.0556814705649552e-05 / Validation loss: 0.00014736080716126713 / Long term Validation loss: 0.1630\n",
      "Epoch: [8343/10000] Training loss: 1.0555052878483115e-05 / Validation loss: 0.00014734523698508228 / Long term Validation loss: 0.1630\n",
      "Epoch: [8344/10000] Training loss: 1.0553290828962604e-05 / Validation loss: 0.0001473296623567 / Long term Validation loss: 0.1630\n",
      "Epoch: [8345/10000] Training loss: 1.0551528556857024e-05 / Validation loss: 0.00014731407948110036 / Long term Validation loss: 0.1630\n",
      "Epoch: [8346/10000] Training loss: 1.0549766061938124e-05 / Validation loss: 0.0001472985012807757 / Long term Validation loss: 0.1630\n",
      "Epoch: [8347/10000] Training loss: 1.0548003343981299e-05 / Validation loss: 0.00014728290705869187 / Long term Validation loss: 0.1630\n",
      "Epoch: [8348/10000] Training loss: 1.0546240402762644e-05 / Validation loss: 0.00014726732403464005 / Long term Validation loss: 0.1630\n",
      "Epoch: [8349/10000] Training loss: 1.0544477238063559e-05 / Validation loss: 0.0001472517198402156 / Long term Validation loss: 0.1630\n",
      "Epoch: [8350/10000] Training loss: 1.0542713849664598e-05 / Validation loss: 0.0001472361307933742 / Long term Validation loss: 0.1630\n",
      "Epoch: [8351/10000] Training loss: 1.0540950237352689e-05 / Validation loss: 0.00014722051780841693 / Long term Validation loss: 0.1630\n",
      "Epoch: [8352/10000] Training loss: 1.0539186400912972e-05 / Validation loss: 0.00014720492177981706 / Long term Validation loss: 0.1630\n",
      "Epoch: [8353/10000] Training loss: 1.05374223401375e-05 / Validation loss: 0.00014718930090898207 / Long term Validation loss: 0.1630\n",
      "Epoch: [8354/10000] Training loss: 1.053565805481607e-05 / Validation loss: 0.00014717369726027164 / Long term Validation loss: 0.1630\n",
      "Epoch: [8355/10000] Training loss: 1.053389354474552e-05 / Validation loss: 0.00014715806911376567 / Long term Validation loss: 0.1630\n",
      "Epoch: [8356/10000] Training loss: 1.0532128809720294e-05 / Validation loss: 0.00014714245751078548 / Long term Validation loss: 0.1630\n",
      "Epoch: [8357/10000] Training loss: 1.053036384954174e-05 / Validation loss: 0.00014712682241943374 / Long term Validation loss: 0.1630\n",
      "Epoch: [8358/10000] Training loss: 1.0528598664008937e-05 / Validation loss: 0.00014711120276809516 / Long term Validation loss: 0.1630\n",
      "Epoch: [8359/10000] Training loss: 1.0526833252927505e-05 / Validation loss: 0.00014709556082797355 / Long term Validation loss: 0.1630\n",
      "Epoch: [8360/10000] Training loss: 1.0525067616101012e-05 / Validation loss: 0.0001470799332143086 / Long term Validation loss: 0.1630\n",
      "Epoch: [8361/10000] Training loss: 1.0523301753339264e-05 / Validation loss: 0.00014706428435336204 / Long term Validation loss: 0.1630\n",
      "Epoch: [8362/10000] Training loss: 1.0521535664450183e-05 / Validation loss: 0.0001470486490038677 / Long term Validation loss: 0.1631\n",
      "Epoch: [8363/10000] Training loss: 1.0519769349247614e-05 / Validation loss: 0.00014703299304185007 / Long term Validation loss: 0.1631\n",
      "Epoch: [8364/10000] Training loss: 1.051800280754366e-05 / Validation loss: 0.00014701735029237916 / Long term Validation loss: 0.1631\n",
      "Epoch: [8365/10000] Training loss: 1.0516236039156173e-05 / Validation loss: 0.000147001686968453 / Long term Validation loss: 0.1631\n",
      "Epoch: [8366/10000] Training loss: 1.0514469043901205e-05 / Validation loss: 0.0001469860372372713 / Long term Validation loss: 0.1631\n",
      "Epoch: [8367/10000] Training loss: 1.051270182160056e-05 / Validation loss: 0.00014697036620618081 / Long term Validation loss: 0.1631\n",
      "Epoch: [8368/10000] Training loss: 1.0510934372074097e-05 / Validation loss: 0.00014695470998645768 / Long term Validation loss: 0.1631\n",
      "Epoch: [8369/10000] Training loss: 1.0509166695147486e-05 / Validation loss: 0.00014693903079914233 / Long term Validation loss: 0.1631\n",
      "Epoch: [8370/10000] Training loss: 1.0507398790644156e-05 / Validation loss: 0.0001469233686861452 / Long term Validation loss: 0.1631\n",
      "Epoch: [8371/10000] Training loss: 1.0505630658393682e-05 / Validation loss: 0.00014690768075774336 / Long term Validation loss: 0.1631\n",
      "Epoch: [8372/10000] Training loss: 1.0503862298222817e-05 / Validation loss: 0.00014689201350980078 / Long term Validation loss: 0.1631\n",
      "Epoch: [8373/10000] Training loss: 1.0502093709965059e-05 / Validation loss: 0.0001468763160581323 / Long term Validation loss: 0.1631\n",
      "Epoch: [8374/10000] Training loss: 1.0500324893450208e-05 / Validation loss: 0.0001468606446860025 / Long term Validation loss: 0.1631\n",
      "Epoch: [8375/10000] Training loss: 1.0498555848515766e-05 / Validation loss: 0.00014684493662002032 / Long term Validation loss: 0.1631\n",
      "Epoch: [8376/10000] Training loss: 1.0496786574994285e-05 / Validation loss: 0.00014682926251712814 / Long term Validation loss: 0.1631\n",
      "Epoch: [8377/10000] Training loss: 1.0495017072727421e-05 / Validation loss: 0.00014681354226041193 / Long term Validation loss: 0.1631\n",
      "Epoch: [8378/10000] Training loss: 1.0493247341550037e-05 / Validation loss: 0.00014679786740976418 / Long term Validation loss: 0.1631\n",
      "Epoch: [8379/10000] Training loss: 1.0491477381308244e-05 / Validation loss: 0.00014678213263503181 / Long term Validation loss: 0.1631\n",
      "Epoch: [8380/10000] Training loss: 1.0489707191838746e-05 / Validation loss: 0.00014676645994342452 / Long term Validation loss: 0.1631\n",
      "Epoch: [8381/10000] Training loss: 1.048793677299253e-05 / Validation loss: 0.0001467507071613687 / Long term Validation loss: 0.1631\n",
      "Epoch: [8382/10000] Training loss: 1.0486166124607442e-05 / Validation loss: 0.00014673504098999959 / Long term Validation loss: 0.1631\n",
      "Epoch: [8383/10000] Training loss: 1.0484395246540227e-05 / Validation loss: 0.00014671926488855775 / Long term Validation loss: 0.1631\n",
      "Epoch: [8384/10000] Training loss: 1.0482624138628891e-05 / Validation loss: 0.00014670361189674856 / Long term Validation loss: 0.1631\n",
      "Epoch: [8385/10000] Training loss: 1.0480852800737274e-05 / Validation loss: 0.00014668780426524582 / Long term Validation loss: 0.1631\n",
      "Epoch: [8386/10000] Training loss: 1.0479081232702376e-05 / Validation loss: 0.00014667217477779026 / Long term Validation loss: 0.1631\n",
      "Epoch: [8387/10000] Training loss: 1.0477309434397469e-05 / Validation loss: 0.00014665632274661548 / Long term Validation loss: 0.1631\n",
      "Epoch: [8388/10000] Training loss: 1.0475537405657044e-05 / Validation loss: 0.00014664073300937227 / Long term Validation loss: 0.1631\n",
      "Epoch: [8389/10000] Training loss: 1.0473765146368165e-05 / Validation loss: 0.00014662481614097106 / Long term Validation loss: 0.1631\n",
      "Epoch: [8390/10000] Training loss: 1.0471992656361428e-05 / Validation loss: 0.0001466092920844956 / Long term Validation loss: 0.1631\n",
      "Epoch: [8391/10000] Training loss: 1.0470219935546107e-05 / Validation loss: 0.0001465932775007989 / Long term Validation loss: 0.1631\n",
      "Epoch: [8392/10000] Training loss: 1.046844698374915e-05 / Validation loss: 0.00014657786108306794 / Long term Validation loss: 0.1631\n",
      "Epoch: [8393/10000] Training loss: 1.0466673800919708e-05 / Validation loss: 0.0001465616951999117 / Long term Validation loss: 0.1631\n",
      "Epoch: [8394/10000] Training loss: 1.0464900386888277e-05 / Validation loss: 0.00014654645521641345 / Long term Validation loss: 0.1631\n",
      "Epoch: [8395/10000] Training loss: 1.0463126741683845e-05 / Validation loss: 0.0001465300495639449 / Long term Validation loss: 0.1631\n",
      "Epoch: [8396/10000] Training loss: 1.0461352865170999e-05 / Validation loss: 0.0001465151002916228 / Long term Validation loss: 0.1631\n",
      "Epoch: [8397/10000] Training loss: 1.0459578757556482e-05 / Validation loss: 0.00014649830691998813 / Long term Validation loss: 0.1631\n",
      "Epoch: [8398/10000] Training loss: 1.0457804418843834e-05 / Validation loss: 0.00014648384064548864 / Long term Validation loss: 0.1631\n",
      "Epoch: [8399/10000] Training loss: 1.0456029849673871e-05 / Validation loss: 0.0001464664089827414 / Long term Validation loss: 0.1631\n",
      "Epoch: [8400/10000] Training loss: 1.0454255050525103e-05 / Validation loss: 0.0001464527533942916 / Long term Validation loss: 0.1631\n",
      "Epoch: [8401/10000] Training loss: 1.045248002318311e-05 / Validation loss: 0.00014643425370761324 / Long term Validation loss: 0.1631\n",
      "Epoch: [8402/10000] Training loss: 1.0450704769656138e-05 / Validation loss: 0.0001464219742894768 / Long term Validation loss: 0.1631\n",
      "Epoch: [8403/10000] Training loss: 1.0448929294939266e-05 / Validation loss: 0.00014640166037237633 / Long term Validation loss: 0.1631\n",
      "Epoch: [8404/10000] Training loss: 1.0447153605883149e-05 / Validation loss: 0.00014639174513602247 / Long term Validation loss: 0.1631\n",
      "Epoch: [8405/10000] Training loss: 1.0445377716927615e-05 / Validation loss: 0.00014636830523413597 / Long term Validation loss: 0.1632\n",
      "Epoch: [8406/10000] Training loss: 1.0443601650284894e-05 / Validation loss: 0.00014636250171242966 / Long term Validation loss: 0.1632\n",
      "Epoch: [8407/10000] Training loss: 1.0441825449299624e-05 / Validation loss: 0.00014633360176046165 / Long term Validation loss: 0.1632\n",
      "Epoch: [8408/10000] Training loss: 1.0440049185504885e-05 / Validation loss: 0.00014633503858595362 / Long term Validation loss: 0.1632\n",
      "Epoch: [8409/10000] Training loss: 1.0438272993681318e-05 / Validation loss: 0.0001462964754733347 / Long term Validation loss: 0.1632\n",
      "Epoch: [8410/10000] Training loss: 1.0436497106473655e-05 / Validation loss: 0.00014631082155041 / Long term Validation loss: 0.1632\n",
      "Epoch: [8411/10000] Training loss: 1.043472195646118e-05 / Validation loss: 0.00014625493668026497 / Long term Validation loss: 0.1632\n",
      "Epoch: [8412/10000] Training loss: 1.0432948313245599e-05 / Validation loss: 0.00014629258689833024 / Long term Validation loss: 0.1632\n",
      "Epoch: [8413/10000] Training loss: 1.0431177605533738e-05 / Validation loss: 0.00014620526279244996 / Long term Validation loss: 0.1632\n",
      "Epoch: [8414/10000] Training loss: 1.0429412431418742e-05 / Validation loss: 0.00014628550566911272 / Long term Validation loss: 0.1632\n",
      "Epoch: [8415/10000] Training loss: 1.0427657636389056e-05 / Validation loss: 0.000146140422939486 / Long term Validation loss: 0.1632\n",
      "Epoch: [8416/10000] Training loss: 1.0425922184764493e-05 / Validation loss: 0.00014629947927931685 / Long term Validation loss: 0.1632\n",
      "Epoch: [8417/10000] Training loss: 1.042422292780356e-05 / Validation loss: 0.0001460470321852159 / Long term Validation loss: 0.1632\n",
      "Epoch: [8418/10000] Training loss: 1.0422591497975602e-05 / Validation loss: 0.0001463537507870795 / Long term Validation loss: 0.1632\n",
      "Epoch: [8419/10000] Training loss: 1.0421087931460272e-05 / Validation loss: 0.00014589948033263584 / Long term Validation loss: 0.1632\n",
      "Epoch: [8420/10000] Training loss: 1.0419826407198605e-05 / Validation loss: 0.0001464864056986517 / Long term Validation loss: 0.1632\n",
      "Epoch: [8421/10000] Training loss: 1.04190258957916e-05 / Validation loss: 0.0001456487995530879 / Long term Validation loss: 0.1633\n",
      "Epoch: [8422/10000] Training loss: 1.041910795056147e-05 / Validation loss: 0.00014677468931843167 / Long term Validation loss: 0.1633\n",
      "Epoch: [8423/10000] Training loss: 1.0420889746571133e-05 / Validation loss: 0.00014520254575502042 / Long term Validation loss: 0.1633\n",
      "Epoch: [8424/10000] Training loss: 1.0425962869391243e-05 / Validation loss: 0.00014738096010300544 / Long term Validation loss: 0.1637\n",
      "Epoch: [8425/10000] Training loss: 1.0437445875156219e-05 / Validation loss: 0.00014439312639564085 / Long term Validation loss: 0.1635\n",
      "Epoch: [8426/10000] Training loss: 1.0461480463591935e-05 / Validation loss: 0.0001486654317902715 / Long term Validation loss: 0.1644\n",
      "Epoch: [8427/10000] Training loss: 1.0510225101582913e-05 / Validation loss: 0.00014295034420668898 / Long term Validation loss: 0.1635\n",
      "Epoch: [8428/10000] Training loss: 1.0607868094841959e-05 / Validation loss: 0.0001514934902548219 / Long term Validation loss: 0.1655\n",
      "Epoch: [8429/10000] Training loss: 1.0802707705256506e-05 / Validation loss: 0.0001405990458882119 / Long term Validation loss: 0.1625\n",
      "Epoch: [8430/10000] Training loss: 1.1191502765521112e-05 / Validation loss: 0.00015817100195598754 / Long term Validation loss: 0.1650\n",
      "Epoch: [8431/10000] Training loss: 1.1967916636617723e-05 / Validation loss: 0.0001378935845199159 / Long term Validation loss: 0.1612\n",
      "Epoch: [8432/10000] Training loss: 1.351801081856148e-05 / Validation loss: 0.00017548966236859858 / Long term Validation loss: 0.1604\n",
      "Epoch: [8433/10000] Training loss: 1.6597470971877407e-05 / Validation loss: 0.00014027608650839032 / Long term Validation loss: 0.1552\n",
      "Epoch: [8434/10000] Training loss: 2.2632051384276353e-05 / Validation loss: 0.00022358103093476068 / Long term Validation loss: 0.1610\n",
      "Epoch: [8435/10000] Training loss: 3.40632966014977e-05 / Validation loss: 0.00017021558362799807 / Long term Validation loss: 0.1600\n",
      "Epoch: [8436/10000] Training loss: 5.4205169842356156e-05 / Validation loss: 0.00033954433119872204 / Long term Validation loss: 0.1679\n",
      "Epoch: [8437/10000] Training loss: 8.450029273768328e-05 / Validation loss: 0.00024688356531809213 / Long term Validation loss: 0.1729\n",
      "Epoch: [8438/10000] Training loss: 0.00011623380169732208 / Validation loss: 0.0004234651667830836 / Long term Validation loss: 0.1702\n",
      "Epoch: [8439/10000] Training loss: 0.00012244958198425338 / Validation loss: 0.00020142276586465973 / Long term Validation loss: 0.1673\n",
      "Epoch: [8440/10000] Training loss: 8.11895029385118e-05 / Validation loss: 0.00020591543668246393 / Long term Validation loss: 0.1571\n",
      "Epoch: [8441/10000] Training loss: 2.4797979662805766e-05 / Validation loss: 0.00017138808039621737 / Long term Validation loss: 0.1593\n",
      "Epoch: [8442/10000] Training loss: 1.3548325922794825e-05 / Validation loss: 0.0001651780715288392 / Long term Validation loss: 0.1549\n",
      "Epoch: [8443/10000] Training loss: 4.719643930797455e-05 / Validation loss: 0.00031236883150767856 / Long term Validation loss: 0.1640\n",
      "Epoch: [8444/10000] Training loss: 6.673823420336927e-05 / Validation loss: 0.00016058771255304607 / Long term Validation loss: 0.1541\n",
      "Epoch: [8445/10000] Training loss: 4.072907521743509e-05 / Validation loss: 0.0001662422804121608 / Long term Validation loss: 0.1625\n",
      "Epoch: [8446/10000] Training loss: 1.1773429373973155e-05 / Validation loss: 0.00020335249904426177 / Long term Validation loss: 0.1556\n",
      "Epoch: [8447/10000] Training loss: 2.2218640068704805e-05 / Validation loss: 0.0001636323843474404 / Long term Validation loss: 0.1536\n",
      "Epoch: [8448/10000] Training loss: 4.362735703209117e-05 / Validation loss: 0.00023520061895201263 / Long term Validation loss: 0.1561\n",
      "Epoch: [8449/10000] Training loss: 3.438110745485645e-05 / Validation loss: 0.0001454893074824469 / Long term Validation loss: 0.1630\n",
      "Epoch: [8450/10000] Training loss: 1.2769914872594204e-05 / Validation loss: 0.00014370385380575596 / Long term Validation loss: 0.1606\n",
      "Epoch: [8451/10000] Training loss: 1.629892976142516e-05 / Validation loss: 0.00022406424874655103 / Long term Validation loss: 0.1555\n",
      "Epoch: [8452/10000] Training loss: 3.1317779061309e-05 / Validation loss: 0.0001479335886929183 / Long term Validation loss: 0.1521\n",
      "Epoch: [8453/10000] Training loss: 2.6190412499419725e-05 / Validation loss: 0.00016240403106928 / Long term Validation loss: 0.1644\n",
      "Epoch: [8454/10000] Training loss: 1.1881053081465137e-05 / Validation loss: 0.00017237307245858 / Long term Validation loss: 0.1593\n",
      "Epoch: [8455/10000] Training loss: 1.4559708526587216e-05 / Validation loss: 0.0001459921679054518 / Long term Validation loss: 0.1508\n",
      "Epoch: [8456/10000] Training loss: 2.435301939451336e-05 / Validation loss: 0.00018771353793475685 / Long term Validation loss: 0.1575\n",
      "Epoch: [8457/10000] Training loss: 2.016678512261567e-05 / Validation loss: 0.0001434604197989734 / Long term Validation loss: 0.1634\n",
      "Epoch: [8458/10000] Training loss: 1.1035392988558627e-05 / Validation loss: 0.00013983955695836506 / Long term Validation loss: 0.1593\n",
      "Epoch: [8459/10000] Training loss: 1.3769753554394326e-05 / Validation loss: 0.0001838287164069745 / Long term Validation loss: 0.1581\n",
      "Epoch: [8460/10000] Training loss: 1.9871497943309307e-05 / Validation loss: 0.0001389569227614359 / Long term Validation loss: 0.1549\n",
      "Epoch: [8461/10000] Training loss: 1.6204875893706105e-05 / Validation loss: 0.0001482760853117863 / Long term Validation loss: 0.1658\n",
      "Epoch: [8462/10000] Training loss: 1.0605947116615769e-05 / Validation loss: 0.00016110381752273796 / Long term Validation loss: 0.1637\n",
      "Epoch: [8463/10000] Training loss: 1.3190088576951921e-05 / Validation loss: 0.00013862229483007832 / Long term Validation loss: 0.1532\n",
      "Epoch: [8464/10000] Training loss: 1.6794909000821028e-05 / Validation loss: 0.00016286407340696573 / Long term Validation loss: 0.1629\n",
      "Epoch: [8465/10000] Training loss: 1.3751963348704339e-05 / Validation loss: 0.00014400186704465465 / Long term Validation loss: 0.1643\n",
      "Epoch: [8466/10000] Training loss: 1.0452999790778142e-05 / Validation loss: 0.00013834966655537298 / Long term Validation loss: 0.1597\n",
      "Epoch: [8467/10000] Training loss: 1.263013186010741e-05 / Validation loss: 0.00016605620744096017 / Long term Validation loss: 0.1612\n",
      "Epoch: [8468/10000] Training loss: 1.4648112809479996e-05 / Validation loss: 0.00013843316349253273 / Long term Validation loss: 0.1603\n",
      "Epoch: [8469/10000] Training loss: 1.2282978218190468e-05 / Validation loss: 0.00014439401945625697 / Long term Validation loss: 0.1647\n",
      "Epoch: [8470/10000] Training loss: 1.0418154870467019e-05 / Validation loss: 0.00015701287457709046 / Long term Validation loss: 0.1662\n",
      "Epoch: [8471/10000] Training loss: 1.2110199021012435e-05 / Validation loss: 0.0001379800186027749 / Long term Validation loss: 0.1596\n",
      "Epoch: [8472/10000] Training loss: 1.3174454459061908e-05 / Validation loss: 0.00015434481296297832 / Long term Validation loss: 0.1669\n",
      "Epoch: [8473/10000] Training loss: 1.1432747469362325e-05 / Validation loss: 0.00014673675293773659 / Long term Validation loss: 0.1652\n",
      "Epoch: [8474/10000] Training loss: 1.0421563004710868e-05 / Validation loss: 0.00013974619918959307 / Long term Validation loss: 0.1621\n",
      "Epoch: [8475/10000] Training loss: 1.1661737673166314e-05 / Validation loss: 0.00015881328146399778 / Long term Validation loss: 0.1655\n",
      "Epoch: [8476/10000] Training loss: 1.2190951583615072e-05 / Validation loss: 0.0001414862626468853 / Long term Validation loss: 0.1629\n",
      "Epoch: [8477/10000] Training loss: 1.0960534289690287e-05 / Validation loss: 0.00014470940857311016 / Long term Validation loss: 0.1643\n",
      "Epoch: [8478/10000] Training loss: 1.042554320250586e-05 / Validation loss: 0.00015488996119817968 / Long term Validation loss: 0.1660\n",
      "Epoch: [8479/10000] Training loss: 1.1294439751344812e-05 / Validation loss: 0.00014015396962397216 / Long term Validation loss: 0.1624\n",
      "Epoch: [8480/10000] Training loss: 1.1545381467631518e-05 / Validation loss: 0.00015110745379083384 / Long term Validation loss: 0.1656\n",
      "Epoch: [8481/10000] Training loss: 1.070195301107432e-05 / Validation loss: 0.00014793577866161132 / Long term Validation loss: 0.1644\n",
      "Epoch: [8482/10000] Training loss: 1.041671605130705e-05 / Validation loss: 0.00014131467731765752 / Long term Validation loss: 0.1621\n",
      "Epoch: [8483/10000] Training loss: 1.1006977015682422e-05 / Validation loss: 0.00015412127950590476 / Long term Validation loss: 0.1656\n",
      "Epoch: [8484/10000] Training loss: 1.112681130576035e-05 / Validation loss: 0.00014318965442053458 / Long term Validation loss: 0.1628\n",
      "Epoch: [8485/10000] Training loss: 1.0560106059605687e-05 / Validation loss: 0.0001447597088028727 / Long term Validation loss: 0.1636\n",
      "Epoch: [8486/10000] Training loss: 1.039751800101388e-05 / Validation loss: 0.00015177306750433896 / Long term Validation loss: 0.1654\n",
      "Epoch: [8487/10000] Training loss: 1.0790542401525555e-05 / Validation loss: 0.00014147181284116173 / Long term Validation loss: 0.1620\n",
      "Epoch: [8488/10000] Training loss: 1.0856816741734447e-05 / Validation loss: 0.00014876483606425327 / Long term Validation loss: 0.1643\n",
      "Epoch: [8489/10000] Training loss: 1.0481563962281719e-05 / Validation loss: 0.00014694656860494577 / Long term Validation loss: 0.1635\n",
      "Epoch: [8490/10000] Training loss: 1.0376162328567863e-05 / Validation loss: 0.00014206184754264833 / Long term Validation loss: 0.1623\n",
      "Epoch: [8491/10000] Training loss: 1.0633970688563884e-05 / Validation loss: 0.00015043400632934982 / Long term Validation loss: 0.1650\n",
      "Epoch: [8492/10000] Training loss: 1.068169225333571e-05 / Validation loss: 0.00014330863543878149 / Long term Validation loss: 0.1629\n",
      "Epoch: [8493/10000] Training loss: 1.0436165860777793e-05 / Validation loss: 0.00014450803083043852 / Long term Validation loss: 0.1631\n",
      "Epoch: [8494/10000] Training loss: 1.0357648027012376e-05 / Validation loss: 0.0001488088060640523 / Long term Validation loss: 0.1643\n",
      "Epoch: [8495/10000] Training loss: 1.0523787699710677e-05 / Validation loss: 0.00014205888869166347 / Long term Validation loss: 0.1623\n",
      "Epoch: [8496/10000] Training loss: 1.0565586278488816e-05 / Validation loss: 0.00014727383250636981 / Long term Validation loss: 0.1635\n",
      "Epoch: [8497/10000] Training loss: 1.0407267215497619e-05 / Validation loss: 0.00014570467705996135 / Long term Validation loss: 0.1632\n",
      "Epoch: [8498/10000] Training loss: 1.0342953808490941e-05 / Validation loss: 0.00014278649982611135 / Long term Validation loss: 0.1628\n",
      "Epoch: [8499/10000] Training loss: 1.0446953144487102e-05 / Validation loss: 0.00014826117461102324 / Long term Validation loss: 0.1641\n",
      "Epoch: [8500/10000] Training loss: 1.0485766976398643e-05 / Validation loss: 0.00014338430206047516 / Long term Validation loss: 0.1630\n",
      "Epoch: [8501/10000] Training loss: 1.0386209928534583e-05 / Validation loss: 0.00014477263292213556 / Long term Validation loss: 0.1630\n",
      "Epoch: [8502/10000] Training loss: 1.0331912673821789e-05 / Validation loss: 0.00014705267727018923 / Long term Validation loss: 0.1635\n",
      "Epoch: [8503/10000] Training loss: 1.039392144251361e-05 / Validation loss: 0.00014279019349865428 / Long term Validation loss: 0.1630\n",
      "Epoch: [8504/10000] Training loss: 1.0429140574137062e-05 / Validation loss: 0.00014668823668526784 / Long term Validation loss: 0.1633\n",
      "Epoch: [8505/10000] Training loss: 1.0369265585010364e-05 / Validation loss: 0.00014505098991687132 / Long term Validation loss: 0.1630\n",
      "Epoch: [8506/10000] Training loss: 1.0324008166289886e-05 / Validation loss: 0.00014369762613620818 / Long term Validation loss: 0.1630\n",
      "Epoch: [8507/10000] Training loss: 1.0357818305148132e-05 / Validation loss: 0.00014713921747918447 / Long term Validation loss: 0.1637\n",
      "Epoch: [8508/10000] Training loss: 1.0388052019734753e-05 / Validation loss: 0.0001436980298026332 / Long term Validation loss: 0.1629\n",
      "Epoch: [8509/10000] Training loss: 1.0354714575874483e-05 / Validation loss: 0.00014526419936112262 / Long term Validation loss: 0.1629\n",
      "Epoch: [8510/10000] Training loss: 1.0318278472342411e-05 / Validation loss: 0.00014608115097924343 / Long term Validation loss: 0.1632\n",
      "Epoch: [8511/10000] Training loss: 1.0333465566956089e-05 / Validation loss: 0.0001435451545034455 / Long term Validation loss: 0.1629\n",
      "Epoch: [8512/10000] Training loss: 1.035755852236417e-05 / Validation loss: 0.00014636498895899996 / Long term Validation loss: 0.1633\n",
      "Epoch: [8513/10000] Training loss: 1.0341435465595183e-05 / Validation loss: 0.00014467144361723255 / Long term Validation loss: 0.1629\n",
      "Epoch: [8514/10000] Training loss: 1.0313522342411963e-05 / Validation loss: 0.00014439000927550378 / Long term Validation loss: 0.1630\n",
      "Epoch: [8515/10000] Training loss: 1.0316952288064316e-05 / Validation loss: 0.00014628712090677295 / Long term Validation loss: 0.1633\n",
      "Epoch: [8516/10000] Training loss: 1.0334443023767153e-05 / Validation loss: 0.00014391153283681345 / Long term Validation loss: 0.1630\n",
      "Epoch: [8517/10000] Training loss: 1.0328851802863187e-05 / Validation loss: 0.00014546537404528257 / Long term Validation loss: 0.1629\n",
      "Epoch: [8518/10000] Training loss: 1.0308821545026268e-05 / Validation loss: 0.00014532298376798724 / Long term Validation loss: 0.1629\n",
      "Epoch: [8519/10000] Training loss: 1.0305519301899143e-05 / Validation loss: 0.00014404487531955492 / Long term Validation loss: 0.1630\n",
      "Epoch: [8520/10000] Training loss: 1.031669085642205e-05 / Validation loss: 0.0001459193449227112 / Long term Validation loss: 0.1632\n",
      "Epoch: [8521/10000] Training loss: 1.031685477021805e-05 / Validation loss: 0.00014436629641585138 / Long term Validation loss: 0.1629\n",
      "Epoch: [8522/10000] Training loss: 1.030367865699919e-05 / Validation loss: 0.00014475455358017126 / Long term Validation loss: 0.1628\n",
      "Epoch: [8523/10000] Training loss: 1.029728473758534e-05 / Validation loss: 0.00014552437136152238 / Long term Validation loss: 0.1630\n",
      "Epoch: [8524/10000] Training loss: 1.0303006395058876e-05 / Validation loss: 0.00014405221617432183 / Long term Validation loss: 0.1629\n",
      "Epoch: [8525/10000] Training loss: 1.0305568653908537e-05 / Validation loss: 0.00014539988228098007 / Long term Validation loss: 0.1629\n",
      "Epoch: [8526/10000] Training loss: 1.0297869915639049e-05 / Validation loss: 0.00014476317881146185 / Long term Validation loss: 0.1628\n",
      "Epoch: [8527/10000] Training loss: 1.0290865620436034e-05 / Validation loss: 0.00014439575157036122 / Long term Validation loss: 0.1629\n",
      "Epoch: [8528/10000] Training loss: 1.0292383619026794e-05 / Validation loss: 0.00014545963270691712 / Long term Validation loss: 0.1630\n",
      "Epoch: [8529/10000] Training loss: 1.0295144269820419e-05 / Validation loss: 0.000144257122601648 / Long term Validation loss: 0.1629\n",
      "Epoch: [8530/10000] Training loss: 1.0291350517121584e-05 / Validation loss: 0.0001449662046037397 / Long term Validation loss: 0.1628\n",
      "Epoch: [8531/10000] Training loss: 1.0285228549057181e-05 / Validation loss: 0.00014499658156214433 / Long term Validation loss: 0.1628\n",
      "Epoch: [8532/10000] Training loss: 1.028395595262898e-05 / Validation loss: 0.00014428914517908482 / Long term Validation loss: 0.1629\n",
      "Epoch: [8533/10000] Training loss: 1.0285698287266784e-05 / Validation loss: 0.0001452614721420352 / Long term Validation loss: 0.1629\n",
      "Epoch: [8534/10000] Training loss: 1.0284258090457609e-05 / Validation loss: 0.0001444995854627186 / Long term Validation loss: 0.1628\n",
      "Epoch: [8535/10000] Training loss: 1.0279683531117486e-05 / Validation loss: 0.00014469739349738927 / Long term Validation loss: 0.1628\n",
      "Epoch: [8536/10000] Training loss: 1.0276972833374944e-05 / Validation loss: 0.00014509228727471802 / Long term Validation loss: 0.1628\n",
      "Epoch: [8537/10000] Training loss: 1.0277283982024995e-05 / Validation loss: 0.0001443478405620761 / Long term Validation loss: 0.1629\n",
      "Epoch: [8538/10000] Training loss: 1.0276873659468832e-05 / Validation loss: 0.00014505562118646351 / Long term Validation loss: 0.1628\n",
      "Epoch: [8539/10000] Training loss: 1.027386851455854e-05 / Validation loss: 0.00014469238589760424 / Long term Validation loss: 0.1628\n",
      "Epoch: [8540/10000] Training loss: 1.0270799883740606e-05 / Validation loss: 0.00014455530740383542 / Long term Validation loss: 0.1628\n",
      "Epoch: [8541/10000] Training loss: 1.0269834721019772e-05 / Validation loss: 0.00014506066530191599 / Long term Validation loss: 0.1628\n",
      "Epoch: [8542/10000] Training loss: 1.0269510087910007e-05 / Validation loss: 0.00014443544165005436 / Long term Validation loss: 0.1628\n",
      "Epoch: [8543/10000] Training loss: 1.0267682157409964e-05 / Validation loss: 0.00014485772336272158 / Long term Validation loss: 0.1628\n",
      "Epoch: [8544/10000] Training loss: 1.0264933169495112e-05 / Validation loss: 0.00014478085072228527 / Long term Validation loss: 0.1628\n",
      "Epoch: [8545/10000] Training loss: 1.0263150901222112e-05 / Validation loss: 0.00014448346399792555 / Long term Validation loss: 0.1628\n",
      "Epoch: [8546/10000] Training loss: 1.026240799197097e-05 / Validation loss: 0.00014495653712551083 / Long term Validation loss: 0.1628\n",
      "Epoch: [8547/10000] Training loss: 1.0261205054901333e-05 / Validation loss: 0.00014450568839330467 / Long term Validation loss: 0.1628\n",
      "Epoch: [8548/10000] Training loss: 1.0259035274478162e-05 / Validation loss: 0.00014469573980211738 / Long term Validation loss: 0.1628\n",
      "Epoch: [8549/10000] Training loss: 1.0256950461242808e-05 / Validation loss: 0.00014478205122940083 / Long term Validation loss: 0.1628\n",
      "Epoch: [8550/10000] Training loss: 1.0255682115848237e-05 / Validation loss: 0.00014444066904170912 / Long term Validation loss: 0.1628\n",
      "Epoch: [8551/10000] Training loss: 1.0254613024571591e-05 / Validation loss: 0.00014481498880343944 / Long term Validation loss: 0.1628\n",
      "Epoch: [8552/10000] Training loss: 1.0252956678547797e-05 / Validation loss: 0.0001445268057478259 / Long term Validation loss: 0.1628\n",
      "Epoch: [8553/10000] Training loss: 1.025095320137332e-05 / Validation loss: 0.0001445614524683871 / Long term Validation loss: 0.1628\n",
      "Epoch: [8554/10000] Training loss: 1.0249314222686289e-05 / Validation loss: 0.0001447227516794892 / Long term Validation loss: 0.1628\n",
      "Epoch: [8555/10000] Training loss: 1.0248084949364753e-05 / Validation loss: 0.00014441010901080863 / Long term Validation loss: 0.1628\n",
      "Epoch: [8556/10000] Training loss: 1.024670789115474e-05 / Validation loss: 0.00014467986304022597 / Long term Validation loss: 0.1628\n",
      "Epoch: [8557/10000] Training loss: 1.0244952973752852e-05 / Validation loss: 0.0001445203653876971 / Long term Validation loss: 0.1628\n",
      "Epoch: [8558/10000] Training loss: 1.0243187004553161e-05 / Validation loss: 0.00014446795782666573 / Long term Validation loss: 0.1628\n",
      "Epoch: [8559/10000] Training loss: 1.0241725961753054e-05 / Validation loss: 0.00014464586028735697 / Long term Validation loss: 0.1628\n",
      "Epoch: [8560/10000] Training loss: 1.0240389195467436e-05 / Validation loss: 0.00014438826421619996 / Long term Validation loss: 0.1628\n",
      "Epoch: [8561/10000] Training loss: 1.0238855635156934e-05 / Validation loss: 0.00014456674969560312 / Long term Validation loss: 0.1628\n",
      "Epoch: [8562/10000] Training loss: 1.023715018902586e-05 / Validation loss: 0.00014449357146878282 / Long term Validation loss: 0.1628\n",
      "Epoch: [8563/10000] Training loss: 1.0235540127457921e-05 / Validation loss: 0.0001444021911900956 / Long term Validation loss: 0.1628\n",
      "Epoch: [8564/10000] Training loss: 1.0234108211291809e-05 / Validation loss: 0.00014456730064756483 / Long term Validation loss: 0.1628\n",
      "Epoch: [8565/10000] Training loss: 1.0232671645026797e-05 / Validation loss: 0.00014436981126370927 / Long term Validation loss: 0.1628\n",
      "Epoch: [8566/10000] Training loss: 1.0231091233788825e-05 / Validation loss: 0.00014448113506487984 / Long term Validation loss: 0.1628\n",
      "Epoch: [8567/10000] Training loss: 1.0229457923781015e-05 / Validation loss: 0.0001444601474348154 / Long term Validation loss: 0.1628\n",
      "Epoch: [8568/10000] Training loss: 1.0227920436034955e-05 / Validation loss: 0.0001443584059832278 / Long term Validation loss: 0.1628\n",
      "Epoch: [8569/10000] Training loss: 1.0226466974015759e-05 / Validation loss: 0.0001444964224978628 / Long term Validation loss: 0.1628\n",
      "Epoch: [8570/10000] Training loss: 1.022497293733132e-05 / Validation loss: 0.00014434828211332037 / Long term Validation loss: 0.1628\n",
      "Epoch: [8571/10000] Training loss: 1.0223394840142337e-05 / Validation loss: 0.00014441132387650133 / Long term Validation loss: 0.1628\n",
      "Epoch: [8572/10000] Training loss: 1.0221814116833907e-05 / Validation loss: 0.00014441553414312823 / Long term Validation loss: 0.1628\n",
      "Epoch: [8573/10000] Training loss: 1.0220300997802763e-05 / Validation loss: 0.00014431755411895846 / Long term Validation loss: 0.1628\n",
      "Epoch: [8574/10000] Training loss: 1.0218820926274784e-05 / Validation loss: 0.00014442597382632817 / Long term Validation loss: 0.1629\n",
      "Epoch: [8575/10000] Training loss: 1.021730197646116e-05 / Validation loss: 0.0001443156778213254 / Long term Validation loss: 0.1628\n",
      "Epoch: [8576/10000] Training loss: 1.0215739311155464e-05 / Validation loss: 0.00014434786565253402 / Long term Validation loss: 0.1629\n",
      "Epoch: [8577/10000] Training loss: 1.021418849160464e-05 / Validation loss: 0.00014436156766121477 / Long term Validation loss: 0.1629\n",
      "Epoch: [8578/10000] Training loss: 1.0212679025510436e-05 / Validation loss: 0.00014427370456747367 / Long term Validation loss: 0.1628\n",
      "Epoch: [8579/10000] Training loss: 1.021118006172718e-05 / Validation loss: 0.00014435473258968056 / Long term Validation loss: 0.1629\n",
      "Epoch: [8580/10000] Training loss: 1.020965322635929e-05 / Validation loss: 0.000144270397793685 / Long term Validation loss: 0.1629\n",
      "Epoch: [8581/10000] Training loss: 1.0208104808712715e-05 / Validation loss: 0.00014428315760760634 / Long term Validation loss: 0.1629\n",
      "Epoch: [8582/10000] Training loss: 1.0206568682399066e-05 / Validation loss: 0.00014429691354241514 / Long term Validation loss: 0.1629\n",
      "Epoch: [8583/10000] Training loss: 1.0205056366904315e-05 / Validation loss: 0.0001442211667272481 / Long term Validation loss: 0.1629\n",
      "Epoch: [8584/10000] Training loss: 1.0203546356243056e-05 / Validation loss: 0.00014428030555706697 / Long term Validation loss: 0.1629\n",
      "Epoch: [8585/10000] Training loss: 1.0202018575390697e-05 / Validation loss: 0.00014421449206508577 / Long term Validation loss: 0.1629\n",
      "Epoch: [8586/10000] Training loss: 1.0200480037654416e-05 / Validation loss: 0.0001442173388784905 / Long term Validation loss: 0.1629\n",
      "Epoch: [8587/10000] Training loss: 1.0198950470951657e-05 / Validation loss: 0.00014422782926644295 / Long term Validation loss: 0.1629\n",
      "Epoch: [8588/10000] Training loss: 1.0197434449406996e-05 / Validation loss: 0.0001441645282421634 / Long term Validation loss: 0.1629\n",
      "Epoch: [8589/10000] Training loss: 1.0195918520852983e-05 / Validation loss: 0.0001442069787009738 / Long term Validation loss: 0.1629\n",
      "Epoch: [8590/10000] Training loss: 1.0194391860625299e-05 / Validation loss: 0.0001441537109294721 / Long term Validation loss: 0.1629\n",
      "Epoch: [8591/10000] Training loss: 1.0192859415928115e-05 / Validation loss: 0.00014415186329917764 / Long term Validation loss: 0.1629\n",
      "Epoch: [8592/10000] Training loss: 1.0191332564257709e-05 / Validation loss: 0.00014415747506327868 / Long term Validation loss: 0.1629\n",
      "Epoch: [8593/10000] Training loss: 1.0189813538738383e-05 / Validation loss: 0.00014410558361220964 / Long term Validation loss: 0.1629\n",
      "Epoch: [8594/10000] Training loss: 1.0188294433260643e-05 / Validation loss: 0.00014413589316741702 / Long term Validation loss: 0.1629\n",
      "Epoch: [8595/10000] Training loss: 1.0186769024334436e-05 / Validation loss: 0.0001440917319472954 / Long term Validation loss: 0.1629\n",
      "Epoch: [8596/10000] Training loss: 1.01852401056061e-05 / Validation loss: 0.00014408877157464503 / Long term Validation loss: 0.1629\n",
      "Epoch: [8597/10000] Training loss: 1.0183714272269292e-05 / Validation loss: 0.00014408957031601395 / Long term Validation loss: 0.1629\n",
      "Epoch: [8598/10000] Training loss: 1.0182193064113833e-05 / Validation loss: 0.00014404760746927932 / Long term Validation loss: 0.1629\n",
      "Epoch: [8599/10000] Training loss: 1.018067208030293e-05 / Validation loss: 0.00014406858782428633 / Long term Validation loss: 0.1629\n",
      "Epoch: [8600/10000] Training loss: 1.0179147491422037e-05 / Validation loss: 0.0001440305282091076 / Long term Validation loss: 0.1629\n",
      "Epoch: [8601/10000] Training loss: 1.0177620528492232e-05 / Validation loss: 0.00014402744728953136 / Long term Validation loss: 0.1629\n",
      "Epoch: [8602/10000] Training loss: 1.0176095007950638e-05 / Validation loss: 0.00014402328187114233 / Long term Validation loss: 0.1629\n",
      "Epoch: [8603/10000] Training loss: 1.0174572237123618e-05 / Validation loss: 0.0001439893827091523 / Long term Validation loss: 0.1629\n",
      "Epoch: [8604/10000] Training loss: 1.0173049951665196e-05 / Validation loss: 0.00014400290610127662 / Long term Validation loss: 0.1630\n",
      "Epoch: [8605/10000] Training loss: 1.0171525701345902e-05 / Validation loss: 0.0001439692914945831 / Long term Validation loss: 0.1630\n",
      "Epoch: [8606/10000] Training loss: 1.0169999756956724e-05 / Validation loss: 0.00014396656704926736 / Long term Validation loss: 0.1630\n",
      "Epoch: [8607/10000] Training loss: 1.0168474258448722e-05 / Validation loss: 0.0001439579266187249 / Long term Validation loss: 0.1630\n",
      "Epoch: [8608/10000] Training loss: 1.0166950333326954e-05 / Validation loss: 0.0001439305393001975 / Long term Validation loss: 0.1630\n",
      "Epoch: [8609/10000] Training loss: 1.016542698744447e-05 / Validation loss: 0.0001439376502814649 / Long term Validation loss: 0.1630\n",
      "Epoch: [8610/10000] Training loss: 1.0163902670251782e-05 / Validation loss: 0.0001439072356639625 / Long term Validation loss: 0.1630\n",
      "Epoch: [8611/10000] Training loss: 1.0162377153609574e-05 / Validation loss: 0.0001439044548052905 / Long term Validation loss: 0.1630\n",
      "Epoch: [8612/10000] Training loss: 1.01608515336289e-05 / Validation loss: 0.00014389179247782527 / Long term Validation loss: 0.1630\n",
      "Epoch: [8613/10000] Training loss: 1.0159326709458997e-05 / Validation loss: 0.00014386961558060567 / Long term Validation loss: 0.1630\n",
      "Epoch: [8614/10000] Training loss: 1.0157802405842528e-05 / Validation loss: 0.0001438712106529212 / Long term Validation loss: 0.1630\n",
      "Epoch: [8615/10000] Training loss: 1.0156277712285327e-05 / Validation loss: 0.0001438436958280364 / Long term Validation loss: 0.1630\n",
      "Epoch: [8616/10000] Training loss: 1.0154752219109522e-05 / Validation loss: 0.00014384064154315823 / Long term Validation loss: 0.1630\n",
      "Epoch: [8617/10000] Training loss: 1.015322637627689e-05 / Validation loss: 0.00014382493449924068 / Long term Validation loss: 0.1630\n",
      "Epoch: [8618/10000] Training loss: 1.0151700814268329e-05 / Validation loss: 0.0001438070589598452 / Long term Validation loss: 0.1630\n",
      "Epoch: [8619/10000] Training loss: 1.0150175615358745e-05 / Validation loss: 0.00014380384909919844 / Long term Validation loss: 0.1630\n",
      "Epoch: [8620/10000] Training loss: 1.0148650328518066e-05 / Validation loss: 0.0001437791188964061 / Long term Validation loss: 0.1630\n",
      "Epoch: [8621/10000] Training loss: 1.0147124560803312e-05 / Validation loss: 0.00014377526805162157 / Long term Validation loss: 0.1630\n",
      "Epoch: [8622/10000] Training loss: 1.014559839248829e-05 / Validation loss: 0.00014375745703229845 / Long term Validation loss: 0.1630\n",
      "Epoch: [8623/10000] Training loss: 1.0144072187986452e-05 / Validation loss: 0.0001437430322873804 / Long term Validation loss: 0.1630\n",
      "Epoch: [8624/10000] Training loss: 1.0142546150668422e-05 / Validation loss: 0.00014373575798929455 / Long term Validation loss: 0.1630\n",
      "Epoch: [8625/10000] Training loss: 1.0141020133996267e-05 / Validation loss: 0.00014371398746089868 / Long term Validation loss: 0.1630\n",
      "Epoch: [8626/10000] Training loss: 1.0139493862960707e-05 / Validation loss: 0.00014370896053550063 / Long term Validation loss: 0.1630\n",
      "Epoch: [8627/10000] Training loss: 1.0137967249193186e-05 / Validation loss: 0.00014369014267496624 / Long term Validation loss: 0.1630\n",
      "Epoch: [8628/10000] Training loss: 1.013644043947904e-05 / Validation loss: 0.00014367832111445033 / Long term Validation loss: 0.1630\n",
      "Epoch: [8629/10000] Training loss: 1.0134913617610263e-05 / Validation loss: 0.0001436676460286248 / Long term Validation loss: 0.1631\n",
      "Epoch: [8630/10000] Training loss: 1.0133386802332473e-05 / Validation loss: 0.00014364877476542515 / Long term Validation loss: 0.1631\n",
      "Epoch: [8631/10000] Training loss: 1.0131859859443133e-05 / Validation loss: 0.00014364203061128765 / Long term Validation loss: 0.1631\n",
      "Epoch: [8632/10000] Training loss: 1.0130332668566591e-05 / Validation loss: 0.00014362309854925616 / Long term Validation loss: 0.1631\n",
      "Epoch: [8633/10000] Training loss: 1.0128805237672043e-05 / Validation loss: 0.00014361289502725676 / Long term Validation loss: 0.1631\n",
      "Epoch: [8634/10000] Training loss: 1.0127277670412994e-05 / Validation loss: 0.00014359956540665932 / Long term Validation loss: 0.1631\n",
      "Epoch: [8635/10000] Training loss: 1.0125750038510382e-05 / Validation loss: 0.0001435834159577239 / Long term Validation loss: 0.1631\n",
      "Epoch: [8636/10000] Training loss: 1.0124222316243922e-05 / Validation loss: 0.00014357463075625264 / Long term Validation loss: 0.1631\n",
      "Epoch: [8637/10000] Training loss: 1.0122694423714728e-05 / Validation loss: 0.00014355640529656547 / Long term Validation loss: 0.1631\n",
      "Epoch: [8638/10000] Training loss: 1.0121166313050859e-05 / Validation loss: 0.0001435468158346564 / Long term Validation loss: 0.1631\n",
      "Epoch: [8639/10000] Training loss: 1.0119638009235025e-05 / Validation loss: 0.00014353163006642492 / Long term Validation loss: 0.1631\n",
      "Epoch: [8640/10000] Training loss: 1.0118109567175977e-05 / Validation loss: 0.0001435176981389532 / Long term Validation loss: 0.1631\n",
      "Epoch: [8641/10000] Training loss: 1.011658101287984e-05 / Validation loss: 0.000143506699484739 / Long term Validation loss: 0.1631\n",
      "Epoch: [8642/10000] Training loss: 1.011505232195942e-05 / Validation loss: 0.00014348972301177516 / Long term Validation loss: 0.1631\n",
      "Epoch: [8643/10000] Training loss: 1.0113523451117783e-05 / Validation loss: 0.00014347981184060905 / Long term Validation loss: 0.1631\n",
      "Epoch: [8644/10000] Training loss: 1.0111994382638727e-05 / Validation loss: 0.0001434636832689925 / Long term Validation loss: 0.1631\n",
      "Epoch: [8645/10000] Training loss: 1.0110465132655905e-05 / Validation loss: 0.00014345131458125564 / Long term Validation loss: 0.1631\n",
      "Epoch: [8646/10000] Training loss: 1.0108935729589408e-05 / Validation loss: 0.0001434383312292796 / Long term Validation loss: 0.1631\n",
      "Epoch: [8647/10000] Training loss: 1.0107406182684052e-05 / Validation loss: 0.0001434229057925827 / Long term Validation loss: 0.1631\n",
      "Epoch: [8648/10000] Training loss: 1.0105876476531178e-05 / Validation loss: 0.00014341198019775285 / Long term Validation loss: 0.1631\n",
      "Epoch: [8649/10000] Training loss: 1.010434658914102e-05 / Validation loss: 0.00014339578400876754 / Long term Validation loss: 0.1631\n",
      "Epoch: [8650/10000] Training loss: 1.0102816510975109e-05 / Validation loss: 0.00014338414581616525 / Long term Validation loss: 0.1631\n",
      "Epoch: [8651/10000] Training loss: 1.0101286251110521e-05 / Validation loss: 0.0001433696817497343 / Long term Validation loss: 0.1631\n",
      "Epoch: [8652/10000] Training loss: 1.0099755822838013e-05 / Validation loss: 0.00014335572200389475 / Long term Validation loss: 0.1631\n",
      "Epoch: [8653/10000] Training loss: 1.0098225230775905e-05 / Validation loss: 0.00014334343126794926 / Long term Validation loss: 0.1632\n",
      "Epoch: [8654/10000] Training loss: 1.009669446701689e-05 / Validation loss: 0.00014332788466114076 / Long term Validation loss: 0.1632\n",
      "Epoch: [8655/10000] Training loss: 1.00951635192618e-05 / Validation loss: 0.00014331619075567788 / Long term Validation loss: 0.1632\n",
      "Epoch: [8656/10000] Training loss: 1.0093632382017386e-05 / Validation loss: 0.00014330100299256375 / Long term Validation loss: 0.1632\n",
      "Epoch: [8657/10000] Training loss: 1.0092101057906895e-05 / Validation loss: 0.00014328810425087897 / Long term Validation loss: 0.1632\n",
      "Epoch: [8658/10000] Training loss: 1.0090569553729564e-05 / Validation loss: 0.00014327450636861038 / Long term Validation loss: 0.1632\n",
      "Epoch: [8659/10000] Training loss: 1.0089037872062905e-05 / Validation loss: 0.00014325996553758195 / Long term Validation loss: 0.1632\n",
      "Epoch: [8660/10000] Training loss: 1.008750600933137e-05 / Validation loss: 0.0001432475843843819 / Long term Validation loss: 0.1632\n",
      "Epoch: [8661/10000] Training loss: 1.0085973959164084e-05 / Validation loss: 0.00014323241081750664 / Long term Validation loss: 0.1632\n",
      "Epoch: [8662/10000] Training loss: 1.0084441716856845e-05 / Validation loss: 0.00014321992011977428 / Long term Validation loss: 0.1632\n",
      "Epoch: [8663/10000] Training loss: 1.0082909282690864e-05 / Validation loss: 0.0001432054277110586 / Long term Validation loss: 0.1632\n",
      "Epoch: [8664/10000] Training loss: 1.008137665903743e-05 / Validation loss: 0.00014319183828780747 / Long term Validation loss: 0.1632\n",
      "Epoch: [8665/10000] Training loss: 1.00798438477688e-05 / Validation loss: 0.00014317850864963125 / Long term Validation loss: 0.1632\n",
      "Epoch: [8666/10000] Training loss: 1.0078310847627179e-05 / Validation loss: 0.0001431638959933005 / Long term Validation loss: 0.1632\n",
      "Epoch: [8667/10000] Training loss: 1.0076777655096034e-05 / Validation loss: 0.00014315116900396048 / Long term Validation loss: 0.1632\n",
      "Epoch: [8668/10000] Training loss: 1.0075244267073978e-05 / Validation loss: 0.0001431363877839484 / Long term Validation loss: 0.1632\n",
      "Epoch: [8669/10000] Training loss: 1.0073710681970521e-05 / Validation loss: 0.00014312332662299646 / Long term Validation loss: 0.1632\n",
      "Epoch: [8670/10000] Training loss: 1.007217690041669e-05 / Validation loss: 0.00014310917705781084 / Long term Validation loss: 0.1632\n",
      "Epoch: [8671/10000] Training loss: 1.0070642923043012e-05 / Validation loss: 0.00014309527604279513 / Long term Validation loss: 0.1632\n",
      "Epoch: [8672/10000] Training loss: 1.0069108749637858e-05 / Validation loss: 0.00014308189965935806 / Long term Validation loss: 0.1632\n",
      "Epoch: [8673/10000] Training loss: 1.0067574378591632e-05 / Validation loss: 0.0001430673768268438 / Long term Validation loss: 0.1632\n",
      "Epoch: [8674/10000] Training loss: 1.0066039807629391e-05 / Validation loss: 0.00014305428940322175 / Long term Validation loss: 0.1632\n",
      "Epoch: [8675/10000] Training loss: 1.006450503520728e-05 / Validation loss: 0.000143039766930957 / Long term Validation loss: 0.1632\n",
      "Epoch: [8676/10000] Training loss: 1.0062970060452455e-05 / Validation loss: 0.00014302635237446715 / Long term Validation loss: 0.1633\n",
      "Epoch: [8677/10000] Training loss: 1.0061434883414889e-05 / Validation loss: 0.00014301231199298633 / Long term Validation loss: 0.1633\n",
      "Epoch: [8678/10000] Training loss: 1.0059899503841364e-05 / Validation loss: 0.00014299830141448997 / Long term Validation loss: 0.1633\n",
      "Epoch: [8679/10000] Training loss: 1.005836392102751e-05 / Validation loss: 0.00014298476776424424 / Long term Validation loss: 0.1633\n",
      "Epoch: [8680/10000] Training loss: 1.0056828133719277e-05 / Validation loss: 0.00014297035839889056 / Long term Validation loss: 0.1633\n",
      "Epoch: [8681/10000] Training loss: 1.0055292140432294e-05 / Validation loss: 0.00014295698128439168 / Long term Validation loss: 0.1633\n",
      "Epoch: [8682/10000] Training loss: 1.0053755940155558e-05 / Validation loss: 0.00014294259714561119 / Long term Validation loss: 0.1633\n",
      "Epoch: [8683/10000] Training loss: 1.0052219532097612e-05 / Validation loss: 0.00014292897664892377 / Long term Validation loss: 0.1633\n",
      "Epoch: [8684/10000] Training loss: 1.0050682915918265e-05 / Validation loss: 0.00014291492650350083 / Long term Validation loss: 0.1633\n",
      "Epoch: [8685/10000] Training loss: 1.00491460910654e-05 / Validation loss: 0.00014290089684653703 / Long term Validation loss: 0.1633\n",
      "Epoch: [8686/10000] Training loss: 1.004760905681926e-05 / Validation loss: 0.00014288719075134564 / Long term Validation loss: 0.1633\n",
      "Epoch: [8687/10000] Training loss: 1.0046071812226664e-05 / Validation loss: 0.00014287288085406086 / Long term Validation loss: 0.1633\n",
      "Epoch: [8688/10000] Training loss: 1.0044534356210607e-05 / Validation loss: 0.0001428592898224011 / Long term Validation loss: 0.1633\n",
      "Epoch: [8689/10000] Training loss: 1.0042996687940794e-05 / Validation loss: 0.00014284497517522572 / Long term Validation loss: 0.1633\n",
      "Epoch: [8690/10000] Training loss: 1.0041458806628846e-05 / Validation loss: 0.0001428312332088891 / Long term Validation loss: 0.1633\n",
      "Epoch: [8691/10000] Training loss: 1.0039920711757193e-05 / Validation loss: 0.00014281712702538698 / Long term Validation loss: 0.1633\n",
      "Epoch: [8692/10000] Training loss: 1.0038382402692266e-05 / Validation loss: 0.00014280310845219232 / Long term Validation loss: 0.1633\n",
      "Epoch: [8693/10000] Training loss: 1.0036843878780404e-05 / Validation loss: 0.00014278924096917307 / Long term Validation loss: 0.1633\n",
      "Epoch: [8694/10000] Training loss: 1.0035305139248246e-05 / Validation loss: 0.00014277500724051566 / Long term Validation loss: 0.1633\n",
      "Epoch: [8695/10000] Training loss: 1.0033766183246561e-05 / Validation loss: 0.00014276124737253273 / Long term Validation loss: 0.1633\n",
      "Epoch: [8696/10000] Training loss: 1.0032227010034605e-05 / Validation loss: 0.00014274696809592349 / Long term Validation loss: 0.1633\n",
      "Epoch: [8697/10000] Training loss: 1.003068761885186e-05 / Validation loss: 0.00014273313851800542 / Long term Validation loss: 0.1633\n",
      "Epoch: [8698/10000] Training loss: 1.002914800911299e-05 / Validation loss: 0.0001427189668460598 / Long term Validation loss: 0.1633\n",
      "Epoch: [8699/10000] Training loss: 1.0027608180167927e-05 / Validation loss: 0.0001427049591437678 / Long term Validation loss: 0.1634\n",
      "Epoch: [8700/10000] Training loss: 1.0026068131416468e-05 / Validation loss: 0.00014269094670779704 / Long term Validation loss: 0.1634\n",
      "Epoch: [8701/10000] Training loss: 1.002452786218562e-05 / Validation loss: 0.00014267676831484694 / Long term Validation loss: 0.1634\n",
      "Epoch: [8702/10000] Training loss: 1.0022987371771607e-05 / Validation loss: 0.00014266285857347327 / Long term Validation loss: 0.1634\n",
      "Epoch: [8703/10000] Training loss: 1.002144665950202e-05 / Validation loss: 0.000142648602195324 / Long term Validation loss: 0.1634\n",
      "Epoch: [8704/10000] Training loss: 1.00199057246721e-05 / Validation loss: 0.00014263468592571007 / Long term Validation loss: 0.1634\n",
      "Epoch: [8705/10000] Training loss: 1.0018364566681657e-05 / Validation loss: 0.00014262045934123713 / Long term Validation loss: 0.1634\n",
      "Epoch: [8706/10000] Training loss: 1.0016823184887633e-05 / Validation loss: 0.00014260644571829927 / Long term Validation loss: 0.1634\n",
      "Epoch: [8707/10000] Training loss: 1.0015281578719483e-05 / Validation loss: 0.00014259231111823383 / Long term Validation loss: 0.1634\n",
      "Epoch: [8708/10000] Training loss: 1.0013739747557586e-05 / Validation loss: 0.00014257817177156797 / Long term Validation loss: 0.1634\n",
      "Epoch: [8709/10000] Training loss: 1.0012197690793736e-05 / Validation loss: 0.00014256412426348703 / Long term Validation loss: 0.1634\n",
      "Epoch: [8710/10000] Training loss: 1.0010655407812407e-05 / Validation loss: 0.0001425498938051083 / Long term Validation loss: 0.1634\n",
      "Epoch: [8711/10000] Training loss: 1.0009112897979451e-05 / Validation loss: 0.00014253587946184884 / Long term Validation loss: 0.1634\n",
      "Epoch: [8712/10000] Training loss: 1.0007570160708462e-05 / Validation loss: 0.00014252162367230578 / Long term Validation loss: 0.1634\n",
      "Epoch: [8713/10000] Training loss: 1.0006027195382931e-05 / Validation loss: 0.00014250757735104512 / Long term Validation loss: 0.1634\n",
      "Epoch: [8714/10000] Training loss: 1.000448400144856e-05 / Validation loss: 0.00014249335396529676 / Long term Validation loss: 0.1634\n",
      "Epoch: [8715/10000] Training loss: 1.0002940578315792e-05 / Validation loss: 0.00014247923314158122 / Long term Validation loss: 0.1634\n",
      "Epoch: [8716/10000] Training loss: 1.0001396925432697e-05 / Validation loss: 0.0001424650666906746 / Long term Validation loss: 0.1634\n",
      "Epoch: [8717/10000] Training loss: 9.999853042225033e-06 / Validation loss: 0.00014245086621694813 / Long term Validation loss: 0.1634\n",
      "Epoch: [8718/10000] Training loss: 9.998308928125675e-06 / Validation loss: 0.0001424367449464094 / Long term Validation loss: 0.1634\n",
      "Epoch: [8719/10000] Training loss: 9.996764582575414e-06 / Validation loss: 0.00014242249085891497 / Long term Validation loss: 0.1634\n",
      "Epoch: [8720/10000] Training loss: 9.995220005002221e-06 / Validation loss: 0.00014240838073567347 / Long term Validation loss: 0.1635\n",
      "Epoch: [8721/10000] Training loss: 9.993675194867653e-06 / Validation loss: 0.00014239411154932477 / Long term Validation loss: 0.1635\n",
      "Epoch: [8722/10000] Training loss: 9.992130151610404e-06 / Validation loss: 0.0001423799761873386 / Long term Validation loss: 0.1635\n",
      "Epoch: [8723/10000] Training loss: 9.990584874707664e-06 / Validation loss: 0.00014236572355559623 / Long term Validation loss: 0.1635\n",
      "Epoch: [8724/10000] Training loss: 9.989039363613384e-06 / Validation loss: 0.00014235153988492635 / Long term Validation loss: 0.1635\n",
      "Epoch: [8725/10000] Training loss: 9.98749361780783e-06 / Validation loss: 0.0001423373173387657 / Long term Validation loss: 0.1635\n",
      "Epoch: [8726/10000] Training loss: 9.985947636758718e-06 / Validation loss: 0.00014232308177503295 / Long term Validation loss: 0.1635\n",
      "Epoch: [8727/10000] Training loss: 9.984401419942786e-06 / Validation loss: 0.00014230888395925893 / Long term Validation loss: 0.1635\n",
      "Epoch: [8728/10000] Training loss: 9.982854966841548e-06 / Validation loss: 0.00014229460921831743 / Long term Validation loss: 0.1635\n",
      "Epoch: [8729/10000] Training loss: 9.981308276931265e-06 / Validation loss: 0.00014228041871717275 / Long term Validation loss: 0.1635\n",
      "Epoch: [8730/10000] Training loss: 9.979761349707517e-06 / Validation loss: 0.00014226612510609275 / Long term Validation loss: 0.1635\n",
      "Epoch: [8731/10000] Training loss: 9.978214184652148e-06 / Validation loss: 0.00014225192183371627 / Long term Validation loss: 0.1635\n",
      "Epoch: [8732/10000] Training loss: 9.976666781271809e-06 / Validation loss: 0.00014223762798583532 / Long term Validation loss: 0.1635\n",
      "Epoch: [8733/10000] Training loss: 9.9751191390577e-06 / Validation loss: 0.0001422233969019334 / Long term Validation loss: 0.1635\n",
      "Epoch: [8734/10000] Training loss: 9.97357125752223e-06 / Validation loss: 0.0001422091137106032 / Long term Validation loss: 0.1635\n",
      "Epoch: [8735/10000] Training loss: 9.972023136167124e-06 / Validation loss: 0.0001421948486866183 / Long term Validation loss: 0.1635\n",
      "Epoch: [8736/10000] Training loss: 9.970474774506701e-06 / Validation loss: 0.00014218057773559203 / Long term Validation loss: 0.1635\n",
      "Epoch: [8737/10000] Training loss: 9.96892617205349e-06 / Validation loss: 0.00014216628139075244 / Long term Validation loss: 0.1635\n",
      "Epoch: [8738/10000] Training loss: 9.967377328323107e-06 / Validation loss: 0.00014215201696866418 / Long term Validation loss: 0.1635\n",
      "Epoch: [8739/10000] Training loss: 9.965828242838675e-06 / Validation loss: 0.0001421376976704777 / Long term Validation loss: 0.1635\n",
      "Epoch: [8740/10000] Training loss: 9.964278915118803e-06 / Validation loss: 0.00014212343046443455 / Long term Validation loss: 0.1635\n",
      "Epoch: [8741/10000] Training loss: 9.962729344695885e-06 / Validation loss: 0.00014210909831460268 / Long term Validation loss: 0.1635\n",
      "Epoch: [8742/10000] Training loss: 9.961179531093696e-06 / Validation loss: 0.00014209481906153222 / Long term Validation loss: 0.1636\n",
      "Epoch: [8743/10000] Training loss: 9.959629473851659e-06 / Validation loss: 0.0001420804825311672 / Long term Validation loss: 0.1636\n",
      "Epoch: [8744/10000] Training loss: 9.958079172500263e-06 / Validation loss: 0.00014206618459506581 / Long term Validation loss: 0.1636\n",
      "Epoch: [8745/10000] Training loss: 9.956528626583744e-06 / Validation loss: 0.0001420518487087743 / Long term Validation loss: 0.1636\n",
      "Epoch: [8746/10000] Training loss: 9.954977835640123e-06 / Validation loss: 0.00014203752920116216 / Long term Validation loss: 0.1636\n",
      "Epoch: [8747/10000] Training loss: 9.953426799216929e-06 / Validation loss: 0.00014202319527780377 / Long term Validation loss: 0.1636\n",
      "Epoch: [8748/10000] Training loss: 9.95187551685997e-06 / Validation loss: 0.00014200885482396804 / Long term Validation loss: 0.1636\n",
      "Epoch: [8749/10000] Training loss: 9.950323988119622e-06 / Validation loss: 0.00014199452124827587 / Long term Validation loss: 0.1636\n",
      "Epoch: [8750/10000] Training loss: 9.94877221254924e-06 / Validation loss: 0.00014198016286141548 / Long term Validation loss: 0.1636\n",
      "Epoch: [8751/10000] Training loss: 9.947220189702297e-06 / Validation loss: 0.0001419658262981167 / Long term Validation loss: 0.1636\n",
      "Epoch: [8752/10000] Training loss: 9.945667919139016e-06 / Validation loss: 0.00014195145398883667 / Long term Validation loss: 0.1636\n",
      "Epoch: [8753/10000] Training loss: 9.944115400416584e-06 / Validation loss: 0.00014193711061649214 / Long term Validation loss: 0.1636\n",
      "Epoch: [8754/10000] Training loss: 9.942562633101139e-06 / Validation loss: 0.0001419227282487838 / Long term Validation loss: 0.1636\n",
      "Epoch: [8755/10000] Training loss: 9.941009616754184e-06 / Validation loss: 0.00014190837473920283 / Long term Validation loss: 0.1636\n",
      "Epoch: [8756/10000] Training loss: 9.939456350946787e-06 / Validation loss: 0.0001418939853423329 / Long term Validation loss: 0.1636\n",
      "Epoch: [8757/10000] Training loss: 9.937902835245275e-06 / Validation loss: 0.0001418796194189598 / Long term Validation loss: 0.1636\n",
      "Epoch: [8758/10000] Training loss: 9.936349069224764e-06 / Validation loss: 0.00014186522491109729 / Long term Validation loss: 0.1636\n",
      "Epoch: [8759/10000] Training loss: 9.934795052456672e-06 / Validation loss: 0.000141850845463585 / Long term Validation loss: 0.1636\n",
      "Epoch: [8760/10000] Training loss: 9.933240784519586e-06 / Validation loss: 0.0001418364466668165 / Long term Validation loss: 0.1636\n",
      "Epoch: [8761/10000] Training loss: 9.931686264990077e-06 / Validation loss: 0.0001418220535562105 / Long term Validation loss: 0.1636\n",
      "Epoch: [8762/10000] Training loss: 9.930131493449855e-06 / Validation loss: 0.0001418076504104535 / Long term Validation loss: 0.1637\n",
      "Epoch: [8763/10000] Training loss: 9.928576469480492e-06 / Validation loss: 0.00014179324416075789 / Long term Validation loss: 0.1637\n",
      "Epoch: [8764/10000] Training loss: 9.927021192666701e-06 / Validation loss: 0.0001417788360511341 / Long term Validation loss: 0.1637\n",
      "Epoch: [8765/10000] Training loss: 9.925465662594826e-06 / Validation loss: 0.00014176441755729433 / Long term Validation loss: 0.1637\n",
      "Epoch: [8766/10000] Training loss: 9.923909878852524e-06 / Validation loss: 0.00014175000364455116 / Long term Validation loss: 0.1637\n",
      "Epoch: [8767/10000] Training loss: 9.922353841030573e-06 / Validation loss: 0.00014173557392964218 / Long term Validation loss: 0.1637\n",
      "Epoch: [8768/10000] Training loss: 9.920797548719629e-06 / Validation loss: 0.00014172115338846104 / Long term Validation loss: 0.1637\n",
      "Epoch: [8769/10000] Training loss: 9.919241001514543e-06 / Validation loss: 0.00014170671340972785 / Long term Validation loss: 0.1637\n",
      "Epoch: [8770/10000] Training loss: 9.917684199008983e-06 / Validation loss: 0.00014169228555338482 / Long term Validation loss: 0.1637\n",
      "Epoch: [8771/10000] Training loss: 9.916127140801518e-06 / Validation loss: 0.00014167783607533412 / Long term Validation loss: 0.1637\n",
      "Epoch: [8772/10000] Training loss: 9.914569826488858e-06 / Validation loss: 0.00014166340040903862 / Long term Validation loss: 0.1637\n",
      "Epoch: [8773/10000] Training loss: 9.913012255672988e-06 / Validation loss: 0.0001416489419607022 / Long term Validation loss: 0.1637\n",
      "Epoch: [8774/10000] Training loss: 9.911454427953599e-06 / Validation loss: 0.00014163449820540618 / Long term Validation loss: 0.1637\n",
      "Epoch: [8775/10000] Training loss: 9.909896342935848e-06 / Validation loss: 0.00014162003110026826 / Long term Validation loss: 0.1637\n",
      "Epoch: [8776/10000] Training loss: 9.908338000222346e-06 / Validation loss: 0.0001416055791927728 / Long term Validation loss: 0.1637\n",
      "Epoch: [8777/10000] Training loss: 9.9067793994212e-06 / Validation loss: 0.00014159110356295982 / Long term Validation loss: 0.1637\n",
      "Epoch: [8778/10000] Training loss: 9.905220540137834e-06 / Validation loss: 0.00014157664362726737 / Long term Validation loss: 0.1637\n",
      "Epoch: [8779/10000] Training loss: 9.903661421983164e-06 / Validation loss: 0.0001415621594419317 / Long term Validation loss: 0.1637\n",
      "Epoch: [8780/10000] Training loss: 9.90210204456526e-06 / Validation loss: 0.00014154769174934203 / Long term Validation loss: 0.1637\n",
      "Epoch: [8781/10000] Training loss: 9.900542407497725e-06 / Validation loss: 0.00014153319882136722 / Long term Validation loss: 0.1637\n",
      "Epoch: [8782/10000] Training loss: 9.898982510391092e-06 / Validation loss: 0.0001415187237707427 / Long term Validation loss: 0.1637\n",
      "Epoch: [8783/10000] Training loss: 9.897422352861584e-06 / Validation loss: 0.00014150422176177833 / Long term Validation loss: 0.1638\n",
      "Epoch: [8784/10000] Training loss: 9.89586193452197e-06 / Validation loss: 0.00014148973989422446 / Long term Validation loss: 0.1638\n",
      "Epoch: [8785/10000] Training loss: 9.894301254991072e-06 / Validation loss: 0.00014147522830705467 / Long term Validation loss: 0.1638\n",
      "Epoch: [8786/10000] Training loss: 9.892740313883678e-06 / Validation loss: 0.00014146074034671567 / Long term Validation loss: 0.1638\n",
      "Epoch: [8787/10000] Training loss: 9.891179110821141e-06 / Validation loss: 0.00014144621848166582 / Long term Validation loss: 0.1638\n",
      "Epoch: [8788/10000] Training loss: 9.889617645420011e-06 / Validation loss: 0.00014143172539781015 / Long term Validation loss: 0.1638\n",
      "Epoch: [8789/10000] Training loss: 9.888055917304267e-06 / Validation loss: 0.00014141719226012968 / Long term Validation loss: 0.1638\n",
      "Epoch: [8790/10000] Training loss: 9.886493926091864e-06 / Validation loss: 0.0001414026953688006 / Long term Validation loss: 0.1638\n",
      "Epoch: [8791/10000] Training loss: 9.884931671409508e-06 / Validation loss: 0.00014138814952325892 / Long term Validation loss: 0.1638\n",
      "Epoch: [8792/10000] Training loss: 9.88336915287619e-06 / Validation loss: 0.00014137365066199827 / Long term Validation loss: 0.1638\n",
      "Epoch: [8793/10000] Training loss: 9.88180637012156e-06 / Validation loss: 0.00014135909001593278 / Long term Validation loss: 0.1638\n",
      "Epoch: [8794/10000] Training loss: 9.880243322765181e-06 / Validation loss: 0.00014134459182714454 / Long term Validation loss: 0.1638\n",
      "Epoch: [8795/10000] Training loss: 9.878680010439996e-06 / Validation loss: 0.00014133001329050268 / Long term Validation loss: 0.1638\n",
      "Epoch: [8796/10000] Training loss: 9.877116432765505e-06 / Validation loss: 0.0001413155196614465 / Long term Validation loss: 0.1638\n",
      "Epoch: [8797/10000] Training loss: 9.875552589378635e-06 / Validation loss: 0.00014130091859908766 / Long term Validation loss: 0.1638\n",
      "Epoch: [8798/10000] Training loss: 9.873988479897974e-06 / Validation loss: 0.00014128643535039112 / Long term Validation loss: 0.1638\n",
      "Epoch: [8799/10000] Training loss: 9.872424103965501e-06 / Validation loss: 0.0001412718047021243 / Long term Validation loss: 0.1638\n",
      "Epoch: [8800/10000] Training loss: 9.870859461197849e-06 / Validation loss: 0.00014125734069327594 / Long term Validation loss: 0.1638\n",
      "Epoch: [8801/10000] Training loss: 9.869294551243939e-06 / Validation loss: 0.00014124266955912687 / Long term Validation loss: 0.1638\n",
      "Epoch: [8802/10000] Training loss: 9.867729373717052e-06 / Validation loss: 0.00014122823849168145 / Long term Validation loss: 0.1639\n",
      "Epoch: [8803/10000] Training loss: 9.866163928276459e-06 / Validation loss: 0.0001412135098282747 / Long term Validation loss: 0.1639\n",
      "Epoch: [8804/10000] Training loss: 9.864598214530716e-06 / Validation loss: 0.00014119913321113787 / Long term Validation loss: 0.1639\n",
      "Epoch: [8805/10000] Training loss: 9.86303223215577e-06 / Validation loss: 0.00014118432002312558 / Long term Validation loss: 0.1639\n",
      "Epoch: [8806/10000] Training loss: 9.861465980755007e-06 / Validation loss: 0.000141170032095225 / Long term Validation loss: 0.1639\n",
      "Epoch: [8807/10000] Training loss: 9.85989946003387e-06 / Validation loss: 0.00014115509106187051 / Long term Validation loss: 0.1639\n",
      "Epoch: [8808/10000] Training loss: 9.85833266959425e-06 / Validation loss: 0.00014114094706371477 / Long term Validation loss: 0.1639\n",
      "Epoch: [8809/10000] Training loss: 9.856765609198977e-06 / Validation loss: 0.00014112580776050296 / Long term Validation loss: 0.1639\n",
      "Epoch: [8810/10000] Training loss: 9.855198278466389e-06 / Validation loss: 0.00014111189800384123 / Long term Validation loss: 0.1639\n",
      "Epoch: [8811/10000] Training loss: 9.853630677282097e-06 / Validation loss: 0.00014109644447309798 / Long term Validation loss: 0.1639\n",
      "Epoch: [8812/10000] Training loss: 9.852062805343688e-06 / Validation loss: 0.00014108291854715007 / Long term Validation loss: 0.1639\n",
      "Epoch: [8813/10000] Training loss: 9.850494662823553e-06 / Validation loss: 0.00014106695742253187 / Long term Validation loss: 0.1639\n",
      "Epoch: [8814/10000] Training loss: 9.848926249698366e-06 / Validation loss: 0.0001410540663032974 / Long term Validation loss: 0.1639\n",
      "Epoch: [8815/10000] Training loss: 9.847357566864361e-06 / Validation loss: 0.00014103727104626367 / Long term Validation loss: 0.1639\n",
      "Epoch: [8816/10000] Training loss: 9.845788615193157e-06 / Validation loss: 0.0001410254411720122 / Long term Validation loss: 0.1639\n",
      "Epoch: [8817/10000] Training loss: 9.844219397528065e-06 / Validation loss: 0.00014100725342474534 / Long term Validation loss: 0.1639\n",
      "Epoch: [8818/10000] Training loss: 9.842649917525406e-06 / Validation loss: 0.0001409972184710556 / Long term Validation loss: 0.1639\n",
      "Epoch: [8819/10000] Training loss: 9.841080183542556e-06 / Validation loss: 0.00014097667161213072 / Long term Validation loss: 0.1639\n",
      "Epoch: [8820/10000] Training loss: 9.839510207866866e-06 / Validation loss: 0.00014096970953052942 / Long term Validation loss: 0.1640\n",
      "Epoch: [8821/10000] Training loss: 9.837940015138268e-06 / Validation loss: 0.00014094510959856225 / Long term Validation loss: 0.1640\n",
      "Epoch: [8822/10000] Training loss: 9.836369644634568e-06 / Validation loss: 0.00014094347373371175 / Long term Validation loss: 0.1640\n",
      "Epoch: [8823/10000] Training loss: 9.83479917079215e-06 / Validation loss: 0.0001409118161131024 / Long term Validation loss: 0.1640\n",
      "Epoch: [8824/10000] Training loss: 9.833228718618854e-06 / Validation loss: 0.0001409195279690868 / Long term Validation loss: 0.1640\n",
      "Epoch: [8825/10000] Training loss: 9.831658519565364e-06 / Validation loss: 0.00014087541951934526 / Long term Validation loss: 0.1640\n",
      "Epoch: [8826/10000] Training loss: 9.830088976218655e-06 / Validation loss: 0.00014089974267329915 / Long term Validation loss: 0.1640\n",
      "Epoch: [8827/10000] Training loss: 9.828520828989877e-06 / Validation loss: 0.00014083338893967792 / Long term Validation loss: 0.1640\n",
      "Epoch: [8828/10000] Training loss: 9.826955398358588e-06 / Validation loss: 0.00014088759985648257 / Long term Validation loss: 0.1640\n",
      "Epoch: [8829/10000] Training loss: 9.825395117620173e-06 / Validation loss: 0.00014078100783445306 / Long term Validation loss: 0.1640\n",
      "Epoch: [8830/10000] Training loss: 9.823844410072708e-06 / Validation loss: 0.0001408896637963841 / Long term Validation loss: 0.1641\n",
      "Epoch: [8831/10000] Training loss: 9.822311479947315e-06 / Validation loss: 0.00014070940823250662 / Long term Validation loss: 0.1639\n",
      "Epoch: [8832/10000] Training loss: 9.82081147697571e-06 / Validation loss: 0.00014091848108789 / Long term Validation loss: 0.1641\n",
      "Epoch: [8833/10000] Training loss: 9.81937275138609e-06 / Validation loss: 0.00014060180180589692 / Long term Validation loss: 0.1639\n",
      "Epoch: [8834/10000] Training loss: 9.818048378406378e-06 / Validation loss: 0.00014099842098324298 / Long term Validation loss: 0.1643\n",
      "Epoch: [8835/10000] Training loss: 9.816938686513783e-06 / Validation loss: 0.00014042630912796712 / Long term Validation loss: 0.1638\n",
      "Epoch: [8836/10000] Training loss: 9.8162338861487e-06 / Validation loss: 0.0001411777912182757 / Long term Validation loss: 0.1645\n",
      "Epoch: [8837/10000] Training loss: 9.816297381481572e-06 / Validation loss: 0.0001401226764701815 / Long term Validation loss: 0.1638\n",
      "Epoch: [8838/10000] Training loss: 9.817826393668338e-06 / Validation loss: 0.00014155510289480972 / Long term Validation loss: 0.1649\n",
      "Epoch: [8839/10000] Training loss: 9.822167495335066e-06 / Validation loss: 0.0001395794220424203 / Long term Validation loss: 0.1640\n",
      "Epoch: [8840/10000] Training loss: 9.8319337735352e-06 / Validation loss: 0.00014233984980622537 / Long term Validation loss: 0.1654\n",
      "Epoch: [8841/10000] Training loss: 9.85222631747296e-06 / Validation loss: 0.0001386026245522286 / Long term Validation loss: 0.1643\n",
      "Epoch: [8842/10000] Training loss: 9.89305087972282e-06 / Validation loss: 0.00014400650401863644 / Long term Validation loss: 0.1658\n",
      "Epoch: [8843/10000] Training loss: 9.974136370056638e-06 / Validation loss: 0.0001369128118697452 / Long term Validation loss: 0.1638\n",
      "Epoch: [8844/10000] Training loss: 1.0134538760863191e-05 / Validation loss: 0.0001477295337631415 / Long term Validation loss: 0.1660\n",
      "Epoch: [8845/10000] Training loss: 1.0451839489185756e-05 / Validation loss: 0.00013438476912001494 / Long term Validation loss: 0.1626\n",
      "Epoch: [8846/10000] Training loss: 1.1080245127764479e-05 / Validation loss: 0.0001567409197228554 / Long term Validation loss: 0.1643\n",
      "Epoch: [8847/10000] Training loss: 1.2325332947007693e-05 / Validation loss: 0.00013251621542307806 / Long term Validation loss: 0.1625\n",
      "Epoch: [8848/10000] Training loss: 1.4785031311384129e-05 / Validation loss: 0.00018068188575826827 / Long term Validation loss: 0.1623\n",
      "Epoch: [8849/10000] Training loss: 1.959505823188936e-05 / Validation loss: 0.000140582881214345 / Long term Validation loss: 0.1557\n",
      "Epoch: [8850/10000] Training loss: 2.8763824478663183e-05 / Validation loss: 0.00024610569764690985 / Long term Validation loss: 0.1668\n",
      "Epoch: [8851/10000] Training loss: 4.528747028347527e-05 / Validation loss: 0.00018733219552280106 / Long term Validation loss: 0.1683\n",
      "Epoch: [8852/10000] Training loss: 7.168212658439425e-05 / Validation loss: 0.0003768600967214874 / Long term Validation loss: 0.1718\n",
      "Epoch: [8853/10000] Training loss: 0.00010406846195197841 / Validation loss: 0.00025195950954538804 / Long term Validation loss: 0.1749\n",
      "Epoch: [8854/10000] Training loss: 0.00012239555567831372 / Validation loss: 0.00036986839548203913 / Long term Validation loss: 0.1708\n",
      "Epoch: [8855/10000] Training loss: 9.864511636483772e-05 / Validation loss: 0.00015371493928072882 / Long term Validation loss: 0.1568\n",
      "Epoch: [8856/10000] Training loss: 4.185652326625945e-05 / Validation loss: 0.0001488231296838546 / Long term Validation loss: 0.1633\n",
      "Epoch: [8857/10000] Training loss: 1.0009481107559272e-05 / Validation loss: 0.00022114817811295167 / Long term Validation loss: 0.1597\n",
      "Epoch: [8858/10000] Training loss: 3.107276509286556e-05 / Validation loss: 0.00017903701591631048 / Long term Validation loss: 0.1635\n",
      "Epoch: [8859/10000] Training loss: 6.244267374591074e-05 / Validation loss: 0.0002778503652380264 / Long term Validation loss: 0.1644\n",
      "Epoch: [8860/10000] Training loss: 5.3556771622572695e-05 / Validation loss: 0.00014080576552404048 / Long term Validation loss: 0.1619\n",
      "Epoch: [8861/10000] Training loss: 1.9101118692926577e-05 / Validation loss: 0.00014112095028554823 / Long term Validation loss: 0.1628\n",
      "Epoch: [8862/10000] Training loss: 1.2743163792295846e-05 / Validation loss: 0.00023416489342932402 / Long term Validation loss: 0.1590\n",
      "Epoch: [8863/10000] Training loss: 3.5452577495751305e-05 / Validation loss: 0.00015706598502684898 / Long term Validation loss: 0.1559\n",
      "Epoch: [8864/10000] Training loss: 4.0999054283709355e-05 / Validation loss: 0.00019056474323021343 / Long term Validation loss: 0.1570\n",
      "Epoch: [8865/10000] Training loss: 1.9889966141402875e-05 / Validation loss: 0.0001543313807007323 / Long term Validation loss: 0.1649\n",
      "Epoch: [8866/10000] Training loss: 1.0505802879855705e-05 / Validation loss: 0.00014234551295762705 / Long term Validation loss: 0.1553\n",
      "Epoch: [8867/10000] Training loss: 2.4461450815691508e-05 / Validation loss: 0.00021558512885989857 / Long term Validation loss: 0.1577\n",
      "Epoch: [8868/10000] Training loss: 3.0350702150208973e-05 / Validation loss: 0.00013799502915013654 / Long term Validation loss: 0.1589\n",
      "Epoch: [8869/10000] Training loss: 1.6976933258614636e-05 / Validation loss: 0.00014160116435932695 / Long term Validation loss: 0.1629\n",
      "Epoch: [8870/10000] Training loss: 1.016711082115041e-05 / Validation loss: 0.0001820269158300442 / Long term Validation loss: 0.1591\n",
      "Epoch: [8871/10000] Training loss: 1.933695672889501e-05 / Validation loss: 0.00014024843958412026 / Long term Validation loss: 0.1523\n",
      "Epoch: [8872/10000] Training loss: 2.3223444355794488e-05 / Validation loss: 0.0001642830849714421 / Long term Validation loss: 0.1607\n",
      "Epoch: [8873/10000] Training loss: 1.430063991454562e-05 / Validation loss: 0.00014510261081412672 / Long term Validation loss: 0.1666\n",
      "Epoch: [8874/10000] Training loss: 1.0115500644097716e-05 / Validation loss: 0.00013438879868580758 / Long term Validation loss: 0.1547\n",
      "Epoch: [8875/10000] Training loss: 1.6369059902584153e-05 / Validation loss: 0.0001747403424772397 / Long term Validation loss: 0.1602\n",
      "Epoch: [8876/10000] Training loss: 1.855135538986911e-05 / Validation loss: 0.00013330804391486903 / Long term Validation loss: 0.1595\n",
      "Epoch: [8877/10000] Training loss: 1.24746533074684e-05 / Validation loss: 0.00013646130391072422 / Long term Validation loss: 0.1633\n",
      "Epoch: [8878/10000] Training loss: 1.0120095637223408e-05 / Validation loss: 0.00016118408566858427 / Long term Validation loss: 0.1619\n",
      "Epoch: [8879/10000] Training loss: 1.4404952954641641e-05 / Validation loss: 0.0001331370984957334 / Long term Validation loss: 0.1553\n",
      "Epoch: [8880/10000] Training loss: 1.5477901996180015e-05 / Validation loss: 0.00014990024807625093 / Long term Validation loss: 0.1686\n",
      "Epoch: [8881/10000] Training loss: 1.1351074238294923e-05 / Validation loss: 0.00014300089342245698 / Long term Validation loss: 0.1676\n",
      "Epoch: [8882/10000] Training loss: 1.0095230698026665e-05 / Validation loss: 0.0001326374396605863 / Long term Validation loss: 0.1595\n",
      "Epoch: [8883/10000] Training loss: 1.2997506924464173e-05 / Validation loss: 0.0001583855018120644 / Long term Validation loss: 0.1631\n",
      "Epoch: [8884/10000] Training loss: 1.3458268198014259e-05 / Validation loss: 0.0001344458840951941 / Long term Validation loss: 0.1631\n",
      "Epoch: [8885/10000] Training loss: 1.0680035480315894e-05 / Validation loss: 0.0001365461333240081 / Long term Validation loss: 0.1634\n",
      "Epoch: [8886/10000] Training loss: 1.0047175316931905e-05 / Validation loss: 0.00015377009433812904 / Long term Validation loss: 0.1657\n",
      "Epoch: [8887/10000] Training loss: 1.1995476199807835e-05 / Validation loss: 0.00013318612274863218 / Long term Validation loss: 0.1602\n",
      "Epoch: [8888/10000] Training loss: 1.2146491363786216e-05 / Validation loss: 0.00014604464821553993 / Long term Validation loss: 0.1674\n",
      "Epoch: [8889/10000] Training loss: 1.0295539352482297e-05 / Validation loss: 0.00014391221777586443 / Long term Validation loss: 0.1670\n",
      "Epoch: [8890/10000] Training loss: 9.990024955450476e-06 / Validation loss: 0.0001344038982376569 / Long term Validation loss: 0.1622\n",
      "Epoch: [8891/10000] Training loss: 1.1285573897546535e-05 / Validation loss: 0.0001522718427436228 / Long term Validation loss: 0.1667\n",
      "Epoch: [8892/10000] Training loss: 1.131029937935889e-05 / Validation loss: 0.00013737865756200288 / Long term Validation loss: 0.1636\n",
      "Epoch: [8893/10000] Training loss: 1.008481053862058e-05 / Validation loss: 0.0001383355179341187 / Long term Validation loss: 0.1641\n",
      "Epoch: [8894/10000] Training loss: 9.929320636131626e-06 / Validation loss: 0.0001499611610838393 / Long term Validation loss: 0.1667\n",
      "Epoch: [8895/10000] Training loss: 1.0784213131085814e-05 / Validation loss: 0.00013520765584680298 / Long term Validation loss: 0.1623\n",
      "Epoch: [8896/10000] Training loss: 1.0775476795285351e-05 / Validation loss: 0.00014441915108926058 / Long term Validation loss: 0.1663\n",
      "Epoch: [8897/10000] Training loss: 9.967213597716086e-06 / Validation loss: 0.00014330861911327554 / Long term Validation loss: 0.1660\n",
      "Epoch: [8898/10000] Training loss: 9.870349670949399e-06 / Validation loss: 0.00013590075315854524 / Long term Validation loss: 0.1620\n",
      "Epoch: [8899/10000] Training loss: 1.0431875175091601e-05 / Validation loss: 0.00014791505404643154 / Long term Validation loss: 0.1661\n",
      "Epoch: [8900/10000] Training loss: 1.0432211097812637e-05 / Validation loss: 0.00013817382310500818 / Long term Validation loss: 0.1641\n",
      "Epoch: [8901/10000] Training loss: 9.900428358495125e-06 / Validation loss: 0.00013889512944463 / Long term Validation loss: 0.1641\n",
      "Epoch: [8902/10000] Training loss: 9.819844368894425e-06 / Validation loss: 0.00014599091776820145 / Long term Validation loss: 0.1661\n",
      "Epoch: [8903/10000] Training loss: 1.0187117832110162e-05 / Validation loss: 0.0001361043536028623 / Long term Validation loss: 0.1625\n",
      "Epoch: [8904/10000] Training loss: 1.0209084193159294e-05 / Validation loss: 0.00014273368024308406 / Long term Validation loss: 0.1655\n",
      "Epoch: [8905/10000] Training loss: 9.861817666570356e-06 / Validation loss: 0.00014131343734839202 / Long term Validation loss: 0.1644\n",
      "Epoch: [8906/10000] Training loss: 9.783147252908275e-06 / Validation loss: 0.00013659244588353877 / Long term Validation loss: 0.1627\n",
      "Epoch: [8907/10000] Training loss: 1.0020069031850444e-05 / Validation loss: 0.00014449163577797048 / Long term Validation loss: 0.1662\n",
      "Epoch: [8908/10000] Training loss: 1.0060416757261131e-05 / Validation loss: 0.00013779458817975883 / Long term Validation loss: 0.1638\n",
      "Epoch: [8909/10000] Training loss: 9.837406897177447e-06 / Validation loss: 0.00013906061421223824 / Long term Validation loss: 0.1639\n",
      "Epoch: [8910/10000] Training loss: 9.759431949546194e-06 / Validation loss: 0.00014293707226798176 / Long term Validation loss: 0.1658\n",
      "Epoch: [8911/10000] Training loss: 9.907179466803952e-06 / Validation loss: 0.00013667377043133675 / Long term Validation loss: 0.1631\n",
      "Epoch: [8912/10000] Training loss: 9.957105993408911e-06 / Validation loss: 0.00014178585067450267 / Long term Validation loss: 0.1650\n",
      "Epoch: [8913/10000] Training loss: 9.818772028716626e-06 / Validation loss: 0.0001399292248708215 / Long term Validation loss: 0.1637\n",
      "Epoch: [8914/10000] Training loss: 9.745196322442297e-06 / Validation loss: 0.0001375574758142587 / Long term Validation loss: 0.1637\n",
      "Epoch: [8915/10000] Training loss: 9.83147634298938e-06 / Validation loss: 0.00014262888264411473 / Long term Validation loss: 0.1657\n",
      "Epoch: [8916/10000] Training loss: 9.88239412156238e-06 / Validation loss: 0.0001378585153832396 / Long term Validation loss: 0.1637\n",
      "Epoch: [8917/10000] Training loss: 9.801915319411838e-06 / Validation loss: 0.00013968147543588533 / Long term Validation loss: 0.1636\n",
      "Epoch: [8918/10000] Training loss: 9.737354684764779e-06 / Validation loss: 0.0001413145938122286 / Long term Validation loss: 0.1648\n",
      "Epoch: [8919/10000] Training loss: 9.781964718375999e-06 / Validation loss: 0.00013757036090603967 / Long term Validation loss: 0.1637\n",
      "Epoch: [8920/10000] Training loss: 9.827471602283483e-06 / Validation loss: 0.00014149027802997302 / Long term Validation loss: 0.1650\n",
      "Epoch: [8921/10000] Training loss: 9.785817419986278e-06 / Validation loss: 0.00013937954712801606 / Long term Validation loss: 0.1635\n",
      "Epoch: [8922/10000] Training loss: 9.733321663644571e-06 / Validation loss: 0.00013868752757466293 / Long term Validation loss: 0.1637\n",
      "Epoch: [8923/10000] Training loss: 9.75065663502852e-06 / Validation loss: 0.00014161722780448157 / Long term Validation loss: 0.1652\n",
      "Epoch: [8924/10000] Training loss: 9.786957116560906e-06 / Validation loss: 0.0001382549363269022 / Long term Validation loss: 0.1638\n",
      "Epoch: [8925/10000] Training loss: 9.77011428099507e-06 / Validation loss: 0.000140261618856109 / Long term Validation loss: 0.1640\n",
      "Epoch: [8926/10000] Training loss: 9.730755488061127e-06 / Validation loss: 0.00014035877477445332 / Long term Validation loss: 0.1642\n",
      "Epoch: [8927/10000] Training loss: 9.731458619898186e-06 / Validation loss: 0.00013838924044668645 / Long term Validation loss: 0.1638\n",
      "Epoch: [8928/10000] Training loss: 9.757050749988835e-06 / Validation loss: 0.0001411085398261274 / Long term Validation loss: 0.1649\n",
      "Epoch: [8929/10000] Training loss: 9.754666222457956e-06 / Validation loss: 0.00013903775954781142 / Long term Validation loss: 0.1635\n",
      "Epoch: [8930/10000] Training loss: 9.72787789070147e-06 / Validation loss: 0.00013939538311933967 / Long term Validation loss: 0.1636\n",
      "Epoch: [8931/10000] Training loss: 9.719847452524787e-06 / Validation loss: 0.00014068571567078727 / Long term Validation loss: 0.1647\n",
      "Epoch: [8932/10000] Training loss: 9.735135629158064e-06 / Validation loss: 0.00013852783862083193 / Long term Validation loss: 0.1637\n",
      "Epoch: [8933/10000] Training loss: 9.739759719419252e-06 / Validation loss: 0.00014035274365106556 / Long term Validation loss: 0.1644\n",
      "Epoch: [8934/10000] Training loss: 9.72378744069394e-06 / Validation loss: 0.00013959090300513408 / Long term Validation loss: 0.1637\n",
      "Epoch: [8935/10000] Training loss: 9.712694585093864e-06 / Validation loss: 0.00013890638858689012 / Long term Validation loss: 0.1635\n",
      "Epoch: [8936/10000] Training loss: 9.71940447607021e-06 / Validation loss: 0.0001404951931625659 / Long term Validation loss: 0.1646\n",
      "Epoch: [8937/10000] Training loss: 9.725953666460774e-06 / Validation loss: 0.00013880327724698243 / Long term Validation loss: 0.1635\n",
      "Epoch: [8938/10000] Training loss: 9.71824580301056e-06 / Validation loss: 0.00013969317715057083 / Long term Validation loss: 0.1638\n",
      "Epoch: [8939/10000] Training loss: 9.707778842050263e-06 / Validation loss: 0.0001398700243545964 / Long term Validation loss: 0.1639\n",
      "Epoch: [8940/10000] Training loss: 9.708358761217585e-06 / Validation loss: 0.00013877697532312244 / Long term Validation loss: 0.1635\n",
      "Epoch: [8941/10000] Training loss: 9.713752374351085e-06 / Validation loss: 0.00014015748595118193 / Long term Validation loss: 0.1642\n",
      "Epoch: [8942/10000] Training loss: 9.711426522059622e-06 / Validation loss: 0.00013913434165512256 / Long term Validation loss: 0.1635\n",
      "Epoch: [8943/10000] Training loss: 9.703498652471714e-06 / Validation loss: 0.00013930246728430763 / Long term Validation loss: 0.1636\n",
      "Epoch: [8944/10000] Training loss: 9.700550524041392e-06 / Validation loss: 0.0001399477290008882 / Long term Validation loss: 0.1640\n",
      "Epoch: [8945/10000] Training loss: 9.703418344775251e-06 / Validation loss: 0.00013885737537185048 / Long term Validation loss: 0.1634\n",
      "Epoch: [8946/10000] Training loss: 9.703780868464779e-06 / Validation loss: 0.0001398274256300559 / Long term Validation loss: 0.1639\n",
      "Epoch: [8947/10000] Training loss: 9.698866562968446e-06 / Validation loss: 0.00013939398835815012 / Long term Validation loss: 0.1636\n",
      "Epoch: [8948/10000] Training loss: 9.69465731779251e-06 / Validation loss: 0.00013913111279659628 / Long term Validation loss: 0.1635\n",
      "Epoch: [8949/10000] Training loss: 9.694946033507478e-06 / Validation loss: 0.0001399071135576048 / Long term Validation loss: 0.1640\n",
      "Epoch: [8950/10000] Training loss: 9.6959251869388e-06 / Validation loss: 0.00013903681591067405 / Long term Validation loss: 0.1635\n",
      "Epoch: [8951/10000] Training loss: 9.693500996255273e-06 / Validation loss: 0.00013959696199933736 / Long term Validation loss: 0.1637\n",
      "Epoch: [8952/10000] Training loss: 9.689599491377159e-06 / Validation loss: 0.000139564686419803 / Long term Validation loss: 0.1637\n",
      "Epoch: [8953/10000] Training loss: 9.688056655484314e-06 / Validation loss: 0.00013910187003620612 / Long term Validation loss: 0.1635\n",
      "Epoch: [8954/10000] Training loss: 9.688423874161191e-06 / Validation loss: 0.00013979770854836559 / Long term Validation loss: 0.1639\n",
      "Epoch: [8955/10000] Training loss: 9.68747936019186e-06 / Validation loss: 0.00013919136402900235 / Long term Validation loss: 0.1635\n",
      "Epoch: [8956/10000] Training loss: 9.684619925887882e-06 / Validation loss: 0.00013942313844907345 / Long term Validation loss: 0.1637\n",
      "Epoch: [8957/10000] Training loss: 9.68225323235051e-06 / Validation loss: 0.00013960819437278262 / Long term Validation loss: 0.1638\n",
      "Epoch: [8958/10000] Training loss: 9.681606172017262e-06 / Validation loss: 0.00013910914652817227 / Long term Validation loss: 0.1635\n",
      "Epoch: [8959/10000] Training loss: 9.68112498918108e-06 / Validation loss: 0.00013964886482418267 / Long term Validation loss: 0.1639\n",
      "Epoch: [8960/10000] Training loss: 9.679324191054791e-06 / Validation loss: 0.00013928126508412682 / Long term Validation loss: 0.1636\n",
      "Epoch: [8961/10000] Training loss: 9.676969456006371e-06 / Validation loss: 0.00013929824376024782 / Long term Validation loss: 0.1636\n",
      "Epoch: [8962/10000] Training loss: 9.67550776398393e-06 / Validation loss: 0.0001395679231708512 / Long term Validation loss: 0.1638\n",
      "Epoch: [8963/10000] Training loss: 9.674807579515199e-06 / Validation loss: 0.00013911817127966336 / Long term Validation loss: 0.1636\n",
      "Epoch: [8964/10000] Training loss: 9.673658955280236e-06 / Validation loss: 0.0001394913406225304 / Long term Validation loss: 0.1638\n",
      "Epoch: [8965/10000] Training loss: 9.671747607405166e-06 / Validation loss: 0.0001392983525509547 / Long term Validation loss: 0.1637\n",
      "Epoch: [8966/10000] Training loss: 9.66993518134772e-06 / Validation loss: 0.00013919592925886506 / Long term Validation loss: 0.1636\n",
      "Epoch: [8967/10000] Training loss: 9.66878189856594e-06 / Validation loss: 0.0001394729628225917 / Long term Validation loss: 0.1638\n",
      "Epoch: [8968/10000] Training loss: 9.667797335039296e-06 / Validation loss: 0.00013911070084476643 / Long term Validation loss: 0.1636\n",
      "Epoch: [8969/10000] Training loss: 9.666349639152509e-06 / Validation loss: 0.00013935126257059278 / Long term Validation loss: 0.1638\n",
      "Epoch: [8970/10000] Training loss: 9.664596560644785e-06 / Validation loss: 0.00013927712575119428 / Long term Validation loss: 0.1638\n",
      "Epoch: [8971/10000] Training loss: 9.663105369843295e-06 / Validation loss: 0.00013912775574157567 / Long term Validation loss: 0.1637\n",
      "Epoch: [8972/10000] Training loss: 9.661967144703082e-06 / Validation loss: 0.00013937559293890643 / Long term Validation loss: 0.1639\n",
      "Epoch: [8973/10000] Training loss: 9.660760853077945e-06 / Validation loss: 0.0001391019498572869 / Long term Validation loss: 0.1637\n",
      "Epoch: [8974/10000] Training loss: 9.659246122042679e-06 / Validation loss: 0.00013924529777086607 / Long term Validation loss: 0.1638\n",
      "Epoch: [8975/10000] Training loss: 9.657664709743718e-06 / Validation loss: 0.00013923902872642083 / Long term Validation loss: 0.1638\n",
      "Epoch: [8976/10000] Training loss: 9.656307784627582e-06 / Validation loss: 0.00013908240616975889 / Long term Validation loss: 0.1637\n",
      "Epoch: [8977/10000] Training loss: 9.655100849121454e-06 / Validation loss: 0.00013928784955673956 / Long term Validation loss: 0.1639\n",
      "Epoch: [8978/10000] Training loss: 9.653778459752608e-06 / Validation loss: 0.00013908989102791197 / Long term Validation loss: 0.1637\n",
      "Epoch: [8979/10000] Training loss: 9.652287795859723e-06 / Validation loss: 0.000139170589656695 / Long term Validation loss: 0.1638\n",
      "Epoch: [8980/10000] Training loss: 9.650819443298372e-06 / Validation loss: 0.00013919911230321616 / Long term Validation loss: 0.1638\n",
      "Epoch: [8981/10000] Training loss: 9.64949841904906e-06 / Validation loss: 0.00013905590706170454 / Long term Validation loss: 0.1638\n",
      "Epoch: [8982/10000] Training loss: 9.648226454839564e-06 / Validation loss: 0.00013921781921673882 / Long term Validation loss: 0.1639\n",
      "Epoch: [8983/10000] Training loss: 9.646859172329318e-06 / Validation loss: 0.00013907496964442098 / Long term Validation loss: 0.1638\n",
      "Epoch: [8984/10000] Training loss: 9.645410205883137e-06 / Validation loss: 0.00013911480540803697 / Long term Validation loss: 0.1638\n",
      "Epoch: [8985/10000] Training loss: 9.644001751167184e-06 / Validation loss: 0.00013915343487626568 / Long term Validation loss: 0.1639\n",
      "Epoch: [8986/10000] Training loss: 9.642679183505522e-06 / Validation loss: 0.00013902891151771216 / Long term Validation loss: 0.1638\n",
      "Epoch: [8987/10000] Training loss: 9.641365474375266e-06 / Validation loss: 0.00013915163052396129 / Long term Validation loss: 0.1639\n",
      "Epoch: [8988/10000] Training loss: 9.63998762477878e-06 / Validation loss: 0.00013904678404839843 / Long term Validation loss: 0.1638\n",
      "Epoch: [8989/10000] Training loss: 9.638572399735342e-06 / Validation loss: 0.00013906308301235213 / Long term Validation loss: 0.1639\n",
      "Epoch: [8990/10000] Training loss: 9.637191383928246e-06 / Validation loss: 0.00013910010837916398 / Long term Validation loss: 0.1639\n",
      "Epoch: [8991/10000] Training loss: 9.635858060986844e-06 / Validation loss: 0.00013899549087113277 / Long term Validation loss: 0.1638\n",
      "Epoch: [8992/10000] Training loss: 9.634521562840386e-06 / Validation loss: 0.00013908614221169808 / Long term Validation loss: 0.1639\n",
      "Epoch: [8993/10000] Training loss: 9.633145507822733e-06 / Validation loss: 0.0001390056182958199 / Long term Validation loss: 0.1639\n",
      "Epoch: [8994/10000] Training loss: 9.631752195460071e-06 / Validation loss: 0.00013900849265951516 / Long term Validation loss: 0.1639\n",
      "Epoch: [8995/10000] Training loss: 9.630382271461413e-06 / Validation loss: 0.00013903735728763983 / Long term Validation loss: 0.1639\n",
      "Epoch: [8996/10000] Training loss: 9.629038832858048e-06 / Validation loss: 0.00013895063975904033 / Long term Validation loss: 0.1639\n",
      "Epoch: [8997/10000] Training loss: 9.62769083429017e-06 / Validation loss: 0.00013901682460188696 / Long term Validation loss: 0.1640\n",
      "Epoch: [8998/10000] Training loss: 9.626319324098099e-06 / Validation loss: 0.00013895271516821807 / Long term Validation loss: 0.1639\n",
      "Epoch: [8999/10000] Training loss: 9.624939009461932e-06 / Validation loss: 0.00013895043533128343 / Long term Validation loss: 0.1639\n",
      "Epoch: [9000/10000] Training loss: 9.623573068722773e-06 / Validation loss: 0.00013897062786349512 / Long term Validation loss: 0.1640\n",
      "Epoch: [9001/10000] Training loss: 9.622222325232723e-06 / Validation loss: 0.00013890045045290573 / Long term Validation loss: 0.1639\n",
      "Epoch: [9002/10000] Training loss: 9.620868608215723e-06 / Validation loss: 0.00013894890354598826 / Long term Validation loss: 0.1640\n",
      "Epoch: [9003/10000] Training loss: 9.619501101611742e-06 / Validation loss: 0.00013889579082650402 / Long term Validation loss: 0.1640\n",
      "Epoch: [9004/10000] Training loss: 9.618128235866426e-06 / Validation loss: 0.00013889235570388602 / Long term Validation loss: 0.1640\n",
      "Epoch: [9005/10000] Training loss: 9.61676350022957e-06 / Validation loss: 0.0001389038631845213 / Long term Validation loss: 0.1640\n",
      "Epoch: [9006/10000] Training loss: 9.615407857669884e-06 / Validation loss: 0.00013884780202768973 / Long term Validation loss: 0.1640\n",
      "Epoch: [9007/10000] Training loss: 9.61405102956407e-06 / Validation loss: 0.00013888334008351513 / Long term Validation loss: 0.1640\n",
      "Epoch: [9008/10000] Training loss: 9.612686202265432e-06 / Validation loss: 0.00013883825505003043 / Long term Validation loss: 0.1640\n",
      "Epoch: [9009/10000] Training loss: 9.611317544391704e-06 / Validation loss: 0.00013883623442482722 / Long term Validation loss: 0.1640\n",
      "Epoch: [9010/10000] Training loss: 9.609953024184338e-06 / Validation loss: 0.00013884037606071582 / Long term Validation loss: 0.1640\n",
      "Epoch: [9011/10000] Training loss: 9.608594087189502e-06 / Validation loss: 0.000138796398197453 / Long term Validation loss: 0.1640\n",
      "Epoch: [9012/10000] Training loss: 9.607235216430717e-06 / Validation loss: 0.00013882197227125837 / Long term Validation loss: 0.1641\n",
      "Epoch: [9013/10000] Training loss: 9.605871850083744e-06 / Validation loss: 0.00013878245886172474 / Long term Validation loss: 0.1640\n",
      "Epoch: [9014/10000] Training loss: 9.604505548870926e-06 / Validation loss: 0.00013878189730157152 / Long term Validation loss: 0.1641\n",
      "Epoch: [9015/10000] Training loss: 9.60314095779302e-06 / Validation loss: 0.00013877912355662818 / Long term Validation loss: 0.1641\n",
      "Epoch: [9016/10000] Training loss: 9.601779747144279e-06 / Validation loss: 0.00013874475975755878 / Long term Validation loss: 0.1641\n",
      "Epoch: [9017/10000] Training loss: 9.600419257215394e-06 / Validation loss: 0.00013876197385914791 / Long term Validation loss: 0.1641\n",
      "Epoch: [9018/10000] Training loss: 9.599056429318997e-06 / Validation loss: 0.0001387267608240672 / Long term Validation loss: 0.1641\n",
      "Epoch: [9019/10000] Training loss: 9.597691384671766e-06 / Validation loss: 0.00013872741232341985 / Long term Validation loss: 0.1641\n",
      "Epoch: [9020/10000] Training loss: 9.596326660523793e-06 / Validation loss: 0.00013871880820287417 / Long term Validation loss: 0.1641\n",
      "Epoch: [9021/10000] Training loss: 9.594963823809595e-06 / Validation loss: 0.00013869226770200888 / Long term Validation loss: 0.1641\n",
      "Epoch: [9022/10000] Training loss: 9.593601888034658e-06 / Validation loss: 0.00013870208839716842 / Long term Validation loss: 0.1641\n",
      "Epoch: [9023/10000] Training loss: 9.592238927698409e-06 / Validation loss: 0.00013867035406660744 / Long term Validation loss: 0.1641\n",
      "Epoch: [9024/10000] Training loss: 9.590874409161292e-06 / Validation loss: 0.0001386712143937957 / Long term Validation loss: 0.1641\n",
      "Epoch: [9025/10000] Training loss: 9.589509513311653e-06 / Validation loss: 0.00013865767381881114 / Long term Validation loss: 0.1642\n",
      "Epoch: [9026/10000] Training loss: 9.588145470552224e-06 / Validation loss: 0.00013863735432817175 / Long term Validation loss: 0.1642\n",
      "Epoch: [9027/10000] Training loss: 9.586782195178551e-06 / Validation loss: 0.00013864060398468699 / Long term Validation loss: 0.1642\n",
      "Epoch: [9028/10000] Training loss: 9.585418636336963e-06 / Validation loss: 0.00013861238842768152 / Long term Validation loss: 0.1642\n",
      "Epoch: [9029/10000] Training loss: 9.584054107394448e-06 / Validation loss: 0.0001386128276487888 / Long term Validation loss: 0.1642\n",
      "Epoch: [9030/10000] Training loss: 9.582688961230562e-06 / Validation loss: 0.00013859585962793175 / Long term Validation loss: 0.1642\n",
      "Epoch: [9031/10000] Training loss: 9.581323990452012e-06 / Validation loss: 0.00013858064688160524 / Long term Validation loss: 0.1642\n",
      "Epoch: [9032/10000] Training loss: 9.579959491746227e-06 / Validation loss: 0.00013857815281254377 / Long term Validation loss: 0.1642\n",
      "Epoch: [9033/10000] Training loss: 9.578595042650424e-06 / Validation loss: 0.000138553589963251 / Long term Validation loss: 0.1642\n",
      "Epoch: [9034/10000] Training loss: 9.577230087615847e-06 / Validation loss: 0.00013855273091591157 / Long term Validation loss: 0.1642\n",
      "Epoch: [9035/10000] Training loss: 9.575864551641038e-06 / Validation loss: 0.00013853370561698076 / Long term Validation loss: 0.1642\n",
      "Epoch: [9036/10000] Training loss: 9.574498813304243e-06 / Validation loss: 0.00013852239949551067 / Long term Validation loss: 0.1642\n",
      "Epoch: [9037/10000] Training loss: 9.573133221973232e-06 / Validation loss: 0.0001385150686355274 / Long term Validation loss: 0.1642\n",
      "Epoch: [9038/10000] Training loss: 9.571767735479821e-06 / Validation loss: 0.0001384944019908955 / Long term Validation loss: 0.1642\n",
      "Epoch: [9039/10000] Training loss: 9.570402039116747e-06 / Validation loss: 0.00013849162472669298 / Long term Validation loss: 0.1643\n",
      "Epoch: [9040/10000] Training loss: 9.569035922001689e-06 / Validation loss: 0.0001384720393334812 / Long term Validation loss: 0.1643\n",
      "Epoch: [9041/10000] Training loss: 9.567669463658222e-06 / Validation loss: 0.00013846341761307137 / Long term Validation loss: 0.1643\n",
      "Epoch: [9042/10000] Training loss: 9.566302897464924e-06 / Validation loss: 0.00013845219872488302 / Long term Validation loss: 0.1643\n",
      "Epoch: [9043/10000] Training loss: 9.564936339047103e-06 / Validation loss: 0.00013843524917883654 / Long term Validation loss: 0.1643\n",
      "Epoch: [9044/10000] Training loss: 9.563569693573279e-06 / Validation loss: 0.00013842987445707996 / Long term Validation loss: 0.1643\n",
      "Epoch: [9045/10000] Training loss: 9.56220279035623e-06 / Validation loss: 0.00013841087126269187 / Long term Validation loss: 0.1643\n",
      "Epoch: [9046/10000] Training loss: 9.560835561574883e-06 / Validation loss: 0.00013840352899457123 / Long term Validation loss: 0.1643\n",
      "Epoch: [9047/10000] Training loss: 9.559468089393355e-06 / Validation loss: 0.00013838951191137537 / Long term Validation loss: 0.1643\n",
      "Epoch: [9048/10000] Training loss: 9.558100490153419e-06 / Validation loss: 0.0001383758237559433 / Long term Validation loss: 0.1643\n",
      "Epoch: [9049/10000] Training loss: 9.556732795218482e-06 / Validation loss: 0.00013836758688211186 / Long term Validation loss: 0.1643\n",
      "Epoch: [9050/10000] Training loss: 9.555364935027392e-06 / Validation loss: 0.0001383501163621025 / Long term Validation loss: 0.1643\n",
      "Epoch: [9051/10000] Training loss: 9.553996822970203e-06 / Validation loss: 0.00013834273620626 / Long term Validation loss: 0.1644\n",
      "Epoch: [9052/10000] Training loss: 9.552628441229087e-06 / Validation loss: 0.00013832710599701763 / Long term Validation loss: 0.1644\n",
      "Epoch: [9053/10000] Training loss: 9.551259839070433e-06 / Validation loss: 0.000138315791413487 / Long term Validation loss: 0.1644\n",
      "Epoch: [9054/10000] Training loss: 9.549891073883447e-06 / Validation loss: 0.0001383047594972135 / Long term Validation loss: 0.1644\n",
      "Epoch: [9055/10000] Training loss: 9.548522152674782e-06 / Validation loss: 0.00013828929841629235 / Long term Validation loss: 0.1644\n",
      "Epoch: [9056/10000] Training loss: 9.547153034458443e-06 / Validation loss: 0.00013828078997856508 / Long term Validation loss: 0.1644\n",
      "Epoch: [9057/10000] Training loss: 9.545783676637372e-06 / Validation loss: 0.00013826478616224647 / Long term Validation loss: 0.1644\n",
      "Epoch: [9058/10000] Training loss: 9.544414070794515e-06 / Validation loss: 0.00013825479582762446 / Long term Validation loss: 0.1644\n",
      "Epoch: [9059/10000] Training loss: 9.543044243878455e-06 / Validation loss: 0.00013824159443128614 / Long term Validation loss: 0.1644\n",
      "Epoch: [9060/10000] Training loss: 9.541674223348396e-06 / Validation loss: 0.0001382282061659537 / Long term Validation loss: 0.1644\n",
      "Epoch: [9061/10000] Training loss: 9.540304012289555e-06 / Validation loss: 0.00013821795251165467 / Long term Validation loss: 0.1644\n",
      "Epoch: [9062/10000] Training loss: 9.538933589772827e-06 / Validation loss: 0.00013820261384934107 / Long term Validation loss: 0.1644\n",
      "Epoch: [9063/10000] Training loss: 9.537562932107562e-06 / Validation loss: 0.000138192801430727 / Long term Validation loss: 0.1644\n",
      "Epoch: [9064/10000] Training loss: 9.536192033915912e-06 / Validation loss: 0.00013817835406118603 / Long term Validation loss: 0.1645\n",
      "Epoch: [9065/10000] Training loss: 9.534820906118908e-06 / Validation loss: 0.00013816656762802448 / Long term Validation loss: 0.1645\n",
      "Epoch: [9066/10000] Training loss: 9.533449563336522e-06 / Validation loss: 0.0001381544986967266 / Long term Validation loss: 0.1645\n",
      "Epoch: [9067/10000] Training loss: 9.532078008301483e-06 / Validation loss: 0.00013814046754108698 / Long term Validation loss: 0.1645\n",
      "Epoch: [9068/10000] Training loss: 9.530706231030867e-06 / Validation loss: 0.00013812992708134132 / Long term Validation loss: 0.1645\n",
      "Epoch: [9069/10000] Training loss: 9.529334218850545e-06 / Validation loss: 0.0001381153024678445 / Long term Validation loss: 0.1645\n",
      "Epoch: [9070/10000] Training loss: 9.527961965425137e-06 / Validation loss: 0.0001381043112253512 / Long term Validation loss: 0.1645\n",
      "Epoch: [9071/10000] Training loss: 9.526589474642894e-06 / Validation loss: 0.00013809088915087638 / Long term Validation loss: 0.1645\n",
      "Epoch: [9072/10000] Training loss: 9.525216753005189e-06 / Validation loss: 0.00013807821622071947 / Long term Validation loss: 0.1645\n",
      "Epoch: [9073/10000] Training loss: 9.523843803505736e-06 / Validation loss: 0.00013806643786235228 / Long term Validation loss: 0.1645\n",
      "Epoch: [9074/10000] Training loss: 9.522470622355681e-06 / Validation loss: 0.00013805244505344226 / Long term Validation loss: 0.1645\n",
      "Epoch: [9075/10000] Training loss: 9.521097202227195e-06 / Validation loss: 0.00013804132807124055 / Long term Validation loss: 0.1645\n",
      "Epoch: [9076/10000] Training loss: 9.519723538227106e-06 / Validation loss: 0.00013802734069034427 / Long term Validation loss: 0.1645\n",
      "Epoch: [9077/10000] Training loss: 9.518349629403353e-06 / Validation loss: 0.00013801555908754087 / Long term Validation loss: 0.1645\n",
      "Epoch: [9078/10000] Training loss: 9.516975478513927e-06 / Validation loss: 0.00013800261512819482 / Long term Validation loss: 0.1646\n",
      "Epoch: [9079/10000] Training loss: 9.515601087344801e-06 / Validation loss: 0.0001379896063109435 / Long term Validation loss: 0.1646\n",
      "Epoch: [9080/10000] Training loss: 9.514226455017233e-06 / Validation loss: 0.00013797771378568937 / Long term Validation loss: 0.1646\n",
      "Epoch: [9081/10000] Training loss: 9.512851578088781e-06 / Validation loss: 0.0001379639483875887 / Long term Validation loss: 0.1646\n",
      "Epoch: [9082/10000] Training loss: 9.511476452509823e-06 / Validation loss: 0.00013795230094131613 / Long term Validation loss: 0.1646\n",
      "Epoch: [9083/10000] Training loss: 9.510101076326828e-06 / Validation loss: 0.00013793870773939952 / Long term Validation loss: 0.1646\n",
      "Epoch: [9084/10000] Training loss: 9.508725449151905e-06 / Validation loss: 0.00013792646304454292 / Long term Validation loss: 0.1646\n",
      "Epoch: [9085/10000] Training loss: 9.507349571846603e-06 / Validation loss: 0.0001379136414196337 / Long term Validation loss: 0.1646\n",
      "Epoch: [9086/10000] Training loss: 9.50597344424889e-06 / Validation loss: 0.00013790054683998534 / Long term Validation loss: 0.1646\n",
      "Epoch: [9087/10000] Training loss: 9.504597064909572e-06 / Validation loss: 0.00013788839971790848 / Long term Validation loss: 0.1646\n",
      "Epoch: [9088/10000] Training loss: 9.503220431520258e-06 / Validation loss: 0.00013787484664333762 / Long term Validation loss: 0.1646\n",
      "Epoch: [9089/10000] Training loss: 9.501843541680726e-06 / Validation loss: 0.00013786280027271337 / Long term Validation loss: 0.1646\n",
      "Epoch: [9090/10000] Training loss: 9.500466394171055e-06 / Validation loss: 0.00013784940855368058 / Long term Validation loss: 0.1646\n",
      "Epoch: [9091/10000] Training loss: 9.499088988286048e-06 / Validation loss: 0.00013783692024430978 / Long term Validation loss: 0.1646\n",
      "Epoch: [9092/10000] Training loss: 9.497711323874093e-06 / Validation loss: 0.00013782405962264578 / Long term Validation loss: 0.1647\n",
      "Epoch: [9093/10000] Training loss: 9.4963334002042e-06 / Validation loss: 0.00013781098646828687 / Long term Validation loss: 0.1647\n",
      "Epoch: [9094/10000] Training loss: 9.494955216009068e-06 / Validation loss: 0.00013779857982051412 / Long term Validation loss: 0.1647\n",
      "Epoch: [9095/10000] Training loss: 9.493576769691874e-06 / Validation loss: 0.00013778518842412333 / Long term Validation loss: 0.1647\n",
      "Epoch: [9096/10000] Training loss: 9.492198059567358e-06 / Validation loss: 0.00013777286063909768 / Long term Validation loss: 0.1647\n",
      "Epoch: [9097/10000] Training loss: 9.49081908454617e-06 / Validation loss: 0.00013775956073626855 / Long term Validation loss: 0.1647\n",
      "Epoch: [9098/10000] Training loss: 9.489439843679126e-06 / Validation loss: 0.00013774694973925591 / Long term Validation loss: 0.1647\n",
      "Epoch: [9099/10000] Training loss: 9.488060336375607e-06 / Validation loss: 0.0001377339974263314 / Long term Validation loss: 0.1647\n",
      "Epoch: [9100/10000] Training loss: 9.48668056176276e-06 / Validation loss: 0.00013772098471569609 / Long term Validation loss: 0.1647\n",
      "Epoch: [9101/10000] Training loss: 9.485300518786584e-06 / Validation loss: 0.00013770835478789075 / Long term Validation loss: 0.1647\n",
      "Epoch: [9102/10000] Training loss: 9.48392020621924e-06 / Validation loss: 0.00013769508737366545 / Long term Validation loss: 0.1647\n",
      "Epoch: [9103/10000] Training loss: 9.482539622728128e-06 / Validation loss: 0.0001376825519303725 / Long term Validation loss: 0.1647\n",
      "Epoch: [9104/10000] Training loss: 9.481158767257623e-06 / Validation loss: 0.000137669293349646 / Long term Validation loss: 0.1647\n",
      "Epoch: [9105/10000] Training loss: 9.479777638759125e-06 / Validation loss: 0.00013765660511182694 / Long term Validation loss: 0.1647\n",
      "Epoch: [9106/10000] Training loss: 9.47839623645061e-06 / Validation loss: 0.00013764355036556382 / Long term Validation loss: 0.1647\n",
      "Epoch: [9107/10000] Training loss: 9.477014559414713e-06 / Validation loss: 0.00013763059348809704 / Long term Validation loss: 0.1648\n",
      "Epoch: [9108/10000] Training loss: 9.475632606728464e-06 / Validation loss: 0.00013761776929175956 / Long term Validation loss: 0.1648\n",
      "Epoch: [9109/10000] Training loss: 9.474250377352927e-06 / Validation loss: 0.00013760459735397324 / Long term Validation loss: 0.1648\n",
      "Epoch: [9110/10000] Training loss: 9.472867870173203e-06 / Validation loss: 0.00013759188488493787 / Long term Validation loss: 0.1648\n",
      "Epoch: [9111/10000] Training loss: 9.471485084177426e-06 / Validation loss: 0.00013757865296450556 / Long term Validation loss: 0.1648\n",
      "Epoch: [9112/10000] Training loss: 9.470102018314321e-06 / Validation loss: 0.00013756588777895625 / Long term Validation loss: 0.1648\n",
      "Epoch: [9113/10000] Training loss: 9.468718671715356e-06 / Validation loss: 0.00013755274384443658 / Long term Validation loss: 0.1648\n",
      "Epoch: [9114/10000] Training loss: 9.467335043437532e-06 / Validation loss: 0.0001375398161385399 / Long term Validation loss: 0.1648\n",
      "Epoch: [9115/10000] Training loss: 9.46595113261403e-06 / Validation loss: 0.00013752682234446738 / Long term Validation loss: 0.1648\n",
      "Epoch: [9116/10000] Training loss: 9.464566938296103e-06 / Validation loss: 0.00013751372273728676 / Long term Validation loss: 0.1648\n",
      "Epoch: [9117/10000] Training loss: 9.463182459516257e-06 / Validation loss: 0.00013750084193403495 / Long term Validation loss: 0.1648\n",
      "Epoch: [9118/10000] Training loss: 9.461797695318823e-06 / Validation loss: 0.0001374876433656559 / Long term Validation loss: 0.1648\n",
      "Epoch: [9119/10000] Training loss: 9.460412644707902e-06 / Validation loss: 0.0001374747811762538 / Long term Validation loss: 0.1648\n",
      "Epoch: [9120/10000] Training loss: 9.459027306786127e-06 / Validation loss: 0.0001374615835401703 / Long term Validation loss: 0.1648\n",
      "Epoch: [9121/10000] Training loss: 9.457641680604928e-06 / Validation loss: 0.00013744864924815364 / Long term Validation loss: 0.1648\n",
      "Epoch: [9122/10000] Training loss: 9.456255765307857e-06 / Validation loss: 0.0001374355244163488 / Long term Validation loss: 0.1649\n",
      "Epoch: [9123/10000] Training loss: 9.454869559979563e-06 / Validation loss: 0.00013742247447151216 / Long term Validation loss: 0.1649\n",
      "Epoch: [9124/10000] Training loss: 9.453483063739152e-06 / Validation loss: 0.00013740943844753464 / Long term Validation loss: 0.1649\n",
      "Epoch: [9125/10000] Training loss: 9.45209627567819e-06 / Validation loss: 0.00013739628595295811 / Long term Validation loss: 0.1649\n",
      "Epoch: [9126/10000] Training loss: 9.450709194878189e-06 / Validation loss: 0.00013738330439115964 / Long term Validation loss: 0.1649\n",
      "Epoch: [9127/10000] Training loss: 9.449321820449608e-06 / Validation loss: 0.00013737009979632905 / Long term Validation loss: 0.1649\n",
      "Epoch: [9128/10000] Training loss: 9.447934151471115e-06 / Validation loss: 0.00013735711551998508 / Long term Validation loss: 0.1649\n",
      "Epoch: [9129/10000] Training loss: 9.446546187083741e-06 / Validation loss: 0.00013734391568467045 / Long term Validation loss: 0.1649\n",
      "Epoch: [9130/10000] Training loss: 9.445157926386615e-06 / Validation loss: 0.00013733087953268905 / Long term Validation loss: 0.1649\n",
      "Epoch: [9131/10000] Training loss: 9.443769368533752e-06 / Validation loss: 0.00013731772209521565 / Long term Validation loss: 0.1649\n",
      "Epoch: [9132/10000] Training loss: 9.442380512642342e-06 / Validation loss: 0.0001373046121290568 / Long term Validation loss: 0.1649\n",
      "Epoch: [9133/10000] Training loss: 9.440991357855595e-06 / Validation loss: 0.00013729150454074387 / Long term Validation loss: 0.1649\n",
      "Epoch: [9134/10000] Training loss: 9.439601903303926e-06 / Validation loss: 0.00013727832846732033 / Long term Validation loss: 0.1649\n",
      "Epoch: [9135/10000] Training loss: 9.438212148116022e-06 / Validation loss: 0.00013726525218483825 / Long term Validation loss: 0.1649\n",
      "Epoch: [9136/10000] Training loss: 9.43682209143834e-06 / Validation loss: 0.00013725203701256248 / Long term Validation loss: 0.1649\n",
      "Epoch: [9137/10000] Training loss: 9.435431732397056e-06 / Validation loss: 0.00013723896104630417 / Long term Validation loss: 0.1649\n",
      "Epoch: [9138/10000] Training loss: 9.434041070155135e-06 / Validation loss: 0.00013722573812206815 / Long term Validation loss: 0.1650\n",
      "Epoch: [9139/10000] Training loss: 9.432650103846853e-06 / Validation loss: 0.00013721263393170463 / Long term Validation loss: 0.1650\n",
      "Epoch: [9140/10000] Training loss: 9.431258832643847e-06 / Validation loss: 0.00013719942652970683 / Long term Validation loss: 0.1650\n",
      "Epoch: [9141/10000] Training loss: 9.42986725569169e-06 / Validation loss: 0.00013718627793779403 / Long term Validation loss: 0.1650\n",
      "Epoch: [9142/10000] Training loss: 9.428475372160806e-06 / Validation loss: 0.00013717309511817517 / Long term Validation loss: 0.1650\n",
      "Epoch: [9143/10000] Training loss: 9.427083181208188e-06 / Validation loss: 0.00013715990085063807 / Long term Validation loss: 0.1650\n",
      "Epoch: [9144/10000] Training loss: 9.425690681998909e-06 / Validation loss: 0.00013714673789259362 / Long term Validation loss: 0.1650\n",
      "Epoch: [9145/10000] Training loss: 9.424297873701602e-06 / Validation loss: 0.00013713350813868386 / Long term Validation loss: 0.1650\n",
      "Epoch: [9146/10000] Training loss: 9.42290475547836e-06 / Validation loss: 0.00013712035155861294 / Long term Validation loss: 0.1650\n",
      "Epoch: [9147/10000] Training loss: 9.421511326508736e-06 / Validation loss: 0.0001371071017776455 / Long term Validation loss: 0.1650\n",
      "Epoch: [9148/10000] Training loss: 9.42011758595645e-06 / Validation loss: 0.00013709393592947338 / Long term Validation loss: 0.1650\n",
      "Epoch: [9149/10000] Training loss: 9.418723533008989e-06 / Validation loss: 0.00013708068087621826 / Long term Validation loss: 0.1650\n",
      "Epoch: [9150/10000] Training loss: 9.417329166835202e-06 / Validation loss: 0.00013706749337863894 / Long term Validation loss: 0.1650\n",
      "Epoch: [9151/10000] Training loss: 9.415934486626222e-06 / Validation loss: 0.00013705424307375517 / Long term Validation loss: 0.1650\n",
      "Epoch: [9152/10000] Training loss: 9.414539491557872e-06 / Validation loss: 0.0001370410275605704 / Long term Validation loss: 0.1651\n",
      "Epoch: [9153/10000] Training loss: 9.413144180821441e-06 / Validation loss: 0.00013702778575024654 / Long term Validation loss: 0.1651\n",
      "Epoch: [9154/10000] Training loss: 9.411748553600458e-06 / Validation loss: 0.0001370145419596685 / Long term Validation loss: 0.1651\n",
      "Epoch: [9155/10000] Training loss: 9.410352609085027e-06 / Validation loss: 0.00013700130679235737 / Long term Validation loss: 0.1651\n",
      "Epoch: [9156/10000] Training loss: 9.408956346466314e-06 / Validation loss: 0.00013698803896280203 / Long term Validation loss: 0.1651\n",
      "Epoch: [9157/10000] Training loss: 9.407559764933793e-06 / Validation loss: 0.00013697480506150628 / Long term Validation loss: 0.1651\n",
      "Epoch: [9158/10000] Training loss: 9.406162863685399e-06 / Validation loss: 0.00013696151971556936 / Long term Validation loss: 0.1651\n",
      "Epoch: [9159/10000] Training loss: 9.404765641911349e-06 / Validation loss: 0.00013694828058252525 / Long term Validation loss: 0.1651\n",
      "Epoch: [9160/10000] Training loss: 9.403368098814834e-06 / Validation loss: 0.00013693498444243928 / Long term Validation loss: 0.1651\n",
      "Epoch: [9161/10000] Training loss: 9.401970233588296e-06 / Validation loss: 0.00013692173433626104 / Long term Validation loss: 0.1651\n",
      "Epoch: [9162/10000] Training loss: 9.400572045438311e-06 / Validation loss: 0.00013690843279307212 / Long term Validation loss: 0.1651\n",
      "Epoch: [9163/10000] Training loss: 9.39917353356061e-06 / Validation loss: 0.0001368951677407528 / Long term Validation loss: 0.1651\n",
      "Epoch: [9164/10000] Training loss: 9.397774697163582e-06 / Validation loss: 0.0001368818640929858 / Long term Validation loss: 0.1651\n",
      "Epoch: [9165/10000] Training loss: 9.396375535446884e-06 / Validation loss: 0.00013686858215676431 / Long term Validation loss: 0.1651\n",
      "Epoch: [9166/10000] Training loss: 9.39497604761961e-06 / Validation loss: 0.00013685527760729034 / Long term Validation loss: 0.1651\n",
      "Epoch: [9167/10000] Training loss: 9.39357623288562e-06 / Validation loss: 0.0001368419786710409 / Long term Validation loss: 0.1652\n",
      "Epoch: [9168/10000] Training loss: 9.392176090454244e-06 / Validation loss: 0.00013682867282450157 / Long term Validation loss: 0.1652\n",
      "Epoch: [9169/10000] Training loss: 9.390775619533369e-06 / Validation loss: 0.00013681535809964752 / Long term Validation loss: 0.1652\n",
      "Epoch: [9170/10000] Training loss: 9.38937481933248e-06 / Validation loss: 0.0001368020496060222 / Long term Validation loss: 0.1652\n",
      "Epoch: [9171/10000] Training loss: 9.387973689063154e-06 / Validation loss: 0.00013678872101152394 / Long term Validation loss: 0.1652\n",
      "Epoch: [9172/10000] Training loss: 9.38657222793519e-06 / Validation loss: 0.00013677540812037637 / Long term Validation loss: 0.1652\n",
      "Epoch: [9173/10000] Training loss: 9.385170435163194e-06 / Validation loss: 0.00013676206771194482 / Long term Validation loss: 0.1652\n",
      "Epoch: [9174/10000] Training loss: 9.383768309957581e-06 / Validation loss: 0.0001367487486838152 / Long term Validation loss: 0.1652\n",
      "Epoch: [9175/10000] Training loss: 9.38236585153533e-06 / Validation loss: 0.00013673539827819975 / Long term Validation loss: 0.1652\n",
      "Epoch: [9176/10000] Training loss: 9.380963059107722e-06 / Validation loss: 0.000136722071672198 / Long term Validation loss: 0.1652\n",
      "Epoch: [9177/10000] Training loss: 9.37955993189349e-06 / Validation loss: 0.0001367087126898329 / Long term Validation loss: 0.1652\n",
      "Epoch: [9178/10000] Training loss: 9.378156469104982e-06 / Validation loss: 0.00013669537751879158 / Long term Validation loss: 0.1652\n",
      "Epoch: [9179/10000] Training loss: 9.376752669962143e-06 / Validation loss: 0.00013668201095930283 / Long term Validation loss: 0.1652\n",
      "Epoch: [9180/10000] Training loss: 9.375348533678478e-06 / Validation loss: 0.0001366686666987014 / Long term Validation loss: 0.1652\n",
      "Epoch: [9181/10000] Training loss: 9.373944059474766e-06 / Validation loss: 0.0001366552931648795 / Long term Validation loss: 0.1652\n",
      "Epoch: [9182/10000] Training loss: 9.37253924656563e-06 / Validation loss: 0.0001366419396594188 / Long term Validation loss: 0.1653\n",
      "Epoch: [9183/10000] Training loss: 9.371134094172356e-06 / Validation loss: 0.00013662855941128226 / Long term Validation loss: 0.1653\n",
      "Epoch: [9184/10000] Training loss: 9.369728601510587e-06 / Validation loss: 0.0001366151967643653 / Long term Validation loss: 0.1653\n",
      "Epoch: [9185/10000] Training loss: 9.368322767801961e-06 / Validation loss: 0.00013660180981064996 / Long term Validation loss: 0.1653\n",
      "Epoch: [9186/10000] Training loss: 9.366916592262922e-06 / Validation loss: 0.00013658843830816468 / Long term Validation loss: 0.1653\n",
      "Epoch: [9187/10000] Training loss: 9.365510074115287e-06 / Validation loss: 0.0001365750445097418 / Long term Validation loss: 0.1653\n",
      "Epoch: [9188/10000] Training loss: 9.364103212576158e-06 / Validation loss: 0.00013656166456824115 / Long term Validation loss: 0.1653\n",
      "Epoch: [9189/10000] Training loss: 9.362696006867421e-06 / Validation loss: 0.00013654826370696288 / Long term Validation loss: 0.1653\n",
      "Epoch: [9190/10000] Training loss: 9.361288456206579e-06 / Validation loss: 0.00013653487582322975 / Long term Validation loss: 0.1653\n",
      "Epoch: [9191/10000] Training loss: 9.359880559815517e-06 / Validation loss: 0.00013652146762072615 / Long term Validation loss: 0.1653\n",
      "Epoch: [9192/10000] Training loss: 9.358472316911882e-06 / Validation loss: 0.00013650807233234704 / Long term Validation loss: 0.1653\n",
      "Epoch: [9193/10000] Training loss: 9.357063726717498e-06 / Validation loss: 0.00013649465644340343 / Long term Validation loss: 0.1653\n",
      "Epoch: [9194/10000] Training loss: 9.355654788449904e-06 / Validation loss: 0.00013648125432709157 / Long term Validation loss: 0.1653\n",
      "Epoch: [9195/10000] Training loss: 9.354245501330818e-06 / Validation loss: 0.0001364678303293161 / Long term Validation loss: 0.1653\n",
      "Epoch: [9196/10000] Training loss: 9.352835864577373e-06 / Validation loss: 0.00013645442204102796 / Long term Validation loss: 0.1653\n",
      "Epoch: [9197/10000] Training loss: 9.351425877411203e-06 / Validation loss: 0.00013644098940971831 / Long term Validation loss: 0.1653\n",
      "Epoch: [9198/10000] Training loss: 9.350015539048721e-06 / Validation loss: 0.00013642757574743549 / Long term Validation loss: 0.1654\n",
      "Epoch: [9199/10000] Training loss: 9.348604848711483e-06 / Validation loss: 0.0001364141337905601 / Long term Validation loss: 0.1654\n",
      "Epoch: [9200/10000] Training loss: 9.347193805614903e-06 / Validation loss: 0.0001364007157730835 / Long term Validation loss: 0.1654\n",
      "Epoch: [9201/10000] Training loss: 9.345782408980489e-06 / Validation loss: 0.00013638726351670522 / Long term Validation loss: 0.1654\n",
      "Epoch: [9202/10000] Training loss: 9.344370658022285e-06 / Validation loss: 0.00013637384250306923 / Long term Validation loss: 0.1654\n",
      "Epoch: [9203/10000] Training loss: 9.34295855196189e-06 / Validation loss: 0.0001363603785263419 / Long term Validation loss: 0.1654\n",
      "Epoch: [9204/10000] Training loss: 9.341546090011495e-06 / Validation loss: 0.00013634695641497784 / Long term Validation loss: 0.1654\n",
      "Epoch: [9205/10000] Training loss: 9.340133271393135e-06 / Validation loss: 0.0001363334786107093 / Long term Validation loss: 0.1654\n",
      "Epoch: [9206/10000] Training loss: 9.3387200953165e-06 / Validation loss: 0.0001363200581559383 / Long term Validation loss: 0.1654\n",
      "Epoch: [9207/10000] Training loss: 9.337306561004552e-06 / Validation loss: 0.00013630656335312686 / Long term Validation loss: 0.1654\n",
      "Epoch: [9208/10000] Training loss: 9.335892667663742e-06 / Validation loss: 0.00013629314865313928 / Long term Validation loss: 0.1654\n",
      "Epoch: [9209/10000] Training loss: 9.334478414518766e-06 / Validation loss: 0.0001362796320031023 / Long term Validation loss: 0.1654\n",
      "Epoch: [9210/10000] Training loss: 9.333063800771758e-06 / Validation loss: 0.00013626622926973602 / Long term Validation loss: 0.1654\n",
      "Epoch: [9211/10000] Training loss: 9.331648825650576e-06 / Validation loss: 0.00013625268325298664 / Long term Validation loss: 0.1654\n",
      "Epoch: [9212/10000] Training loss: 9.330233488351604e-06 / Validation loss: 0.00013623930206477697 / Long term Validation loss: 0.1654\n",
      "Epoch: [9213/10000] Training loss: 9.328817788108353e-06 / Validation loss: 0.00013622571487964785 / Long term Validation loss: 0.1654\n",
      "Epoch: [9214/10000] Training loss: 9.327401724109616e-06 / Validation loss: 0.0001362123702509608 / Long term Validation loss: 0.1655\n",
      "Epoch: [9215/10000] Training loss: 9.325985295599017e-06 / Validation loss: 0.00013619872315781529 / Long term Validation loss: 0.1655\n",
      "Epoch: [9216/10000] Training loss: 9.324568501755756e-06 / Validation loss: 0.00013618543897934925 / Long term Validation loss: 0.1655\n",
      "Epoch: [9217/10000] Training loss: 9.323151341842152e-06 / Validation loss: 0.00013617170185446334 / Long term Validation loss: 0.1655\n",
      "Epoch: [9218/10000] Training loss: 9.321733815026851e-06 / Validation loss: 0.00013615851667095729 / Long term Validation loss: 0.1655\n",
      "Epoch: [9219/10000] Training loss: 9.32031592060833e-06 / Validation loss: 0.0001361446404781877 / Long term Validation loss: 0.1655\n",
      "Epoch: [9220/10000] Training loss: 9.318897657749315e-06 / Validation loss: 0.00013613161731318164 / Long term Validation loss: 0.1655\n",
      "Epoch: [9221/10000] Training loss: 9.317479025823073e-06 / Validation loss: 0.00013611752122615157 / Long term Validation loss: 0.1655\n",
      "Epoch: [9222/10000] Training loss: 9.316060024010873e-06 / Validation loss: 0.0001361047644910341 / Long term Validation loss: 0.1655\n",
      "Epoch: [9223/10000] Training loss: 9.314640651853154e-06 / Validation loss: 0.00013609031362016655 / Long term Validation loss: 0.1655\n",
      "Epoch: [9224/10000] Training loss: 9.313220908638733e-06 / Validation loss: 0.0001360779985396532 / Long term Validation loss: 0.1655\n",
      "Epoch: [9225/10000] Training loss: 9.31180079431245e-06 / Validation loss: 0.00013606296496613532 / Long term Validation loss: 0.1655\n",
      "Epoch: [9226/10000] Training loss: 9.310380308562327e-06 / Validation loss: 0.0001360513893462695 / Long term Validation loss: 0.1655\n",
      "Epoch: [9227/10000] Training loss: 9.308959452384742e-06 / Validation loss: 0.00013603538318327467 / Long term Validation loss: 0.1655\n",
      "Epoch: [9228/10000] Training loss: 9.307538226790462e-06 / Validation loss: 0.00013602505949887733 / Long term Validation loss: 0.1655\n",
      "Epoch: [9229/10000] Training loss: 9.30611663568342e-06 / Validation loss: 0.00013600740558141874 / Long term Validation loss: 0.1655\n",
      "Epoch: [9230/10000] Training loss: 9.304694684303201e-06 / Validation loss: 0.0001359992266082436 / Long term Validation loss: 0.1656\n",
      "Epoch: [9231/10000] Training loss: 9.303272385007462e-06 / Validation loss: 0.0001359787415193741 / Long term Validation loss: 0.1656\n",
      "Epoch: [9232/10000] Training loss: 9.301849756477138e-06 / Validation loss: 0.00013597428152997348 / Long term Validation loss: 0.1656\n",
      "Epoch: [9233/10000] Training loss: 9.300426836678026e-06 / Validation loss: 0.00013594886603006225 / Long term Validation loss: 0.1656\n",
      "Epoch: [9234/10000] Training loss: 9.299003687359798e-06 / Validation loss: 0.00013595093450123177 / Long term Validation loss: 0.1656\n",
      "Epoch: [9235/10000] Training loss: 9.297580426772832e-06 / Validation loss: 0.0001359168205282379 / Long term Validation loss: 0.1656\n",
      "Epoch: [9236/10000] Training loss: 9.2961572567645e-06 / Validation loss: 0.00013593049117895898 / Long term Validation loss: 0.1656\n",
      "Epoch: [9237/10000] Training loss: 9.294734554929086e-06 / Validation loss: 0.00013588083595373327 / Long term Validation loss: 0.1656\n",
      "Epoch: [9238/10000] Training loss: 9.293312987463923e-06 / Validation loss: 0.00013591538012915105 / Long term Validation loss: 0.1656\n",
      "Epoch: [9239/10000] Training loss: 9.29189379269481e-06 / Validation loss: 0.00013583761417226533 / Long term Validation loss: 0.1656\n",
      "Epoch: [9240/10000] Training loss: 9.290479208020526e-06 / Validation loss: 0.00013591017363140173 / Long term Validation loss: 0.1657\n",
      "Epoch: [9241/10000] Training loss: 9.289073401434838e-06 / Validation loss: 0.00013578094852377084 / Long term Validation loss: 0.1656\n",
      "Epoch: [9242/10000] Training loss: 9.287684042793887e-06 / Validation loss: 0.00013592359212296866 / Long term Validation loss: 0.1657\n",
      "Epoch: [9243/10000] Training loss: 9.286325514664842e-06 / Validation loss: 0.00013569906626691234 / Long term Validation loss: 0.1656\n",
      "Epoch: [9244/10000] Training loss: 9.28502468730934e-06 / Validation loss: 0.0001359725122529223 / Long term Validation loss: 0.1658\n",
      "Epoch: [9245/10000] Training loss: 9.283832396261515e-06 / Validation loss: 0.0001355695219076959 / Long term Validation loss: 0.1655\n",
      "Epoch: [9246/10000] Training loss: 9.282844888437226e-06 / Validation loss: 0.00013609018191270412 / Long term Validation loss: 0.1659\n",
      "Epoch: [9247/10000] Training loss: 9.282246078822468e-06 / Validation loss: 0.0001353495348451005 / Long term Validation loss: 0.1655\n",
      "Epoch: [9248/10000] Training loss: 9.282388464655554e-06 / Validation loss: 0.00013634367713902303 / Long term Validation loss: 0.1660\n",
      "Epoch: [9249/10000] Training loss: 9.283952727027157e-06 / Validation loss: 0.00013495851264378977 / Long term Validation loss: 0.1653\n",
      "Epoch: [9250/10000] Training loss: 9.288258647725599e-06 / Validation loss: 0.00013687306757513028 / Long term Validation loss: 0.1662\n",
      "Epoch: [9251/10000] Training loss: 9.297882009258975e-06 / Validation loss: 0.00013425023068418815 / Long term Validation loss: 0.1651\n",
      "Epoch: [9252/10000] Training loss: 9.317872052925469e-06 / Validation loss: 0.0001379865101310277 / Long term Validation loss: 0.1663\n",
      "Epoch: [9253/10000] Training loss: 9.358186142680731e-06 / Validation loss: 0.00013298645601351797 / Long term Validation loss: 0.1652\n",
      "Epoch: [9254/10000] Training loss: 9.438539084475403e-06 / Validation loss: 0.0001404162803912825 / Long term Validation loss: 0.1668\n",
      "Epoch: [9255/10000] Training loss: 9.598167638310645e-06 / Validation loss: 0.00013090691398307984 / Long term Validation loss: 0.1638\n",
      "Epoch: [9256/10000] Training loss: 9.915317361091771e-06 / Validation loss: 0.00014608911301175785 / Long term Validation loss: 0.1679\n",
      "Epoch: [9257/10000] Training loss: 1.0546402968206547e-05 / Validation loss: 0.000128380459277954 / Long term Validation loss: 0.1628\n",
      "Epoch: [9258/10000] Training loss: 1.1802456916770438e-05 / Validation loss: 0.00016063008083657074 / Long term Validation loss: 0.1647\n",
      "Epoch: [9259/10000] Training loss: 1.4296219527634383e-05 / Validation loss: 0.00012965951818348195 / Long term Validation loss: 0.1604\n",
      "Epoch: [9260/10000] Training loss: 1.919454114366141e-05 / Validation loss: 0.00020098354669420023 / Long term Validation loss: 0.1665\n",
      "Epoch: [9261/10000] Training loss: 2.8580734567789036e-05 / Validation loss: 0.00015366254684928574 / Long term Validation loss: 0.1631\n",
      "Epoch: [9262/10000] Training loss: 4.555834357934456e-05 / Validation loss: 0.000304044067238495 / Long term Validation loss: 0.1720\n",
      "Epoch: [9263/10000] Training loss: 7.280362882864355e-05 / Validation loss: 0.00022832598713699444 / Long term Validation loss: 0.1756\n",
      "Epoch: [9264/10000] Training loss: 0.00010614002476398638 / Validation loss: 0.00041762087795224433 / Long term Validation loss: 0.1748\n",
      "Epoch: [9265/10000] Training loss: 0.00012477830077501023 / Validation loss: 0.00021819969180577518 / Long term Validation loss: 0.1740\n",
      "Epoch: [9266/10000] Training loss: 9.923348753004722e-05 / Validation loss: 0.00023817167649425603 / Long term Validation loss: 0.1662\n",
      "Epoch: [9267/10000] Training loss: 4.0618477664365e-05 / Validation loss: 0.00014051995382453758 / Long term Validation loss: 0.1624\n",
      "Epoch: [9268/10000] Training loss: 9.42132270269981e-06 / Validation loss: 0.00014381668079393054 / Long term Validation loss: 0.1588\n",
      "Epoch: [9269/10000] Training loss: 3.2725661844964056e-05 / Validation loss: 0.0002976062911940125 / Long term Validation loss: 0.1681\n",
      "Epoch: [9270/10000] Training loss: 6.413380605957726e-05 / Validation loss: 0.0001657225615865145 / Long term Validation loss: 0.1631\n",
      "Epoch: [9271/10000] Training loss: 5.2416675815795837e-05 / Validation loss: 0.00018085669333181833 / Long term Validation loss: 0.1594\n",
      "Epoch: [9272/10000] Training loss: 1.7173537763637345e-05 / Validation loss: 0.00016807469475486967 / Long term Validation loss: 0.1605\n",
      "Epoch: [9273/10000] Training loss: 1.344867582876127e-05 / Validation loss: 0.00015035411451244939 / Long term Validation loss: 0.1584\n",
      "Epoch: [9274/10000] Training loss: 3.7301374205049415e-05 / Validation loss: 0.00024143913292572756 / Long term Validation loss: 0.1625\n",
      "Epoch: [9275/10000] Training loss: 4.032133010226504e-05 / Validation loss: 0.00013513493780239108 / Long term Validation loss: 0.1616\n",
      "Epoch: [9276/10000] Training loss: 1.7713313493402266e-05 / Validation loss: 0.00013639002564522833 / Long term Validation loss: 0.1621\n",
      "Epoch: [9277/10000] Training loss: 1.0698911240270088e-05 / Validation loss: 0.0002014291549464524 / Long term Validation loss: 0.1590\n",
      "Epoch: [9278/10000] Training loss: 2.6050460303037744e-05 / Validation loss: 0.00014267551258883982 / Long term Validation loss: 0.1559\n",
      "Epoch: [9279/10000] Training loss: 2.9671479007143484e-05 / Validation loss: 0.00016708079772116428 / Long term Validation loss: 0.1608\n",
      "Epoch: [9280/10000] Training loss: 1.498713552661622e-05 / Validation loss: 0.0001472998222312089 / Long term Validation loss: 0.1684\n",
      "Epoch: [9281/10000] Training loss: 1.0193985252294266e-05 / Validation loss: 0.0001346293449001501 / Long term Validation loss: 0.1554\n",
      "Epoch: [9282/10000] Training loss: 2.0457434388469216e-05 / Validation loss: 0.00018479533255603574 / Long term Validation loss: 0.1608\n",
      "Epoch: [9283/10000] Training loss: 2.2502440368892833e-05 / Validation loss: 0.0001301695885670763 / Long term Validation loss: 0.1601\n",
      "Epoch: [9284/10000] Training loss: 1.2634222982657007e-05 / Validation loss: 0.00013138392603753365 / Long term Validation loss: 0.1635\n",
      "Epoch: [9285/10000] Training loss: 1.0099913984488546e-05 / Validation loss: 0.00016667617167353987 / Long term Validation loss: 0.1621\n",
      "Epoch: [9286/10000] Training loss: 1.706945243625868e-05 / Validation loss: 0.00013009799632463095 / Long term Validation loss: 0.1533\n",
      "Epoch: [9287/10000] Training loss: 1.774769861433707e-05 / Validation loss: 0.00014598196303202622 / Long term Validation loss: 0.1681\n",
      "Epoch: [9288/10000] Training loss: 1.1111293409040034e-05 / Validation loss: 0.0001409135153455086 / Long term Validation loss: 0.1695\n",
      "Epoch: [9289/10000] Training loss: 1.0066426279910743e-05 / Validation loss: 0.00012837410441159602 / Long term Validation loss: 0.1559\n",
      "Epoch: [9290/10000] Training loss: 1.4745326044387014e-05 / Validation loss: 0.00015808714181872772 / Long term Validation loss: 0.1637\n",
      "Epoch: [9291/10000] Training loss: 1.4652462348609537e-05 / Validation loss: 0.00012949566046637284 / Long term Validation loss: 0.1626\n",
      "Epoch: [9292/10000] Training loss: 1.0227877835311095e-05 / Validation loss: 0.00012995714022134543 / Long term Validation loss: 0.1638\n",
      "Epoch: [9293/10000] Training loss: 9.982491592564797e-06 / Validation loss: 0.0001534950632868864 / Long term Validation loss: 0.1643\n",
      "Epoch: [9294/10000] Training loss: 1.307069436073611e-05 / Validation loss: 0.000127845299631212 / Long term Validation loss: 0.1600\n",
      "Epoch: [9295/10000] Training loss: 1.2634446185130716e-05 / Validation loss: 0.00013976124258145394 / Long term Validation loss: 0.1686\n",
      "Epoch: [9296/10000] Training loss: 9.734274290145993e-06 / Validation loss: 0.00014094512143438494 / Long term Validation loss: 0.1684\n",
      "Epoch: [9297/10000] Training loss: 9.870163113163154e-06 / Validation loss: 0.0001283091992594177 / Long term Validation loss: 0.1602\n",
      "Epoch: [9298/10000] Training loss: 1.1875800743139061e-05 / Validation loss: 0.00014885765440549986 / Long term Validation loss: 0.1667\n",
      "Epoch: [9299/10000] Training loss: 1.1356545591377093e-05 / Validation loss: 0.0001326811338299798 / Long term Validation loss: 0.1648\n",
      "Epoch: [9300/10000] Training loss: 9.481021268059153e-06 / Validation loss: 0.0001316948727207468 / Long term Validation loss: 0.1638\n",
      "Epoch: [9301/10000] Training loss: 9.748549050965168e-06 / Validation loss: 0.0001484194107896351 / Long term Validation loss: 0.1671\n",
      "Epoch: [9302/10000] Training loss: 1.1037242847419455e-05 / Validation loss: 0.000130100822284237 / Long term Validation loss: 0.1629\n",
      "Epoch: [9303/10000] Training loss: 1.0566917803999566e-05 / Validation loss: 0.00013853791800149087 / Long term Validation loss: 0.1671\n",
      "Epoch: [9304/10000] Training loss: 9.361052020251778e-06 / Validation loss: 0.00014108300911517776 / Long term Validation loss: 0.1673\n",
      "Epoch: [9305/10000] Training loss: 9.623839133766756e-06 / Validation loss: 0.00013014788911460453 / Long term Validation loss: 0.1625\n",
      "Epoch: [9306/10000] Training loss: 1.0448140455540875e-05 / Validation loss: 0.00014405613912302041 / Long term Validation loss: 0.1675\n",
      "Epoch: [9307/10000] Training loss: 1.0081630373108485e-05 / Validation loss: 0.00013434701347759206 / Long term Validation loss: 0.1651\n",
      "Epoch: [9308/10000] Training loss: 9.302374518389876e-06 / Validation loss: 0.00013260706704031267 / Long term Validation loss: 0.1642\n",
      "Epoch: [9309/10000] Training loss: 9.506520230453824e-06 / Validation loss: 0.0001436795637881068 / Long term Validation loss: 0.1675\n",
      "Epoch: [9310/10000] Training loss: 1.0039748182308922e-05 / Validation loss: 0.00013128176777173274 / Long term Validation loss: 0.1631\n",
      "Epoch: [9311/10000] Training loss: 9.783715609469772e-06 / Validation loss: 0.00013700887549780832 / Long term Validation loss: 0.1665\n",
      "Epoch: [9312/10000] Training loss: 9.27289504221207e-06 / Validation loss: 0.0001387544156187874 / Long term Validation loss: 0.1671\n",
      "Epoch: [9313/10000] Training loss: 9.407220240501277e-06 / Validation loss: 0.00013094183869236082 / Long term Validation loss: 0.1630\n",
      "Epoch: [9314/10000] Training loss: 9.759688302030576e-06 / Validation loss: 0.0001401810200067966 / Long term Validation loss: 0.1669\n",
      "Epoch: [9315/10000] Training loss: 9.599814454043042e-06 / Validation loss: 0.00013385795057067808 / Long term Validation loss: 0.1648\n",
      "Epoch: [9316/10000] Training loss: 9.258778108298082e-06 / Validation loss: 0.0001327557599018722 / Long term Validation loss: 0.1647\n",
      "Epoch: [9317/10000] Training loss: 9.332500252865265e-06 / Validation loss: 0.00013963611639292335 / Long term Validation loss: 0.1668\n",
      "Epoch: [9318/10000] Training loss: 9.570033360654906e-06 / Validation loss: 0.0001315400478457567 / Long term Validation loss: 0.1639\n",
      "Epoch: [9319/10000] Training loss: 9.482123759789399e-06 / Validation loss: 0.00013592460074483178 / Long term Validation loss: 0.1658\n",
      "Epoch: [9320/10000] Training loss: 9.251366165824924e-06 / Validation loss: 0.0001364933610237222 / Long term Validation loss: 0.1664\n",
      "Epoch: [9321/10000] Training loss: 9.279896243022068e-06 / Validation loss: 0.00013167418055022793 / Long term Validation loss: 0.1642\n",
      "Epoch: [9322/10000] Training loss: 9.440596074150516e-06 / Validation loss: 0.00013803767255870193 / Long term Validation loss: 0.1670\n",
      "Epoch: [9323/10000] Training loss: 9.401916329607306e-06 / Validation loss: 0.00013352439488172312 / Long term Validation loss: 0.1648\n",
      "Epoch: [9324/10000] Training loss: 9.245669337657005e-06 / Validation loss: 0.0001335109711188616 / Long term Validation loss: 0.1648\n",
      "Epoch: [9325/10000] Training loss: 9.244477155902058e-06 / Validation loss: 0.00013753966453117772 / Long term Validation loss: 0.1667\n",
      "Epoch: [9326/10000] Training loss: 9.351662026042341e-06 / Validation loss: 0.00013228176785386625 / Long term Validation loss: 0.1644\n",
      "Epoch: [9327/10000] Training loss: 9.343699682560358e-06 / Validation loss: 0.0001359147120380906 / Long term Validation loss: 0.1660\n",
      "Epoch: [9328/10000] Training loss: 9.239953563644786e-06 / Validation loss: 0.00013547288929066027 / Long term Validation loss: 0.1655\n",
      "Epoch: [9329/10000] Training loss: 9.222170642851577e-06 / Validation loss: 0.0001328492950030492 / Long term Validation loss: 0.1647\n",
      "Epoch: [9330/10000] Training loss: 9.291107911612127e-06 / Validation loss: 0.00013715604420594756 / Long term Validation loss: 0.1665\n",
      "Epoch: [9331/10000] Training loss: 9.30019434954833e-06 / Validation loss: 0.00013371169345829085 / Long term Validation loss: 0.1649\n",
      "Epoch: [9332/10000] Training loss: 9.234020125023628e-06 / Validation loss: 0.0001345105668878532 / Long term Validation loss: 0.1650\n",
      "Epoch: [9333/10000] Training loss: 9.209047575255522e-06 / Validation loss: 0.00013648820350841648 / Long term Validation loss: 0.1663\n",
      "Epoch: [9334/10000] Training loss: 9.25036282628238e-06 / Validation loss: 0.00013314955171736024 / Long term Validation loss: 0.1647\n",
      "Epoch: [9335/10000] Training loss: 9.266868938560007e-06 / Validation loss: 0.00013604809647546222 / Long term Validation loss: 0.1661\n",
      "Epoch: [9336/10000] Training loss: 9.227407678301926e-06 / Validation loss: 0.00013490685049834345 / Long term Validation loss: 0.1651\n",
      "Epoch: [9337/10000] Training loss: 9.201470431204986e-06 / Validation loss: 0.000133780579334164 / Long term Validation loss: 0.1650\n",
      "Epoch: [9338/10000] Training loss: 9.223073755150534e-06 / Validation loss: 0.00013639481105374334 / Long term Validation loss: 0.1662\n",
      "Epoch: [9339/10000] Training loss: 9.240702658550093e-06 / Validation loss: 0.00013377337409621715 / Long term Validation loss: 0.1650\n",
      "Epoch: [9340/10000] Training loss: 9.219747240080836e-06 / Validation loss: 0.00013498109161414317 / Long term Validation loss: 0.1653\n",
      "Epoch: [9341/10000] Training loss: 9.196711162663046e-06 / Validation loss: 0.00013552594161014622 / Long term Validation loss: 0.1658\n",
      "Epoch: [9342/10000] Training loss: 9.204879427118932e-06 / Validation loss: 0.00013361942281024888 / Long term Validation loss: 0.1650\n",
      "Epoch: [9343/10000] Training loss: 9.219938893361044e-06 / Validation loss: 0.00013575204790281593 / Long term Validation loss: 0.1660\n",
      "Epoch: [9344/10000] Training loss: 9.21117595889393e-06 / Validation loss: 0.00013435051757199396 / Long term Validation loss: 0.1650\n",
      "Epoch: [9345/10000] Training loss: 9.19310305100984e-06 / Validation loss: 0.00013424580715030955 / Long term Validation loss: 0.1650\n",
      "Epoch: [9346/10000] Training loss: 9.192906384537169e-06 / Validation loss: 0.00013555019500948703 / Long term Validation loss: 0.1659\n",
      "Epoch: [9347/10000] Training loss: 9.20362148362651e-06 / Validation loss: 0.00013374650298784746 / Long term Validation loss: 0.1650\n",
      "Epoch: [9348/10000] Training loss: 9.202075416646709e-06 / Validation loss: 0.00013504227293735052 / Long term Validation loss: 0.1655\n",
      "Epoch: [9349/10000] Training loss: 9.18957267102311e-06 / Validation loss: 0.00013474312281101396 / Long term Validation loss: 0.1652\n",
      "Epoch: [9350/10000] Training loss: 9.18500727526873e-06 / Validation loss: 0.0001339431954279173 / Long term Validation loss: 0.1650\n",
      "Epoch: [9351/10000] Training loss: 9.191006507810359e-06 / Validation loss: 0.00013532933399297564 / Long term Validation loss: 0.1658\n",
      "Epoch: [9352/10000] Training loss: 9.192865240835015e-06 / Validation loss: 0.00013406279759359922 / Long term Validation loss: 0.1650\n",
      "Epoch: [9353/10000] Training loss: 9.185427509334466e-06 / Validation loss: 0.0001345717546115592 / Long term Validation loss: 0.1651\n",
      "Epoch: [9354/10000] Training loss: 9.179438750354958e-06 / Validation loss: 0.00013495259794438674 / Long term Validation loss: 0.1654\n",
      "Epoch: [9355/10000] Training loss: 9.181362530058329e-06 / Validation loss: 0.00013394352326118333 / Long term Validation loss: 0.1650\n",
      "Epoch: [9356/10000] Training loss: 9.183967083235233e-06 / Validation loss: 0.00013504264983455854 / Long term Validation loss: 0.1655\n",
      "Epoch: [9357/10000] Training loss: 9.180403572517939e-06 / Validation loss: 0.0001343757918586814 / Long term Validation loss: 0.1651\n",
      "Epoch: [9358/10000] Training loss: 9.174903825757346e-06 / Validation loss: 0.00013433501718076657 / Long term Validation loss: 0.1650\n",
      "Epoch: [9359/10000] Training loss: 9.173979186530585e-06 / Validation loss: 0.00013500935093792386 / Long term Validation loss: 0.1655\n",
      "Epoch: [9360/10000] Training loss: 9.175789667007273e-06 / Validation loss: 0.00013410559372517333 / Long term Validation loss: 0.1650\n",
      "Epoch: [9361/10000] Training loss: 9.1746338753912e-06 / Validation loss: 0.00013481245362224922 / Long term Validation loss: 0.1653\n",
      "Epoch: [9362/10000] Training loss: 9.170553855746515e-06 / Validation loss: 0.00013461391516530712 / Long term Validation loss: 0.1652\n",
      "Epoch: [9363/10000] Training loss: 9.168146379429776e-06 / Validation loss: 0.0001342631398008648 / Long term Validation loss: 0.1651\n",
      "Epoch: [9364/10000] Training loss: 9.168579252571594e-06 / Validation loss: 0.00013494904963152706 / Long term Validation loss: 0.1655\n",
      "Epoch: [9365/10000] Training loss: 9.168460330208004e-06 / Validation loss: 0.0001342688489073657 / Long term Validation loss: 0.1651\n",
      "Epoch: [9366/10000] Training loss: 9.16592194119614e-06 / Validation loss: 0.00013461441118308419 / Long term Validation loss: 0.1652\n",
      "Epoch: [9367/10000] Training loss: 9.163173801606604e-06 / Validation loss: 0.00013470369854396523 / Long term Validation loss: 0.1653\n",
      "Epoch: [9368/10000] Training loss: 9.16234140126755e-06 / Validation loss: 0.00013424199065441622 / Long term Validation loss: 0.1651\n",
      "Epoch: [9369/10000] Training loss: 9.162256302595078e-06 / Validation loss: 0.0001348096206539095 / Long term Validation loss: 0.1654\n",
      "Epoch: [9370/10000] Training loss: 9.160863107172556e-06 / Validation loss: 0.0001343724875728887 / Long term Validation loss: 0.1651\n",
      "Epoch: [9371/10000] Training loss: 9.158486462316377e-06 / Validation loss: 0.00013445974965287916 / Long term Validation loss: 0.1652\n",
      "Epoch: [9372/10000] Training loss: 9.156878006147367e-06 / Validation loss: 0.00013468647815646754 / Long term Validation loss: 0.1654\n",
      "Epoch: [9373/10000] Training loss: 9.156316726539534e-06 / Validation loss: 0.00013423544952883323 / Long term Validation loss: 0.1651\n",
      "Epoch: [9374/10000] Training loss: 9.155479826373112e-06 / Validation loss: 0.0001346425104976992 / Long term Validation loss: 0.1654\n",
      "Epoch: [9375/10000] Training loss: 9.153718716289974e-06 / Validation loss: 0.00013440185054414271 / Long term Validation loss: 0.1652\n",
      "Epoch: [9376/10000] Training loss: 9.151885589290834e-06 / Validation loss: 0.0001343341691764195 / Long term Validation loss: 0.1652\n",
      "Epoch: [9377/10000] Training loss: 9.15078354369351e-06 / Validation loss: 0.00013460058116046973 / Long term Validation loss: 0.1654\n",
      "Epoch: [9378/10000] Training loss: 9.149996064693101e-06 / Validation loss: 0.0001342248036895613 / Long term Validation loss: 0.1652\n",
      "Epoch: [9379/10000] Training loss: 9.148734065505592e-06 / Validation loss: 0.0001344920275389841 / Long term Validation loss: 0.1654\n",
      "Epoch: [9380/10000] Training loss: 9.14705829152647e-06 / Validation loss: 0.00013439416453633334 / Long term Validation loss: 0.1653\n",
      "Epoch: [9381/10000] Training loss: 9.14562405474707e-06 / Validation loss: 0.00013425671328539697 / Long term Validation loss: 0.1653\n",
      "Epoch: [9382/10000] Training loss: 9.1446165313868e-06 / Validation loss: 0.00013450948400426873 / Long term Validation loss: 0.1654\n",
      "Epoch: [9383/10000] Training loss: 9.143575337029629e-06 / Validation loss: 0.00013422330334961322 / Long term Validation loss: 0.1653\n",
      "Epoch: [9384/10000] Training loss: 9.142181227615336e-06 / Validation loss: 0.00013438181696195256 / Long term Validation loss: 0.1654\n",
      "Epoch: [9385/10000] Training loss: 9.140684719939572e-06 / Validation loss: 0.0001343698040183939 / Long term Validation loss: 0.1654\n",
      "Epoch: [9386/10000] Training loss: 9.139437125807419e-06 / Validation loss: 0.00013421261573603492 / Long term Validation loss: 0.1653\n",
      "Epoch: [9387/10000] Training loss: 9.138372011896392e-06 / Validation loss: 0.00013442781877893238 / Long term Validation loss: 0.1654\n",
      "Epoch: [9388/10000] Training loss: 9.137180659995679e-06 / Validation loss: 0.00013422326562858712 / Long term Validation loss: 0.1653\n",
      "Epoch: [9389/10000] Training loss: 9.1357939296928e-06 / Validation loss: 0.0001343086045718478 / Long term Validation loss: 0.1654\n",
      "Epoch: [9390/10000] Training loss: 9.134432192132222e-06 / Validation loss: 0.00013434229417619718 / Long term Validation loss: 0.1654\n",
      "Epoch: [9391/10000] Training loss: 9.133240399505717e-06 / Validation loss: 0.00013419261871581645 / Long term Validation loss: 0.1653\n",
      "Epoch: [9392/10000] Training loss: 9.132101015973464e-06 / Validation loss: 0.00013436296000368398 / Long term Validation loss: 0.1655\n",
      "Epoch: [9393/10000] Training loss: 9.130848276092372e-06 / Validation loss: 0.000134219175467774 / Long term Validation loss: 0.1654\n",
      "Epoch: [9394/10000] Training loss: 9.129506276673459e-06 / Validation loss: 0.00013425536076212054 / Long term Validation loss: 0.1654\n",
      "Epoch: [9395/10000] Training loss: 9.12821854341852e-06 / Validation loss: 0.00013430430715593493 / Long term Validation loss: 0.1655\n",
      "Epoch: [9396/10000] Training loss: 9.127027821869922e-06 / Validation loss: 0.00013417135926326387 / Long term Validation loss: 0.1654\n",
      "Epoch: [9397/10000] Training loss: 9.125837442075897e-06 / Validation loss: 0.0001342993910003175 / Long term Validation loss: 0.1655\n",
      "Epoch: [9398/10000] Training loss: 9.124569930110497e-06 / Validation loss: 0.0001341981785935777 / Long term Validation loss: 0.1654\n",
      "Epoch: [9399/10000] Training loss: 9.123267879068586e-06 / Validation loss: 0.00013420582986040737 / Long term Validation loss: 0.1655\n",
      "Epoch: [9400/10000] Training loss: 9.122012949245788e-06 / Validation loss: 0.00013425515316075785 / Long term Validation loss: 0.1655\n",
      "Epoch: [9401/10000] Training loss: 9.120808484910478e-06 / Validation loss: 0.0001341422488167088 / Long term Validation loss: 0.1655\n",
      "Epoch: [9402/10000] Training loss: 9.119590363315725e-06 / Validation loss: 0.0001342352352967395 / Long term Validation loss: 0.1655\n",
      "Epoch: [9403/10000] Training loss: 9.118325544642589e-06 / Validation loss: 0.00013416129343682564 / Long term Validation loss: 0.1655\n",
      "Epoch: [9404/10000] Training loss: 9.117049619746137e-06 / Validation loss: 0.00013415314281049927 / Long term Validation loss: 0.1655\n",
      "Epoch: [9405/10000] Training loss: 9.115806356690629e-06 / Validation loss: 0.00013419483061680932 / Long term Validation loss: 0.1655\n",
      "Epoch: [9406/10000] Training loss: 9.11458851183794e-06 / Validation loss: 0.00013410108010889597 / Long term Validation loss: 0.1655\n",
      "Epoch: [9407/10000] Training loss: 9.113357299345896e-06 / Validation loss: 0.00013416803419243861 / Long term Validation loss: 0.1655\n",
      "Epoch: [9408/10000] Training loss: 9.112099108637065e-06 / Validation loss: 0.00013411245412026424 / Long term Validation loss: 0.1655\n",
      "Epoch: [9409/10000] Training loss: 9.110838114188166e-06 / Validation loss: 0.00013409883408772183 / Long term Validation loss: 0.1655\n",
      "Epoch: [9410/10000] Training loss: 9.109597630180897e-06 / Validation loss: 0.0001341318209939351 / Long term Validation loss: 0.1656\n",
      "Epoch: [9411/10000] Training loss: 9.108370254636437e-06 / Validation loss: 0.00013405586504440812 / Long term Validation loss: 0.1655\n",
      "Epoch: [9412/10000] Training loss: 9.107133360063792e-06 / Validation loss: 0.00013410444839918374 / Long term Validation loss: 0.1656\n",
      "Epoch: [9413/10000] Training loss: 9.105880920298016e-06 / Validation loss: 0.00013406049668095697 / Long term Validation loss: 0.1656\n",
      "Epoch: [9414/10000] Training loss: 9.104627815745666e-06 / Validation loss: 0.00013404627556510234 / Long term Validation loss: 0.1656\n",
      "Epoch: [9415/10000] Training loss: 9.103386929582645e-06 / Validation loss: 0.00013407019024664127 / Long term Validation loss: 0.1656\n",
      "Epoch: [9416/10000] Training loss: 9.1021533075167e-06 / Validation loss: 0.00013400921034187967 / Long term Validation loss: 0.1656\n",
      "Epoch: [9417/10000] Training loss: 9.100913851370442e-06 / Validation loss: 0.0001340450494689802 / Long term Validation loss: 0.1656\n",
      "Epoch: [9418/10000] Training loss: 9.09966524141444e-06 / Validation loss: 0.00013400882414460499 / Long term Validation loss: 0.1656\n",
      "Epoch: [9419/10000] Training loss: 9.098416074121888e-06 / Validation loss: 0.0001339969740575741 / Long term Validation loss: 0.1656\n",
      "Epoch: [9420/10000] Training loss: 9.097173911341267e-06 / Validation loss: 0.0001340128065099422 / Long term Validation loss: 0.1656\n",
      "Epoch: [9421/10000] Training loss: 9.095936168405583e-06 / Validation loss: 0.00013396403931199575 / Long term Validation loss: 0.1656\n",
      "Epoch: [9422/10000] Training loss: 9.094695262974806e-06 / Validation loss: 0.000133990318784511 / Long term Validation loss: 0.1656\n",
      "Epoch: [9423/10000] Training loss: 9.09344883148483e-06 / Validation loss: 0.00013395841909088596 / Long term Validation loss: 0.1656\n",
      "Epoch: [9424/10000] Training loss: 9.09220152427259e-06 / Validation loss: 0.00013394905180219864 / Long term Validation loss: 0.1657\n",
      "Epoch: [9425/10000] Training loss: 9.090958024273328e-06 / Validation loss: 0.0001339570678504446 / Long term Validation loss: 0.1657\n",
      "Epoch: [9426/10000] Training loss: 9.089717401720119e-06 / Validation loss: 0.00013391770926410998 / Long term Validation loss: 0.1657\n",
      "Epoch: [9427/10000] Training loss: 9.088475323977838e-06 / Validation loss: 0.00013393643335164667 / Long term Validation loss: 0.1657\n",
      "Epoch: [9428/10000] Training loss: 9.087229876729581e-06 / Validation loss: 0.00013390722565131025 / Long term Validation loss: 0.1657\n",
      "Epoch: [9429/10000] Training loss: 9.085983320279831e-06 / Validation loss: 0.00013390035216307607 / Long term Validation loss: 0.1657\n",
      "Epoch: [9430/10000] Training loss: 9.08473861300854e-06 / Validation loss: 0.00013390155723604845 / Long term Validation loss: 0.1657\n",
      "Epoch: [9431/10000] Training loss: 9.083495792269887e-06 / Validation loss: 0.00013386968281262488 / Long term Validation loss: 0.1657\n",
      "Epoch: [9432/10000] Training loss: 9.082252503605383e-06 / Validation loss: 0.00013388199233768308 / Long term Validation loss: 0.1657\n",
      "Epoch: [9433/10000] Training loss: 9.08100720284672e-06 / Validation loss: 0.00013385444263004184 / Long term Validation loss: 0.1657\n",
      "Epoch: [9434/10000] Training loss: 9.07976075470636e-06 / Validation loss: 0.00013384934582036083 / Long term Validation loss: 0.1657\n",
      "Epoch: [9435/10000] Training loss: 9.078514970957511e-06 / Validation loss: 0.00013384463498186783 / Long term Validation loss: 0.1657\n",
      "Epoch: [9436/10000] Training loss: 9.077270339743008e-06 / Validation loss: 0.00013381891604967612 / Long term Validation loss: 0.1657\n",
      "Epoch: [9437/10000] Training loss: 9.076025731889132e-06 / Validation loss: 0.00013382585371012163 / Long term Validation loss: 0.1657\n",
      "Epoch: [9438/10000] Training loss: 9.074779995850984e-06 / Validation loss: 0.0001338001069202783 / Long term Validation loss: 0.1657\n",
      "Epoch: [9439/10000] Training loss: 9.073533240394294e-06 / Validation loss: 0.00013379647417546267 / Long term Validation loss: 0.1657\n",
      "Epoch: [9440/10000] Training loss: 9.07228646881371e-06 / Validation loss: 0.00013378722018140686 / Long term Validation loss: 0.1657\n",
      "Epoch: [9441/10000] Training loss: 9.071040272387052e-06 / Validation loss: 0.00013376681899184638 / Long term Validation loss: 0.1658\n",
      "Epoch: [9442/10000] Training loss: 9.069794258635489e-06 / Validation loss: 0.00013376904392015858 / Long term Validation loss: 0.1658\n",
      "Epoch: [9443/10000] Training loss: 9.068547675571634e-06 / Validation loss: 0.00013374530135278488 / Long term Validation loss: 0.1658\n",
      "Epoch: [9444/10000] Training loss: 9.067300294614077e-06 / Validation loss: 0.00013374238008034668 / Long term Validation loss: 0.1658\n",
      "Epoch: [9445/10000] Training loss: 9.066052559276868e-06 / Validation loss: 0.00013372972652592 / Long term Validation loss: 0.1658\n",
      "Epoch: [9446/10000] Training loss: 9.064804959652111e-06 / Validation loss: 0.00013371384396872857 / Long term Validation loss: 0.1658\n",
      "Epoch: [9447/10000] Training loss: 9.063557497454594e-06 / Validation loss: 0.00013371193927440203 / Long term Validation loss: 0.1658\n",
      "Epoch: [9448/10000] Training loss: 9.06230977720343e-06 / Validation loss: 0.0001336906720276131 / Long term Validation loss: 0.1658\n",
      "Epoch: [9449/10000] Training loss: 9.061061498864378e-06 / Validation loss: 0.00013368778842623222 / Long term Validation loss: 0.1658\n",
      "Epoch: [9450/10000] Training loss: 9.059812758630365e-06 / Validation loss: 0.0001336729239337305 / Long term Validation loss: 0.1658\n",
      "Epoch: [9451/10000] Training loss: 9.058563858636047e-06 / Validation loss: 0.00013366065436722495 / Long term Validation loss: 0.1658\n",
      "Epoch: [9452/10000] Training loss: 9.057314952212964e-06 / Validation loss: 0.00013365496812784436 / Long term Validation loss: 0.1658\n",
      "Epoch: [9453/10000] Training loss: 9.056065908974838e-06 / Validation loss: 0.00013363630945777786 / Long term Validation loss: 0.1658\n",
      "Epoch: [9454/10000] Training loss: 9.054816502168144e-06 / Validation loss: 0.0001336325300687948 / Long term Validation loss: 0.1658\n",
      "Epoch: [9455/10000] Training loss: 9.053566661631224e-06 / Validation loss: 0.00013361644500555392 / Long term Validation loss: 0.1658\n",
      "Epoch: [9456/10000] Training loss: 9.052316506069885e-06 / Validation loss: 0.00013360675464547275 / Long term Validation loss: 0.1658\n",
      "Epoch: [9457/10000] Training loss: 9.051066187015985e-06 / Validation loss: 0.0001335978024602281 / Long term Validation loss: 0.1658\n",
      "Epoch: [9458/10000] Training loss: 9.049815723562347e-06 / Validation loss: 0.0001335818475569742 / Long term Validation loss: 0.1658\n",
      "Epoch: [9459/10000] Training loss: 9.048565007963552e-06 / Validation loss: 0.00013357649124089654 / Long term Validation loss: 0.1658\n",
      "Epoch: [9460/10000] Training loss: 9.047313937513279e-06 / Validation loss: 0.00013356019361828846 / Long term Validation loss: 0.1658\n",
      "Epoch: [9461/10000] Training loss: 9.046062509307468e-06 / Validation loss: 0.00013355198504722416 / Long term Validation loss: 0.1659\n",
      "Epoch: [9462/10000] Training loss: 9.044810804741315e-06 / Validation loss: 0.00013354040169576855 / Long term Validation loss: 0.1659\n",
      "Epoch: [9463/10000] Training loss: 9.04355888719241e-06 / Validation loss: 0.0001335269151667301 / Long term Validation loss: 0.1659\n",
      "Epoch: [9464/10000] Training loss: 9.04230674380326e-06 / Validation loss: 0.00013351948487555295 / Long term Validation loss: 0.1659\n",
      "Epoch: [9465/10000] Training loss: 9.041054310913812e-06 / Validation loss: 0.0001335038354624921 / Long term Validation loss: 0.1659\n",
      "Epoch: [9466/10000] Training loss: 9.039801541503683e-06 / Validation loss: 0.00013349607993927545 / Long term Validation loss: 0.1659\n",
      "Epoch: [9467/10000] Training loss: 9.038548447380425e-06 / Validation loss: 0.00013348279425133132 / Long term Validation loss: 0.1659\n",
      "Epoch: [9468/10000] Training loss: 9.037295071019261e-06 / Validation loss: 0.00013347138822739248 / Long term Validation loss: 0.1659\n",
      "Epoch: [9469/10000] Training loss: 9.036041439956267e-06 / Validation loss: 0.00013346183994532694 / Long term Validation loss: 0.1659\n",
      "Epoch: [9470/10000] Training loss: 9.034787540812883e-06 / Validation loss: 0.00013344749831584738 / Long term Validation loss: 0.1659\n",
      "Epoch: [9471/10000] Training loss: 9.033533337915533e-06 / Validation loss: 0.0001334392917664305 / Long term Validation loss: 0.1659\n",
      "Epoch: [9472/10000] Training loss: 9.032278810226902e-06 / Validation loss: 0.0001334252770305411 / Long term Validation loss: 0.1659\n",
      "Epoch: [9473/10000] Training loss: 9.031023963619394e-06 / Validation loss: 0.00013341524244104324 / Long term Validation loss: 0.1659\n",
      "Epoch: [9474/10000] Training loss: 9.029768820668005e-06 / Validation loss: 0.00013340385939543224 / Long term Validation loss: 0.1659\n",
      "Epoch: [9475/10000] Training loss: 9.028513393268177e-06 / Validation loss: 0.0001333910908040014 / Long term Validation loss: 0.1659\n",
      "Epoch: [9476/10000] Training loss: 9.02725767338795e-06 / Validation loss: 0.00013338180967780607 / Long term Validation loss: 0.1659\n",
      "Epoch: [9477/10000] Training loss: 9.02600164269363e-06 / Validation loss: 0.00013336800100832285 / Long term Validation loss: 0.1659\n",
      "Epoch: [9478/10000] Training loss: 9.024745288142077e-06 / Validation loss: 0.00013335851995598873 / Long term Validation loss: 0.1659\n",
      "Epoch: [9479/10000] Training loss: 9.023488612412175e-06 / Validation loss: 0.0001333459282027442 / Long term Validation loss: 0.1659\n",
      "Epoch: [9480/10000] Training loss: 9.022231625019843e-06 / Validation loss: 0.00013333456198325188 / Long term Validation loss: 0.1659\n",
      "Epoch: [9481/10000] Training loss: 9.0209743326329e-06 / Validation loss: 0.00013332393013226396 / Long term Validation loss: 0.1659\n",
      "Epoch: [9482/10000] Training loss: 9.019716731603686e-06 / Validation loss: 0.0001333109367889878 / Long term Validation loss: 0.1659\n",
      "Epoch: [9483/10000] Training loss: 9.018458811721106e-06 / Validation loss: 0.000133301175909403 / Long term Validation loss: 0.1659\n",
      "Epoch: [9484/10000] Training loss: 9.017200565438491e-06 / Validation loss: 0.00013328812722599486 / Long term Validation loss: 0.1659\n",
      "Epoch: [9485/10000] Training loss: 9.015941991053658e-06 / Validation loss: 0.00013327761766762506 / Long term Validation loss: 0.1660\n",
      "Epoch: [9486/10000] Training loss: 9.01468309290952e-06 / Validation loss: 0.00013326580213261847 / Long term Validation loss: 0.1660\n",
      "Epoch: [9487/10000] Training loss: 9.01342387401934e-06 / Validation loss: 0.00013325384771655993 / Long term Validation loss: 0.1660\n",
      "Epoch: [9488/10000] Training loss: 9.012164333285816e-06 / Validation loss: 0.00013324326540710052 / Long term Validation loss: 0.1660\n",
      "Epoch: [9489/10000] Training loss: 9.010904465728816e-06 / Validation loss: 0.0001332304671442297 / Long term Validation loss: 0.1660\n",
      "Epoch: [9490/10000] Training loss: 9.009644265638346e-06 / Validation loss: 0.00013322010314647736 / Long term Validation loss: 0.1660\n",
      "Epoch: [9491/10000] Training loss: 9.008383730830037e-06 / Validation loss: 0.00013320760198675495 / Long term Validation loss: 0.1660\n",
      "Epoch: [9492/10000] Training loss: 9.00712286141408e-06 / Validation loss: 0.0001331964540439742 / Long term Validation loss: 0.1660\n",
      "Epoch: [9493/10000] Training loss: 9.005861658989249e-06 / Validation loss: 0.00013318491270048448 / Long term Validation loss: 0.1660\n",
      "Epoch: [9494/10000] Training loss: 9.004600123169904e-06 / Validation loss: 0.00013317277605039813 / Long term Validation loss: 0.1660\n",
      "Epoch: [9495/10000] Training loss: 9.003338251455437e-06 / Validation loss: 0.00013316196315393437 / Long term Validation loss: 0.1660\n",
      "Epoch: [9496/10000] Training loss: 9.002076040517576e-06 / Validation loss: 0.00013314941787824684 / Long term Validation loss: 0.1660\n",
      "Epoch: [9497/10000] Training loss: 9.000813487323842e-06 / Validation loss: 0.00013313857580476407 / Long term Validation loss: 0.1660\n",
      "Epoch: [9498/10000] Training loss: 8.999550590943008e-06 / Validation loss: 0.00013312637896409699 / Long term Validation loss: 0.1660\n",
      "Epoch: [9499/10000] Training loss: 8.998287350959724e-06 / Validation loss: 0.0001331149093033492 / Long term Validation loss: 0.1660\n",
      "Epoch: [9500/10000] Training loss: 8.997023767265144e-06 / Validation loss: 0.00013310339398660666 / Long term Validation loss: 0.1660\n",
      "Epoch: [9501/10000] Training loss: 8.995759838581548e-06 / Validation loss: 0.00013309126931666717 / Long term Validation loss: 0.1660\n",
      "Epoch: [9502/10000] Training loss: 8.994495562742413e-06 / Validation loss: 0.0001330801933589629 / Long term Validation loss: 0.1660\n",
      "Epoch: [9503/10000] Training loss: 8.99323093757785e-06 / Validation loss: 0.00013306785570478573 / Long term Validation loss: 0.1660\n",
      "Epoch: [9504/10000] Training loss: 8.99196596107631e-06 / Validation loss: 0.0001330567005926082 / Long term Validation loss: 0.1660\n",
      "Epoch: [9505/10000] Training loss: 8.990700632329189e-06 / Validation loss: 0.00013304464415394527 / Long term Validation loss: 0.1660\n",
      "Epoch: [9506/10000] Training loss: 8.98943495042278e-06 / Validation loss: 0.00013303304147832702 / Long term Validation loss: 0.1660\n",
      "Epoch: [9507/10000] Training loss: 8.988168914590351e-06 / Validation loss: 0.00013302145473720342 / Long term Validation loss: 0.1660\n",
      "Epoch: [9508/10000] Training loss: 8.986902523485632e-06 / Validation loss: 0.00013300941022317353 / Long term Validation loss: 0.1660\n",
      "Epoch: [9509/10000] Training loss: 8.985635775386232e-06 / Validation loss: 0.00013299811362824102 / Long term Validation loss: 0.1660\n",
      "Epoch: [9510/10000] Training loss: 8.984368668665348e-06 / Validation loss: 0.00013298592320564742 / Long term Validation loss: 0.1660\n",
      "Epoch: [9511/10000] Training loss: 8.983101201683207e-06 / Validation loss: 0.00013297457152579322 / Long term Validation loss: 0.1660\n",
      "Epoch: [9512/10000] Training loss: 8.98183337339766e-06 / Validation loss: 0.00013296256078004721 / Long term Validation loss: 0.1660\n",
      "Epoch: [9513/10000] Training loss: 8.980565182664782e-06 / Validation loss: 0.00013295090759012359 / Long term Validation loss: 0.1661\n",
      "Epoch: [9514/10000] Training loss: 8.979296628508682e-06 / Validation loss: 0.00013293921126538645 / Long term Validation loss: 0.1661\n",
      "Epoch: [9515/10000] Training loss: 8.978027709661528e-06 / Validation loss: 0.00013292724617404095 / Long term Validation loss: 0.1661\n",
      "Epoch: [9516/10000] Training loss: 8.976758424701615e-06 / Validation loss: 0.00013291576276295628 / Long term Validation loss: 0.1661\n",
      "Epoch: [9517/10000] Training loss: 8.975488772251974e-06 / Validation loss: 0.00013290366572187674 / Long term Validation loss: 0.1661\n",
      "Epoch: [9518/10000] Training loss: 8.974218750855687e-06 / Validation loss: 0.0001328921733986756 / Long term Validation loss: 0.1661\n",
      "Epoch: [9519/10000] Training loss: 8.972948359391089e-06 / Validation loss: 0.0001328801612691829 / Long term Validation loss: 0.1661\n",
      "Epoch: [9520/10000] Training loss: 8.971677596634031e-06 / Validation loss: 0.00013286848156381992 / Long term Validation loss: 0.1661\n",
      "Epoch: [9521/10000] Training loss: 8.970406461544027e-06 / Validation loss: 0.0001328566684473567 / Long term Validation loss: 0.1661\n",
      "Epoch: [9522/10000] Training loss: 8.96913495291612e-06 / Validation loss: 0.00013284476418211143 / Long term Validation loss: 0.1661\n",
      "Epoch: [9523/10000] Training loss: 8.967863069513653e-06 / Validation loss: 0.0001328331161844963 / Long term Validation loss: 0.1661\n",
      "Epoch: [9524/10000] Training loss: 8.966590810091097e-06 / Validation loss: 0.00013282108147569182 / Long term Validation loss: 0.1661\n",
      "Epoch: [9525/10000] Training loss: 8.9653181733274e-06 / Validation loss: 0.0001328094691000083 / Long term Validation loss: 0.1661\n",
      "Epoch: [9526/10000] Training loss: 8.964045158080167e-06 / Validation loss: 0.00013279744572140687 / Long term Validation loss: 0.1661\n",
      "Epoch: [9527/10000] Training loss: 8.962771763118415e-06 / Validation loss: 0.00013278573909695867 / Long term Validation loss: 0.1661\n",
      "Epoch: [9528/10000] Training loss: 8.96149798737809e-06 / Validation loss: 0.0001327738266953706 / Long term Validation loss: 0.1661\n",
      "Epoch: [9529/10000] Training loss: 8.960223829687331e-06 / Validation loss: 0.00013276196842557922 / Long term Validation loss: 0.1661\n",
      "Epoch: [9530/10000] Training loss: 8.958949288919165e-06 / Validation loss: 0.00013275018014086433 / Long term Validation loss: 0.1661\n",
      "Epoch: [9531/10000] Training loss: 8.957674363905359e-06 / Validation loss: 0.0001327382004933015 / Long term Validation loss: 0.1661\n",
      "Epoch: [9532/10000] Training loss: 8.956399053442937e-06 / Validation loss: 0.0001327264758321831 / Long term Validation loss: 0.1661\n",
      "Epoch: [9533/10000] Training loss: 8.955123356403815e-06 / Validation loss: 0.00013271445685661816 / Long term Validation loss: 0.1661\n",
      "Epoch: [9534/10000] Training loss: 8.95384727159397e-06 / Validation loss: 0.00013270270991230259 / Long term Validation loss: 0.1661\n",
      "Epoch: [9535/10000] Training loss: 8.952570797942076e-06 / Validation loss: 0.00013269073136814723 / Long term Validation loss: 0.1661\n",
      "Epoch: [9536/10000] Training loss: 8.95129393429812e-06 / Validation loss: 0.00013267890033505941 / Long term Validation loss: 0.1661\n",
      "Epoch: [9537/10000] Training loss: 8.950016679593546e-06 / Validation loss: 0.00013266700059723916 / Long term Validation loss: 0.1661\n",
      "Epoch: [9538/10000] Training loss: 8.948739032704855e-06 / Validation loss: 0.00013265507317457897 / Long term Validation loss: 0.1661\n",
      "Epoch: [9539/10000] Training loss: 8.947460992524645e-06 / Validation loss: 0.0001326432405976932 / Long term Validation loss: 0.1661\n",
      "Epoch: [9540/10000] Training loss: 8.94618255795328e-06 / Validation loss: 0.00013263124880270232 / Long term Validation loss: 0.1661\n",
      "Epoch: [9541/10000] Training loss: 8.944903727861286e-06 / Validation loss: 0.0001326194390828719 / Long term Validation loss: 0.1661\n",
      "Epoch: [9542/10000] Training loss: 8.943624501183522e-06 / Validation loss: 0.00013260743399831908 / Long term Validation loss: 0.1661\n",
      "Epoch: [9543/10000] Training loss: 8.94234487680187e-06 / Validation loss: 0.0001325955980751006 / Long term Validation loss: 0.1661\n",
      "Epoch: [9544/10000] Training loss: 8.941064853675044e-06 / Validation loss: 0.00013258362210113265 / Long term Validation loss: 0.1661\n",
      "Epoch: [9545/10000] Training loss: 8.939784430708493e-06 / Validation loss: 0.00013257172908753058 / Long term Validation loss: 0.1661\n",
      "Epoch: [9546/10000] Training loss: 8.938503606858316e-06 / Validation loss: 0.00013255979943328701 / Long term Validation loss: 0.1661\n",
      "Epoch: [9547/10000] Training loss: 8.937222381052325e-06 / Validation loss: 0.00013254784584124365 / Long term Validation loss: 0.1662\n",
      "Epoch: [9548/10000] Training loss: 8.935940752230975e-06 / Validation loss: 0.00013253595346546576 / Long term Validation loss: 0.1662\n",
      "Epoch: [9549/10000] Training loss: 8.93465871934542e-06 / Validation loss: 0.00013252395836849348 / Long term Validation loss: 0.1662\n",
      "Epoch: [9550/10000] Training loss: 8.933376281328344e-06 / Validation loss: 0.00013251207818086474 / Long term Validation loss: 0.1662\n",
      "Epoch: [9551/10000] Training loss: 8.932093437154176e-06 / Validation loss: 0.0001325000701148559 / Long term Validation loss: 0.1662\n",
      "Epoch: [9552/10000] Training loss: 8.930810185761803e-06 / Validation loss: 0.00013248817481460875 / Long term Validation loss: 0.1662\n",
      "Epoch: [9553/10000] Training loss: 8.929526526140766e-06 / Validation loss: 0.0001324761782329991 / Long term Validation loss: 0.1662\n",
      "Epoch: [9554/10000] Training loss: 8.92824245724459e-06 / Validation loss: 0.00013246424928929564 / Long term Validation loss: 0.1662\n",
      "Epoch: [9555/10000] Training loss: 8.926957978065958e-06 / Validation loss: 0.00013245227635214578 / Long term Validation loss: 0.1662\n",
      "Epoch: [9556/10000] Training loss: 8.925673087576028e-06 / Validation loss: 0.00013244030883115222 / Long term Validation loss: 0.1662\n",
      "Epoch: [9557/10000] Training loss: 8.924387784763759e-06 / Validation loss: 0.000132428358213272 / Long term Validation loss: 0.1662\n",
      "Epoch: [9558/10000] Training loss: 8.923102068618938e-06 / Validation loss: 0.00013241635937050886 / Long term Validation loss: 0.1662\n",
      "Epoch: [9559/10000] Training loss: 8.92181593812794e-06 / Validation loss: 0.00013240442028164144 / Long term Validation loss: 0.1662\n",
      "Epoch: [9560/10000] Training loss: 8.92052939229822e-06 / Validation loss: 0.00013239240411788095 / Long term Validation loss: 0.1662\n",
      "Epoch: [9561/10000] Training loss: 8.919242430119056e-06 / Validation loss: 0.00013238046236859274 / Long term Validation loss: 0.1662\n",
      "Epoch: [9562/10000] Training loss: 8.917955050611616e-06 / Validation loss: 0.0001323684432812869 / Long term Validation loss: 0.1662\n",
      "Epoch: [9563/10000] Training loss: 8.916667252773527e-06 / Validation loss: 0.00013235648671699188 / Long term Validation loss: 0.1662\n",
      "Epoch: [9564/10000] Training loss: 8.915379035634092e-06 / Validation loss: 0.00013234447485127652 / Long term Validation loss: 0.1662\n",
      "Epoch: [9565/10000] Training loss: 8.914090398202929e-06 / Validation loss: 0.00013233249665543925 / Long term Validation loss: 0.1662\n",
      "Epoch: [9566/10000] Training loss: 8.912801339512889e-06 / Validation loss: 0.00013232049605673527 / Long term Validation loss: 0.1662\n",
      "Epoch: [9567/10000] Training loss: 8.911511858587477e-06 / Validation loss: 0.0001323084955157084 / Long term Validation loss: 0.1662\n",
      "Epoch: [9568/10000] Training loss: 8.910221954461262e-06 / Validation loss: 0.0001322965047089027 / Long term Validation loss: 0.1662\n",
      "Epoch: [9569/10000] Training loss: 8.908931626171993e-06 / Validation loss: 0.00013228448588791097 / Long term Validation loss: 0.1662\n",
      "Epoch: [9570/10000] Training loss: 8.90764087275669e-06 / Validation loss: 0.00013227249981481989 / Long term Validation loss: 0.1662\n",
      "Epoch: [9571/10000] Training loss: 8.906349693266154e-06 / Validation loss: 0.00013226046917099533 / Long term Validation loss: 0.1662\n",
      "Epoch: [9572/10000] Training loss: 8.905058086742056e-06 / Validation loss: 0.0001322484814836699 / Long term Validation loss: 0.1662\n",
      "Epoch: [9573/10000] Training loss: 8.903766052246052e-06 / Validation loss: 0.00013223644554130726 / Long term Validation loss: 0.1662\n",
      "Epoch: [9574/10000] Training loss: 8.902473588826843e-06 / Validation loss: 0.0001322244505739132 / Long term Validation loss: 0.1662\n",
      "Epoch: [9575/10000] Training loss: 8.90118069555447e-06 / Validation loss: 0.00013221241437433057 / Long term Validation loss: 0.1662\n",
      "Epoch: [9576/10000] Training loss: 8.89988737148665e-06 / Validation loss: 0.00013220040838609077 / Long term Validation loss: 0.1662\n",
      "Epoch: [9577/10000] Training loss: 8.898593615699798e-06 / Validation loss: 0.00013218837483955058 / Long term Validation loss: 0.1662\n",
      "Epoch: [9578/10000] Training loss: 8.897299427261963e-06 / Validation loss: 0.00013217635638018422 / Long term Validation loss: 0.1662\n",
      "Epoch: [9579/10000] Training loss: 8.896004805254713e-06 / Validation loss: 0.00013216432629914302 / Long term Validation loss: 0.1662\n",
      "Epoch: [9580/10000] Training loss: 8.894709748756979e-06 / Validation loss: 0.00013215229585748667 / Long term Validation loss: 0.1662\n",
      "Epoch: [9581/10000] Training loss: 8.89341425685516e-06 / Validation loss: 0.00013214026841308925 / Long term Validation loss: 0.1662\n",
      "Epoch: [9582/10000] Training loss: 8.892118328638951e-06 / Validation loss: 0.0001321282277249943 / Long term Validation loss: 0.1662\n",
      "Epoch: [9583/10000] Training loss: 8.890821963199952e-06 / Validation loss: 0.0001321162011241047 / Long term Validation loss: 0.1662\n",
      "Epoch: [9584/10000] Training loss: 8.889525159638026e-06 / Validation loss: 0.00013210415248568886 / Long term Validation loss: 0.1662\n",
      "Epoch: [9585/10000] Training loss: 8.888227917050716e-06 / Validation loss: 0.00013209212466355462 / Long term Validation loss: 0.1663\n",
      "Epoch: [9586/10000] Training loss: 8.886930234547292e-06 / Validation loss: 0.00013208007040133572 / Long term Validation loss: 0.1663\n",
      "Epoch: [9587/10000] Training loss: 8.885632111232064e-06 / Validation loss: 0.00013206803953367486 / Long term Validation loss: 0.1663\n",
      "Epoch: [9588/10000] Training loss: 8.884333546222856e-06 / Validation loss: 0.0001320559816393647 / Long term Validation loss: 0.1663\n",
      "Epoch: [9589/10000] Training loss: 8.883034538631523e-06 / Validation loss: 0.00013204394638898878 / Long term Validation loss: 0.1663\n",
      "Epoch: [9590/10000] Training loss: 8.881735087583726e-06 / Validation loss: 0.0001320318863144716 / Long term Validation loss: 0.1663\n",
      "Epoch: [9591/10000] Training loss: 8.880435192199404e-06 / Validation loss: 0.00013201984587390067 / Long term Validation loss: 0.1663\n",
      "Epoch: [9592/10000] Training loss: 8.879134851611542e-06 / Validation loss: 0.00013200778449871852 / Long term Validation loss: 0.1663\n",
      "Epoch: [9593/10000] Training loss: 8.87783406494856e-06 / Validation loss: 0.00013199573853954168 / Long term Validation loss: 0.1663\n",
      "Epoch: [9594/10000] Training loss: 8.876532831350464e-06 / Validation loss: 0.00013198367627853083 / Long term Validation loss: 0.1663\n",
      "Epoch: [9595/10000] Training loss: 8.875231149954406e-06 / Validation loss: 0.0001319716248623807 / Long term Validation loss: 0.1663\n",
      "Epoch: [9596/10000] Training loss: 8.873929019907315e-06 / Validation loss: 0.00013195956182253 / Long term Validation loss: 0.1663\n",
      "Epoch: [9597/10000] Training loss: 8.872626440355112e-06 / Validation loss: 0.00013194750527666113 / Long term Validation loss: 0.1663\n",
      "Epoch: [9598/10000] Training loss: 8.871323410451728e-06 / Validation loss: 0.0001319354413849282 / Long term Validation loss: 0.1663\n",
      "Epoch: [9599/10000] Training loss: 8.870019929351836e-06 / Validation loss: 0.0001319233801602915 / Long term Validation loss: 0.1663\n",
      "Epoch: [9600/10000] Training loss: 8.868715996216435e-06 / Validation loss: 0.0001319113152520614 / Long term Validation loss: 0.1663\n",
      "Epoch: [9601/10000] Training loss: 8.86741161020889e-06 / Validation loss: 0.00013189924981120275 / Long term Validation loss: 0.1663\n",
      "Epoch: [9602/10000] Training loss: 8.866106770497443e-06 / Validation loss: 0.00013188718370648993 / Long term Validation loss: 0.1663\n",
      "Epoch: [9603/10000] Training loss: 8.864801476254027e-06 / Validation loss: 0.0001318751144683066 / Long term Validation loss: 0.1663\n",
      "Epoch: [9604/10000] Training loss: 8.863495726654328e-06 / Validation loss: 0.0001318630470405742 / Long term Validation loss: 0.1663\n",
      "Epoch: [9605/10000] Training loss: 8.862189520878818e-06 / Validation loss: 0.00013185097436076786 / Long term Validation loss: 0.1663\n",
      "Epoch: [9606/10000] Training loss: 8.860882858110728e-06 / Validation loss: 0.000131838905577047 / Long term Validation loss: 0.1663\n",
      "Epoch: [9607/10000] Training loss: 8.859575737539066e-06 / Validation loss: 0.0001318268297296365 / Long term Validation loss: 0.1663\n",
      "Epoch: [9608/10000] Training loss: 8.85826815835478e-06 / Validation loss: 0.00013181475965611585 / Long term Validation loss: 0.1663\n",
      "Epoch: [9609/10000] Training loss: 8.856960119755372e-06 / Validation loss: 0.0001318026808099466 / Long term Validation loss: 0.1663\n",
      "Epoch: [9610/10000] Training loss: 8.85565162093959e-06 / Validation loss: 0.00013179060960836865 / Long term Validation loss: 0.1663\n",
      "Epoch: [9611/10000] Training loss: 8.854342661113517e-06 / Validation loss: 0.00013177852781319063 / Long term Validation loss: 0.1663\n",
      "Epoch: [9612/10000] Training loss: 8.853033239483758e-06 / Validation loss: 0.00013176645575402158 / Long term Validation loss: 0.1663\n",
      "Epoch: [9613/10000] Training loss: 8.851723355265115e-06 / Validation loss: 0.0001317543709356217 / Long term Validation loss: 0.1663\n",
      "Epoch: [9614/10000] Training loss: 8.850413007672033e-06 / Validation loss: 0.00013174229842859245 / Long term Validation loss: 0.1663\n",
      "Epoch: [9615/10000] Training loss: 8.849102195928213e-06 / Validation loss: 0.00013173021037004624 / Long term Validation loss: 0.1663\n",
      "Epoch: [9616/10000] Training loss: 8.847790919255943e-06 / Validation loss: 0.00013171813800210127 / Long term Validation loss: 0.1663\n",
      "Epoch: [9617/10000] Training loss: 8.846479176888074e-06 / Validation loss: 0.00013170604629111584 / Long term Validation loss: 0.1663\n",
      "Epoch: [9618/10000] Training loss: 8.84516696805463e-06 / Validation loss: 0.00013169397487999355 / Long term Validation loss: 0.1663\n",
      "Epoch: [9619/10000] Training loss: 8.843854291997931e-06 / Validation loss: 0.00013168187881916913 / Long term Validation loss: 0.1663\n",
      "Epoch: [9620/10000] Training loss: 8.842541147955569e-06 / Validation loss: 0.00013166980951082036 / Long term Validation loss: 0.1663\n",
      "Epoch: [9621/10000] Training loss: 8.84122753517981e-06 / Validation loss: 0.00013165770798594749 / Long term Validation loss: 0.1663\n",
      "Epoch: [9622/10000] Training loss: 8.839913452915441e-06 / Validation loss: 0.00013164564242532868 / Long term Validation loss: 0.1663\n",
      "Epoch: [9623/10000] Training loss: 8.838598900425405e-06 / Validation loss: 0.00013163353370105715 / Long term Validation loss: 0.1663\n",
      "Epoch: [9624/10000] Training loss: 8.837283876961155e-06 / Validation loss: 0.00013162147430501235 / Long term Validation loss: 0.1663\n",
      "Epoch: [9625/10000] Training loss: 8.835968381797259e-06 / Validation loss: 0.0001316093556887225 / Long term Validation loss: 0.1663\n",
      "Epoch: [9626/10000] Training loss: 8.834652414190996e-06 / Validation loss: 0.00013159730607413547 / Long term Validation loss: 0.1663\n",
      "Epoch: [9627/10000] Training loss: 8.833335973430046e-06 / Validation loss: 0.00013158517336478404 / Long term Validation loss: 0.1664\n",
      "Epoch: [9628/10000] Training loss: 8.83201905877616e-06 / Validation loss: 0.00013157313903972758 / Long term Validation loss: 0.1664\n",
      "Epoch: [9629/10000] Training loss: 8.830701669532424e-06 / Validation loss: 0.00013156098563418608 / Long term Validation loss: 0.1664\n",
      "Epoch: [9630/10000] Training loss: 8.82938380496299e-06 / Validation loss: 0.00013154897513721305 / Long term Validation loss: 0.1664\n",
      "Epoch: [9631/10000] Training loss: 8.828065464390039e-06 / Validation loss: 0.0001315367905688566 / Long term Validation loss: 0.1664\n",
      "Epoch: [9632/10000] Training loss: 8.826746647077153e-06 / Validation loss: 0.0001315248173589283 / Long term Validation loss: 0.1664\n",
      "Epoch: [9633/10000] Training loss: 8.825427352371683e-06 / Validation loss: 0.0001315125848672563 / Long term Validation loss: 0.1664\n",
      "Epoch: [9634/10000] Training loss: 8.824107579532497e-06 / Validation loss: 0.00013150067047962974 / Long term Validation loss: 0.1664\n",
      "Epoch: [9635/10000] Training loss: 8.822787327943075e-06 / Validation loss: 0.00013148836291549808 / Long term Validation loss: 0.1664\n",
      "Epoch: [9636/10000] Training loss: 8.821466596852568e-06 / Validation loss: 0.00013147654229229505 / Long term Validation loss: 0.1664\n",
      "Epoch: [9637/10000] Training loss: 8.820145385701332e-06 / Validation loss: 0.00013146411515072563 / Long term Validation loss: 0.1664\n",
      "Epoch: [9638/10000] Training loss: 8.818823693725832e-06 / Validation loss: 0.0001314524457585574 / Long term Validation loss: 0.1664\n",
      "Epoch: [9639/10000] Training loss: 8.817501520466016e-06 / Validation loss: 0.00013143982520118767 / Long term Validation loss: 0.1664\n",
      "Epoch: [9640/10000] Training loss: 8.81617886515522e-06 / Validation loss: 0.0001314284028047588 / Long term Validation loss: 0.1664\n",
      "Epoch: [9641/10000] Training loss: 8.81485572752898e-06 / Validation loss: 0.00013141546483480432 / Long term Validation loss: 0.1664\n",
      "Epoch: [9642/10000] Training loss: 8.813532106875943e-06 / Validation loss: 0.00013140445108084645 / Long term Validation loss: 0.1664\n",
      "Epoch: [9643/10000] Training loss: 8.81220800336205e-06 / Validation loss: 0.00013139098492323068 / Long term Validation loss: 0.1664\n",
      "Epoch: [9644/10000] Training loss: 8.81088341655638e-06 / Validation loss: 0.00013138065610663978 / Long term Validation loss: 0.1664\n",
      "Epoch: [9645/10000] Training loss: 8.809558347675584e-06 / Validation loss: 0.00013136629910326237 / Long term Validation loss: 0.1664\n",
      "Epoch: [9646/10000] Training loss: 8.808232797339209e-06 / Validation loss: 0.00013135713333745327 / Long term Validation loss: 0.1664\n",
      "Epoch: [9647/10000] Training loss: 8.806906769561252e-06 / Validation loss: 0.00013134125394140893 / Long term Validation loss: 0.1664\n",
      "Epoch: [9648/10000] Training loss: 8.805580268543544e-06 / Validation loss: 0.00013133408869104007 / Long term Validation loss: 0.1664\n",
      "Epoch: [9649/10000] Training loss: 8.804253306276253e-06 / Validation loss: 0.00013131557390408207 / Long term Validation loss: 0.1664\n",
      "Epoch: [9650/10000] Training loss: 8.802925898797502e-06 / Validation loss: 0.0001313118937740049 / Long term Validation loss: 0.1664\n",
      "Epoch: [9651/10000] Training loss: 8.801598082079927e-06 / Validation loss: 0.00013128875882879653 / Long term Validation loss: 0.1664\n",
      "Epoch: [9652/10000] Training loss: 8.80026991107157e-06 / Validation loss: 0.00013129122700513608 / Long term Validation loss: 0.1664\n",
      "Epoch: [9653/10000] Training loss: 8.798941496928845e-06 / Validation loss: 0.00013125989102357545 / Long term Validation loss: 0.1664\n",
      "Epoch: [9654/10000] Training loss: 8.797613023526847e-06 / Validation loss: 0.0001312733412687475 / Long term Validation loss: 0.1664\n",
      "Epoch: [9655/10000] Training loss: 8.796284845913107e-06 / Validation loss: 0.00013122726899159368 / Long term Validation loss: 0.1664\n",
      "Epoch: [9656/10000] Training loss: 8.794957581488905e-06 / Validation loss: 0.00013126057740885902 / Long term Validation loss: 0.1664\n",
      "Epoch: [9657/10000] Training loss: 8.793632400054295e-06 / Validation loss: 0.00013118770617885457 / Long term Validation loss: 0.1664\n",
      "Epoch: [9658/10000] Training loss: 8.792311403700518e-06 / Validation loss: 0.00013125736175814584 / Long term Validation loss: 0.1664\n",
      "Epoch: [9659/10000] Training loss: 8.790998556873712e-06 / Validation loss: 0.00013113517944493108 / Long term Validation loss: 0.1664\n",
      "Epoch: [9660/10000] Training loss: 8.78970114987452e-06 / Validation loss: 0.00013127217212375504 / Long term Validation loss: 0.1664\n",
      "Epoch: [9661/10000] Training loss: 8.78843297156583e-06 / Validation loss: 0.00013105821505424452 / Long term Validation loss: 0.1664\n",
      "Epoch: [9662/10000] Training loss: 8.78721982867985e-06 / Validation loss: 0.00013132148303900986 / Long term Validation loss: 0.1664\n",
      "Epoch: [9663/10000] Training loss: 8.786110866389804e-06 / Validation loss: 0.0001309348463048136 / Long term Validation loss: 0.1665\n",
      "Epoch: [9664/10000] Training loss: 8.785199409382374e-06 / Validation loss: 0.0001314378806245633 / Long term Validation loss: 0.1664\n",
      "Epoch: [9665/10000] Training loss: 8.784664762510449e-06 / Validation loss: 0.00013072303747581194 / Long term Validation loss: 0.1666\n",
      "Epoch: [9666/10000] Training loss: 8.784851804812565e-06 / Validation loss: 0.00013168736911230606 / Long term Validation loss: 0.1664\n",
      "Epoch: [9667/10000] Training loss: 8.786429720603016e-06 / Validation loss: 0.00013034330639514325 / Long term Validation loss: 0.1668\n",
      "Epoch: [9668/10000] Training loss: 8.790700857205547e-06 / Validation loss: 0.00013220835515534272 / Long term Validation loss: 0.1667\n",
      "Epoch: [9669/10000] Training loss: 8.800218325772355e-06 / Validation loss: 0.00012965101546092442 / Long term Validation loss: 0.1667\n",
      "Epoch: [9670/10000] Training loss: 8.820005679643264e-06 / Validation loss: 0.00013330676634814355 / Long term Validation loss: 0.1674\n",
      "Epoch: [9671/10000] Training loss: 8.860012080205695e-06 / Validation loss: 0.00012841037625029215 / Long term Validation loss: 0.1662\n",
      "Epoch: [9672/10000] Training loss: 8.940007600164395e-06 / Validation loss: 0.0001357137590954984 / Long term Validation loss: 0.1678\n",
      "Epoch: [9673/10000] Training loss: 9.099506773508625e-06 / Validation loss: 0.00012636729169313962 / Long term Validation loss: 0.1648\n",
      "Epoch: [9674/10000] Training loss: 9.417578117564691e-06 / Validation loss: 0.00014136717312118678 / Long term Validation loss: 0.1692\n",
      "Epoch: [9675/10000] Training loss: 1.0052987258489092e-05 / Validation loss: 0.00012391971692073245 / Long term Validation loss: 0.1630\n",
      "Epoch: [9676/10000] Training loss: 1.1322406277027745e-05 / Validation loss: 0.00015597494090009915 / Long term Validation loss: 0.1672\n",
      "Epoch: [9677/10000] Training loss: 1.3852827877307112e-05 / Validation loss: 0.00012547761413797884 / Long term Validation loss: 0.1627\n",
      "Epoch: [9678/10000] Training loss: 1.8840894083554994e-05 / Validation loss: 0.00019690092762127684 / Long term Validation loss: 0.1704\n",
      "Epoch: [9679/10000] Training loss: 2.843619484488264e-05 / Validation loss: 0.0001506343108413496 / Long term Validation loss: 0.1667\n",
      "Epoch: [9680/10000] Training loss: 4.5834795843937926e-05 / Validation loss: 0.00030226856892826356 / Long term Validation loss: 0.1739\n",
      "Epoch: [9681/10000] Training loss: 7.381060067572589e-05 / Validation loss: 0.00022791318114324922 / Long term Validation loss: 0.1782\n",
      "Epoch: [9682/10000] Training loss: 0.00010787041423608163 / Validation loss: 0.0004173573456275013 / Long term Validation loss: 0.1760\n",
      "Epoch: [9683/10000] Training loss: 0.00012640115922602174 / Validation loss: 0.00021500064389630077 / Long term Validation loss: 0.1758\n",
      "Epoch: [9684/10000] Training loss: 9.90479277819492e-05 / Validation loss: 0.00023109825131212256 / Long term Validation loss: 0.1683\n",
      "Epoch: [9685/10000] Training loss: 3.898123954234301e-05 / Validation loss: 0.00013802491372943613 / Long term Validation loss: 0.1636\n",
      "Epoch: [9686/10000] Training loss: 8.947519772911053e-06 / Validation loss: 0.0001419468749525567 / Long term Validation loss: 0.1616\n",
      "Epoch: [9687/10000] Training loss: 3.439519672959461e-05 / Validation loss: 0.00029735492844075445 / Long term Validation loss: 0.1698\n",
      "Epoch: [9688/10000] Training loss: 6.537675953121653e-05 / Validation loss: 0.000161097617114118 / Long term Validation loss: 0.1664\n",
      "Epoch: [9689/10000] Training loss: 5.094532064032713e-05 / Validation loss: 0.00017229806260718502 / Long term Validation loss: 0.1610\n",
      "Epoch: [9690/10000] Training loss: 1.537499718140723e-05 / Validation loss: 0.0001684634817974677 / Long term Validation loss: 0.1612\n",
      "Epoch: [9691/10000] Training loss: 1.4296473842154189e-05 / Validation loss: 0.00014839277960324037 / Long term Validation loss: 0.1614\n",
      "Epoch: [9692/10000] Training loss: 3.882762750133501e-05 / Validation loss: 0.0002353476152303556 / Long term Validation loss: 0.1645\n",
      "Epoch: [9693/10000] Training loss: 3.929069253157569e-05 / Validation loss: 0.00013037690745890785 / Long term Validation loss: 0.1618\n",
      "Epoch: [9694/10000] Training loss: 1.574721440795023e-05 / Validation loss: 0.00013068325106018255 / Long term Validation loss: 0.1621\n",
      "Epoch: [9695/10000] Training loss: 1.1086099869157073e-05 / Validation loss: 0.00020069698624329554 / Long term Validation loss: 0.1616\n",
      "Epoch: [9696/10000] Training loss: 2.7302206528891998e-05 / Validation loss: 0.00013815556497733875 / Long term Validation loss: 0.1583\n",
      "Epoch: [9697/10000] Training loss: 2.874497518096573e-05 / Validation loss: 0.00015769863064818827 / Long term Validation loss: 0.1631\n",
      "Epoch: [9698/10000] Training loss: 1.3276688732755263e-05 / Validation loss: 0.0001455306595756971 / Long term Validation loss: 0.1674\n",
      "Epoch: [9699/10000] Training loss: 1.0399156528181225e-05 / Validation loss: 0.00013107416528446988 / Long term Validation loss: 0.1569\n",
      "Epoch: [9700/10000] Training loss: 2.1256884257738437e-05 / Validation loss: 0.00017796862294734768 / Long term Validation loss: 0.1638\n",
      "Epoch: [9701/10000] Training loss: 2.160205050861486e-05 / Validation loss: 0.00012548419015033873 / Long term Validation loss: 0.1603\n",
      "Epoch: [9702/10000] Training loss: 1.1273517759087384e-05 / Validation loss: 0.0001254724546788537 / Long term Validation loss: 0.1618\n",
      "Epoch: [9703/10000] Training loss: 1.0214412306059034e-05 / Validation loss: 0.0001637736700187671 / Long term Validation loss: 0.1651\n",
      "Epoch: [9704/10000] Training loss: 1.747831720493178e-05 / Validation loss: 0.0001253014589969305 / Long term Validation loss: 0.1548\n",
      "Epoch: [9705/10000] Training loss: 1.6856837419188446e-05 / Validation loss: 0.00013838041263741674 / Long term Validation loss: 0.1703\n",
      "Epoch: [9706/10000] Training loss: 1.0033998313090422e-05 / Validation loss: 0.00013861480576998975 / Long term Validation loss: 0.1707\n",
      "Epoch: [9707/10000] Training loss: 1.0096731552577794e-05 / Validation loss: 0.0001242009365431882 / Long term Validation loss: 0.1559\n",
      "Epoch: [9708/10000] Training loss: 1.4845640485445531e-05 / Validation loss: 0.0001521787007510168 / Long term Validation loss: 0.1662\n",
      "Epoch: [9709/10000] Training loss: 1.3785659814187771e-05 / Validation loss: 0.0001258806934826427 / Long term Validation loss: 0.1641\n",
      "Epoch: [9710/10000] Training loss: 9.34996078293959e-06 / Validation loss: 0.00012481110340795433 / Long term Validation loss: 0.1627\n",
      "Epoch: [9711/10000] Training loss: 9.923484497167952e-06 / Validation loss: 0.00015041027019189226 / Long term Validation loss: 0.1661\n",
      "Epoch: [9712/10000] Training loss: 1.295304785606812e-05 / Validation loss: 0.00012365424470690318 / Long term Validation loss: 0.1605\n",
      "Epoch: [9713/10000] Training loss: 1.1803054338654209e-05 / Validation loss: 0.00013367460490165675 / Long term Validation loss: 0.1687\n",
      "Epoch: [9714/10000] Training loss: 8.995489790673393e-06 / Validation loss: 0.00013886613602022744 / Long term Validation loss: 0.1695\n",
      "Epoch: [9715/10000] Training loss: 9.735283716891105e-06 / Validation loss: 0.00012413499403195208 / Long term Validation loss: 0.1607\n",
      "Epoch: [9716/10000] Training loss: 1.1614218494890421e-05 / Validation loss: 0.00014361846965986474 / Long term Validation loss: 0.1680\n",
      "Epoch: [9717/10000] Training loss: 1.0577863859998963e-05 / Validation loss: 0.00012980896648049663 / Long term Validation loss: 0.1662\n",
      "Epoch: [9718/10000] Training loss: 8.841427371474809e-06 / Validation loss: 0.0001267910459415876 / Long term Validation loss: 0.1637\n",
      "Epoch: [9719/10000] Training loss: 9.543184623205726e-06 / Validation loss: 0.00014487920705940979 / Long term Validation loss: 0.1675\n",
      "Epoch: [9720/10000] Training loss: 1.0685004220019753e-05 / Validation loss: 0.00012623541118930644 / Long term Validation loss: 0.1634\n",
      "Epoch: [9721/10000] Training loss: 9.847399940477335e-06 / Validation loss: 0.00013272264263312921 / Long term Validation loss: 0.1670\n",
      "Epoch: [9722/10000] Training loss: 8.783570902038914e-06 / Validation loss: 0.00013824354508224805 / Long term Validation loss: 0.1686\n",
      "Epoch: [9723/10000] Training loss: 9.350989585051271e-06 / Validation loss: 0.0001256267617112437 / Long term Validation loss: 0.1628\n",
      "Epoch: [9724/10000] Training loss: 1.0039055012731424e-05 / Validation loss: 0.0001385152355573617 / Long term Validation loss: 0.1684\n",
      "Epoch: [9725/10000] Training loss: 9.416410129886325e-06 / Validation loss: 0.00013096971307048359 / Long term Validation loss: 0.1666\n",
      "Epoch: [9726/10000] Training loss: 8.758746595543208e-06 / Validation loss: 0.00012731391151483764 / Long term Validation loss: 0.1645\n",
      "Epoch: [9727/10000] Training loss: 9.174559256366119e-06 / Validation loss: 0.00013925667366844802 / Long term Validation loss: 0.1688\n",
      "Epoch: [9728/10000] Training loss: 9.599563411614215e-06 / Validation loss: 0.00012706747358908927 / Long term Validation loss: 0.1646\n",
      "Epoch: [9729/10000] Training loss: 9.165612212039403e-06 / Validation loss: 0.00013109156744871368 / Long term Validation loss: 0.1672\n",
      "Epoch: [9730/10000] Training loss: 8.74629915140054e-06 / Validation loss: 0.00013497946272084991 / Long term Validation loss: 0.1682\n",
      "Epoch: [9731/10000] Training loss: 9.029040548750707e-06 / Validation loss: 0.00012606753438725292 / Long term Validation loss: 0.1637\n",
      "Epoch: [9732/10000] Training loss: 9.304901153432004e-06 / Validation loss: 0.0001345870518633854 / Long term Validation loss: 0.1681\n",
      "Epoch: [9733/10000] Training loss: 9.019526530192276e-06 / Validation loss: 0.00012993796284780442 / Long term Validation loss: 0.1666\n",
      "Epoch: [9734/10000] Training loss: 8.740235357227322e-06 / Validation loss: 0.00012736481691595994 / Long term Validation loss: 0.1652\n",
      "Epoch: [9735/10000] Training loss: 8.920011742046245e-06 / Validation loss: 0.0001350445935316991 / Long term Validation loss: 0.1682\n",
      "Epoch: [9736/10000] Training loss: 9.108559126743541e-06 / Validation loss: 0.00012720562044662273 / Long term Validation loss: 0.1652\n",
      "Epoch: [9737/10000] Training loss: 8.929915892789347e-06 / Validation loss: 0.00013036929916105986 / Long term Validation loss: 0.1669\n",
      "Epoch: [9738/10000] Training loss: 8.736101131039856e-06 / Validation loss: 0.00013256384409478993 / Long term Validation loss: 0.1678\n",
      "Epoch: [9739/10000] Training loss: 8.84167133510427e-06 / Validation loss: 0.00012691859738597533 / Long term Validation loss: 0.1651\n",
      "Epoch: [9740/10000] Training loss: 8.974925413678673e-06 / Validation loss: 0.0001329607124928979 / Long term Validation loss: 0.1678\n",
      "Epoch: [9741/10000] Training loss: 8.869873213296358e-06 / Validation loss: 0.00012959243631862748 / Long term Validation loss: 0.1665\n",
      "Epoch: [9742/10000] Training loss: 8.73213955357178e-06 / Validation loss: 0.00012842015447306237 / Long term Validation loss: 0.1659\n",
      "Epoch: [9743/10000] Training loss: 8.787366876979045e-06 / Validation loss: 0.00013325398734354975 / Long term Validation loss: 0.1675\n",
      "Epoch: [9744/10000] Training loss: 8.882812794127954e-06 / Validation loss: 0.00012804601918758244 / Long term Validation loss: 0.1655\n",
      "Epoch: [9745/10000] Training loss: 8.82658901801546e-06 / Validation loss: 0.00013084003763000758 / Long term Validation loss: 0.1665\n",
      "Epoch: [9746/10000] Training loss: 8.72871185518271e-06 / Validation loss: 0.00013162030947074316 / Long term Validation loss: 0.1667\n",
      "Epoch: [9747/10000] Training loss: 8.751765026946338e-06 / Validation loss: 0.0001282399002726516 / Long term Validation loss: 0.1656\n",
      "Epoch: [9748/10000] Training loss: 8.81945055594242e-06 / Validation loss: 0.00013250948358313553 / Long term Validation loss: 0.1671\n",
      "Epoch: [9749/10000] Training loss: 8.794302355443768e-06 / Validation loss: 0.00012974303975216473 / Long term Validation loss: 0.1664\n",
      "Epoch: [9750/10000] Training loss: 8.725828869105888e-06 / Validation loss: 0.00012960125401658158 / Long term Validation loss: 0.1663\n",
      "Epoch: [9751/10000] Training loss: 8.729286863800252e-06 / Validation loss: 0.00013231564639711073 / Long term Validation loss: 0.1669\n",
      "Epoch: [9752/10000] Training loss: 8.775655045718227e-06 / Validation loss: 0.00012880118043303565 / Long term Validation loss: 0.1658\n",
      "Epoch: [9753/10000] Training loss: 8.769012651836317e-06 / Validation loss: 0.0001311679264359218 / Long term Validation loss: 0.1662\n",
      "Epoch: [9754/10000] Training loss: 8.722622449246313e-06 / Validation loss: 0.0001308816164938544 / Long term Validation loss: 0.1661\n",
      "Epoch: [9755/10000] Training loss: 8.71518580230797e-06 / Validation loss: 0.0001290590309870324 / Long term Validation loss: 0.1660\n",
      "Epoch: [9756/10000] Training loss: 8.744975816520683e-06 / Validation loss: 0.0001318496678850378 / Long term Validation loss: 0.1667\n",
      "Epoch: [9757/10000] Training loss: 8.748306102946404e-06 / Validation loss: 0.0001295455442892042 / Long term Validation loss: 0.1663\n",
      "Epoch: [9758/10000] Training loss: 8.718593932898494e-06 / Validation loss: 0.0001300699901064443 / Long term Validation loss: 0.1665\n",
      "Epoch: [9759/10000] Training loss: 8.706280997556794e-06 / Validation loss: 0.00013126467580891767 / Long term Validation loss: 0.1664\n",
      "Epoch: [9760/10000] Training loss: 8.723427364243609e-06 / Validation loss: 0.00012904743241030483 / Long term Validation loss: 0.1661\n",
      "Epoch: [9761/10000] Training loss: 8.731047150998375e-06 / Validation loss: 0.0001309605403327787 / Long term Validation loss: 0.1662\n",
      "Epoch: [9762/10000] Training loss: 8.713740729121472e-06 / Validation loss: 0.00013010332077700322 / Long term Validation loss: 0.1664\n",
      "Epoch: [9763/10000] Training loss: 8.700575188992183e-06 / Validation loss: 0.00012942380578549932 / Long term Validation loss: 0.1663\n",
      "Epoch: [9764/10000] Training loss: 8.708455301916971e-06 / Validation loss: 0.0001310638500784127 / Long term Validation loss: 0.1663\n",
      "Epoch: [9765/10000] Training loss: 8.716649080592213e-06 / Validation loss: 0.0001293149901582844 / Long term Validation loss: 0.1663\n",
      "Epoch: [9766/10000] Training loss: 8.708075856495744e-06 / Validation loss: 0.00013021772684254415 / Long term Validation loss: 0.1663\n",
      "Epoch: [9767/10000] Training loss: 8.696501288892252e-06 / Validation loss: 0.00013044795082659593 / Long term Validation loss: 0.1662\n",
      "Epoch: [9768/10000] Training loss: 8.698087779654629e-06 / Validation loss: 0.00012928775634833093 / Long term Validation loss: 0.1663\n",
      "Epoch: [9769/10000] Training loss: 8.704659347422774e-06 / Validation loss: 0.00013072051486661716 / Long term Validation loss: 0.1662\n",
      "Epoch: [9770/10000] Training loss: 8.701658349656802e-06 / Validation loss: 0.0001297219590296177 / Long term Validation loss: 0.1664\n",
      "Epoch: [9771/10000] Training loss: 8.692870440361674e-06 / Validation loss: 0.00012981561641939264 / Long term Validation loss: 0.1664\n",
      "Epoch: [9772/10000] Training loss: 8.690758432861268e-06 / Validation loss: 0.0001305774322146414 / Long term Validation loss: 0.1661\n",
      "Epoch: [9773/10000] Training loss: 8.694774677719578e-06 / Validation loss: 0.00012943261819918663 / Long term Validation loss: 0.1663\n",
      "Epoch: [9774/10000] Training loss: 8.694759562861774e-06 / Validation loss: 0.00013039323951795 / Long term Validation loss: 0.1661\n",
      "Epoch: [9775/10000] Training loss: 8.688999081256626e-06 / Validation loss: 0.00013006858839245924 / Long term Validation loss: 0.1663\n",
      "Epoch: [9776/10000] Training loss: 8.685298020764327e-06 / Validation loss: 0.0001296859361488469 / Long term Validation loss: 0.1664\n",
      "Epoch: [9777/10000] Training loss: 8.686761244143548e-06 / Validation loss: 0.0001305601331800162 / Long term Validation loss: 0.1661\n",
      "Epoch: [9778/10000] Training loss: 8.687800792962111e-06 / Validation loss: 0.00012968684265072654 / Long term Validation loss: 0.1664\n",
      "Epoch: [9779/10000] Training loss: 8.68461649091153e-06 / Validation loss: 0.00013015850102333828 / Long term Validation loss: 0.1663\n",
      "Epoch: [9780/10000] Training loss: 8.680804627372338e-06 / Validation loss: 0.00013026760477672226 / Long term Validation loss: 0.1662\n",
      "Epoch: [9781/10000] Training loss: 8.68028943811409e-06 / Validation loss: 0.00012968274413877748 / Long term Validation loss: 0.1664\n",
      "Epoch: [9782/10000] Training loss: 8.681140076558638e-06 / Validation loss: 0.0001304165739341266 / Long term Validation loss: 0.1661\n",
      "Epoch: [9783/10000] Training loss: 8.679706741903886e-06 / Validation loss: 0.00012986804652991544 / Long term Validation loss: 0.1663\n",
      "Epoch: [9784/10000] Training loss: 8.67660191303836e-06 / Validation loss: 0.00012996563442213313 / Long term Validation loss: 0.1663\n",
      "Epoch: [9785/10000] Training loss: 8.674922268960773e-06 / Validation loss: 0.0001302886957360048 / Long term Validation loss: 0.1661\n",
      "Epoch: [9786/10000] Training loss: 8.675000110393818e-06 / Validation loss: 0.0001297058001888278 / Long term Validation loss: 0.1664\n",
      "Epoch: [9787/10000] Training loss: 8.674431339312267e-06 / Validation loss: 0.00013022184139575575 / Long term Validation loss: 0.1662\n",
      "Epoch: [9788/10000] Training loss: 8.672278915742943e-06 / Validation loss: 0.0001299554027839118 / Long term Validation loss: 0.1663\n",
      "Epoch: [9789/10000] Training loss: 8.670210601219494e-06 / Validation loss: 0.00012982589429836884 / Long term Validation loss: 0.1663\n",
      "Epoch: [9790/10000] Training loss: 8.669460775123156e-06 / Validation loss: 0.00013020238015903373 / Long term Validation loss: 0.1662\n",
      "Epoch: [9791/10000] Training loss: 8.669048588218524e-06 / Validation loss: 0.00012971922531944253 / Long term Validation loss: 0.1664\n",
      "Epoch: [9792/10000] Training loss: 8.667686061465329e-06 / Validation loss: 0.00013002581471119022 / Long term Validation loss: 0.1662\n",
      "Epoch: [9793/10000] Training loss: 8.665769589869316e-06 / Validation loss: 0.0001299577413407028 / Long term Validation loss: 0.1663\n",
      "Epoch: [9794/10000] Training loss: 8.664457116949667e-06 / Validation loss: 0.00012972883477406347 / Long term Validation loss: 0.1664\n",
      "Epoch: [9795/10000] Training loss: 8.663793911155856e-06 / Validation loss: 0.00013007856871440502 / Long term Validation loss: 0.1662\n",
      "Epoch: [9796/10000] Training loss: 8.662861830402937e-06 / Validation loss: 0.00012973148786116605 / Long term Validation loss: 0.1664\n",
      "Epoch: [9797/10000] Training loss: 8.661324186354272e-06 / Validation loss: 0.00012988364240888842 / Long term Validation loss: 0.1663\n",
      "Epoch: [9798/10000] Training loss: 8.65980759611762e-06 / Validation loss: 0.00012993536594710955 / Long term Validation loss: 0.1663\n",
      "Epoch: [9799/10000] Training loss: 8.658792992655203e-06 / Validation loss: 0.00012968784176578516 / Long term Validation loss: 0.1664\n",
      "Epoch: [9800/10000] Training loss: 8.657940261527616e-06 / Validation loss: 0.00012997252788130722 / Long term Validation loss: 0.1662\n",
      "Epoch: [9801/10000] Training loss: 8.656743841518318e-06 / Validation loss: 0.00012974541681488622 / Long term Validation loss: 0.1663\n",
      "Epoch: [9802/10000] Training loss: 8.655298333878266e-06 / Validation loss: 0.00012979339372756768 / Long term Validation loss: 0.1663\n",
      "Epoch: [9803/10000] Training loss: 8.654044115684027e-06 / Validation loss: 0.00012990070229144745 / Long term Validation loss: 0.1662\n",
      "Epoch: [9804/10000] Training loss: 8.653064741109073e-06 / Validation loss: 0.00012967616445915263 / Long term Validation loss: 0.1663\n",
      "Epoch: [9805/10000] Training loss: 8.65203656049947e-06 / Validation loss: 0.0001298907521595251 / Long term Validation loss: 0.1662\n",
      "Epoch: [9806/10000] Training loss: 8.650771037028488e-06 / Validation loss: 0.00012975566230578425 / Long term Validation loss: 0.1663\n",
      "Epoch: [9807/10000] Training loss: 8.649456495212273e-06 / Validation loss: 0.0001297428360753995 / Long term Validation loss: 0.1663\n",
      "Epoch: [9808/10000] Training loss: 8.648317004219703e-06 / Validation loss: 0.00012986387153360744 / Long term Validation loss: 0.1662\n",
      "Epoch: [9809/10000] Training loss: 8.647288321648774e-06 / Validation loss: 0.0001296748770438581 / Long term Validation loss: 0.1663\n",
      "Epoch: [9810/10000] Training loss: 8.646164611820626e-06 / Validation loss: 0.0001298240737270391 / Long term Validation loss: 0.1662\n",
      "Epoch: [9811/10000] Training loss: 8.644914823217417e-06 / Validation loss: 0.00012974723700565313 / Long term Validation loss: 0.1663\n",
      "Epoch: [9812/10000] Training loss: 8.643691522996736e-06 / Validation loss: 0.0001297010201928091 / Long term Validation loss: 0.1663\n",
      "Epoch: [9813/10000] Training loss: 8.642582486831427e-06 / Validation loss: 0.00012981220765084927 / Long term Validation loss: 0.1662\n",
      "Epoch: [9814/10000] Training loss: 8.641501511959796e-06 / Validation loss: 0.00012965987569047785 / Long term Validation loss: 0.1663\n",
      "Epoch: [9815/10000] Training loss: 8.640341401053642e-06 / Validation loss: 0.00012975870081139088 / Long term Validation loss: 0.1662\n",
      "Epoch: [9816/10000] Training loss: 8.639125488472813e-06 / Validation loss: 0.00012971767805928055 / Long term Validation loss: 0.1663\n",
      "Epoch: [9817/10000] Training loss: 8.637949860243539e-06 / Validation loss: 0.00012965874213261 / Long term Validation loss: 0.1663\n",
      "Epoch: [9818/10000] Training loss: 8.636837959831608e-06 / Validation loss: 0.00012975146996379317 / Long term Validation loss: 0.1662\n",
      "Epoch: [9819/10000] Training loss: 8.635722894240117e-06 / Validation loss: 0.00012963086455960096 / Long term Validation loss: 0.1663\n",
      "Epoch: [9820/10000] Training loss: 8.634555589694108e-06 / Validation loss: 0.00012969318111495995 / Long term Validation loss: 0.1662\n",
      "Epoch: [9821/10000] Training loss: 8.633367447608075e-06 / Validation loss: 0.00012967127717904047 / Long term Validation loss: 0.1662\n",
      "Epoch: [9822/10000] Training loss: 8.632212253162098e-06 / Validation loss: 0.00012961135982305925 / Long term Validation loss: 0.1663\n",
      "Epoch: [9823/10000] Training loss: 8.631089882820819e-06 / Validation loss: 0.00012968506956310142 / Long term Validation loss: 0.1662\n",
      "Epoch: [9824/10000] Training loss: 8.62995700922477e-06 / Validation loss: 0.0001295913918873394 / Long term Validation loss: 0.1663\n",
      "Epoch: [9825/10000] Training loss: 8.628792937196576e-06 / Validation loss: 0.0001296311943282265 / Long term Validation loss: 0.1662\n",
      "Epoch: [9826/10000] Training loss: 8.627622486925931e-06 / Validation loss: 0.0001296200085383688 / Long term Validation loss: 0.1662\n",
      "Epoch: [9827/10000] Training loss: 8.626474315988297e-06 / Validation loss: 0.00012956635586987582 / Long term Validation loss: 0.1663\n",
      "Epoch: [9828/10000] Training loss: 8.625342655085237e-06 / Validation loss: 0.0001296235243451112 / Long term Validation loss: 0.1662\n",
      "Epoch: [9829/10000] Training loss: 8.624201565371816e-06 / Validation loss: 0.00012955052883580634 / Long term Validation loss: 0.1663\n",
      "Epoch: [9830/10000] Training loss: 8.623042494123094e-06 / Validation loss: 0.0001295761597292329 / Long term Validation loss: 0.1662\n",
      "Epoch: [9831/10000] Training loss: 8.621881921005188e-06 / Validation loss: 0.0001295690948582942 / Long term Validation loss: 0.1662\n",
      "Epoch: [9832/10000] Training loss: 8.620735158695878e-06 / Validation loss: 0.00012952366500648801 / Long term Validation loss: 0.1663\n",
      "Epoch: [9833/10000] Training loss: 8.619596975424664e-06 / Validation loss: 0.00012956704205012513 / Long term Validation loss: 0.1662\n",
      "Epoch: [9834/10000] Training loss: 8.61845229579608e-06 / Validation loss: 0.00012950962630754417 / Long term Validation loss: 0.1663\n",
      "Epoch: [9835/10000] Training loss: 8.617297226631903e-06 / Validation loss: 0.000129527067663965 / Long term Validation loss: 0.1662\n",
      "Epoch: [9836/10000] Training loss: 8.616141794288854e-06 / Validation loss: 0.0001295207199945613 / Long term Validation loss: 0.1662\n",
      "Epoch: [9837/10000] Training loss: 8.61499453480364e-06 / Validation loss: 0.00012948346832200605 / Long term Validation loss: 0.1662\n",
      "Epoch: [9838/10000] Training loss: 8.613852102312892e-06 / Validation loss: 0.00012951502731807672 / Long term Validation loss: 0.1662\n",
      "Epoch: [9839/10000] Training loss: 8.612705772984383e-06 / Validation loss: 0.00012946814739517103 / Long term Validation loss: 0.1662\n",
      "Epoch: [9840/10000] Training loss: 8.611553293416259e-06 / Validation loss: 0.00012947985809397996 / Long term Validation loss: 0.1662\n",
      "Epoch: [9841/10000] Training loss: 8.6104004044033e-06 / Validation loss: 0.00012947154767018184 / Long term Validation loss: 0.1662\n",
      "Epoch: [9842/10000] Training loss: 8.609252162799815e-06 / Validation loss: 0.00012944118269325513 / Long term Validation loss: 0.1662\n",
      "Epoch: [9843/10000] Training loss: 8.608106892084307e-06 / Validation loss: 0.00012946284940233883 / Long term Validation loss: 0.1662\n",
      "Epoch: [9844/10000] Training loss: 8.606959545524385e-06 / Validation loss: 0.0001294235973472134 / Long term Validation loss: 0.1662\n",
      "Epoch: [9845/10000] Training loss: 8.605808461255324e-06 / Validation loss: 0.00012943171191182731 / Long term Validation loss: 0.1662\n",
      "Epoch: [9846/10000] Training loss: 8.60465671284743e-06 / Validation loss: 0.00012942086710197764 / Long term Validation loss: 0.1662\n",
      "Epoch: [9847/10000] Training loss: 8.603507450026163e-06 / Validation loss: 0.0001293964522440621 / Long term Validation loss: 0.1662\n",
      "Epoch: [9848/10000] Training loss: 8.602360125441966e-06 / Validation loss: 0.00012940987810251982 / Long term Validation loss: 0.1662\n",
      "Epoch: [9849/10000] Training loss: 8.601211869801784e-06 / Validation loss: 0.0001293761951548954 / Long term Validation loss: 0.1662\n",
      "Epoch: [9850/10000] Training loss: 8.600061326741082e-06 / Validation loss: 0.00012938164872378462 / Long term Validation loss: 0.1662\n",
      "Epoch: [9851/10000] Training loss: 8.598909962794615e-06 / Validation loss: 0.00012936825535783122 / Long term Validation loss: 0.1662\n",
      "Epoch: [9852/10000] Training loss: 8.597759764544283e-06 / Validation loss: 0.0001293490204925035 / Long term Validation loss: 0.1662\n",
      "Epoch: [9853/10000] Training loss: 8.59661082605005e-06 / Validation loss: 0.0001293559522537617 / Long term Validation loss: 0.1662\n",
      "Epoch: [9854/10000] Training loss: 8.595461607431722e-06 / Validation loss: 0.00012932716856574664 / Long term Validation loss: 0.1662\n",
      "Epoch: [9855/10000] Training loss: 8.59431102218895e-06 / Validation loss: 0.00012933091388342207 / Long term Validation loss: 0.1662\n",
      "Epoch: [9856/10000] Training loss: 8.59315960385342e-06 / Validation loss: 0.0001293157380607471 / Long term Validation loss: 0.1662\n",
      "Epoch: [9857/10000] Training loss: 8.59200855428433e-06 / Validation loss: 0.00012930101325714133 / Long term Validation loss: 0.1662\n",
      "Epoch: [9858/10000] Training loss: 8.590858251552735e-06 / Validation loss: 0.00012930262514985644 / Long term Validation loss: 0.1662\n",
      "Epoch: [9859/10000] Training loss: 8.589707977848991e-06 / Validation loss: 0.00012927806417608955 / Long term Validation loss: 0.1662\n",
      "Epoch: [9860/10000] Training loss: 8.58855693556885e-06 / Validation loss: 0.00012928015263226269 / Long term Validation loss: 0.1662\n",
      "Epoch: [9861/10000] Training loss: 8.587405161539234e-06 / Validation loss: 0.0001292637696483174 / Long term Validation loss: 0.1662\n",
      "Epoch: [9862/10000] Training loss: 8.586253303803567e-06 / Validation loss: 0.00012925271020362134 / Long term Validation loss: 0.1662\n",
      "Epoch: [9863/10000] Training loss: 8.585101789278439e-06 / Validation loss: 0.00012924996807831736 / Long term Validation loss: 0.1662\n",
      "Epoch: [9864/10000] Training loss: 8.58395038925203e-06 / Validation loss: 0.00012922930429054477 / Long term Validation loss: 0.1662\n",
      "Epoch: [9865/10000] Training loss: 8.582798595599946e-06 / Validation loss: 0.00012922964583977984 / Long term Validation loss: 0.1662\n",
      "Epoch: [9866/10000] Training loss: 8.581646233171204e-06 / Validation loss: 0.00012921270658909406 / Long term Validation loss: 0.1662\n",
      "Epoch: [9867/10000] Training loss: 8.580493570649306e-06 / Validation loss: 0.00012920428963083129 / Long term Validation loss: 0.1662\n",
      "Epoch: [9868/10000] Training loss: 8.579340948735167e-06 / Validation loss: 0.00012919777643676035 / Long term Validation loss: 0.1662\n",
      "Epoch: [9869/10000] Training loss: 8.578188392246539e-06 / Validation loss: 0.0001291804525441274 / Long term Validation loss: 0.1662\n",
      "Epoch: [9870/10000] Training loss: 8.577035645023018e-06 / Validation loss: 0.00012917861049152585 / Long term Validation loss: 0.1662\n",
      "Epoch: [9871/10000] Training loss: 8.575882498103135e-06 / Validation loss: 0.0001291616736475515 / Long term Validation loss: 0.1662\n",
      "Epoch: [9872/10000] Training loss: 8.574728991201483e-06 / Validation loss: 0.00012915490348312203 / Long term Validation loss: 0.1662\n",
      "Epoch: [9873/10000] Training loss: 8.573575326455617e-06 / Validation loss: 0.0001291453727760618 / Long term Validation loss: 0.1662\n",
      "Epoch: [9874/10000] Training loss: 8.57242161722562e-06 / Validation loss: 0.00012913103547461307 / Long term Validation loss: 0.1662\n",
      "Epoch: [9875/10000] Training loss: 8.57126778902802e-06 / Validation loss: 0.00012912682089522975 / Long term Validation loss: 0.1662\n",
      "Epoch: [9876/10000] Training loss: 8.570113692383195e-06 / Validation loss: 0.00012911053737231935 / Long term Validation loss: 0.1663\n",
      "Epoch: [9877/10000] Training loss: 8.568959264643764e-06 / Validation loss: 0.00012910446368355596 / Long term Validation loss: 0.1663\n",
      "Epoch: [9878/10000] Training loss: 8.567804579921865e-06 / Validation loss: 0.00012909269305167543 / Long term Validation loss: 0.1663\n",
      "Epoch: [9879/10000] Training loss: 8.5666497393675e-06 / Validation loss: 0.00012908083653266953 / Long term Validation loss: 0.1663\n",
      "Epoch: [9880/10000] Training loss: 8.565494766029733e-06 / Validation loss: 0.0001290741959636348 / Long term Validation loss: 0.1663\n",
      "Epoch: [9881/10000] Training loss: 8.564339594554052e-06 / Validation loss: 0.00012905915330003307 / Long term Validation loss: 0.1663\n",
      "Epoch: [9882/10000] Training loss: 8.563184150445297e-06 / Validation loss: 0.00012905300936692324 / Long term Validation loss: 0.1663\n",
      "Epoch: [9883/10000] Training loss: 8.562028427335025e-06 / Validation loss: 0.00012903998712969595 / Long term Validation loss: 0.1663\n",
      "Epoch: [9884/10000] Training loss: 8.560872473479925e-06 / Validation loss: 0.00012903010611920007 / Long term Validation loss: 0.1663\n",
      "Epoch: [9885/10000] Training loss: 8.559716336160049e-06 / Validation loss: 0.0001290212991331024 / Long term Validation loss: 0.1663\n",
      "Epoch: [9886/10000] Training loss: 8.558560012189067e-06 / Validation loss: 0.00012900789080912844 / Long term Validation loss: 0.1663\n",
      "Epoch: [9887/10000] Training loss: 8.557403459983314e-06 / Validation loss: 0.00012900099649922693 / Long term Validation loss: 0.1663\n",
      "Epoch: [9888/10000] Training loss: 8.556246647260207e-06 / Validation loss: 0.00012898760370840497 / Long term Validation loss: 0.1663\n",
      "Epoch: [9889/10000] Training loss: 8.555089575452728e-06 / Validation loss: 0.0001289789551861768 / Long term Validation loss: 0.1663\n",
      "Epoch: [9890/10000] Training loss: 8.553932273662083e-06 / Validation loss: 0.00012896840904170258 / Long term Validation loss: 0.1663\n",
      "Epoch: [9891/10000] Training loss: 8.55277476167856e-06 / Validation loss: 0.0001289567068349931 / Long term Validation loss: 0.1663\n",
      "Epoch: [9892/10000] Training loss: 8.551617033831056e-06 / Validation loss: 0.00012894862294226257 / Long term Validation loss: 0.1663\n",
      "Epoch: [9893/10000] Training loss: 8.55045906814663e-06 / Validation loss: 0.00012893564558578183 / Long term Validation loss: 0.1663\n",
      "Epoch: [9894/10000] Training loss: 8.549300847281244e-06 / Validation loss: 0.00012892744408000466 / Long term Validation loss: 0.1663\n",
      "Epoch: [9895/10000] Training loss: 8.548142374644855e-06 / Validation loss: 0.00012891577709329586 / Long term Validation loss: 0.1663\n",
      "Epoch: [9896/10000] Training loss: 8.546983663466455e-06 / Validation loss: 0.00012890548793710995 / Long term Validation loss: 0.1663\n",
      "Epoch: [9897/10000] Training loss: 8.545824724040154e-06 / Validation loss: 0.00012889601888377517 / Long term Validation loss: 0.1663\n",
      "Epoch: [9898/10000] Training loss: 8.54466555292936e-06 / Validation loss: 0.00012888392170799497 / Long term Validation loss: 0.1663\n",
      "Epoch: [9899/10000] Training loss: 8.543506137875703e-06 / Validation loss: 0.0001288754004015371 / Long term Validation loss: 0.1663\n",
      "Epoch: [9900/10000] Training loss: 8.542346470496574e-06 / Validation loss: 0.00012886330195007405 / Long term Validation loss: 0.1663\n",
      "Epoch: [9901/10000] Training loss: 8.541186550192892e-06 / Validation loss: 0.00012885388048909668 / Long term Validation loss: 0.1663\n",
      "Epoch: [9902/10000] Training loss: 8.540026384203627e-06 / Validation loss: 0.00012884321320530504 / Long term Validation loss: 0.1663\n",
      "Epoch: [9903/10000] Training loss: 8.538865977264422e-06 / Validation loss: 0.00012883216824926397 / Long term Validation loss: 0.1663\n",
      "Epoch: [9904/10000] Training loss: 8.5377053282729e-06 / Validation loss: 0.00012882283575513806 / Long term Validation loss: 0.1663\n",
      "Epoch: [9905/10000] Training loss: 8.536544431478723e-06 / Validation loss: 0.000128810950785186 / Long term Validation loss: 0.1663\n",
      "Epoch: [9906/10000] Training loss: 8.535383280872834e-06 / Validation loss: 0.00012880173333045268 / Long term Validation loss: 0.1663\n",
      "Epoch: [9907/10000] Training loss: 8.534221875791355e-06 / Validation loss: 0.0001287903254922311 / Long term Validation loss: 0.1663\n",
      "Epoch: [9908/10000] Training loss: 8.533060218206031e-06 / Validation loss: 0.00012878013655574425 / Long term Validation loss: 0.1663\n",
      "Epoch: [9909/10000] Training loss: 8.531898311245028e-06 / Validation loss: 0.0001287698540602103 / Long term Validation loss: 0.1663\n",
      "Epoch: [9910/10000] Training loss: 8.53073615483895e-06 / Validation loss: 0.0001287586050101598 / Long term Validation loss: 0.1663\n",
      "Epoch: [9911/10000] Training loss: 8.529573746124164e-06 / Validation loss: 0.00012874903942220982 / Long term Validation loss: 0.1663\n",
      "Epoch: [9912/10000] Training loss: 8.528411082028471e-06 / Validation loss: 0.00012873750026271325 / Long term Validation loss: 0.1663\n",
      "Epoch: [9913/10000] Training loss: 8.527248160387547e-06 / Validation loss: 0.00012872773942703824 / Long term Validation loss: 0.1663\n",
      "Epoch: [9914/10000] Training loss: 8.526084981990753e-06 / Validation loss: 0.00012871674612776061 / Long term Validation loss: 0.1663\n",
      "Epoch: [9915/10000] Training loss: 8.524921547743645e-06 / Validation loss: 0.00012870620493854776 / Long term Validation loss: 0.1663\n",
      "Epoch: [9916/10000] Training loss: 8.523757858272509e-06 / Validation loss: 0.0001286959938102256 / Long term Validation loss: 0.1663\n",
      "Epoch: [9917/10000] Training loss: 8.522593912554018e-06 / Validation loss: 0.0001286848019076464 / Long term Validation loss: 0.1663\n",
      "Epoch: [9918/10000] Training loss: 8.521429708555616e-06 / Validation loss: 0.00012867495663985106 / Long term Validation loss: 0.1663\n",
      "Epoch: [9919/10000] Training loss: 8.52026524495443e-06 / Validation loss: 0.00012866370704449822 / Long term Validation loss: 0.1663\n",
      "Epoch: [9920/10000] Training loss: 8.519100520841762e-06 / Validation loss: 0.00012865361280077193 / Long term Validation loss: 0.1663\n",
      "Epoch: [9921/10000] Training loss: 8.517935536719213e-06 / Validation loss: 0.00012864281635676977 / Long term Validation loss: 0.1663\n",
      "Epoch: [9922/10000] Training loss: 8.516770292685166e-06 / Validation loss: 0.00012863215957322816 / Long term Validation loss: 0.1663\n",
      "Epoch: [9923/10000] Training loss: 8.5156047885912e-06 / Validation loss: 0.00012862188545935708 / Long term Validation loss: 0.1663\n",
      "Epoch: [9924/10000] Training loss: 8.514439023610133e-06 / Validation loss: 0.00012861081931627482 / Long term Validation loss: 0.1663\n",
      "Epoch: [9925/10000] Training loss: 8.513272996516063e-06 / Validation loss: 0.00012860074076862074 / Long term Validation loss: 0.1663\n",
      "Epoch: [9926/10000] Training loss: 8.512106706684688e-06 / Validation loss: 0.00012858967535328298 / Long term Validation loss: 0.1663\n",
      "Epoch: [9927/10000] Training loss: 8.510940153535582e-06 / Validation loss: 0.00012857938788963852 / Long term Validation loss: 0.1663\n",
      "Epoch: [9928/10000] Training loss: 8.509773337226487e-06 / Validation loss: 0.0001285686446899158 / Long term Validation loss: 0.1663\n",
      "Epoch: [9929/10000] Training loss: 8.508606257568648e-06 / Validation loss: 0.00012855796452033506 / Long term Validation loss: 0.1663\n",
      "Epoch: [9930/10000] Training loss: 8.507438914311336e-06 / Validation loss: 0.00012854757153400362 / Long term Validation loss: 0.1663\n",
      "Epoch: [9931/10000] Training loss: 8.50627130692258e-06 / Validation loss: 0.0001285366126890647 / Long term Validation loss: 0.1663\n",
      "Epoch: [9932/10000] Training loss: 8.505103434663974e-06 / Validation loss: 0.00012852634862428342 / Long term Validation loss: 0.1663\n",
      "Epoch: [9933/10000] Training loss: 8.50393529716947e-06 / Validation loss: 0.00012851538159736932 / Long term Validation loss: 0.1663\n",
      "Epoch: [9934/10000] Training loss: 8.502766893980422e-06 / Validation loss: 0.00012850497832206305 / Long term Validation loss: 0.1663\n",
      "Epoch: [9935/10000] Training loss: 8.501598225090047e-06 / Validation loss: 0.00012849421828209332 / Long term Validation loss: 0.1663\n",
      "Epoch: [9936/10000] Training loss: 8.500429290259024e-06 / Validation loss: 0.0001284835468566907 / Long term Validation loss: 0.1663\n",
      "Epoch: [9937/10000] Training loss: 8.499260089305391e-06 / Validation loss: 0.0001284730268832499 / Long term Validation loss: 0.1663\n",
      "Epoch: [9938/10000] Training loss: 8.49809062190229e-06 / Validation loss: 0.00012846214939426338 / Long term Validation loss: 0.1664\n",
      "Epoch: [9939/10000] Training loss: 8.496920887610065e-06 / Validation loss: 0.00012845173916207993 / Long term Validation loss: 0.1664\n",
      "Epoch: [9940/10000] Training loss: 8.495750886190124e-06 / Validation loss: 0.0001284408274400954 / Long term Validation loss: 0.1664\n",
      "Epoch: [9941/10000] Training loss: 8.494580617288962e-06 / Validation loss: 0.00012843035005405683 / Long term Validation loss: 0.1664\n",
      "Epoch: [9942/10000] Training loss: 8.493410080853726e-06 / Validation loss: 0.00012841955641326004 / Long term Validation loss: 0.1664\n",
      "Epoch: [9943/10000] Training loss: 8.492239276672451e-06 / Validation loss: 0.00012840890680331817 / Long term Validation loss: 0.1664\n",
      "Epoch: [9944/10000] Training loss: 8.491068204651403e-06 / Validation loss: 0.00012839827785573194 / Long term Validation loss: 0.1664\n",
      "Epoch: [9945/10000] Training loss: 8.489896864595314e-06 / Validation loss: 0.00012838746995779066 / Long term Validation loss: 0.1664\n",
      "Epoch: [9946/10000] Training loss: 8.488725256271594e-06 / Validation loss: 0.00012837694329427419 / Long term Validation loss: 0.1664\n",
      "Epoch: [9947/10000] Training loss: 8.487553379535082e-06 / Validation loss: 0.00012836607553317779 / Long term Validation loss: 0.1664\n",
      "Epoch: [9948/10000] Training loss: 8.486381234149653e-06 / Validation loss: 0.0001283555397311746 / Long term Validation loss: 0.1664\n",
      "Epoch: [9949/10000] Training loss: 8.485208820074626e-06 / Validation loss: 0.00012834472003579796 / Long term Validation loss: 0.1664\n",
      "Epoch: [9950/10000] Training loss: 8.484036137154408e-06 / Validation loss: 0.00012833408787942007 / Long term Validation loss: 0.1664\n",
      "Epoch: [9951/10000] Training loss: 8.4828631853709e-06 / Validation loss: 0.00012832337185714264 / Long term Validation loss: 0.1664\n",
      "Epoch: [9952/10000] Training loss: 8.48168996461917e-06 / Validation loss: 0.00012831262280438765 / Long term Validation loss: 0.1664\n",
      "Epoch: [9953/10000] Training loss: 8.480516474821412e-06 / Validation loss: 0.00012830199596291058 / Long term Validation loss: 0.1664\n",
      "Epoch: [9954/10000] Training loss: 8.479342715914119e-06 / Validation loss: 0.00012829117237178142 / Long term Validation loss: 0.1664\n",
      "Epoch: [9955/10000] Training loss: 8.478168687788102e-06 / Validation loss: 0.00012828057392552173 / Long term Validation loss: 0.1664\n",
      "Epoch: [9956/10000] Training loss: 8.47699439044296e-06 / Validation loss: 0.00012826974472492848 / Long term Validation loss: 0.1664\n",
      "Epoch: [9957/10000] Training loss: 8.475819823799216e-06 / Validation loss: 0.00012825910915916782 / Long term Validation loss: 0.1664\n",
      "Epoch: [9958/10000] Training loss: 8.474644987899621e-06 / Validation loss: 0.00012824832858714325 / Long term Validation loss: 0.1664\n",
      "Epoch: [9959/10000] Training loss: 8.473469882711401e-06 / Validation loss: 0.00012823761920214243 / Long term Validation loss: 0.1664\n",
      "Epoch: [9960/10000] Training loss: 8.472294508271699e-06 / Validation loss: 0.00012822690336500785 / Long term Validation loss: 0.1664\n",
      "Epoch: [9961/10000] Training loss: 8.471118864589194e-06 / Validation loss: 0.0001282161235227194 / Long term Validation loss: 0.1664\n",
      "Epoch: [9962/10000] Training loss: 8.469942951678994e-06 / Validation loss: 0.0001282054517458063 / Long term Validation loss: 0.1664\n",
      "Epoch: [9963/10000] Training loss: 8.468766769594278e-06 / Validation loss: 0.00012819463425550512 / Long term Validation loss: 0.1664\n",
      "Epoch: [9964/10000] Training loss: 8.467590318350412e-06 / Validation loss: 0.0001281839675195702 / Long term Validation loss: 0.1664\n",
      "Epoch: [9965/10000] Training loss: 8.466413598043211e-06 / Validation loss: 0.00012817315281534545 / Long term Validation loss: 0.1664\n",
      "Epoch: [9966/10000] Training loss: 8.465236608712985e-06 / Validation loss: 0.000128162455669086 / Long term Validation loss: 0.1664\n",
      "Epoch: [9967/10000] Training loss: 8.464059350478874e-06 / Validation loss: 0.0001281516719057453 / Long term Validation loss: 0.1664\n",
      "Epoch: [9968/10000] Training loss: 8.462881823415624e-06 / Validation loss: 0.00012814092703753788 / Long term Validation loss: 0.1664\n",
      "Epoch: [9969/10000] Training loss: 8.461704027644687e-06 / Validation loss: 0.00012813018080720323 / Long term Validation loss: 0.1664\n",
      "Epoch: [9970/10000] Training loss: 8.46052596327729e-06 / Validation loss: 0.00012811939214616376 / Long term Validation loss: 0.1664\n",
      "Epoch: [9971/10000] Training loss: 8.45934763043249e-06 / Validation loss: 0.0001281086710521328 / Long term Validation loss: 0.1664\n",
      "Epoch: [9972/10000] Training loss: 8.458169029258263e-06 / Validation loss: 0.0001280978572890312 / Long term Validation loss: 0.1664\n",
      "Epoch: [9973/10000] Training loss: 8.456990159881053e-06 / Validation loss: 0.0001280871397426586 / Long term Validation loss: 0.1664\n",
      "Epoch: [9974/10000] Training loss: 8.455811022480766e-06 / Validation loss: 0.00012807632346638665 / Long term Validation loss: 0.1664\n",
      "Epoch: [9975/10000] Training loss: 8.454631617203149e-06 / Validation loss: 0.00012806558938309605 / Long term Validation loss: 0.1664\n",
      "Epoch: [9976/10000] Training loss: 8.453451944249423e-06 / Validation loss: 0.00012805478740655175 / Long term Validation loss: 0.1664\n",
      "Epoch: [9977/10000] Training loss: 8.452272003792016e-06 / Validation loss: 0.00012804402532164583 / Long term Validation loss: 0.1664\n",
      "Epoch: [9978/10000] Training loss: 8.451091796043217e-06 / Validation loss: 0.00012803324381530773 / Long term Validation loss: 0.1664\n",
      "Epoch: [9979/10000] Training loss: 8.44991132120537e-06 / Validation loss: 0.00012802245291934067 / Long term Validation loss: 0.1664\n",
      "Epoch: [9980/10000] Training loss: 8.448730579497875e-06 / Validation loss: 0.00012801168793046585 / Long term Validation loss: 0.1664\n",
      "Epoch: [9981/10000] Training loss: 8.447549571153336e-06 / Validation loss: 0.00012800087581268596 / Long term Validation loss: 0.1664\n",
      "Epoch: [9982/10000] Training loss: 8.446368296400813e-06 / Validation loss: 0.00012799011729118303 / Long term Validation loss: 0.1664\n",
      "Epoch: [9983/10000] Training loss: 8.445186755500609e-06 / Validation loss: 0.00012797929535241515 / Long term Validation loss: 0.1664\n",
      "Epoch: [9984/10000] Training loss: 8.444004948696816e-06 / Validation loss: 0.00012796853203532284 / Long term Validation loss: 0.1664\n",
      "Epoch: [9985/10000] Training loss: 8.442822876272338e-06 / Validation loss: 0.0001279577107957917 / Long term Validation loss: 0.1664\n",
      "Epoch: [9986/10000] Training loss: 8.441640538491266e-06 / Validation loss: 0.00012794693401088378 / Long term Validation loss: 0.1664\n",
      "Epoch: [9987/10000] Training loss: 8.44045793565351e-06 / Validation loss: 0.000127936120019992 / Long term Validation loss: 0.1664\n",
      "Epoch: [9988/10000] Training loss: 8.43927506804652e-06 / Validation loss: 0.00012792532561342376 / Long term Validation loss: 0.1664\n",
      "Epoch: [9989/10000] Training loss: 8.438091935983438e-06 / Validation loss: 0.00012791452057990532 / Long term Validation loss: 0.1664\n",
      "Epoch: [9990/10000] Training loss: 8.436908539776581e-06 / Validation loss: 0.00012790370899588407 / Long term Validation loss: 0.1664\n",
      "Epoch: [9991/10000] Training loss: 8.435724879751298e-06 / Validation loss: 0.00012789291067755718 / Long term Validation loss: 0.1664\n",
      "Epoch: [9992/10000] Training loss: 8.434540956244459e-06 / Validation loss: 0.00012788208567297764 / Long term Validation loss: 0.1664\n",
      "Epoch: [9993/10000] Training loss: 8.433356769594741e-06 / Validation loss: 0.00012787128958214133 / Long term Validation loss: 0.1664\n",
      "Epoch: [9994/10000] Training loss: 8.432172320161861e-06 / Validation loss: 0.00012786045632702428 / Long term Validation loss: 0.1665\n",
      "Epoch: [9995/10000] Training loss: 8.430987608299873e-06 / Validation loss: 0.00012784965746475538 / Long term Validation loss: 0.1665\n",
      "Epoch: [9996/10000] Training loss: 8.42980263438878e-06 / Validation loss: 0.0001278388208166495 / Long term Validation loss: 0.1665\n",
      "Epoch: [9997/10000] Training loss: 8.428617398800241e-06 / Validation loss: 0.00012782801501605057 / Long term Validation loss: 0.1665\n",
      "Epoch: [9998/10000] Training loss: 8.427431901932009e-06 / Validation loss: 0.00012781717846485762 / Long term Validation loss: 0.1665\n",
      "Epoch: [9999/10000] Training loss: 8.426246144175062e-06 / Validation loss: 0.00012780636315910273 / Long term Validation loss: 0.1665\n",
      "Epoch: [10000/10000] Training loss: 8.425060125943029e-06 / Validation loss: 0.00012779552848994865 / Long term Validation loss: 0.1665\n"
     ]
    }
   ],
   "source": [
    "# # # Train LSTM \n",
    "input_size=len(features)\n",
    "hidden_size=4\n",
    "num_layers=1\n",
    "in_seq_len=input_window\n",
    "out_seq_len=output_window\n",
    "output_size=len(features)\n",
    "num_epochs=10000\n",
    "learning_rate= 0.001\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model_LSTM = LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, in_seq_len=in_seq_len, out_seq_len=out_seq_len, output_size=output_size, device=DEVICE).to(DEVICE).double()\n",
    "\n",
    "train_history, val_history, lt_history, r_history = train_MLP(model_LSTM, train_dataloader=train_dataloader, val_dataloader=val_dataloader, val_lt_dataloader=val_lt_dataloader, num_epochs=num_epochs, learning_rate=learning_rate)\n",
    "# train_history, val_history, lt_history, r_history = train_MLP(model_LSTM, train_dataloader=train_dataloader, val_dataloader=val_dataloader, val_lt_dataloader=val_lt_dataloader, num_epochs=num_epochs, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # useful resources: https://www.youtube.com/watch?v=8A6TEjG2DNw (LSTM Time Series Prediction Tutorial using PyTorch in Python | Coronavirus Daily Cases Forecasting)\n",
    "# # LSTM model \n",
    "# class LSTM_bi(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, in_seq_len, out_seq_len, output_size, device):\n",
    "#         super(LSTM_bi, self).__init__()\n",
    "#         '''\n",
    "#         nn.lstm: \n",
    "#         input_size = number of features in input, if batch_first = TRUE (batchSize, seq_len, Hin) FEATURES\n",
    "#         hidden_size = number of features in hidden state\n",
    "#         num_layers\n",
    "#         batch_first = if TRUE (batch, seq, Hin) Hin may mean input features\n",
    "#         h_0 = (D * num_layers, batchSize, Hout)\n",
    "#         c_0 = (D * num_layers, batchSize, Hcell)\n",
    "\n",
    "#         output = (batchSize, seq_len, D * Hout) Hout may mean output features, if batch first = TRUE \n",
    "#         h_0 = (D*num_layers, N, Hout) final state for each element in the batch \n",
    "#         C_0 = (D*num_layers, N, Hcell) final cell state for each element \n",
    "\n",
    "#         nn.linear:\n",
    "#         input_size = (N,*, Hin) Hin = input_features (equal to hidden size therefore is arbitrarily set)\n",
    "#         output_size = (N, *, Hout) Hout = output features (equal to output_size passed in to the model) \n",
    "\n",
    "#         '''\n",
    "#         # Pytorch documentation: \n",
    "#         # >>> rnn = nn.LSTM(10, 20, 2) features, hidden_size, number of layers \n",
    "#         # >>> input = torch.randn(5, 3, 10) in_seq_len, batch_size, hidden_size (batch size should not be input to the mode, inferred from the shape of the input and will be the first number if batch_first = TRUE)\n",
    "#         # >>> h0 = torch.randn(2, 3, 20)\n",
    "#         # >>> c0 = torch.randn(2, 3, 20)\n",
    "#         # >>> output, (hn, cn) = rnn(input, (h0, c0))\n",
    "\n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.in_seq_len = in_seq_len\n",
    "#         self.output_size = output_size\n",
    "#         self.out_seq_len = out_seq_len\n",
    "#         self.device = device\n",
    "        \n",
    "#         # nn.LSTM(features, hidden_size, number of layers)\n",
    "#         self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first = True, bidirectional=True)\n",
    "\n",
    "#         #nn.fc1\n",
    "#         self.fc1 = nn.Linear(in_features=self.hidden_size*2, out_features=(self.output_size * self.out_seq_len))\n",
    "#         # self.fc1 = nn.Linear(in_features=self.hidden_size, out_features=(self.output_size * self.out_seq_len))\n",
    "\n",
    "\n",
    "#     def forward(self, input_data):\n",
    "\n",
    "#         # initialise hidden and cell states after passing through each batch (this is skipped in stateless model)\n",
    "#         h_0 = torch.zeros(self.num_layers*2, input_data.shape[0], self.hidden_size).to(self.device) # input_data.shape[0] is equal to batch size\n",
    "#         c_0 = torch.zeros(self.num_layers*2, input_data.shape[0], self.hidden_size).to(self.device)\n",
    "\n",
    "#         #propagate through LSTM\n",
    "#         lstm_out, (h_out, c_out) = self.lstm(input_data, (h_0, c_0))\n",
    "#         # lsmt_out.shape = (batch_size,seq_length, hidden_size)\n",
    "#         # print('lstm_out[-1][-1]')\n",
    "#         # print(lstm_out[-1][-1])\n",
    "\n",
    "#         # print('h_out[-1][-1]')\n",
    "#         # print(h_out[-1][-1])\n",
    "#         # if lstm_out[-1][-1] == h_out[-1][-1]:\n",
    "#         # #     print(lstm_out[-1][-1])\n",
    "#         # concat_lstm_out = torch.cat((h_out[0,:,:], h_out[-1,:,:]), dim = 1)\n",
    "#         # print(f'concat_lstm_out {concat_lstm_out.shape}')\n",
    "#         # print(f'lsmt_out: {lstm_out[:,-1,:].shape}')\n",
    "#         # print(f'h_out: {h_out[-1].shape}')\n",
    "#         # print(f'h_out: {h_out.shape}')\n",
    "\n",
    "#         # propagate through linear layer \n",
    "#         fc1_out = self.fc1(lstm_out[:,-1,:])\n",
    "#         # fc1_out = self.fc1(lstm_out)\n",
    "#         # print(f'preds.shape (before reshaping): {fc1_out.shape}')\n",
    "        \n",
    "#         preds = fc1_out.reshape(input_data.shape[0], self.out_seq_len, self.output_size)\n",
    "#         # print(f'preds.shape (after reshaping): {preds.shape}')\n",
    "        \n",
    "#         return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, in_seq_len, out_seq_len, device):\n",
    "        super(MLP, self).__init__()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.device = device \n",
    "        self.layers = 3\n",
    "        self.num_units = 200\n",
    "        \n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "\n",
    "        # self.MLP = nn.Sequential(\n",
    "        #     nn.Linear(in_features=self.input_size*self.in_seq_len , out_features=200),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(200, 200),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(200, 200),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(200, self.output_size*self.out_seq_len),\n",
    "        # )\n",
    "\n",
    "        linear_layers_lst = [] #linear layer list\n",
    " \n",
    "        for i in range(self.layers):\n",
    "            linear_layers_lst.append(nn.Linear(self.num_units, self.num_units))\n",
    "            linear_layers_lst.append(nn.ReLU())\n",
    "\n",
    "       \n",
    "        linear_layers = tuple((linear_layers_lst))\n",
    "\n",
    "        self.MLP = nn.Sequential(nn.Linear(in_features=self.input_size*self.in_seq_len, out_features=self.num_units), \n",
    "                                nn.ReLU(), \n",
    "                                *linear_layers, \n",
    "                                nn.Linear(self.num_units, self.output_size*self.out_seq_len))\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f'shape of x: {x.shape}')\n",
    "        out = self.flatten(x)\n",
    "        # x is in the shape [batch_size, in_seq_len, features], but should be reshaped to [batch_size, in_seq_len*features]\n",
    "        # print(f'shape of flatten: {out.shape}')\n",
    "        out = self.MLP(out)\n",
    "        # print(f'shape of MLP: {out.shape}')\n",
    "        out = self.tanh(out)\n",
    "        preds = out.reshape(x.shape[0],  self.out_seq_len,self.output_size)\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP with t2v\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, in_seq_len, out_seq_len, device):\n",
    "        super(MLP, self).__init__()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.device = device \n",
    "        self.layers = 3\n",
    "        self.num_units = 200\n",
    "        \n",
    "        self.t2v = Time2Vec(input_dim=6, embed_dim=600, act_function=torch.sin)\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "\n",
    "        # self.MLP = nn.Sequential(\n",
    "        #     nn.Linear(in_features=self.input_size*self.in_seq_len , out_features=200),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(200, 200),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(200, 200),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(200, self.output_size*self.out_seq_len),\n",
    "        # )\n",
    "\n",
    "        linear_layers_lst = [] #linear layer list\n",
    " \n",
    "        for i in range(self.layers):\n",
    "            linear_layers_lst.append(nn.Linear(self.num_units, self.num_units))\n",
    "            linear_layers_lst.append(nn.ReLU())\n",
    "\n",
    "       \n",
    "        linear_layers = tuple((linear_layers_lst))\n",
    "\n",
    "        self.MLP = nn.Sequential(nn.Linear(in_features=6*self.in_seq_len, out_features=self.num_units), \n",
    "                                nn.ReLU(), \n",
    "                                *linear_layers, \n",
    "                                nn.Linear(self.num_units, self.output_size*self.out_seq_len))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f'shape of x: {x.shape}')\n",
    "        affine_x = self.t2v(x)\n",
    "        # print(f'shape of affine_x: {affine_x.shape}')\n",
    "\n",
    "        out = self.flatten(x)\n",
    "        # x is in the shape [batch_size, in_seq_len, features], but should be reshaped to [batch_size, in_seq_len*features]\n",
    "        # print(f'shape of flatten: {out.shape}')\n",
    "        out = self.MLP(out)\n",
    "        # print(f'shape of MLP: {out.shape}')\n",
    "        preds = out.reshape(x.shape[0],  self.out_seq_len,self.output_size)\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the MLP model using a loss function and a optimiser\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def train_MLP(model, train_dataloader, val_dataloader, val_lt_dataloader, num_epochs, learning_rate):\n",
    "    loss_function = nn.MSELoss(reduction='mean')\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    # train_loss, val_loss = np.zeros(num_epochs), np.zeros(num_epochs)\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    lt_loss = np.zeros(num_epochs)\n",
    "    r_val = np.zeros(num_epochs)\n",
    "\n",
    "\n",
    "    min_val_loss = 1.\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Loop over batch values \n",
    "        runningLoss_train = 0. \n",
    "\n",
    "        for idx, (batch_inputs, batch_targets) in enumerate(train_dataloader):\n",
    "            \n",
    "            # Save batch on GPU\n",
    "            batch_inputs, batch_targets = batch_inputs.to(DEVICE), batch_targets.to(DEVICE)\n",
    "\n",
    "            # Means we are training the model, so uses techniques such as dropout etc., otherwise model.train(model=False)\n",
    "            model.train() \n",
    "\n",
    "            #set gradients to zero\n",
    "            optimiser.zero_grad()\n",
    "            preds = model(batch_inputs)\n",
    "            # print(f'shape of training predictions: {preds.shape}')\n",
    "        \n",
    "            loss = loss_function(preds, batch_targets)\n",
    "            \n",
    "\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            runningLoss_train += loss.item()\n",
    "\n",
    "        train_loss[epoch] = runningLoss_train / len(train_dataloader)\n",
    "\n",
    "        # print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"Training loss: {runningLoss_train/len(train_dataloader)}\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "\n",
    "        model.eval() # means we are evaluating the model, stops process such as dropout etc. \n",
    "        runningLoss_val = 0.\n",
    "\n",
    "        with torch.no_grad(): # makes sure gradient is not stored \n",
    "            for idx, (batch_inputs, batch_targets) in enumerate(val_dataloader):\n",
    "                \n",
    "                batch_inputs, batch_targets = batch_inputs.to(DEVICE), batch_targets.to(DEVICE)\n",
    "                optimiser.zero_grad() #WHY?\n",
    "                preds = model(batch_inputs)\n",
    "                # print(f'shape of validation predictions: {preds.shape}')\n",
    "\n",
    "                loss = loss_function(preds, batch_targets)\n",
    "                runningLoss_val += loss.item()\n",
    "\n",
    "        val_loss[epoch] = runningLoss_val/len(val_dataloader)\n",
    "\n",
    "        # print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"Validation loss: {runningLoss_val/len(val_dataloader)}\")\n",
    "\n",
    "        #Measure long term error \n",
    "        lt_loss_criterion = nn.MSELoss() #long term loss criterion (l1 loss measures MAE for 99 steps in the future)\n",
    "        future_window=200\n",
    "        runningLoss_val_lt = 0.\n",
    "        runningR_val_lt = 0.\n",
    "\n",
    "        with torch.no_grad(): # makes sure gradient is not stored \n",
    "            for idx, (batch_inputs, batch_targets) in enumerate(val_lt_dataloader):\n",
    "\n",
    "                batch_inputs, batch_targets = batch_inputs.to(DEVICE), batch_targets.to(DEVICE)\n",
    "                \n",
    "                future_preds = torch.zeros((batch_inputs.shape[0], 200, len(features))).to(DEVICE) #would store the predicted outputs for 200 timesteps\n",
    "                \n",
    "                # loop to calculate all 200 preds in the future\n",
    "                for t in range(0,future_window):\n",
    "                    optimiser.zero_grad()\n",
    "                    preds = model(batch_inputs)\n",
    "                    # print(f'preds.shape: {preds.shape}')\n",
    "                    # print(f'preds.reshape(-1,1).shape: {preds.squeeze().shape}')\n",
    "                    # print(f'future_preds[:,t,:].shape: {future_preds[:,t,:].shape}')\n",
    "                    future_preds[:,t,:] = preds.reshape(-1,1)\n",
    "                    new_batch_inputs = torch.cat((batch_inputs, preds),1)\n",
    "                    batch_inputs = new_batch_inputs[:,1:,:]\n",
    "                \n",
    "                #Evalute long term predictions\n",
    "                # print(f'future_preds[101:].shape: {future_preds[:,101:,:].shape}')\n",
    "                loss = lt_loss_criterion(future_preds[:,101:,:], batch_targets[:,101:,:])\n",
    "                runningLoss_val_lt += loss.item()\n",
    "\n",
    "                # Spearman's correlation coefficient = covariance(rank(X), rank(Y)) / (stdv(rank(X)) * stdv(rank(Y)))\n",
    "                # all_corr = np.zeros(len(features))\n",
    "\n",
    "                # if output_window == 1:\n",
    "                #     runningR_val_lt = 0\n",
    "                # else:\n",
    "                #     for f in range(len(features)):\n",
    "                #         corr, _ = pearsonr(future_preds[:,101:,f].cpu().detach().numpy().reshape(-1,1).squeeze(), \n",
    "                #                             batch_targets[:,101:,f].cpu().detach().numpy().reshape(-1,1).squeeze())\n",
    "                #         all_corr[f] = corr\n",
    "                    \n",
    "                #     runningR_val_lt += all_corr.mean()\n",
    "                # all_corr  = 0. \n",
    "                \n",
    "                # # print(batch_inputs.shape[0])\n",
    "                # for i in range(batch_inputs.shape[0]):\n",
    "                #         # print(i)\n",
    "                #         corr, _ = pearsonr(future_preds[i,:,0].cpu().detach().numpy().reshape(-1,1).squeeze(), batch_targets[i,:,0].cpu().detach().numpy().reshape(-1,1).squeeze())\n",
    "                #         # print(corr)\n",
    "                #         all_corr += np.abs(corr)\n",
    "                #         # plt.plot(val_targets_np[1,:,f].reshape(-1,1).squeeze())\n",
    "                #         # plt.plot(future_preds_np[1,:,f].reshape(-1,1).squeeze())\n",
    "                #         # val_targets_np[1,:,f].reshape(-1,1).squeeze().shape\n",
    "\n",
    "                # runningR_val_lt += all_corr/batch_inputs.shape[0]\n",
    "\n",
    "                # print(all_corr)\n",
    "        lt_loss[epoch] = runningLoss_val_lt/len(val_lt_dataloader)\n",
    "        # r_val[epoch] = runningR_val_lt / (len(val_lt_dataloader))\n",
    "\n",
    "\n",
    "        # print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"Long term Validation loss: {runningLoss_val_lt/len(val_lt_dataloader)}\")\n",
    "        # print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"R value: {runningR_val_lt/ len(val_lt_dataloader)}\")\n",
    "\n",
    "        if val_loss[epoch] < min_val_loss:\n",
    "\n",
    "            # print('YAY, new best value')\n",
    "            min_val_loss = val_loss[epoch]\n",
    "\n",
    "            torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimiser_state_dict': optimiser.state_dict(),\n",
    "                        'loss': val_loss[epoch],\n",
    "                        }, MODEL_PATH)\n",
    "        \n",
    "        print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", \n",
    "                        f\"Training loss: {runningLoss_train/len(train_dataloader)}\",\n",
    "                        f\"/ Validation loss: {runningLoss_val/len(val_dataloader)}\",\n",
    "                        f\"/ Long term Validation loss: {runningLoss_val_lt/len(val_lt_dataloader):.4f}\",\n",
    "                        # f\"/ R value: {runningR_val_lt/ len(val_lt_dataloader):.4f}\"\n",
    "                        )\n",
    "\n",
    "    return train_loss, val_loss, lt_loss, r_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training the MLP model using a loss function and a optimiser\n",
    "# from scipy.stats import pearsonr\n",
    "\n",
    "# def train_MLP(model, train_dataloader, val_dataloader, val_lt_dataloader, num_epochs, learning_rate):\n",
    "#     loss_function = nn.MSELoss(reduction='mean')\n",
    "#     optimiser = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "#     # train_loss, val_loss = np.zeros(num_epochs), np.zeros(num_epochs)\n",
    "#     train_loss = np.zeros(num_epochs)\n",
    "#     val_loss = np.zeros(num_epochs)\n",
    "#     lt_loss = np.zeros(num_epochs)\n",
    "#     r_val = np.zeros(num_epochs)\n",
    "\n",
    "\n",
    "#     min_val_loss = 1.\n",
    "\n",
    "#     for epoch in tqdm(range(num_epochs)):\n",
    "#         # Loop over batch values \n",
    "#         runningLoss_train = 0. \n",
    "\n",
    "#         for idx, (batch_inputs, batch_targets) in enumerate(train_dataloader):\n",
    "            \n",
    "#             # Save batch on GPU\n",
    "#             batch_inputs, batch_targets = batch_inputs.to(DEVICE), batch_targets.to(DEVICE)\n",
    "\n",
    "#             # Means we are training the model, so uses techniques such as dropout etc., otherwise model.train(model=False)\n",
    "#             model.train() \n",
    "\n",
    "#             #set gradients to zero\n",
    "#             optimiser.zero_grad()\n",
    "#             preds = model(batch_inputs)\n",
    "#             # print(f'shape of training predictions: {preds.shape}')\n",
    "        \n",
    "#             loss = loss_function(preds, batch_targets)\n",
    "            \n",
    "\n",
    "#             loss.backward()\n",
    "#             optimiser.step()\n",
    "#             runningLoss_train += loss.item()\n",
    "\n",
    "#         train_loss[epoch] = runningLoss_train / len(train_dataloader)\n",
    "\n",
    "#         # print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"Training loss: {runningLoss_train/len(train_dataloader)}\")\n",
    "\n",
    "#         # Evaluate on validation set\n",
    "\n",
    "#         model.eval() # means we are evaluating the model, stops process such as dropout etc. \n",
    "#         runningLoss_val = 0.\n",
    "\n",
    "#         with torch.no_grad(): # makes sure gradient is not stored \n",
    "#             for idx, (batch_inputs, batch_targets) in enumerate(val_dataloader):\n",
    "                \n",
    "#                 batch_inputs, batch_targets = batch_inputs.to(DEVICE), batch_targets.to(DEVICE)\n",
    "#                 optimiser.zero_grad() #WHY?\n",
    "#                 preds = model(batch_inputs)\n",
    "#                 # print(f'shape of validation predictions: {preds.shape}')\n",
    "\n",
    "#                 loss = loss_function(preds, batch_targets)\n",
    "#                 runningLoss_val += loss.item()\n",
    "\n",
    "#         val_loss[epoch] = runningLoss_val/len(val_dataloader)\n",
    "\n",
    "#         # print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"Validation loss: {runningLoss_val/len(val_dataloader)}\")\n",
    "\n",
    "#         #Measure long term error \n",
    "#         lt_loss_criterion = nn.MSELoss() #long term loss criterion (l1 loss measures MAE for 99 steps in the future)\n",
    "#         future_window=200\n",
    "#         runningLoss_val_lt = 0.\n",
    "#         runningR_val_lt = 0.\n",
    "\n",
    "#         with torch.no_grad(): # makes sure gradient is not stored \n",
    "#             for idx, (batch_inputs, batch_targets) in enumerate(val_lt_dataloader):\n",
    "\n",
    "#                 batch_inputs, batch_targets = batch_inputs.to(DEVICE), batch_targets.to(DEVICE)\n",
    "                \n",
    "#                 future_preds = torch.zeros((batch_inputs.shape[0], 200, len(features))).to(DEVICE) #would store the predicted outputs for 200 timesteps\n",
    "                \n",
    "#                 # loop to calculate all 200 preds in the future\n",
    "#                 for t in range(0,future_window):\n",
    "#                     optimiser.zero_grad()\n",
    "#                     preds = model(batch_inputs)\n",
    "#                     # print(f'preds.shape: {preds.shape}')\n",
    "#                     # print(f'preds.reshape(-1,1).shape: {preds.squeeze().shape}')\n",
    "#                     # print(f'future_preds[:,t,:].shape: {future_preds[:,t,:].shape}')\n",
    "#                     future_preds[:,t,:] = preds.reshape(-1,1)\n",
    "#                     new_batch_inputs = torch.cat((batch_inputs, preds),1)\n",
    "#                     batch_inputs = new_batch_inputs[:,1:,:]\n",
    "                \n",
    "#                 #Evalute long term predictions\n",
    "#                 # print(f'future_preds[101:].shape: {future_preds[:,101:,:].shape}')\n",
    "#                 loss = lt_loss_criterion(future_preds[:,101:,:], batch_targets[:,101:,:])\n",
    "#                 runningLoss_val_lt += loss.item()\n",
    "\n",
    "#                 # Spearman's correlation coefficient = covariance(rank(X), rank(Y)) / (stdv(rank(X)) * stdv(rank(Y)))\n",
    "#                 all_corr = np.zeros(len(features))\n",
    "\n",
    "#                 if output_window == 1:\n",
    "#                     runningR_val_lt = 0\n",
    "#                 else:\n",
    "#                     for f in range(len(features)):\n",
    "#                         corr, _ = pearsonr(future_preds[:,101:,f].cpu().detach().numpy().reshape(-1,1).squeeze(), \n",
    "#                                             batch_targets[:,101:,f].cpu().detach().numpy().reshape(-1,1).squeeze())\n",
    "#                         all_corr[f] = corr\n",
    "                    \n",
    "#                     runningR_val_lt += all_corr.mean()\n",
    "\n",
    "#                 # print(all_corr)\n",
    "#         lt_loss[epoch] = runningLoss_val_lt/len(val_lt_dataloader)\n",
    "#         r_val[epoch] = runningR_val_lt / (len(val_lt_dataloader))\n",
    "\n",
    "\n",
    "#         # print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"Long term Validation loss: {runningLoss_val_lt/len(val_lt_dataloader)}\")\n",
    "#         # print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"R value: {runningR_val_lt/ len(val_lt_dataloader)}\")\n",
    "\n",
    "#         if val_loss[epoch] < min_val_loss:\n",
    "\n",
    "#             # print('YAY, new best value')\n",
    "#             min_val_loss = val_loss[epoch]\n",
    "\n",
    "#             torch.save({\n",
    "#                         'epoch': epoch,\n",
    "#                         'model_state_dict': model.state_dict(),\n",
    "#                         'optimiser_state_dict': optimiser.state_dict(),\n",
    "#                         'loss': val_loss[epoch],\n",
    "#                         }, MODEL_PATH)\n",
    "        \n",
    "#         print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", \n",
    "#                         f\"Training loss: {runningLoss_train/len(train_dataloader):.7f}\",\n",
    "#                         f\"/ Validation loss: {runningLoss_val/len(val_dataloader):.7f}\",\n",
    "#                         f\"/ Long term Validation loss: {runningLoss_val_lt/len(val_lt_dataloader):.4f}\",\n",
    "#                         f\"/ R value: {runningR_val_lt/ len(val_lt_dataloader):.4f}\")\n",
    "\n",
    "#     return train_loss, val_loss, lt_loss, r_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function \n",
    "def test_MLP(model, dataloader):\n",
    "    loss_function = nn.MSELoss(reduction='mean')\n",
    "    model.eval()\n",
    "    actual_output, pred_output = [], []\n",
    "    running_loss = 0. \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (batch_inputs, batch_targets) in tqdm(enumerate(dataloader)):\n",
    "\n",
    "            batch_inputs, batch_targets = batch_inputs.to(DEVICE), batch_targets.to(DEVICE)\n",
    "\n",
    "            # if idx==0:\n",
    "            #     batch_preds = model(batch_inputs)\n",
    "            #     # print(f'batch shape: {batch_preds.shape}')\n",
    "            #     loss = loss_function(batch_preds, batch_targets)\n",
    "            #     running_loss += loss.item()\n",
    "            #     current_preds = batch_preds\n",
    "            #     all_preds = batch_preds\n",
    "\n",
    "            # else:\n",
    "            #     batch_preds = model(batch_inputs)\n",
    "            #     print(f'batch shape: {batch_preds.shape}')\n",
    "            #     loss = loss_function(batch_preds, batch_targets)\n",
    "            #     running_loss += loss.item()\n",
    "            #     all_preds = torch.cat((current_preds, batch_preds), dim=0)\n",
    "            #     current_preds = batch_preds\n",
    "\n",
    "            batch_preds = model(batch_inputs)\n",
    "            loss = loss_function(batch_preds, batch_targets)\n",
    "            running_loss += loss.item()\n",
    "            actual_output.append(batch_targets)\n",
    "            pred_output.append(batch_preds)\n",
    "\n",
    "\n",
    "            #             lst = []\n",
    "            # print(f'{x.size()}')\n",
    "            # for i in range(10):\n",
    "            #     x += i  # say we do something with x at iteration i\n",
    "            #     lst.append(x)\n",
    "            # # lstt = torch.stack([x for _ in range(10)])\n",
    "            # lstt = torch.stack(lst)\n",
    "            # print(lstt.size())\n",
    "\n",
    "        total_loss = running_loss / len(dataloader)\n",
    "\n",
    "        actual_output_tensor = torch.vstack(actual_output)\n",
    "        pred_output_tensor = torch.vstack(pred_output)\n",
    "    \n",
    "    return pred_output_tensor, actual_output_tensor, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model hyperparameters \n",
    "# Train MLP\n",
    "input_size=len(features)\n",
    "in_seq_len=input_window\n",
    "out_seq_len=output_window\n",
    "output_size=len(features)\n",
    "num_epochs=8\n",
    "learning_rate=0.001\n",
    "\n",
    "model_MLP = MLP(input_size=input_size, in_seq_len=in_seq_len, out_seq_len=out_seq_len, output_size=output_size, device=DEVICE).to(DEVICE)\n",
    "\n",
    "train_history, val_history, lt_history, r_history = train_MLP(model_MLP, train_dataloader=train_dataloader, val_dataloader=val_dataloader, val_lt_dataloader= val_lt_dataloader, num_epochs=num_epochs, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, in_seq_len, out_seq_len, device=DEVICE):\n",
    "        super(CNN, self).__init__()\n",
    "        '''\n",
    "        nn.Conv1d\n",
    "        in_channels: number of features in input \n",
    "        out_channels: number of channels in output\n",
    "        kernel_size: size of convolving layer\n",
    "        stride: convolution stride\n",
    "        padding=0\n",
    "        dilation=1\n",
    "        groups=1\n",
    "        bias=True\n",
    "        padding_mode='zeros'\n",
    "        device=device \n",
    "        dtype=None\n",
    "\n",
    "        nn.MaxPool1d\n",
    "        kernel_size:\n",
    "        stride=None: default is kernel_size\n",
    "        padding=0:\n",
    "        dilation=1\n",
    "        return_indices=False\n",
    "        ceil_mode=False\n",
    "\n",
    "        torch.flatten (reshapes to 1D vector)\n",
    "        input\n",
    "        start_dim=0\n",
    "        end_dim=- 1\n",
    "\n",
    "        nn.Linear \n",
    "        input_size = (N,*, Hin) Hin = input_features (equal to hidden size therefore is arbitrarily set)\n",
    "        output_size = (N, *, Hout) Hout = output features (equal to output_size passed in to the model) \n",
    "        '''\n",
    "        # Pytorch documentation: \n",
    "        # >>> m = nn.Conv1d(16, 33, 3, stride=2) features(in_channels), output_channels, kernel_size\n",
    "        # >>> input = torch.randn(20, 16, 50) batch_size, features, length of sequence \n",
    "        # >>> output = m(input~)\n",
    "\n",
    "\n",
    "        # >>> m = nn.MaxPool1d(3, stride=2) kernel_size, stride\n",
    "        # >>> input = torch.randn(20, 16, 50)\n",
    "        # >>> output = m(input)\n",
    "    \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        # self.kernel_size = kernel_size\n",
    "        # self.stride = stride\n",
    "        self.device = device \n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.conv1_channels = 32\n",
    "        self.conv2_channels = 48\n",
    "        self.conv3_channels = 256\n",
    "        self.conv4_channels = 512\n",
    "        self.kernel_12 = 7\n",
    "        self.kernel_34 = 7\n",
    "        self.padding = 4\n",
    "        self.stride = 1\n",
    "        self.dilation = 1\n",
    "\n",
    "        def calc_shape(input, kernel_size, stride, padding=0, dilation =1, operation = 'conv'):\n",
    "            if operation == 'conv':\n",
    "            # size = ((input - kernel_size + (2 * padding))/stride)  + 1 \n",
    "                size = ((input + 2 * padding - dilation * (kernel_size-1) -1)/stride )+ 1\n",
    "\n",
    "            if operation == 'pool':\n",
    "                size = ((input - kernel_size)/stride)  + 1\n",
    "\n",
    "            return size\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels = self.input_size, out_channels = self.conv1_channels, kernel_size = self.kernel_12, stride = self.stride, padding = self.padding, dilation=self.dilation)\n",
    "        post_conv1 = calc_shape(input = self.in_seq_len, kernel_size = self.kernel_12, stride = self.stride, padding = self.padding,  dilation=self.dilation, operation='conv')\n",
    "        # print(f'post conv1: {post_conv1}')\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels = self.conv1_channels, out_channels = self.conv2_channels, kernel_size = self.kernel_12 , stride = self.stride, padding = self.padding,  dilation=self.dilation)\n",
    "        post_conv2 = calc_shape(input = post_conv1, kernel_size =self.kernel_12 , stride = self.stride, padding = self.padding,  dilation=self.dilation,  operation='conv')\n",
    "        # print(f'post conv2: {post_conv2}')\n",
    "\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size = 2, stride = 2)\n",
    "        post_pool1 = calc_shape(input = post_conv2, kernel_size = 2, stride = 2,  operation='pool')\n",
    "        # print(f'post poo1: {post_pool1}')\n",
    "\n",
    "        self.conv3 = nn.Conv1d(in_channels=self.conv2_channels, out_channels=self.conv3_channels, kernel_size = self.kernel_34, stride = self.stride, padding = self.padding,  dilation=self.dilation )\n",
    "        post_conv3 = calc_shape(input = post_pool1, kernel_size = self.kernel_34, stride = self.stride, padding = self.padding, dilation=self.dilation, operation='conv')\n",
    "        # print(f'post conv3: {post_conv3}')\n",
    "\n",
    "\n",
    "        self.conv4 = nn.Conv1d(in_channels=self.conv3_channels, out_channels=self.conv4_channels, kernel_size = self.kernel_34, stride = self.stride, padding = self.padding,  dilation=self.dilation)\n",
    "        post_conv4 = calc_shape(input = post_conv3, kernel_size = self.kernel_34, stride = self.stride, padding = self.padding,  dilation=self.dilation, operation='conv')\n",
    "        # print(f'post conv4: {post_conv4}')\n",
    "\n",
    "\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size = 2, stride = 2)\n",
    "        post_pool2 = calc_shape(input = post_conv4, kernel_size = 2, stride = 2,  operation='pool')\n",
    "        # print(f'post pool2: {post_pool2}')\n",
    "\n",
    "        # print(f'linear input: {post_pool2 * 32}')\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features = (int(post_pool2) * self.conv4_channels), out_features=(self.output_size * out_seq_len))\n",
    "        #nn.Linear(in_features=self.hidden_size, out_features=(self.output_size * self.out_seq_len))\n",
    "\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        '''\n",
    "        input_data.shape = \n",
    "        '''\n",
    "        # print(f'input_data.shape = {input_data.shape}') #input_data.shape = torch.Size([512, 2, 120])\n",
    "        input = input_data.permute(0,2,1)\n",
    "        out = self.conv1(input)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = F.relu(out)\n",
    "        # print(f'conv1 out shape = {out.shape}') #conv1_out.shape = torch.Size([512, 12, 120])\n",
    "        out = self.pool1(out)\n",
    "        # print(f'pool1 out shape = {out.shape}')\n",
    "        out = self.conv3(out)\n",
    "        # print(f'conv2 out shape: {out.shape}')\n",
    "        out = F.relu(out)\n",
    "        out = self.conv4(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool2(out)\n",
    "        out = torch.flatten(out, start_dim = 1, end_dim=-1) #do not flatten batches \n",
    "        # print(f'flatten layer out shape: {out.shape}') #([512, 720]) \n",
    "        out = self.fc1(out)\n",
    "        # print(f'fc1 out shape: {out.shape}')\n",
    "        preds = out.reshape(input_data.shape[0],  self.out_seq_len, self.output_size)\n",
    "        # print(f'preds.shape (after reshaping): {preds.shape}')\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, in_seq_len, out_seq_len, device=DEVICE):\n",
    "#         super(CNN, self).__init__()\n",
    "#         '''\n",
    "#         nn.Conv1d\n",
    "#         in_channels: number of features in input \n",
    "#         out_channels: number of channels in output\n",
    "#         kernel_size: size of convolving layer\n",
    "#         stride: convolution stride\n",
    "#         padding=0\n",
    "#         dilation=1\n",
    "#         groups=1\n",
    "#         bias=True\n",
    "#         padding_mode='zeros'\n",
    "#         device=device \n",
    "#         dtype=None\n",
    "\n",
    "#         nn.MaxPool1d\n",
    "#         kernel_size:\n",
    "#         stride=None: default is kernel_size\n",
    "#         padding=0:\n",
    "#         dilation=1\n",
    "#         return_indices=False\n",
    "#         ceil_mode=False\n",
    "\n",
    "#         torch.flatten (reshapes to 1D vector)\n",
    "#         input\n",
    "#         start_dim=0\n",
    "#         end_dim=- 1\n",
    "\n",
    "#         nn.Linear \n",
    "#         input_size = (N,*, Hin) Hin = input_features (equal to hidden size therefore is arbitrarily set)\n",
    "#         output_size = (N, *, Hout) Hout = output features (equal to output_size passed in to the model) \n",
    "#         '''\n",
    "#         # Pytorch documentation: \n",
    "#         # >>> m = nn.Conv1d(16, 33, 3, stride=2) features(in_channels), output_channels, kernel_size\n",
    "#         # >>> input = torch.randn(20, 16, 50) batch_size, features, length of sequence \n",
    "#         # >>> output = m(input~)\n",
    "\n",
    "\n",
    "#         # >>> m = nn.MaxPool1d(3, stride=2) kernel_size, stride\n",
    "#         # >>> input = torch.randn(20, 16, 50)\n",
    "#         # >>> output = m(input)\n",
    "    \n",
    "#         self.input_size = input_size\n",
    "#         self.output_size = output_size\n",
    "#         # self.kernel_size = kernel_size\n",
    "#         # self.stride = stride\n",
    "#         self.device = device \n",
    "#         self.in_seq_len = in_seq_len\n",
    "#         self.out_seq_len = out_seq_len\n",
    "#         self.conv1_channels = 16\n",
    "#         self.conv2_channels = 16\n",
    "#         self.conv3_channels = 16\n",
    "#         self.conv4_channels = 16\n",
    "#         self.kernel_12 = 5\n",
    "#         self.kernel_34 = 5\n",
    "#         self.padding = 4\n",
    "#         self.stride = 1\n",
    "#         self.dilation = 1\n",
    "\n",
    "#         def calc_shape(input, kernel_size, stride, padding=0, dilation =1, operation = 'conv'):\n",
    "#             if operation == 'conv':\n",
    "#             # size = ((input - kernel_size + (2 * padding))/stride)  + 1 \n",
    "#                 size = ((input + 2 * padding - dilation * (kernel_size-1) -1)/stride )+ 1\n",
    "\n",
    "#             if operation == 'pool':\n",
    "#                 size = ((input - kernel_size)/stride)  + 1\n",
    "\n",
    "#             return size\n",
    "\n",
    "#         self.conv1 = nn.Conv1d(in_channels = self.input_size, out_channels = self.conv1_channels, kernel_size = self.kernel_12, stride = self.stride, padding = self.padding, dilation=self.dilation)\n",
    "#         post_conv1 = calc_shape(input = self.in_seq_len, kernel_size = self.kernel_12, stride = self.stride, padding = self.padding,  dilation=self.dilation, operation='conv')\n",
    "#         # print(f'post conv1: {post_conv1}')\n",
    "\n",
    "#         self.conv2 = nn.Conv1d(in_channels = self.conv1_channels, out_channels = self.conv2_channels, kernel_size = self.kernel_12 , stride = self.stride, padding = self.padding,  dilation=self.dilation)\n",
    "#         post_conv2 = calc_shape(input = post_conv1, kernel_size =self.kernel_12 , stride = self.stride, padding = self.padding,  dilation=self.dilation,  operation='conv')\n",
    "#         # print(f'post conv2: {post_conv2}')\n",
    "\n",
    "#         self.pool1 = nn.MaxPool1d(kernel_size = 2, stride = 2)\n",
    "#         post_pool1 = calc_shape(input = post_conv2, kernel_size = 2, stride = 2,  operation='pool')\n",
    "#         # print(f'post poo1: {post_pool1}')\n",
    "\n",
    "#         self.conv3 = nn.Conv1d(in_channels=self.conv2_channels, out_channels=self.conv3_channels, kernel_size = self.kernel_34, stride = self.stride, padding = self.padding,  dilation=self.dilation )\n",
    "#         post_conv3 = calc_shape(input = post_pool1, kernel_size = self.kernel_34, stride = self.stride, padding = self.padding, dilation=self.dilation, operation='conv')\n",
    "#         # print(f'post conv3: {post_conv3}')\n",
    "\n",
    "\n",
    "#         self.conv4 = nn.Conv1d(in_channels=self.conv3_channels, out_channels=self.conv4_channels, kernel_size = self.kernel_34, stride = self.stride, padding = self.padding,  dilation=self.dilation)\n",
    "#         post_conv4 = calc_shape(input = post_conv3, kernel_size = self.kernel_34, stride = self.stride, padding = self.padding,  dilation=self.dilation, operation='conv')\n",
    "#         # print(f'post conv4: {post_conv4}')\n",
    "\n",
    "\n",
    "#         self.pool2 = nn.MaxPool1d(kernel_size = 2, stride = 2)\n",
    "#         post_pool2 = calc_shape(input = post_conv4, kernel_size = 2, stride = 2,  operation='pool')\n",
    "#         # print(f'post pool2: {post_pool2}')\n",
    "\n",
    "#         # print(f'linear input: {post_pool2 * 32}')\n",
    "\n",
    "#         self.fc1 = nn.Linear(in_features = (int(post_pool2) * self.conv4_channels), out_features=(self.output_size * out_seq_len))\n",
    "#         #nn.Linear(in_features=self.hidden_size, out_features=(self.output_size * self.out_seq_len))\n",
    "\n",
    "\n",
    "#     def forward(self, input_data):\n",
    "#         '''\n",
    "#         input_data.shape = \n",
    "#         '''\n",
    "#         # print(f'input_data.shape = {input_data.shape}') #input_data.shape = torch.Size([512, 2, 120])\n",
    "#         out = self.conv1(input_data)\n",
    "#         out = F.relu(out)\n",
    "#         out = self.conv2(out)\n",
    "#         out = F.relu(out)\n",
    "#         # print(f'conv1 out shape = {out.shape}') #conv1_out.shape = torch.Size([512, 12, 120])\n",
    "#         out = self.pool1(out)\n",
    "#         # print(f'pool1 out shape = {out.shape}')\n",
    "#         out = self.conv3(out)\n",
    "#         # print(f'conv2 out shape: {out.shape}')\n",
    "#         out = F.relu(out)\n",
    "#         out = self.conv4(out)\n",
    "#         out = F.relu(out)\n",
    "#         out = self.pool2(out)\n",
    "#         out = torch.flatten(out, start_dim = 1, end_dim=-1) #do not flatten batches \n",
    "#         # print(f'flatten layer out shape: {out.shape}') #([512, 720]) \n",
    "#         out = self.fc1(out)\n",
    "#         # print(f'fc1 out shape: {out.shape}')\n",
    "#         preds = out.reshape(input_data.shape[0], self.output_size, self.out_seq_len)\n",
    "#         # print(f'preds.shape (after reshaping): {preds.shape}')\n",
    "\n",
    "#         return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the CNN model using a loss function and a optimiser\n",
    "def train_CNN(model, train_dataloader, val_dataloader, num_epochs, learning_rate):\n",
    "\n",
    "    loss_function = nn.MSELoss()\n",
    "    \n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    # train_loss, val_loss = np.zeros(num_epochs), np.zeros(num_epochs)\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    lt_loss = np.zeros(num_epochs)\n",
    "\n",
    "\n",
    "    min_val_loss = 1.\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Loop over batch values \n",
    "        runningLoss_train = 0. \n",
    "\n",
    "        for idx, (batch_inputs, batch_targets) in enumerate(train_dataloader):\n",
    "            \n",
    "            # Save batch on GPU\n",
    "            batch_inputs, batch_targets = batch_inputs.to(DEVICE), batch_targets.to(DEVICE)\n",
    "\n",
    "            # Means we are training the model, so uses techniques such as dropout etc., otherwise model.train(model=False)\n",
    "            model.train() \n",
    "\n",
    "            #set gradients to zero\n",
    "            optimiser.zero_grad()\n",
    "            preds = model(batch_inputs.permute(0,2,1))\n",
    "        \n",
    "            loss = loss_function(preds, batch_targets.permute(0,2,1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            runningLoss_train += loss.item()\n",
    "\n",
    "        train_loss[epoch] = runningLoss_train / len(train_dataloader)\n",
    "\n",
    "        print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"Training loss: {runningLoss_train/len(train_dataloader)}\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "\n",
    "        model.eval() # means we are evaluating the model, stops process such as dropout etc. \n",
    "        runningLoss_val = 0.\n",
    "\n",
    "        with torch.no_grad(): # makes sure gradient is not stored \n",
    "            for idx, (batch_inputs, batch_targets) in enumerate(val_dataloader):\n",
    "                \n",
    "                batch_inputs, batch_targets = batch_inputs.to(DEVICE), batch_targets.to(DEVICE)\n",
    "                optimiser.zero_grad() #WHY?\n",
    "                preds = model(batch_inputs.permute(0,2,1)) # permute to make the input shape (samples, features, input_window_size)\n",
    "                # print(f'shape of validation predictions: {preds.shape}')\n",
    "\n",
    "                loss = loss_function(preds, batch_targets.permute(0,2,1)) #permute targets to make it same shape as preds\n",
    "                runningLoss_val += loss.item()\n",
    "\n",
    "        val_loss[epoch] = runningLoss_val/len(val_dataloader)                   \n",
    "\n",
    "        print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"Validation loss: {runningLoss_val/len(val_dataloader)}\")\n",
    "\n",
    "        #Measure long term error \n",
    "        lt_loss_criterion = nn.L1Loss() #long term loss criterion (l1 loss measures MAE for 99 steps in the future)\n",
    "        future_window=200\n",
    "        runningLoss_val_lt = 0.\n",
    "        runningR_val_lt = 0.\n",
    "\n",
    "        with torch.no_grad(): # makes sure gradient is not stored \n",
    "            for idx, (batch_inputs, batch_targets) in enumerate(val_lt_dataloader):\n",
    "\n",
    "                batch_inputs, batch_targets = batch_inputs.to(DEVICE), batch_targets.to(DEVICE)\n",
    "                \n",
    "                future_preds = torch.zeros((batch_inputs.shape[0], 200, 6)).to(DEVICE) #would store the predicted outputs for 200 timesteps\n",
    "                \n",
    "                # loop to calculate all 200 preds in the future\n",
    "                for t in range(0,future_window):\n",
    "                    optimiser.zero_grad()\n",
    "                    preds = model(batch_inputs)\n",
    "                    # print(f'preds.shape: {preds.shape}')\n",
    "                    # print(f'future_preds[:,t,:].shape: {future_preds[:,t,:].shape}')\n",
    "                    future_preds[:,t,:] = preds.squeeze()\n",
    "                    new_batch_inputs = torch.cat((batch_inputs, preds),1)\n",
    "                    batch_inputs = new_batch_inputs[:,1:,:]\n",
    "                \n",
    "                #Evalute long term predictions\n",
    "                # print(f'future_preds[101:].shape: {future_preds[:,101:,:].shape}')\n",
    "                loss = lt_loss_criterion(future_preds[:,101:,:], batch_targets[:,101:,:])\n",
    "                runningLoss_val_lt += loss.item()\n",
    "\n",
    "                for f in range(0,len(features)):\n",
    "                    r_val = torch.corrcoef(future_preds[:,101:,f], batch_targets[:,101:,f])\n",
    "                    runningR_val_lt += r_val[1]\n",
    "\n",
    "        lt_loss[epoch] = runningLoss_val_lt/ len(val_lt_dataloader)\n",
    "        r_val[epoch] = runningR_val_lt / (len(val_lt_dataloader) * len(features))\n",
    "\n",
    "\n",
    "        print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"Long term Validation loss: {runningLoss_val_lt/len(val_lt_dataloader)}\")\n",
    "        print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"R value: {runningR_val_lt/ (len(val_lt_dataloader)*len(features))}\")\n",
    "\n",
    "       # Save best model so far\n",
    "        if val_loss[epoch] < min_val_loss:\n",
    "\n",
    "            # print('YAY, new best value')\n",
    "            min_val_loss = val_loss[epoch]\n",
    "\n",
    "            torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimiser_state_dict': optimiser.state_dict(),\n",
    "                        'loss': val_loss[epoch],\n",
    "                        }, MODEL_PATH)\n",
    "                        \n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing function \n",
    "def test_CNN(model, dataloader):\n",
    "    loss_function = nn.MSELoss(reduction='mean')\n",
    "    model.eval()\n",
    "    actual_output, pred_output = [], []\n",
    "    running_loss = 0. \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (batch_inputs, batch_targets) in tqdm(enumerate(dataloader)):\n",
    "\n",
    "            batch_inputs, batch_targets = batch_inputs.to(DEVICE), batch_targets.to(DEVICE)\n",
    "\n",
    "            batch_preds = model(batch_inputs.permute(0,2,1))\n",
    "            loss = loss_function(batch_preds, batch_targets.permute(0,2,1))\n",
    "            running_loss += loss.item()\n",
    "            actual_output.append(batch_targets.permute(0,2,1))\n",
    "            # print(f'actual_output_shape:{batch_targets.permute(0,2,1).shape}')\n",
    "       \n",
    "            pred_output.append(batch_preds)\n",
    "            # print(f'pred_output shape; {batch_preds.shape}')\n",
    "\n",
    "        total_loss = running_loss / len(dataloader)\n",
    "\n",
    "        actual_output_tensor = torch.vstack(actual_output)\n",
    "        pred_output_tensor = torch.vstack(pred_output)\n",
    "    \n",
    "    return pred_output_tensor.permute(0,2,1), actual_output_tensor.permute(0,2,1), total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model hyperparameters \n",
    "# Train CNN\n",
    "input_size=len(features)\n",
    "in_seq_len=input_window\n",
    "out_seq_len=output_window\n",
    "# kernel_size=1\n",
    "num_epochs=30\n",
    "learning_rate=0.001\n",
    "output_size = len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN = CNN(input_size, output_size=output_size, in_seq_len=in_seq_len, out_seq_len=out_seq_len,  device = DEVICE).to(DEVICE)\n",
    "\n",
    "train_history, val_history, lt_history= train_MLP(model_CNN, train_dataloader=train_dataloader, val_dataloader=val_dataloader, val_lt_dataloader = val_lt_dataloader, num_epochs=num_epochs, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hybrid(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers, stride, padding, in_seq_len, out_seq_len, device=DEVICE):\n",
    "        super(hybrid, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding \n",
    "        self.device = device \n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels = self.input_size, out_channels = 48, kernel_size = 7, stride = self.stride, padding = self.padding)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size = 2, stride = 2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=48, out_channels=32, kernel_size = 5, stride = self.stride, padding = self.padding)\n",
    "        self.fc1 = nn.Linear(in_features = 1376, out_features=(self.output_size * in_seq_len))\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first = True)\n",
    "        self.fc2 = nn.Linear(in_features=self.hidden_size, out_features=(self.output_size * self.out_seq_len))\n",
    "\n",
    "        '''\n",
    "        input shape for CNN: [samples, features, in_seq_len]\n",
    "        output shape for CNN: [samples,out_seq_len, features]\n",
    "\n",
    "        input shape for LSTM: [samples,out_seq_len, features]\n",
    "        output shape for LSTM: [samples,out_seq_len, features]\n",
    "        '''\n",
    "    def forward(self, input_data):\n",
    "     \n",
    "        # print(f'input_data.shape = {input_data.shape}') #input_data.shape = torch.Size([512, 2, 120])\n",
    "        out = self.conv1(input_data)\n",
    "        out = F.relu(out)\n",
    "        # print(f'conv1 out shape = {out.shape}') #conv1_out.shape = torch.Size([512, 12, 120])\n",
    "        out = self.pool1(out)\n",
    "        # print(f'pool1 out shape = {out.shape}')\n",
    "        out = self.conv2(out)\n",
    "        # print(f'conv2 out shape: {out.shape}')\n",
    "        out = F.relu(out)\n",
    "        out = torch.flatten(out, start_dim = 1, end_dim=-1) #do not flatten batches \n",
    "        # print(f'flatten layer out shape: {out.shape}') #([512, 720]) \n",
    "        out = self.fc1(out)\n",
    "        # print(f'fc1 out shape: {out.shape}')\n",
    "        # print(f'reshaped output: {out.reshape(input_data.shape[0], self.in_seq_len, self.output_size).shape}')\n",
    "        \n",
    "        h_0 = torch.zeros(self.num_layers, input_data.shape[0], self.hidden_size).to(self.device) # input_data.shape[0] is equal to batch size\n",
    "        c_0 = torch.zeros(self.num_layers, input_data.shape[0], self.hidden_size).to(self.device)\n",
    "\n",
    "        lstm, (h_out, c_out) = self.lstm(out.reshape(input_data.shape[0], self.in_seq_len, self.output_size), (h_0, c_0))\n",
    "        out = self.fc2(h_out[-1])\n",
    "\n",
    "        preds = out.reshape(input_data.shape[0], self.out_seq_len, self.output_size)\n",
    "\n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the CNN model using a loss function and a optimiser\n",
    "def train_hybrid(model, train_dataloader, val_dataloader, num_epochs, learning_rate):\n",
    "    loss_function = nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    # loss2_fun = nn.MSELoss(reduction=\"mean\")\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    # train_loss, val_loss = np.zeros(num_epochs), np.zeros(num_epochs)\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    lt_loss = np.zeros(num_epochs)\n",
    "\n",
    "    min_val_loss = 1.\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Loop over batch values \n",
    "        runningLoss_train = 0. \n",
    "\n",
    "        for idx, (batch_inputs, batch_targets) in enumerate(train_dataloader):\n",
    "            \n",
    "            # Save batch on GPU\n",
    "            batch_inputs, batch_targets = batch_inputs.to(DEVICE), batch_targets.to(DEVICE)\n",
    "\n",
    "            # Means we are training the model, so uses techniques such as dropout etc., otherwise model.train(model=False)\n",
    "            model.train() \n",
    "\n",
    "            #set gradients to zero\n",
    "            optimiser.zero_grad()\n",
    "            preds = model(batch_inputs.permute(0,2,1))\n",
    "            # print(f'shape of training predictions: {preds.shape}')\n",
    "        \n",
    "            loss = loss_function(preds, batch_targets)\n",
    "            # loss_manual = mse_loss(preds, batch_targets.permute(0,2,1), reduction='mean')\n",
    "            # print(f'preds shape: {preds.shape}')\n",
    "            # print(f'targets shape: {batch_targets.permute(0,2,1).shape}')\n",
    "            # print(f'loss automatic: {loss.item()}')\n",
    "            # print(f'loss manual: {loss_manual}')            \n",
    "\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            runningLoss_train += loss.item()\n",
    "\n",
    "        train_loss[epoch] = runningLoss_train / len(train_dataloader)\n",
    "\n",
    "        print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"Training loss: {runningLoss_train/len(train_dataloader)}\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "\n",
    "        model.eval() # means we are evaluating the model, stops process such as dropout etc. \n",
    "        runningLoss_val = 0.\n",
    "\n",
    "        with torch.no_grad(): # makes sure gradient is not stored \n",
    "            for idx, (batch_inputs, batch_targets) in enumerate(val_dataloader):\n",
    "                \n",
    "                batch_inputs, batch_targets = batch_inputs.to(DEVICE), batch_targets.to(DEVICE)\n",
    "                optimiser.zero_grad() #WHY?\n",
    "                preds = model(batch_inputs.permute(0,2,1)) # permute to make the input shape (samples, features, input_window_size)\n",
    "                # print(f'shape of validation predictions: {preds.shape}')\n",
    "\n",
    "                loss = loss_function(preds, batch_targets) #permute targets to make it same shape as preds\n",
    "                runningLoss_val += loss.item()\n",
    "\n",
    "        val_loss[epoch] = runningLoss_val/len(val_dataloader)\n",
    "\n",
    "        print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"Validation loss: {runningLoss_val/len(val_dataloader)}\")\n",
    "\n",
    "         #Measure long term error \n",
    "        lt_loss_criterion = nn.L1Loss() #long term loss criterion (l1 loss measures MAE for 99 steps in the future)\n",
    "        future_window=200\n",
    "        runningLoss_val_lt = 0.\n",
    "\n",
    "        with torch.no_grad(): # makes sure gradient is not stored \n",
    "            for idx, (batch_inputs, batch_targets) in enumerate(val_lt_dataloader):\n",
    "\n",
    "                batch_inputs, batch_targets = batch_inputs.to(DEVICE), batch_targets.to(DEVICE)\n",
    "                \n",
    "                future_preds = torch.zeros((batch_inputs.shape[0], 200, 6)).to(DEVICE) #would store the predicted outputs for 200 timesteps\n",
    "                \n",
    "                # loop to calculate all 200 preds in the future\n",
    "                for t in range(0,future_window):\n",
    "                    optimiser.zero_grad()\n",
    "                    preds = model(batch_inputs.permute(0,2,1))\n",
    "\n",
    "                    # print(f'preds.shape: {preds.shape}')\n",
    "                    # print(f'future_preds[:,t,:].shape: {future_preds[:,t,:].shape}')\n",
    "                    future_preds[:,t,:] = preds.squeeze()\n",
    "                    new_batch_inputs = torch.cat((batch_inputs, preds),1)\n",
    "                    batch_inputs = new_batch_inputs[:,1:,:]\n",
    "                \n",
    "                #Evalute long term predictions\n",
    "                # print(f'future_preds[101:].shape: {future_preds[:,101:,:].shape}')\n",
    "                loss = lt_loss_criterion(future_preds[:,101:,:], batch_targets[:,101:,:])\n",
    "                runningLoss_val_lt += loss.item()\n",
    "\n",
    "        lt_loss[epoch] = runningLoss_val_lt/len(val_lt_dataloader)\n",
    "\n",
    "\n",
    "        print(f\"Epoch: [{epoch + 1}/{num_epochs}]\", f\"Long term Validation loss: {runningLoss_val_lt/len(val_lt_dataloader)}\")\n",
    "\n",
    "\n",
    "        if val_loss[epoch] < min_val_loss:\n",
    "\n",
    "            # print('YAY, new best value')\n",
    "            min_val_loss = val_loss[epoch]\n",
    "\n",
    "            torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimiser_state_dict': optimiser.state_dict(),\n",
    "                        'loss': val_loss[epoch],\n",
    "                        }, MODEL_PATH)\n",
    "                        \n",
    "\n",
    "    return train_loss, val_loss, lt_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model hyperparameters \n",
    "# Train hybrid CNN-LSTM\n",
    "input_size=len(features)\n",
    "in_seq_len=input_window\n",
    "out_seq_len=output_window\n",
    "# kernel_size=1\n",
    "num_epochs=40\n",
    "learning_rate=1e-3\n",
    "output_size = len(features)\n",
    "hidden_size = 100\n",
    "num_layers=2\n",
    "\n",
    "model_hybrid = hybrid(input_size, output_size=output_size, hidden_size=hidden_size, num_layers=num_layers, stride=1, padding=0, in_seq_len=in_seq_len, out_seq_len=out_seq_len,  device = DEVICE).to(DEVICE)\n",
    "\n",
    "train_history, val_history, lt_history = train_hybrid(model_hybrid, train_dataloader=train_dataloader, val_dataloader=val_dataloader, num_epochs=num_epochs, learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Checkpoint Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_CHECKPOINT = r'D:\\Study 2 Results and Models\\Study 2 Model Checkpoints' +  '\\\\' + 'Exp202-2022-08-10-LSTM-In100-Out1' + '.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set parameters of the model \n",
    "input_size=len(features)\n",
    "hidden_size=100\n",
    "num_layers=2\n",
    "in_seq_len=input_window\n",
    "\n",
    "out_seq_len=output_window\n",
    "output_size=len(features)\n",
    "learning_rate= 0.001\n",
    "\n",
    "\n",
    "#Load checkpint model and its parameters\n",
    "checkpoint_model = LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, in_seq_len=in_seq_len, out_seq_len=out_seq_len, output_size=output_size, device=DEVICE).to(DEVICE)\n",
    "\n",
    "# checkpoint_model = CNN(input_size, output_size=output_size, in_seq_len=in_seq_len, out_seq_len=out_seq_len,  device = DEVICE).to(DEVICE)\n",
    "\n",
    "# checkpoint_model = MLP(input_size=input_size, in_seq_len=in_seq_len, out_seq_len=out_seq_len, output_size=output_size, device=DEVICE).to(DEVICE)\n",
    "optimiser = torch.optim.Adam(checkpoint_model.parameters(), lr = learning_rate)\n",
    "\n",
    "checkpoint = torch.load(MODEL_PATH_CHECKPOINT)\n",
    "checkpoint_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimiser.load_state_dict(checkpoint['optimiser_state_dict'])\n",
    "best_epoch = checkpoint['epoch']\n",
    "best_val_loss = checkpoint['loss']\n",
    "\n",
    "checkpoint_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_val_norm, actual_val_norm, val_loss = test_LSTM(checkpoint_model, val_dataloader, DEVICE)\n",
    "# preds_val_norm, actual_val_norm, val_loss = test_CNN(checkpoint_model, val_dataloader)\n",
    "preds_val_norm, actual_val_norm, val_loss = test_MLP(checkpoint_model, val_dataloader)\n",
    "\n",
    "# predicted_values_val_H = pred_val_output_H.cpu().numpy()\n",
    "# actual_values_val_H = actual_val_output_H.cpu().numpy()\n",
    "\n",
    "print(f'val loss: {val_loss}')\n",
    "print(f'Shape of predicted values val normalised: {preds_val_norm.cpu().numpy().shape}')\n",
    "print(f'shape of actual values val normalised: {actual_val_norm.cpu().numpy().shape}')\n",
    "\n",
    "preds_val = denormalise(preds_val_norm.cpu().numpy(), scalars)\n",
    "actual_val = denormalise(actual_val_norm.cpu().numpy(), scalars)\n",
    "\n",
    "print(f'Shape of predicted values test after denormalisation: {preds_val.shape}')\n",
    "print(f'shape of actual values test after denormalisation: {preds_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mse_loss, val_mse_std = mse_loss(preds_val, actual_val, reduction='mean', format='np')\n",
    "val_mae_loss, val_mae_std = mae_loss(preds_val, actual_val, reduction='mean', format='np')\n",
    "\n",
    "print(f'Val MSE Loss: {val_mse_loss}')\n",
    "print(f'Val MSE std: {val_mse_std}')\n",
    "print(f'Val MAE Loss: {val_mae_loss}')\n",
    "print(f'Val MAE std: {val_mae_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = abs(preds_val[:,:,1] -actual_val[:,:,1])\n",
    "plt.hist(diff.squeeze(), 60)\n",
    "plt.xlabel('Error between actual and predictions (degrees)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot MAE for individual steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_timesteps_H = []\n",
    "for s in range(output_window):\n",
    "    # print(s)\n",
    "    _ = mae_loss(np.expand_dims(preds_val[:,s,:],-1), np.expand_dims(actual_val[:,s,:],-1), reduction='mean', format='np')\n",
    "    mae_timesteps_H.append(_)\n",
    "\n",
    "    print(f'MAE loss and standard deviation for timestep {s+1} is: {mae_timesteps_H[s]}')\n",
    "\n",
    "preds_val[:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04888643971911691\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rcParams['figure.figsize'] = 20,10\n",
    "\n",
    "\n",
    "plt.plot(np.log(train_history[:]), label = 'Training loss')\n",
    "plt.plot(np.log(val_history[:]), label = 'Val loss')\n",
    "plt.plot(np.log(lt_history[:]), label='Long term preds loss')\n",
    "# plt.plot(np.log(np.abs(r_history[:])), label='Val R values (absolute value)')\n",
    "plt.axhline(np.min(np.log(lt_history)), label='mininum long term loss')\n",
    "print(np.min(lt_history))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Log MSE Loss')\n",
    "plt.title('Training and Validation MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rcParams['figure.figsize'] = 20,10\n",
    "\n",
    "\n",
    "plt.plot(train_history[:], label = 'Training loss')\n",
    "plt.plot(val_history[:], label = 'Val loss')\n",
    "# plt.plot(lt_history[:], label='Long/ term preds loss')\n",
    "# plt.axhline(np.min(lt_history))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training and Validation MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pred_val_output, actual_val_output, val_loss = test_LSTM(model_LSTM, val_dataloader, DEVICE)\n",
    "\n",
    "# predicted_values_val = pred_val_output.cpu().numpy()\n",
    "# actual_values_val = actual_val_output.cpu().numpy()\n",
    "\n",
    "# print(f'val loss: {val_loss}')\n",
    "# print(f'Shape of predicted values test: {predicted_values_val.shape}')\n",
    "# print(f'shape of actual values test: {actual_values_val.shape}')\n",
    "\n",
    "# predicted_val_denorm = denormalise(predicted_values_val, scalars)\n",
    "# actual_val_denorm = denormalise(actual_values_val, scalars)\n",
    "\n",
    "# print(f'Shape of predicted values test post denormalisation: {predicted_val_denorm.shape}')\n",
    "# print(f'shape of actual values test post denormalisation: {actual_val_denorm.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_mse_loss, val_mse_std = mse_loss(predicted_val_denorm, actual_val_denorm, reduction='mean', format='np')\n",
    "# val_mae_loss, val_mae_std = mae_loss(predicted_val_denorm, actual_val_denorm, reduction='mean', format='np')\n",
    "\n",
    "# print(f'Val MSE Loss: {val_mse_loss}')\n",
    "# print(f'Val MSE std: {val_mse_std}')\n",
    "# print(f'Val MAE Loss: {val_mae_loss}')\n",
    "# print(f'Val MAE std: {val_mae_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset = gaitDataset(X_val, Y_val) #ADJUSTED\n",
    "# # val_dataloader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# print(f\"Train Dataset length: {len(train_dataset)}\")\n",
    "# print(f\"Val Dataset length: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_val_output, actual_val_output, val_loss = test_LSTM(model_LSTM, val_dataloader, DEVICE)\n",
    "\n",
    "# predicted_values_val = pred_val_output.cpu().numpy()\n",
    "# actual_values_val = actual_val_output.cpu().numpy()\n",
    "\n",
    "# print(f'val loss: {val_loss}')\n",
    "# print(f'Shape of predicted values test: {predicted_values_val.shape}')\n",
    "# print(f'shape of actual values test: {actual_values_val.shape}')\n",
    "\n",
    "# predicted_val_denorm = denormalise(predicted_values_val, scalars)\n",
    "# actual_val_denorm = denormalise(actual_values_val, scalars)\n",
    "\n",
    "# print(f'Shape of predicted values test post denormalisation: {predicted_val_denorm.shape}')\n",
    "# print(f'shape of actual values test post denormalisation: {actual_val_denorm.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_mse_loss, val_mse_std = mse_loss(predicted_val_denorm, actual_val_denorm, reduction='mean', format='np')\n",
    "# val_mae_loss, val_mae_std = mae_loss(predicted_val_denorm, actual_val_denorm, reduction='mean', format='np')\n",
    "\n",
    "# print(f'Val MSE Loss: {val_mse_loss}')\n",
    "# print(f'Val MSE std: {val_mse_std}')\n",
    "# print(f'Val MAE Loss: {val_mae_loss}')\n",
    "# print(f'Val MAE std: {val_mae_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_num = 1 #sample number \n",
    "# f = 2 #feature\n",
    "\n",
    "# samples = [2,3,4,3,2]\n",
    "# f_values = [0,1,2,3,4,5]\n",
    "# for s_num in samples:\n",
    "#     for f in f_values:\n",
    "\n",
    "#         input = X_val_data[s_num,:,f]\n",
    "\n",
    "#         # print(f'input length: {len(input)}')\n",
    "#         actual = actual_val[s_num,:,f]\n",
    "#         preds = preds_val[s_num,:,f]\n",
    "        \n",
    "#         past_timepoints = np.arange(0,len(input))\n",
    "#         # print(f'past_timepoints: {past_timepoints}')\n",
    "\n",
    "#         future_timepoints = np.arange(len(input), len(input)+len(preds))\n",
    "#         # print(f'future_timepoints: {future_timepoints}')\n",
    "\n",
    "#         rcParams['figure.figsize'] = 19,6\n",
    "\n",
    "#         # ax = sns.lineplot(x=past_timepoints[-20:], y=input[-20:]) #only plots last 20 values of input\n",
    "#         ax = sns.lineplot(x=past_timepoints[:], y=input[:])\n",
    "\n",
    "#         # ax.fill_between(x=past_timepoints, y1=input_stats, y2=input_stats, alpha=0.1)\n",
    "#         ax = sns.lineplot(x=future_timepoints, y=preds)\n",
    "#         ax = sns.lineplot(x=future_timepoints, y=actual)\n",
    "#         ax.set_xlabel('Time-step', fontsize =15)\n",
    "#         ax.set_ylabel('Angle (degrees)', fontsize =15)\n",
    "#         ax.set_title(f'Sample {s_num} - Feature {f} - {labels_keys[f]}')\n",
    "#         plt.figure()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = r'D:\\Study 2 Data\\Healthy Gait\\Val'\n",
    "# file_dir = f'D:\\Study 2 Data\\Healthy Gait\\evaluate'\n",
    "test_files = os.listdir(file_dir) \n",
    "\n",
    "# Changes the working directory to get the data from their location \n",
    "os.chdir(file_dir)\n",
    "cwd = os.getcwd()\n",
    "\n",
    "print(f'Current working directory is: {cwd}')\n",
    "print(f\"There are {len(test_files)} files in the specified path.\")\n",
    "\n",
    "test_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = create_dataframe(test_files, all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = count_nsamples(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_to_np_converter(data, n_samples, features):\n",
    "    #create a numpy array that stores the data for export\n",
    "    sample_ID = []\n",
    "    # patients = 2\n",
    "    # n_trials = 10\n",
    "    # # samples = patients * n_trials\n",
    "    data_store = np.zeros((n_samples, 2500, len(features)), dtype=np.float32)\n",
    "    i = 0\n",
    "\n",
    "    for p in data['Patient ID'].unique(): #loop over patients \n",
    "        for t in data['Trial'].unique(): #loop over trials starting with trials 1 to trial 9 (inclusive)\n",
    "            pd_array = data[(data['Patient ID'] == p) & (data['Trial'] == t)]\n",
    "            if pd_array.empty:\n",
    "                continue\n",
    "                # print('DataFrame is empty!')\n",
    "                # print(f'Trail {t} does not exist in {p}')\n",
    "            else:\n",
    "                np_array = pd_array.to_numpy()\n",
    "                data_store[i, :np_array.shape[0], :] = np_array[:,3:] #[:,3:] is because we are\n",
    "                sample_ID.append(p+ ' Ts'+str(t)) \n",
    "                i +=1\n",
    "\n",
    "    return pd_array.columns, data_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_columns, test_data_np = pd_to_np_converter(test_data, test_samples, features)\n",
    "\n",
    "print(f'test_data_np.shape: {test_data_np.shape}')\n",
    "\n",
    "test_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the features to be used when creating windows \n",
    "# approx_seq_len = 2500 # appoximate the length of the longest sequence that can be encountered \n",
    "# samples_per_file = ((approx_seq_len - (input_window+output_window)) // stride) + 1 #number of window samples generated per file \n",
    "samples_per_file = 2000\n",
    "\n",
    "\n",
    "# create a zero-filled 3D array with shape (number of samples * number of files, window size, number of features)\n",
    "X_test_windows = np.zeros((samples_per_file*test_samples, input_window, len(features)), dtype=np.float32) #size can be reduced by decreasing train_size \n",
    "Y_test_windows = np.zeros((samples_per_file*test_samples, output_window, len(features)), dtype=np.float32) \n",
    "\n",
    "\n",
    "start_idx = 0 #setting start index to equal zero \n",
    "test_sample_sum = 0\n",
    "test_excluded_samples = []\n",
    "# Create training windows \n",
    "\n",
    "#for i in tqdm(range(train_size)): #Use for including all data including outliers \n",
    "for i in range(test_samples): \n",
    "       \n",
    "    X_values, Y_values = window_generator_fltrd(\n",
    "        test_data_np[i,:,:],\n",
    "        input_window=input_window, \n",
    "        output_window=output_window, \n",
    "        stride=stride, \n",
    "        features=features,\n",
    "        labels=labels\n",
    "        )\n",
    "\n",
    "    end_idx = start_idx + X_values.shape[0]\n",
    "\n",
    "    # print(f'file number [{f}] start index: {start_idx}, end index: {end_idx}, number of samples: {X_values.shape[0]}')\n",
    "\n",
    "    X_test_windows[start_idx:end_idx, :, :] = X_values\n",
    "    Y_test_windows[start_idx:end_idx, :, :] = Y_values\n",
    "\n",
    "    # print(f'number of samples copied: {X_train_data_store[start_idx:end_idx, :, :].shape[0]}')\n",
    "\n",
    "    start_idx = end_idx \n",
    "    test_sample_sum += X_values.shape[0]\n",
    "\n",
    "    # except Exception:\n",
    "    #     exception_msg(i)\n",
    "    #     train_excluded_samples.append(i)\n",
    "\n",
    "# print(f\"Completed storage of training windows samples, which contains {X_train_data_store.shape[0]} samples\")\n",
    "X_test_data = X_test_windows[:end_idx, :, :]\n",
    "Y_test_data = Y_test_windows[:end_idx, :, :]\n",
    "\n",
    "\n",
    "print(f'shape of X_test_windows: {X_test_windows.shape}')\n",
    "print(f'shape of Y_test_windows: {Y_test_windows.shape}')\n",
    "\n",
    "print(f'shape of X_test_data: {X_test_data.shape}')\n",
    "print(f'shape of Y_test_data: {Y_test_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Data for hModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hModel_X_test_norm = normalise_transform(X_test_data, scalars)\n",
    "hModel_Y_test_norm = normalise_transform(Y_test_data, scalars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of the data \n",
    "for f in range(len(features)):\n",
    "    plt.hist(hModel_X_test_norm[:,:,f].reshape(-1,1), bins=50, label = features[f])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of the data \n",
    "for f in range(len(features)):\n",
    "    plt.hist(X_test_data[:,:,f].reshape(-1,1), bins=50, label = features[f])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Tensor \n",
    "# do not store on GPU (yet)\n",
    "hModel_X_test_tensor = torch.from_numpy(hModel_X_test_norm).float()\n",
    "hModel_Y_test_tensor = torch.from_numpy(hModel_Y_test_norm).float()\n",
    "\n",
    "print(f'hModel_X_test_tensor shape: {hModel_X_test_tensor.shape}')\n",
    "print(f'hModel_Y_test_tensor shape: {hModel_Y_test_tensor.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #only use if predicting one feature\n",
    "# hModel_Y_test_tensor = hModel_Y_test_tensor[:,:,1].unsqueeze(-1)\n",
    "\n",
    "# print(f'hModel_Y_test_tensor shape: {hModel_Y_test_tensor.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hModel_test_dataset = gaitDataset(hModel_X_test_tensor, hModel_Y_test_tensor)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle = False)\n",
    "hModel_test_dataloader = DataLoader(hModel_test_dataset, batch_size=32, shuffle = False)\n",
    "# hModel_test_dataloader = DataLoader(hModel_test_dataset, batch_size=16, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Healthy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_MODEL_PATH = r'D:\\Study 2 Results and Models\\Study 2 Model Checkpoints' +  '\\\\' + 'Exp202-2022-08-10-LSTM-In100-Out1' + '.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set parameters of the model \n",
    "input_size=len(features)\n",
    "hidden_size=100\n",
    "num_layers=2\n",
    "in_seq_len=input_window\n",
    "out_seq_len=output_window\n",
    "output_size=len(features)\n",
    "learning_rate= 0.001\n",
    "\n",
    "\n",
    "hModel = LSTM(input_size    =input_size, hidden_size=hidden_size, num_layers=num_layers, in_seq_len=in_seq_len, out_seq_len=out_seq_len, output_size=output_size, device=DEVICE).to(DEVICE)\n",
    "# hModel = MLP(input_size=input_size, in_seq_len=in_seq_len, out_seq_len=out_seq_len, output_size=output_size, device=DEVICE).to(DEVICE)\n",
    "# hModel = CNN(input_size, output_size=output_size, in_seq_len=in_seq_len, out_seq_len=out_seq_len,  device = DEVICE).to(DEVICE)\n",
    "\n",
    "optimiser = torch.optim.Adam(hModel.parameters(), lr = learning_rate)\n",
    "\n",
    "checkpoint = torch.load(H_MODEL_PATH)\n",
    "hModel.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimiser.load_state_dict(checkpoint['optimiser_state_dict'])\n",
    "best_epoch_hModel = checkpoint['epoch']\n",
    "best_val_loss_hModel = checkpoint['loss']\n",
    "\n",
    "hModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_hModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss_hModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hModel_preds_norm, hModel_actual_norm, hmodel_loss = test_LSTM(hModel, hModel_test_dataloader, DEVICE)\n",
    "# hModel_preds_norm, hModel_actual_norm, hmodel_loss = test_CNN(hModel, hModel_test_dataloader)\n",
    "\n",
    "hModel_preds_norm, hModel_actual_norm, hmodel_loss = test_MLP(hModel, hModel_test_dataloader)\n",
    "\n",
    "\n",
    "# H_model_predicted_values = hModel_preds_norm.cpu().numpy()\n",
    "# H_model_actual_values = hModel_actual_norm.cpu().numpy()\n",
    "\n",
    "print(f'hmodel loss: {hmodel_loss}')\n",
    "print(f'Shape of predicted values test: {hModel_preds_norm.cpu().numpy().shape}')\n",
    "print(f'shape of actual values test: {hModel_actual_norm.cpu().numpy().shape}')\n",
    "\n",
    "# hModel_preds = denormalise(hModel_preds_norm.cpu().numpy(), healthy_scalars)\n",
    "hModel_preds = denormalise(hModel_preds_norm.cpu().numpy(), scalars) #UNCOMMENT FOR PREDS shape (batch_size, num_features)\n",
    "hModel_actual = denormalise(hModel_actual_norm.cpu().numpy(), scalars)\n",
    "\n",
    "\n",
    "print(f'Shape of predicted values test post denormalisation: {hModel_preds.shape}')\n",
    "print(f'shape of actual values test post denormalisation: {hModel_actual.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hModel_mse_loss, hModel_mse_std = mse_loss(hModel_preds, hModel_actual, reduction='mean', format='np')\n",
    "hModel_mae_loss, hModel_mae_std = mae_loss(hModel_preds, hModel_actual, reduction='mean', format='np')\n",
    "\n",
    "print('Losses for Healthy trained model')\n",
    "print(f'hmodel MSE Loss: {hModel_mse_loss}')\n",
    "print(f'hmodel MSE std: {hModel_mse_std}')\n",
    "print(f'hmodel MAE Loss: {hModel_mae_loss}')\n",
    "print(f'hmodel MAE std: {hModel_mae_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load CP Data for CP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_dir = r'D:\\Study 2 Data\\CP Gait\\Val'\n",
    "# cp_files = os.listdir(file_dir) \n",
    "\n",
    "# # Changes the working directory to get the data from their location \n",
    "# os.chdir(file_dir)\n",
    "# cwd = os.getcwd()\n",
    "\n",
    "# print(f'Current working directory is: {cwd}')\n",
    "# print(f\"There are {len(cp_files)} files in the specified path.\")\n",
    "\n",
    "# cp_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CP_data = create_dataframe(cp_files, all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CP_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CP_samples = count_nsamples(CP_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pd_to_np_converter(data, n_samples, features):\n",
    "#     #create a numpy array that stores the data for export\n",
    "#     sample_ID = []\n",
    "#     # patients = 2\n",
    "#     # n_trials = 10\n",
    "#     # # samples = patients * n_trials\n",
    "#     data_store = np.zeros((n_samples, 2500, len(features)), dtype=np.float32)\n",
    "#     i = 0\n",
    "\n",
    "#     for p in data['Patient ID'].unique(): #loop over patients \n",
    "#         for t in data['Trial'].unique(): #loop over trials starting with trials 1 to trial 9 (inclusive)\n",
    "#             pd_array = data[(data['Patient ID'] == p) & (data['Trial'] == t)]\n",
    "#             if pd_array.empty:\n",
    "#                 continue\n",
    "#                 # print('DataFrame is empty!')\n",
    "#                 # print(f'Trail {t} does not exist in {p}')\n",
    "#             else:\n",
    "#                 np_array = pd_array.to_numpy()\n",
    "#                 data_store[i, :np_array.shape[0], :] = np_array[:,3:] \n",
    "#                 sample_ID.append(p+ ' Ts'+str(t)) \n",
    "#                 i +=1\n",
    "\n",
    "#     return pd_array.columns, data_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CP_columns, CP_data_np = pd_to_np_converter(CP_data, CP_samples, features)\n",
    "\n",
    "# print(f'CP_data_np.shape: {CP_data_np.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CHECKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n",
    "# # Selecting the features to be used when creating windows \n",
    "# approx_seq_len = 2500 # appoximate the length of the longest sequence that can be encountered \n",
    "# # samples_per_file = ((approx_seq_len - (input_window+output_window)) // stride) + 1 #number of window samples generated per file \n",
    "# samples_per_file = 2000\n",
    "\n",
    "\n",
    "# # create a zero-filled 3D array with shape (number of samples * number of files, window size, number of features)\n",
    "# X_CP_windows = np.zeros((samples_per_file*CP_samples, input_window, len(features)), dtype=np.float32) #size can be reduced by decreasing train_size \n",
    "# Y_CP_windows = np.zeros((samples_per_file*CP_samples, output_window, len(features)), dtype=np.float32) \n",
    "\n",
    "\n",
    "# start_idx = 0 #setting start index to equal zero \n",
    "# CP_sample_sum = 0\n",
    "# CP_excluded_samples = []\n",
    "# # Create training windows \n",
    "\n",
    "# #for i in tqdm(range(train_size)): #Use for including all data including outliers \n",
    "# for i in range(CP_samples): \n",
    "       \n",
    "#     X_values, Y_values = window_generator(\n",
    "#         CP_data_np[i,:,:],\n",
    "#         input_window=input_window, \n",
    "#         output_window=output_window, \n",
    "#         stride=stride, \n",
    "#         features=features,\n",
    "#         labels=labels\n",
    "#         )\n",
    "\n",
    "#     end_idx = start_idx + X_values.shape[0]\n",
    "\n",
    "#     # print(f'file number [{f}] start index: {start_idx}, end index: {end_idx}, number of samples: {X_values.shape[0]}')\n",
    "\n",
    "#     X_CP_windows[start_idx:end_idx, :, :] = X_values\n",
    "#     Y_CP_windows[start_idx:end_idx, :, :] = Y_values\n",
    "\n",
    "#     # print(f'number of samples copied: {X_train_data_store[start_idx:end_idx, :, :].shape[0]}')\n",
    "\n",
    "#     start_idx = end_idx \n",
    "#     CP_sample_sum += X_values.shape[0]\n",
    "\n",
    "#     # except Exception:\n",
    "#     #     exception_msg(i)\n",
    "#     #     train_excluded_samples.append(i)\n",
    "\n",
    "# # print(f\"Completed storage of training windows samples, which contains {X_train_data_store.shape[0]} samples\")\n",
    "# X_CP_data = X_CP_windows[:end_idx, :, :]\n",
    "# Y_CP_data = Y_CP_windows[:end_idx, :, :]\n",
    "\n",
    "\n",
    "# print(f'shape of X_CP_windows: {X_CP_windows.shape}')\n",
    "# print(f'shape of Y_CP_windows: {Y_CP_windows.shape}')\n",
    "\n",
    "# print(f'shape of X_CP_data: {X_CP_data.shape}')\n",
    "# print(f'shape of Y_CP_data: {Y_CP_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpModel_X_test_norm = normalise_transform(X_test_data, cp_scalars)\n",
    "cpModel_Y_test_norm = normalise_transform(Y_test_data, cp_scalars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of the data \n",
    "for f in range(len(features)):\n",
    "    plt.hist(cpModel_X_test_norm[:,:,f].reshape(-1,1), bins=50, label = features[f])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Tensor \n",
    "# do not store on GPU (yet)\n",
    "cpModel_X_test_tensor = torch.from_numpy(cpModel_X_test_norm).float()\n",
    "cpModel_Y_test_tensor = torch.from_numpy(cpModel_Y_test_norm).float()\n",
    "\n",
    "print(f'cpModel_X_test_tensor shape: {cpModel_X_test_tensor.shape}')\n",
    "print(f'cpModel_Y_test_tensor shape: {cpModel_Y_test_tensor.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpModel_test_dataset = gaitDataset(cpModel_X_test_tensor, cpModel_Y_test_tensor)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle = False)\n",
    "cpModel_test_dataloader = DataLoader(cpModel_test_dataset, batch_size=32, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load CP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CP_MODEL_PATH = r'D:\\Study 2 Results and Models\\Study 2 Optimisation\\2022-06-30 Optimisation LSTM CP 1'+  '\\\\' + 'LSTM CP-2022-07-01-trial6' + '.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set parameters of the model \n",
    "input_size=len(features)\n",
    "hidden_size=100\n",
    "num_layers=4\n",
    "in_seq_len=input_window\n",
    "out_seq_len=output_window\n",
    "output_size=len(features)\n",
    "learning_rate= 0.001\n",
    "\n",
    "\n",
    "cpModel = LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, in_seq_len=in_seq_len, out_seq_len=out_seq_len, output_size=output_size, device=DEVICE).to(DEVICE)\n",
    "optimiser = torch.optim.Adam(cpModel.parameters(), lr = learning_rate)\n",
    "\n",
    "checkpoint = torch.load(CP_MODEL_PATH)\n",
    "cpModel.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimiser.load_state_dict(checkpoint['optimiser_state_dict'])\n",
    "best_epoch_cpModel = checkpoint['epoch']\n",
    "best_val_loss_cpModel = checkpoint['loss']\n",
    "\n",
    "cpModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CP_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_cpModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss_cpModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpModel_preds_norm, cpModel_actual_norm, cpModel_loss = test_LSTM(cpModel, cpModel_test_dataloader, DEVICE)\n",
    "\n",
    "# H_model_predicted_values_CP = hModel_preds_norm_CP.cpu().numpy()\n",
    "# H_model_actual_values_CP = hModel_actual_norm_CP.cpu().numpy()\n",
    "\n",
    "print(f'cpModel loss: {cpModel_loss}')\n",
    "print(f'Shape of predicted values test: {cpModel_preds_norm.cpu().numpy().shape}')\n",
    "print(f'shape of actual values test: {cpModel_actual_norm.cpu().numpy().shape}')\n",
    "\n",
    "cpModel_preds = denormalise(cpModel_preds_norm.cpu().numpy(), cp_scalars)\n",
    "cpModel_actual = denormalise(cpModel_actual_norm.cpu().numpy(), cp_scalars)\n",
    "\n",
    "print(f'Shape of predicted values test post denormalisation: {cpModel_preds.shape}')\n",
    "print(f'shape of actual values test post denormalisation: {cpModel_actual.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpModel_mse_loss, cpModel_mse_std = mse_loss(cpModel_preds, cpModel_actual, reduction='mean', format='np')\n",
    "cpModel_mae_loss, cpModel_mae_std = mae_loss(cpModel_preds, cpModel_actual, reduction='mean', format='np')\n",
    "\n",
    "print('Losses for CP trained model')\n",
    "print(f'cpModel MSE Loss: {cpModel_mse_loss}')\n",
    "print(f'hmodel MSE std: {cpModel_mse_std}')\n",
    "print(f'hmodel MAE Loss: {cpModel_mae_loss}')\n",
    "print(f'hmodel MAE std: {cpModel_mae_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Predictions of Healthy trained and CP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_num = 1 #sample number \n",
    "f = 2 #feature\n",
    "\n",
    "sns.set_theme(style='white')\n",
    "\n",
    "\n",
    "samples = [10300]\n",
    "f_values = [0,1,2,3,4,5]\n",
    "for s_num in samples:\n",
    "    for f in f_values:\n",
    "        \n",
    "        # X_CP_norm = normalise_transform(X_CP_data, cp_scalars)\n",
    "        # Y_CP_norm = normalise_transform(Y_CP_data, cp_scalars)\n",
    "\n",
    "        input = X_CP_data[s_num,:,f]\n",
    "\n",
    "        # print(f'input length: {len(input)}')\n",
    "        input_bf_processing = Y_CP_data[s_num,:,f]\n",
    "        actual = cpModel_actual_CP[s_num,:,f]\n",
    "        CP_model_preds = cpModel_preds_CP[s_num,:,f]\n",
    "        H_model_preds = hModel_preds_CP[s_num,:,f]\n",
    "\n",
    "        past_timepoints = np.arange(0,len(input))\n",
    "        # print(f'past_timepoints: {past_timepoints}')\n",
    "\n",
    "        future_timepoints = np.arange(len(input), len(input)+len(CP_model_preds))\n",
    "        # print(f'future_timepoints: {future_timepoints}')\n",
    "\n",
    "        rcParams['figure.figsize'] = 20,6\n",
    "\n",
    "        # ax = sns.lineplot(x=past_timepoints[-20:], y=input[-20:]) #only plots last 20 values of input\n",
    "        ax = sns.lineplot(x=past_timepoints[:], y=input[:])\n",
    "\n",
    "        # ax.fill_between(x=past_timepoints, y1=input_stats, y2=input_stats, alpha=0.1)\n",
    "        ax = sns.lineplot(x=future_timepoints, y=CP_model_preds, label='CP_model_preds')\n",
    "        ax = sns.lineplot(x=future_timepoints, y=H_model_preds, label='H_model_preds')\n",
    "        ax = sns.lineplot(x=future_timepoints, y=actual, label='actual')\n",
    "        # ax = sns.lineplot(x=future_timepoints, y=input_bf_processing, label='input_bf_processing')\n",
    "\n",
    "        # ax.set(font_scale = 1)\n",
    "        # sns.set(rc={'axes.facecolor':'whitegrid', 'figure.facecolor':'whitegrid'})\n",
    " \n",
    "        fontsize = 15\n",
    "        ax.set_xlabel('Time-step', fontsize =15)\n",
    "        ax.set_ylabel('Angle (degrees)', fontsize =15)\n",
    "        ax.set_title(f'Sample {s_num} - Feature {f} - {labels_keys[f]}', fontsize=fontsize)\n",
    "        plt.legend(fontsize = 15)\n",
    "        plt.figure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = cpModel_preds_CP - hModel_preds_CP\n",
    "\n",
    "# rcParams['figure.figsize'] = 20,6\n",
    "p = sns.displot(diff[:,:,2], height=4, aspect=2)\n",
    "p.fig.set_dpi(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_timesteps = []\n",
    "for s in range(output_window):\n",
    "    # print(s)\n",
    "    _ = mae_loss(np.expand_dims(predicted_CP_denorm[:,s,:],-1), np.expand_dims(actual_CP_denorm[:,s,:],-1), reduction='mean', format='np')\n",
    "    mae_timesteps.append(_)\n",
    "\n",
    "    print(f'MAE loss and standard deviation for timestep {s+1} is: {mae_timesteps[s]}')\n",
    "\n",
    "actual_CP_denorm[:,:,:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use for output window > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Only use when output window is greater than 1\n",
    "# s = 00#sample to plot\n",
    "# nsteps_future = 300 #number of steps to predict in the future \n",
    "\n",
    "# with torch.no_grad(): #do not calculate gradients for forward pass\n",
    "#     in_to_model = hModel_X_test_tensor[s].expand((1,-1,-1)).to(DEVICE) \n",
    "#     all_preds=[] \n",
    "\n",
    "#     for _ in range(0,nsteps_future): #loop over the steps to predict in the future\n",
    "#         # print(f'in_to_model: {in_to_model[:,-10:,1]}')\n",
    "#         # print(f'shape in to model: {in_to_model.shape}')\n",
    "#         future_pred = hModel(in_to_model)\n",
    "#         # print(f'future pred: {future_pred.shape}')\n",
    "#         # print(f'shape future preds: {future_pred[:,0,:].unsqueeze(1).shape}')\n",
    "\n",
    "#         all_preds.append(future_pred[:,0,:].unsqueeze(1).tolist())\n",
    "\n",
    "#         new_in_to_model = torch.cat((in_to_model, future_pred[:,0,:].unsqueeze(1)),1)\n",
    "#         in_to_model = new_in_to_model[:,1:,:]\n",
    "#         # print(f'shape in to model: {in_to_model.shape}')\n",
    "\n",
    "# all_future_preds = np.array(all_preds).reshape((nsteps_future,len(features)))\n",
    "\n",
    "# all_future_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #only use for multiple timesteps\n",
    "# input_seq = hModel_X_test_tensor[s].expand((1,-1,-1))\n",
    "\n",
    "\n",
    "# for f in range(len(features)):\n",
    "#     path = r'D:\\Study 2 Results and Models\\Model Performance' + '\\\\'  + 'Exp' + str(exp_ID) + '-' + str(date.today()) + '-' + model + '-In' + str(input_window) + '-Out' + str(output_window) + '-Plot(1)-' + str(features[f]) + '.png'\n",
    "   \n",
    "#     input = input_seq[:,:,f].squeeze()\n",
    "#     print(input.shape)\n",
    "\n",
    "#     predicted_trajectory = all_future_preds[:,f]\n",
    "#     print(predicted_trajectory.shape)\n",
    "\n",
    "#     past_timepoints = np.arange(0,len(input))\n",
    "#     # print(f'past_timepoints: {past_timepoints}')\n",
    "\n",
    "#     future_timepoints = np.arange(len(input), len(input)+len(predicted_trajectory))\n",
    "#     # print(f'future_timepoints: {future_timepoints}')\n",
    "\n",
    "#     actual_seq = hModel_Y_test_tensor[s:s+nsteps_future,0,f].reshape(-1,1).squeeze()\n",
    "#     actual_timepoints = np.arange(len(input), len(input)+len(actual_seq))\n",
    "\n",
    "#     rcParams['figure.figsize'] = 20,6\n",
    "\n",
    "#     # ax = sns.lineplot(x=past_timepoints[-20:], y=input[-20:]) #only plots last 20 values of input\n",
    "#     ax = sns.lineplot(x=past_timepoints[:], y=input[:], label='input_timesteps')\n",
    "\n",
    "#     # ax.fill_between(x=past_timepoints, y1=input_stats, y2=input_stats, alpha=0.1)\n",
    "#     ax = sns.lineplot(x=future_timepoints, y=predicted_trajectory, label='hModel_preds')\n",
    "\n",
    "#     ax = sns.lineplot(x=actual_timepoints, y = actual_seq, label='actual')\n",
    "#     # ax = sns.lineplot(x=future_timepoints, y=H_model_preds, label='H_model_preds')\n",
    "#     # ax = sns.lineplot(x=future_timepoints, y=actual, label='actual')\n",
    "#     # ax = sns.lineplot(x=future_timepoints, y=input_bf_processing, label='input_bf_processing')\n",
    "\n",
    "#     # ax.set(font_scale = 1)\n",
    "#     # sns.set(rc={'axes.facecolor':'whitegrid', 'figure.facecolor':'whitegrid'})\n",
    "\n",
    "#     fontsize = 15\n",
    "#     ax.set_xlabel('Time-step', fontsize =15)\n",
    "#     ax.set_ylabel('Angle (degrees)', fontsize =15)\n",
    "\n",
    "#     plt.legend(fontsize = 15)\n",
    "  \n",
    "#     # plt.savefig(path)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Future loss for 200 timesteps in advance for entire validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_inputs.shape: torch.Size([3179, 100, 1])\n",
      "val_targets.shape: torch.Size([3179, 200, 1])\n"
     ]
    }
   ],
   "source": [
    "val_inputs=X_val_lt.double()\n",
    "val_targets=Y_val_lt.double()\n",
    "\n",
    "print(f'val_inputs.shape: {val_inputs.shape}')\n",
    "print(f'val_targets.shape: {val_targets.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7338], dtype=torch.float64)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val_lt[10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x24c24e15c70>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIoAAAFtCAYAAABoaIFoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAC48ElEQVR4nOzdd5hcZ3k3/u9zzpmyvffeV9rVqnfJsi1bsmyMQRAsehIgEAwhb2J+LikvhoQAeWNiEhNICB0MGGMDAsnGliWrWJLV2/beey8zc8rvj7WE1yq70s7OM+X7uS5flzU75TvSmdk59zz3/QjLsiwQEREREREREVHIU2QHICIiIiIiIiIi/8BCERERERERERERAWChiIiIiIiIiIiI3sRCERERERERERERAWChiIiIiIiIiIiI3sRCERERERERERERAWChiIiIiIiIiIiI3qTJDjCbwcFxmKYlO8a8JSREor9/THYMohvicUr+jscoBQIepxQIeJySv+MxSoEgUI9TRRGIi4u47s/9vlBkmlZQFIoABM3zoODG45T8HY9RCgQ8TikQ8Dglf8djlAJBMB6nbD0jIiIiIiIiIiIALBQREREREREREdGbWCgiIiIiIiIiIiIALBQREREREREREdGbWCgiIiIiIiIiIiIALBQREREREREREdGbWCgiIiIiIiIiIiIALBQREREREREREdGbWCgiIiIiIiIiIiIALBQREREREREREdGbWCgiIiIiIiIiIiIAgCY7AJFHN9DQMYKa1iHUtg3D5THgsKlw2FTERNqxODcei3LiEObg4UpERERERP7NMjwwB9thDrTBGGiDNTYAERYNEREHJSIOaloplMh42TGJrotn3iTN+JQHvz3chFdPt8OjmxAAMpIiERmmYXxKx+CoCxeaBrDvVDtURaAwIwZblqVj9aJkqAoXwxERERERkf+wXONwX3wFngt/gDU1On2hqkFExMOaHAU8k29eU0DNWAxb8UZouSshbA5pmYmuhYUi8jndMHHgTAd+fagR45MebChPxcqSZBRlxSDCabvquvXtwzjfMICTNb34799ewq9ea8D2NdnYVJEGh02V9CyIiIiIiIgAS3fBffLXcF/aB3imoGZVwFa8EUpCFpToFAhl+pzF8kzBHO2F3nACntojmHr1vyGcUXCsfR+04k0QQkh+JkTThGVZluwQN9LfPwbT9OuIc5KUFIXe3lHZMaQbm/TgqV+eRX37CEqzY7FraxGyU6LmdFvTsnC2rg+/P9qM+vYRxEU58OCdhVhdmsw3VS/hcUr+jscoBQIepxQIeJySvwuUY9Qc7sbkH/4D5kA7tII1sC+9F2pizqy3sywTRmcNXG/8EmZ3HdTUYjg2fQRqfKYPUpO3BMpx+naKIpCQEHndn7NQ5COBegB508DIFP7t52fQOzSFP7+3FGsXp9xSgceyLNS0DuGZV2rR0j2G0uxYfPDuYmQkXf9Ap7nhcUr+jscoBQIepxQIeJySvwuEY1RvOo3J/f8NCAVhd34KWtaSm74PyzLhqT4I17FfAO4pONbvgq3sLn4RHiAC4Ti9ltkKRRz0Qj7R2T+OL//4JIbGXPjbB5diXVnqLb/5CSFQkh2Hf/zoanx4ewlae8bwhe+9gRcONkA3TC8nJyIiIiIimsl9bg8mX3oKSnQKInZ+4ZaKRAAghAJ76RZEPPgVaNkVcB35CVyvfQ+WoXs3MNFN4IwiWnA9Q5P4lx+fgqIIPPKBFXNuNZuNogjcsTwDq0qS8LNX6vCbw004VdOHj923CDmp3nkMIiIiIiKit3JXHYDr6M+h5a+G8/ZPQGj2ed+n4oyCc9tn4T7xPNynfwtzuAvOuz8DJSzaC4mJbg5XFNGCcnsMfPNX52FZFh79oPeKRG8VFW7HJ+5fjM++ZwlGJ9z40g9O4PnXuLqIiIiIiIi8y9N0Eq6D34eaWQ7nHZ/0SpHoMiEUOFa/B86tfwmjtxETv/lnmOODXrt/orlioYgWjGVZ+NGL1WjtGcMn7l+M1PjwBX285UVJ+NLH12Lt4hT89kgT/ukHJ9DSHXj9okRERERE5H/0zmpMvfJfUJLyEHb3ZyHUhWnQsRWsRdh9/x+siWFM7P4Ki0Xkc3MuFO3evRv33XcfKioqsGPHDrzwwgs3vP7AwAAee+wxbNq0CWvWrMEnP/lJNDU1zTMuBZIDZzpw+EIX7t+Yi4qCRJ88ZmSYbXp10c4lGBqfXl30m8ONXF1ERERERES3zBzrx+SLT0GJSkb4PX8DYXMs6ONpqUUI2/G3LBaRFHMqFO3ZswcPP/wwNm7ciKeffhpr1qzBI488gr17917z+pZl4aGHHsJrr72Ghx9+GF/72tfQ29uLj3zkIxgeHvbqEyD/1Ng5gp++XIPy/Hi8c1Oezx9/eXESvvSxNVhZkoQXDjbiie+9gdq2IZ/nICIiIiKiwGaZJqZe/W/AMhF2z19DOH2z2/LMYtFXYU6O+ORxieZUKHryySexY8cOPP7449i8eTOeeOIJ7NixA0899dQ1r9/U1IRTp07h85//PN71rnfh9ttvx1NPPYXu7m7s27fPq0+A/I9hmvj+nipEhdvxF/eXQZG0tWNUuB2feqAcf/XeCky5dfzLj0/hh3urMDbpkZKHiIiIiIgCj/vcXhid1XBu+CCU6GSfPvZ0sehvYI31Y/LFf4elu3z6+BSaZi0Utba2oqWlBdu2bZtx+fbt29HQ0IDW1tarbuNyTR+8ERERVy6LiYkBAAwNDc0nLwWAV0+1o7VnDO/fWoTIMJvsOFhWmIgvfXwttq3OwoGzHXjkW0fw28ONmHJzy0kiIiIiIro+o68J7hPPQctbBa14k5QMWmoxnHd+CmZPI6Ze+RYsk2M1aGHNOn2roaEBAJCXN7N9KCcnBwDQ2NiIrKysGT8rLS3F2rVr8fTTTyM/Px9xcXH4yle+gvDwcNx1113eyk5+aHjcjecPNqAsLx4rS5Jkx7nCadewa2sRNlek4VevNeD5g4145WQb7lmbg41LUhEV7r3dCgBgZNyNlu5R9A5Non/EhYHRKUy5DNhtCmyagjC7hry0aJRkxyI+2unVxyYiIiIiovmzdBem9n0bwhkF5+Y/hZDUKQEAtryVsDZ8AK4jP4Hr9Z/AseFDUvNQcJu1UDQ6Or1rVGTkzD7My6uFxsbGrnm7L3zhC/j4xz+Oe++9FwBgt9vx9NNPX1VUouDy7Kt18OgmPnh3sV++cWUkReKz76lAffswnjtQj1+8WodfvVaPlSXJ2FyRhqLMWNi0m9sMcNKlo6lrFA0dw2jsHEVT1wgGRv64JFRVBOKiHAhzaHDrJjy6gfFJHS+fbAMAJMeGYfWiZGxfk+0XK7CIiIiIiAhwn/otzKFOhN37sM/mEt2IvfxumKN98Jx/EUp0CuxLts1+I6JbMGuhyLIsALjqpP/y5Ypy9Ul1fX09du3ahezsbDz++ONwOp34xS9+gb/6q7/Cd77zHaxatWrOARMS5L8gvSUpKUp2hAV1saEfRy504X13FWNJSYrsODeUlBSFdcsy0dQ5ghePNuHVE604dqkbdk1BSU48yvITkJ4UgZhIB2Ii7LDbVExMeTA+qWNkwo22nlG0do+ipWsUHb1jMKdfDkhLjEB5QSIKM2NRmBmL9KQIxEY5oSozXz+GaaGpYxgXGvpxpqYXvz/ajH2n2vHO2/LxrtsKEOnlFU43I9iPUwp8PEYpEPA4pUDA45T8ncxj1DPUg7bzLyKy/DYkL18vLcfbWfd/HN3uIUwc/Rni8goRnrdUdqSQF4zvpbMWiqKipp/021cOjY+Pz/j5W33/+98HAHz3u9+9Mpto48aN+MAHPoAvf/nL+NWvfjXngP39YzAvn4UHsKSkKPT2jsqOsWBMy8LTz55BQrQDdyxNC5jnGqEJ7NyUh/vWZuNi4wBqWodQ3TKEn79cDesGh50Q0yuB0hMjsKIoEQUZMchLi75qRZDp1jHQf+1Vd9EOFRsWJWPDomS0b8jBrw834ed/qMHugw345DvLUJ6f4M2nOifBfpxS4OMxSoGAxykFAh6n5O9kH6OTL38XFgSsigf87rWirP9TKD1t6Hru3xDx7v/r8wHb9Eeyj9NbpSjihotyZi0UXZ5N1NLSgpKSkiuXNzc3z/j5W3V0dKCgoOBKkQiYXpG0cuVK/PCHP5x7egoYp2t60dozhk/cvxgOmyo7zk1z2FSsKE7CiuLpuUpTbh1DY26MTrgxOuGBWzcQ7tAQ5tAQ7rQhOdYJm+a955mRFIlPv6scLd2j+M7uSnz9F2exc0s+7l2X45ctfERERHRjlqnDHOqCOdID4YiAEh4DER4LYeNsQiJ/p3dWQ294A/aV74IS6fsvb2cj7GEI2/45jD//BCZf/AbC3/X3fG8hr5q1UJSTk4PMzEzs3bsXd99995XLX3rpJeTm5iI9Pf2q2+Tl5eH555/H8PDwjGLR2bNnkZGR4aXo5C8sy8JvDjchJT4caxf5d8vZXDntGlLjNaTGh/v0cbNTovB3H16J7++twnMHGtDUOYqPvWMRnPZZX6pENAfW1Bg8TScB1zgsjwuW7oKakA0tbxWEJq/lk4iCg+WegPvci9CbT8Mc7ADMq3dYFTGp0HKWQctZDjWlEEIJvC/YiIKZZZpwHfkpREQ87Et3yI5zXUp0MsK2fhqTe/4fpl79Hzjv/gy/YCavmdPZ50MPPYTHHnsMMTExuP3227Fv3z7s2bMHX//61wEAAwMDaGlpQWFhISIjI/Gnf/qn+M1vfoOPfexj+Iu/+As4nU78+te/xvHjx6/choLHmbo+tPaM4WP3LYKi8M1pvhx2FX9x/2LkpUbhF6/W4z+eO4+//pOlNz1km4j+yHJPwn3+RbjPvQh4Jv/4A1WDx9CBIz+BrWgj7IvvhBKbKi8oEQUkS3fDc/FluM78DnCNQ01fBPuSbVDiM6HEpMJyT8CaGIY5PgijswqeC3+A59xeiLBo2BbdAdviO6GEx8z+QES04PSaQzD7m+G881MQmkN2nBvSMsvgWPsgXEd/BvfZPXAsu1d2JAoScyoU7dy5E263G9/97nfx7LPPIisrC1/96lev7Gi2f/9+PPbYY/jhD3+ItWvXIjMzE8888wz+9V//FY8++igURUFxcTG+973vYcOGDQv6hMi3Lq8mSop1Yl1ZcKwm8gdCCGxbk42ocDv+Z/cl/M9vL+JTD5SzEEd0Czw1h+B6/WewXGPQclfCvuKdUGJSAW16ppjRUQVP5X54Lr0Cz6VX4NjwIdgX3yE5NREFCmOgHZN7n4Q11g81awkcq98DNTH3+jdY/g5Y7knobRfgqTkE96lfw33md9AK18Ox7F4osWk+y05EM1mGDtfJF6AkF0ArWCs7zpzYlmyH0dMA9xvPQk3KhZaxWHYkCgLCsm40slc+DrP2b+fq+/Hvz57Fn+4oxW1Lr25DpPl76XgLfravDrcvz8CHtxUv6JLSYD1OKXjc7DHqvvgKXId/BDW1GI7174eadPVcvcvMiWFMHfhfGK3nYCu9HY6NH4JQ2fZJN4/vpaHD6KnHxJ4nIVQbnHd+Elr6opu+D3OoE+4Lf4Cn+hBgeqAVboBjxTuhxCzsF3A8TsnfyThG3VUH4Hrtewjb8bfQspb49LHnw3JPYuKFL8GaGkX4zi/45VylYBWo76WzDbNmLwvdsunVRI1IiHZiQzlbNRbKtjXZ2LEuG/tPt+O3h5tkxyEKGFeKRNnLEHbf529YJAIAJTwGYdv/GvZl74Cnaj8mdn8F5uSIj9ISUaDR2y5iYvfXIBwRCH/n391SkQgAlNg0ODd9BBEf+H+wLdkOveE4xn/xGKYOfBfmaK+XUxPR9VimAffp3VCS8qBmlsuOc1OEPQzObZ+BZXgw+fLTsAyP7EgU4FgooltW1TyIho4R3Lc+B5rKQ2khvXdLATaUp+LXhxpR2TQgOw6R35tRJLr7IQjVNqfbCUWBY8174dz6aZh9zZjc+++wdNcCpyWiQKO3XcDk3q9DiU5C+DsfhxKdNO/7VMKi4Vy3CxHv/1fYyrbCU3sE4z9/FFMHfwBzrN8LqYnoRvT6Y7BGe2Ff/o6AHAqtxqbDueVjMHsa4Hr9GdlxKMDx7J5u2csn2xAZZsPGJVxNtNCEEPjwthKkxIfjf3ZfwtgkvyUguh69o/ItRaLPzLlI9Fa2gjVwbv1LmL2NmHrlW7BMcwGSElEgMscHMbXv21BiUxF+/2NQwmO9ev9KeCycGz6IiF1fg610CzzVr2H8Z49g6vCPYU4MefWxiGiaZZlwn9kNJS4DWs5y2XFumS1/NWwV98BzaR88NYdlx6EAxkIR3ZL+4SmcqevD5qVpsGnc1tUXHHYVn3xnGUYnPPj+nir4+XgxIiks9ySm9n8HIiYFYXf95bxmDNlyV8Cx4QPQm0/D9fpP+ZojIlimialX/xuW7oLzrk9DOCIW7LGUyPjplrQHvwpb8QZ4Lu3D+DOfx9Trz7AtlsjL9KZTMAc7YF9+P4QI7FNkx5o/gZpWiqmD34fR1yw7DgWowH4VkDQHzrYDFnDHsgzZUUJKTmoU3rOlAKdqevHa2Q7ZcYj8juvoM7DGBxB2+ye8sqWtvfxu2JZsh+fiy/Bc+IMXEhJRIHOf2Q2joxLODR+CGuubTTyUqEQ4b/tzRDz4FWgFa+C58BLGn3kYrmO/gDkVeANUifyNZVlwn/4tRHQKtPw1suPMm1BUOLf+JYQjApN/+E9YrnHZkSgAsVBEN82jm3jtTAeWFiYiMTZMdpyQs21NFhbnxuGZV2rRPTghOw6R39BbzsJT9RrsS++FmlLotft1rHsQWu4KuI7+HEZvo9ful4gCi9FVC/fJF6AVrINWstnnj69EJyPs9k8g4k/+BVruCrjP7sH4M5+H643neCJINA9GZxXMvmbYl90LoQTH6bESHoOwuz8Da3wAk6/+NyyLLfR0c4LjlUA+dbK6ByMTHty5gquJZFCEwMfuWwwhBJ55uVZ2HCK/YE2NYeq170GJy4R95bu8et9CKHBu+RhEeAwm930LlofDrYlCjWV4MLn/fyAiE+Dc/FGpg26V2FSE3fkphL/3n6BlLYH79G8x9szDcJ38NSw3v0Aiulmei68AjgjYCtfLjuJVakohHOvfD6PlLNynd8uOQwGGhSK6aftOtSM5LgyL8+JlRwlZcVEOPLAxD+fq+3Gmrk92HCLpXCd+BWtyFM47PnFLw6tnIxwRcN7xCVjDPXC9/lOv3z8R+TfPpVdhjfTAuekjEHb/WE2txmcg7K6HEP6eL0JLXwT3yecx9szn4Tq9G5ZnSnY8ooBgjg9CbzoFW8ltEJpddhyvsy3eCq1wPdwnnofeel52HAogLBTRTWnpHkVd+zDuXJ4BJQC3jQwmd63KRFpCOH72ci08uiE7DpE05mgvPFUHYCvdDDUxZ8EeR0tfBPvSHfBUHYCn6eSCPQ4R+RfLNQ73qd9AzSiDlrVEdpyrqAnZCNv2Vwh/9xegphTC/cYvMf7M5+E+uweWzhWQRDfiqdwPWBbsi++QHWVBCCHg3PynUOIzMLnvWzBHe2VHogDBQhHdlFdPt8OuKdhYkSY7SsjTVAUfuLsYPUOT2Hu8VXYcImlcJ38NCAH78ncu+GPZV+2EkpgD14HvwZwYXvDHIyL53Gd/D8s1Dsfa98mOckNqUi7C7/k/CH/g76ffp479HOM/fxSeuqPctZHoGixTh6dyP9SsJVCik2XHWTDC5kDY3Z8FLBOTf3galu6WHYkCAAtFNGduj4Hjld1YVZqMCKf3Wzvo5pXlxmNVSRJ+d6QJ/cNcZk6hxxjqgF57GLbFW6FELnw7rFA1OO/4JCzPFFvQiEKAOdYP9/mXoBWtX9AVi96kphQi/N6HEXb/YxDOaEzt+xYmd38FxgC/VCJ6K73xFKzJYdjL7pQdZcEpMSkIu/0vYPY1wXXkx7LjUABgoYjm7ExdHyZdBjaUp8qOQm/x4J1FAIBfHqiXnITI99wnXgBUO+zL7vPZY6px6bAvfwf0+mPQW8/57HGJyPdcJ54HYMGxaqfsKDdNSytB+Lv/LxybPgpjoA0Tz/1fuE7/FpbJ3Y+IAMBz6RWIqCSomRWyo/iElrsc9mXvgKfqNbirDsiOQ36OhSKasyMXuhAX5UBpdpzsKPQWCTFO3L06C8cvdaOtd0x2HCKfMfpboDcch33JNihh0T59bPuy+6DEpmHq0A+5CxpRkDIG2qHXHIat7G4oUYmy49wSoSiwL74DkQ9+FVr+arjfeA6Tv/9XmOODsqMRSWUMtMHorIZt0R0QSuicEttX7YSaUQbX4R/B6G2UHYf8WOi8KmheRsbduNAwgHVlKVAUDrH2N9vXZMPpUPHrg3zDp9DhPvE8YA+DveIenz+2UG1wbP5TWKN9cJ183uePT0QLz3PhRUC1weHDFYsLRTgj4bzzU3Bu+RiMngaM//LvMVF/WnYsImk8lfsBVYOtdLPsKD4lFAXOrZ+CCIvB5B/+E+bUqOxI5KdYKKI5OXapG6ZlYUMZ2878UWSYDdtWZ+NkTS+aukZkxyFacOZQF/Tm07CXb4NwREjJoKWVwFa6BZ7zL8Hoa5aSgYgWhjk5Ak/tEdiKN0A4I2XH8QohBGwlmxGx8wkokfHo+sW/wFNzWHYsIp+zDB16/TFoOSugOKNkx/E5xRmFsLsegjUxjKlXvgXL5O7JdDUWimhOjlzsQk5KFDKSguPDUjC6e1UWIpwaXuCqIgoB7kuvAIoKm+TtbB1r3wfhjMTUoR/Asjj3gyhYeCr3A4YOW/ndsqN4nRKbivD7H0dY9mJM7f8fuM/ukR2JyKeM1vOwpkZhK94gO4o0anI+nJs+AqP9IlzHn5Udh/wQC0U0q/a+cTR3jWI9h1j7tXCnhnvWZuNcfT/q2rltNwUvyzMFT/UhaPmroYTHSs0iHBFwrH0QZk8DPNUHpWYhIu+wDB2eS/ugZpZDjcuQHWdBCHsYUh/8O2j5a+A69nNMHf0ZLMuSHYvIJzy1hyHCoqFmlsuOIpWt9DbYFm+F59xeeOpelx2H/AwLRTSr1y90QRECaxenyI5Cs7hrZRaiw214/rUG2VGIFoyn5jDgmYS97C7ZUQAAWtEGqKnFcB97FtYUB8oTBTq98Q1YE0Owl2+THWVBCc0G59ZPXTlRdJ/9nexIRAvOmhqD3nwGWsFaCEWTHUc6x4b3Q00rwdSB78Loa5Idh/wIC0V0Q6Zl4fWLXSjPj0dMhF12HJqFw65ix7ocVDYPop6riigIWZYFz8VXoCTlQUkukB0HwPTcD8emD8NyT8B1/Jey4xDRPFiWBff5l6DEpELNCv7VBkIocGz8ILSCdXAf/yU8tUdkRyJaUJ6G44Cpw1a8UXYUvyAUDc67HoJwRmHypf+AOclZpzSNhSK6obq2YQyOurCOq4kCxpZl6Yhwath7rEV2FCKvGRiZwo9fqsbXn/wJzKEOuPO3QAj/2YFRjc+CrfxueKoOwOjhij6iQGV218HsbYRtyTYIERofk4VQ4Lz9Y1DTSjF14H+ht1+SHYlowXhqj0CJy4CSkCM7it9QwqIRtu2vYE2OYOrlb8IyddmRyA+Exm9AumUnqnugqQqWFibKjkJz5LRruGNFJk7V9KKzf1x2HKJ5GxiZwj9+9zgOnOlA/sgJjJpOPPGqiYGRKdnRZnCsfBdEeMz0YGuTg62JApH74iuAPRy2otBabSBUG8K2fRZKTComX/oPGIMdsiMReZ053A2zuw5a0Ua/+rLJH6hJuXBu/lMYnVVwHf257DjkB1goousyLQsnq3tRnhePMAd7eAPJXSszoaoKXjzeKjsK0bz9/mgzXG4DEdY4ym1teN1VhHG3wO+P+teW9MIeBse6XTD7muGp2i87DhHdJMs9Cb3pJGyF6yBsDtlxfE44IhC2428gVA1Tr/wXLN0tOxKRV023VgrYitbLjuKXbMUbYSvfBs+FP3CDDmKhiK6vsXMEg6MurCpNkh2FblJ0hB2bKtJw5EInhsdcsuMQzUtj5wgM04IHKo67C3BgqhSGaaGx0//66LWCtVDTF8F1/Jfs8ycKMHrjCcDwwFYUultmK5EJcG75GMyBVs5co6BiWRY8tUegZiyGEhEnO47fcqx7EGr6Ikwd+gFb6UMcC0V0XSereqEqAsvYdhaQtq/JgmFaePlkm+woRPOSlxYNVRGYtBx4ZnwDxqwwqIpAXlq07GhXEULAsfHDgMcF17FnZcchopvgqTkMEZPiN4PyZdFylsFWthWeCy9Bbz0nOw6RV5h9TbBGe2ErWCs7il8TigrnXZ+GCIvB5B/+A+YEN8cJVSwU0TVZloUT1T1YnBuPcKdNdhy6BSlx4VhZkox9p9ox6eJQOgpc967LgcOuQlWm5wmoioDDruLedf45iFKNS4e9Yjv0moPQu2plxyGiOTBH+2B0VsFWtIGzSwA41j4IJS4TU/u/w9WRFBT0hjcAoULLXSE7it9TnFHTw62nxjH18tOwDJ5HhCIWiuiaWrrH0Dc8hVUlbDsLZDvWZmPSpePgWQ6lpMAVH+3EF/98DbYsS0dRViy2LEvHF/98DeKjnbKjXZd9xTshIuLhOvRDWKYhOw4RzcJT9zoAwFYYum1nbyU0O5xbPwXLPQHXwe/LjkM0L5ZlwdN4AmrGIghnpOw4AUFNzIFzy5/D6KqB6/Wfyo5DErBQRNd0oroHihBYXsxCUSDLS4tGYWYM9p1qh2lZsuMQ3bL4aCc+tK0ET/71FnxoW4lfF4kAQNiccKx/P8yBVnguvCQ7DhHdgGVZ0GuPQE0thhLNzz2XqfGZsK/cCb3pFPSWs7LjEN0ys78F1kgPtPzVsqMEFFvhOtgqdsBzaR/cVQdkxyEfY6GIrmJZFt6o6kFpTiwiw9h2FujuWpmJnqFJnK/vlx2FKKRoeaugZi+F643nYY70yI5DRNdh9jbCHOqEFsJDrK/HvmQblNg0TB35CXdBo4ClN54AhMK2s1vgWPMnUDPK4Dr8YxgDnHsaSlgooqu09Y6jZ3ASq0qSZUchL1hRnITYSDuHWhP5mBACzk0fBRQVU699DxZX9RH5JU/tEUDVYONqg6sIVYNjwwdhjfTAfW6v7DhEN82yLHga3oCaXgrFGSU7TsARigLnHX8BYQ/D1CvfhKVzN+VQwUIRXeVkdQ8EwLazIKGpCm5fnoGLjQPo7B+XHYcopCiR8XCsexBGRyU8XLZN5HcsU4defwxaznIIR4TsOH5JyyyHlrcK7tO7YY72yY5DdFPMwXZYw13Q8lbJjhKwlPAYOO/4JMzBTrgO/0R2HPIRForoKmfr+1GQEYOYCLvsKOQlW5ZlQFMF9p1slx2F6Kb0Dk2isTOwd9yxlW6Bmr4IrqM/hzk+KDsOEb2F0VkDa2oUGrfMviHH+vcDAnC9/ozsKEQ3ZXq3MwEtd6XsKAFNyyyDfdl98FS/Bk/dUdlxyAfmXCjavXs37rvvPlRUVGDHjh144YUXrnvdRx99FCUlJdf9j/zX4KgLzV2jWFqYIDsKeVFMhB2rS1Nw6EInJl3c4pICx//+rhLPvlonO8a8CCHgvO3PANNgCxqRn9EbTwKqHVrWEtlR/JoSmQD78vuhN52E3lktOw7RnOmNb0BNLYESHiM7SsCzr3o31JQiTB38Pswxzj4NdnMqFO3ZswcPP/wwNm7ciKeffhpr1qzBI488gr17r92r/OlPfxo///nPZ/z3ta99DYqiYNeuXV59AuRd5+qnlxQvLUiUnIS87a5VmXC5DRw+3yk7CtGcDI+5UNs6hJLsONlR5k2JToZj3ftgtJ6D5/yLsuMQEQDLMqE3n4KWVQ6hOWTH8Xv2JdsgwmLgPvG87ChEc2IMdsAc7ICWz7YzbxCKCuedfwEAmDrwXX7xFeS0uVzpySefxI4dO/D4448DADZv3ozh4WE89dRTuOeee666fnZ2NrKzs6/82TAMfOlLX0JpaSn+7u/+zkvRaSGcretHQrQTGUns0w82eWnRyE+Pxr5T7di6MhNCCNmRiG7oVG0fLAArS4JjXppt8VYY7ZVwHX8Wamox1OR82ZGIQprZ2whrfBDa6vfKjhIQhOaAffk74DryE+jtl6BlLJYdieiG9OZTAMC2My9SopLgWPsgXId+AE/lftgX3yE7Ei2QWVcUtba2oqWlBdu2bZtx+fbt29HQ0IDW1tZZH+RnP/sZLl26hCeeeAJ2O+fe+Cu3x8ClpgEsLUxgESFI3bE8A10DE6huGZIdhWhWJ6t7kBIfjozE4ChcCyHg3PLnEOGxmHzlv2C5OFyeSCa98SQgVGg5y2RHCRi20i0QEfFwnfgVVxOQ39Obz0BJzIUSEfgrk/2JbdHtUDPK4Dr2c5ijvbLj0AKZtVDU0NAAAMjLy5txeU5ODgCgsbHxhrcfHx/HN77xDTzwwAOoqKi41ZzkA1Utg3DrJpYVsu0sWK0uTUa4Q8P+MxxqTf5tbNKDquYhrCpJCqrCtXBEIGzrX8IaG+C8IiKJLMuCp+kU1PRS7nZ2E4Rmh335O2B218FoOy87DtF1mZMjMLvrWQheAFdmL+JyC5opOREthFkLRaOjowCAyMjIGZdHREz/Uh0bG7vh7Z977jmMjIzgk5/85K1mJB85W9cPh01FSXas7Ci0QOw2FRvKU3GyuhcjE27ZcYiu63RtL0zLwori4Gg7eys1pRCONe+B3ngC7pOc9UEkgznUMb1ldu4K2VECjq3kNoioRLhOPM9iN/kto/U8AAta9jLZUYKSEpUIx7pdMDoq4al6TXYcWgCzzii6/Avg7d/oXr5cUW5ca/rJT36CrVu3XrUiaa4SEiJnv1KASEqKkh3huizLwvnGAawoTUZ6WqzsOLSA3n1nEV4+2YazDQPYeUfRVT/35+OUQsf5xkEkx4Vh9ZL0q37/BMMxam19H/qm+jB66jeISkpGzKodsiORlwXDcRrMBqsvYAJAysrboEWF7r/VrR6nI7e9D32/+yYihqoRUbzay6mI/uhWj9Hu1y5AjYxHyqLyoFqZ7E+sxHegs/k43Cd+idRVt0MN53tpMJm1UBT15i/Pt68cGh8fn/Hza6mqqkJTUxMefvjhWw7Y3z8G0wz8byuSkqLQ2zsqO8Z1tXSPom9oEvevz/HrnDR/YapAUWYMfne4ERvLUqC85Zenvx+nFBomXTrO1PTgzhWZ6Oub+bsnmI5Ra/UHoQ0Nov/F/8W4YYctf43sSOQlwXScBqvxi0egJBdgcMoGTIXmv9V8jlMrbQVEVBL6XnsOE3GlXk5GNO1Wj1HL0DFefxq2grVXfY4g71LWvB/mc/8XHXt/AOfmj8qOI0Wg/s5XFHHDRTmztp5dXgnU0tIy4/Lm5uYZP7+W/fv3Izw8HFu2bJlTWJLnbH0/AKCiIEFyEvKF25dloGdwElXNg7KjEF3lbF0fdMMKmt3OrkcoKpxb/xJqahGm9v039JazsiMRhQRztA9mXzN3QpoHoWiwL9kGo7sWRk+97DhEMxid1YBnivOJfECNz4KtbCs8lfth9DXJjkNeNGuhKCcnB5mZmdi7d++My1966SXk5uYiPT39urc9c+YMysvLudNZADhX14e8tGjERDpkRyEfWFWahAinhv1nOmRHIbrKyepexETaUZARIzvKghOaHWHbPwclLh2TL/473Of2cuYH0QLTm08DAGx5nE80H7biTYA9DO5zL8qOQjSD3nIGUG1QMxbLjhISHKveDREWhalDP+Jg6yAya+sZADz00EN47LHHEBMTg9tvvx379u3Dnj178PWvfx0AMDAwgJaWFhQWFs4Yel1TU8PVRAFgbNKDho4R3L8xV3YU8hGbpmLjkjS8crINw+NuxESwmEv+we0xcL6hHxsr0ma0RQYz4YhA+Dsfx9T+78B19Gcw+lvg3PynEJp3XpeWacLsa4LR3wKzvxXmQCsszxSg2SFUG0RYNLSc5dBylkPY+GUBBT+99TxETCqUmFTZUQKasIfBvugOuM/tgTnaCyUquFeBUmCwLAt68xmo6YsgNP5O8wVhD4dj7fswtf870GsOw1ayWXYk8oI5FYp27twJt9uN7373u3j22WeRlZWFr371q7j33nsBTLeYPfbYY/jhD3+ItWvXXrldf38/oqOjFyY5ec2lpgFYAMrz2XYWSrYsS8dLb7TiyIVO7FibIzsOEQCgsnkQbt3E8qJE2VF8SticcN71abhP/xbuE89jor8VjjV/AjVryS0N4bTck9DbLkBvPgOj9RysyzNYbE6o8VkQEfGA4QF0N4zOauj1xwDNDi17GezL74eakOXlZ0jkHyzdDaOjErZFt8uOEhRsZXfBfe5FuC+8DOf698uOQwRzqBPWaC+0pdwkwpe0og1QKvfDdfyX0PLX8IunIDCnQhEA7Nq1C7t27brmz3bu3ImdO3dedfnZs5y3EAguNAwg3KEhLy34prXT9aUlRKAwMwYHz3binjXZ3BGC/MLZ+n44bCpKsuJkR/E5IRQ4VjwANSEbU4d/jMm9T0JJKYRj5buhZiyCENfvFrcsC9ZoL/SWc9BbzsDoqARMA3BEQMuqgJa9FGpyPkRU4lX3Y1kmjK5a6PXH4Kk/Br3pFBxr3wdb+d18X6CgY3RWAYYHWtYS2VGCghIZD61gDTxVB+BY+QCEPVx2JApxevMZAICWvVRukBAjhALn2gcx8Zt/hvviH+BY9g7ZkWie5lwoouBkWRYuNPZjcW4cVGXWkVUUZDZXpOF7v69CbdswirNiZcehEGdZFs7V92FxbhxsWui+H2k5yxGRuQSe6tfgPv1bTP7+XwF7ONTkfKgphRARcdNFIMMDa2oMRl8zzN7GK6uGREwqbOV3Q8tZPn19Rb3h4wmhQEsrgZZWAvvKd2HqwP/C9fpPobedh3PLx6GEB/+sKAodeuv56dkladypy1vsFduh170OT9UB2Cu4ioPkMlrPQknIghLJTglfU1OLoGYvhfvM72FfdAeEI0J2JJoHFopCXHvfOIbG3Gw7C1GrS5Px05drcfBcBwtFJF1b7zgGRlx458br76YZKoSqwb74TtiKN0FvPAGjswZGTx3cJ38N4C3DroWAEpsBNXsZ1OQ8aOmLocTe+twVJSwaYdv/Gp5L++A6+jNMvPBFhD/w91AiQm+FFwUnvfU81PRSr80AI0BNzIWaVgr3hZdhK98OwS8eSRLLPQmjqw72pffIjhKyHKvfg4nn/hHus3vgWPNe2XFoHlgoCnEXGgYAAOV58ZKTkAxOu4a1i1Jw9FIXPnBXsew4FOLO1fcBACoKWLi+TGh22Io2wFa0AcD0h2DLPQEoGoSqXRlI7dXHFAL2sq1QkwswsfsrmNzzJMLf+RhbSijgmSM9sIa7oC2+U3aUoGMr24qpl5+G0XYOWvYy2XEoRBkdVYBlQM0slx0lZKkJ2dAK1sF94SXYyu+CEh4rOxLdIpb8Q9zFxn6kJYQjPtopOwpJsnlpGtweE8cqu2VHoRB3tq4fOalRiI3kAMTrEfYwKJEJUMJjIBwRXi8SvZWalIuwuz8Dc7ADky/9ByxDX7DHIvIFvfU8AEDLqpCcJPhoucshwmLgvrRfdhQKYXrbBUBzQE0plB0lpDlWvRswDLhP/VZ2FJoHFopCmMtjoLp1GOV5/PY+lOWnRSMjKQIHz3bIjkIhbHTCjfr2YSzlaiK/omWWw7nlz2F0VGLqwHdgWabsSES3TG89DxGVBBGTIjtK0BGKBlvJZhitZ2GO9cuOQyFKb7sw3Vq6gF+i0OyUmBTYSjfDU7Uf5mif7Dh0i1goCmE1rUPQDRPl+Ww7C2VCCGyuSEdj5ygaO4Zlx6EQdaFhABaApYWJsqPQ29iKN8K++r3Q647Cc3Gf7DhEt8QyPDA6KqFlLeFufgvEVroFsABP1Wuyo1AIMkd6YI10Q2PbmV+wL38nAMB9bq/kJHSrWCgKYRcaBqCpCocYE9aXpUBTBf5wvEV2FApRZ+v7EB1hR05qlOwodA32ZfdBzSyH641f8ttBCkhGZw2gu9h2toCU6CSomWXwVB+EZRqy41CI0dsuAAALRX5CiYyHVrgBnqrXYE6OyI5Dt4CFohB2obEfJVkxcNhuvHUyBb+ocDuWFSVh/8k26AZbS8i3dMPEhYYBVBQkQOE3/X5JCAHn5o8CloWpQz+AZVmz34jIj+ht5wFFg5q+SHaUoGZbdAes8QEYredkR6EQY7RdhIhMgIi59Z0/ybvsS3cAhhueiy/LjkK3gIWiEDUwMoXO/gmUcT4RvWnTkjSMTrhxpparBci36tuHMeHSOZ/IzylRSXCseS+M1vPQa4/IjkN0U4zW81DTiiFsHJa/kLScpRDhsXBX7pcdhUKIZerQ2y9Byyxna6kfUePSoeWugPviK7A8U7Lj0E1ioShEXWwaAACU53E+EU0rz4tHfLQTh853yo5CIeZcQz9URWBxLt+P/J1t8VYoKYWYev2nXEpOAcOcGIY52A41o0x2lKB3Zah1yzkOtSafMXoaAc8kVLad+R370nsB1zg8lQdkR6GbxEJRiKpsHkR0uA0ZSRGyo5CfUBSBO1dl4XxDPwZHXbLjUAi52DCAwowYhDk02VFoFkJR4LztzwGPC66jP5cdh2hOjI5KAIDGtjOfsJXeBsCCp+aw7CgUIoy284AQ0DIWy45Cb6OmFEJNK4X7/F5Yhi47Dt0EFopCkGVZqGwaRGlOHJdn0gx3rcmGZQGvX+ySHYVCxPC4Gy09Y9x9MYCocemwld8NvfYIjMF22XGIZmV0XALsYVASc2VHCQlKVBLUtFJ4ag9znhn5hN52AUpyAYSDX4D7I/uye2GND0Kve112FLoJLBSFoM7+CQyPu9nmQVfJSIpEYWYMDp3r5Ic78olLb7bBlrENNqDYl90L2Bxwn3xBdhSiWentldDSSiEUfuz1FVvxRljD3TB76mVHoSBnucZh9jZCY2up31Izl0CJz4T7wks8vwgg/I0ZgiqbBwEApTlxkpOQP9q0JA1dAxOo7+D8EVp4FxoGEBlmQ3ZKlOwodBMUZxTs5XdDb3gDRl+z7DhE12WO9MIa7YXKlhSf0vJWAaqd7We04PSOKsCyoGayUOSvhBCwld0Fs78VRned7Dg0RywUhaBLTQNIjHEiOTZMdhTyQ6tLk2G3KTh0rkN2FApylmXhYtMAyvLiobANNuDYK+4B7OFcVUR+Te+4BAAsFPmYsIdBy1sJT/0xWIZHdhwKYkZHJaDZoSbly45CN2ArXA/Yw+C5+IrsKDRHLBSFGNO0UN0yhEVcTUTXEebQsLokGccre+ByG7LjUBBr6x3HyLgbZWyDDUjCEQF7xT3Qm0/D6GmQHYfomoyOSoiwGCix6bKjhBxb8UbAPQG9+YzsKBTEjI5KqKnFECo3xPBnwuaArXgz9MY3YE4MyY5Dc8BCUYhp7h7FhEvHolwWiuj6NlWkYcpt4FRNr+woFMQuNE5vncz5RIHLXn43hDMKrjeekx2F6CqWZcFor4SasYibd0igpi+GCI9l+xktGHNiGOZgO1TuaBgQ7GV3AqYBT9UB2VFoDlgoCjGX5xMtyuGJGV1fUVYsEmOcOHS+U3YUCmIXGweQkRSBuCiH7Ch0i4Q9DPalO2C0X4TR1yQ7DtEM5lAHrMlhnkRKIhQFWuF6GK3nYU5y7iF5n9FZBQDQ+BoPCEpMKtTMcngq98MyddlxaBYsFIWYyqYBZCRGICbCLjsK+TFFCGxakoaq5kH0DU/KjkNByOUxUNM6zLazIGBbdDtgc8J9/g+yoxDNYLRPzyfSOJ9IGlvxRsAyoNcfkx2FgpDRXgnYw6Ak5siOQnNkL9sKa3wQetNp2VFoFiwUhRCPbqK2bZjziWhONpSnwgJw5EKX7CgUhGpah6AbJsrZdhbwhD0ctuKN0OuPce4A+RWjoxIiKglKVJLsKCFLjc+EkpADT+0R2VEoCOkdlVBTSyAUVXYUmiM1aylEZAKHWgcAFopCSEPHMNy6yflENCeJsWFYlBOHw+c7YVmW7DgUZC42DkBTFRRlxcqOQl5gL7sbMHV4Lr0qOwoRAMAyTegdVdAy2JIim61wHczeRpgjPbKjUBAxx/phjXTzNR5ghKLAtvgOGJ1VMIf4ZbQ/Y6EohFxqGoQQQEkWC0U0NxuXpKJ3aAo1rUOyo1CQudg4gOKsGDhs/BYwGCixqVCzl8JT+Sq3wia/YPY3A+4JqOlsO5NNK1gDAPCw/Yy8yOiYnk/E13jgsRVvAoQCT80h2VHoBlgoCiFVLYPITY1CuJPbR9LcrCxOhtOu4vB5VvzJe4bHXGjvG8dizicKKvbyu2FNjkCvPy47ChGMzmoAgJpWIjkJKZEJUFOKOKeIvErvuAThiIQSnyE7Ct0kJTwWatYSeGoOwTIN2XHoOlgoChEuj4GGjhGUZnM1Ec2dw65idWky3qjqwZSbuxOQd1S2XN59ke9HwUTNKIMSlw73+ZfYrkrSGZ3VENEpUCL4PuMPtIK1MAfaYAy0y45CQcCyLBgdVVDTSyEET2cDka3kNlgTQzDaLsiOQtfBV1aIaGgfhmFaKMmOlR2FAszGJWlweQycrO6VHYWCRGXTIMIdGnJSomRHIS8SQsBWvg1mfzOMrhrZcSiEWZYJvasGGlcT+Q0tfxUgBPQGriqi+bNGe2GN9UPljoYBS8teCuGMgqf6oOwodB0sFIWIqpYhCAEUZcbKjkIBpigzBslxYTh8vlN2FAoSlc2DKMmOhaII2VHIy2xF6wF7GDxVB2RHoRBmDrYDrnG2nfkRJTwWavoieOqPccUhzZvefgkAoKaXSk5Ct0qoGrSiDdCbT8OcGpUdh66BhaIQUf3mfKIwB+cT0c0RQmDjkjRUtQyhZ2hSdhwKcL1Dk+gbnuJ8oiAlNAdsheuhN5yA5Z6QHYdC1B/nExVLTkJvpRWshTXcPT1onGgejM5qiLAYKDFpsqPQPNhKNgOmAb32ddlR6BpYKAoBbo+Bhs4R7nZGt2xjeSoEgCNcVUTzVNk8PZ+olPOJgpatZDNguOGpY4sJyWF01kBEJkCJSpIdhd7ClrsSECrfG2heLMuC0VkNNa0EQnBlciBT4zOhJOXBU/0aVxr6IRaKQkB9xwh0g/OJ6NbFRzuxODcORy50weQbOc1DZfMgYiLsSE8Ilx2FFoiSmAslPpNzB0iK6ZPIKqipXE3kb4QzEmpWOfT6Y7AsU3YcClDWWB+s8QGuGAwStpLNMAfaYPZxpaG/YaEoBFS3DHI+Ec3bxiVp6BueQnXLkOwoFKAsy0Jl8yAW5cTxW8AgJoSY/uDX2wBjoE12HAox1nA3rMkRzifyU7b8NbDGB2D2NMiOQgHK6JzeLIGv8eBgK1gLKBo8tYdlR6G3mXOhaPfu3bjvvvtQUVGBHTt24IUXXrjh9U3TxH/9139h69atqKiowP3334/f/e53881Lt6C6ZQjZKVEId3I+Ed26FcVJCHOoHGpNt6yjbxwj424sYttZ0NOKNgCKylVF5HN61/R8Iu545p+0nGXT7w2NJ2RHoQBldFYDjggocRmyo5AXCEcEtOyl0ysNTa409CdzKhTt2bMHDz/8MDZu3Iinn34aa9aswSOPPIK9e/de9zZf/vKX8c1vfhMf+tCH8O1vfxtLly7F3/7t3+LAAe6E4kse3UB9xwhK2XZG82S3qVizKAUnqnsw6dJlx6EAdOnN+UQsFAU/xRkFLWc59NojsAy+X5DvTA+5jYaISZUdha5BOCKgZiyG3niSM0noluhd1dBSiyEEG2OChVa4DtbkCIyOS7Kj0FvM6RX25JNPYseOHXj88cexefNmPPHEE9ixYweeeuqpa16/paUFP/nJT/CP//iP+LM/+zOsX78e//RP/4RVq1bh4EF+u+hLDR0j0A0TJdk8MaP527QkDW6PiRNVPbKjUACqah5EUqwTibFhsqOQD9hKboM1NQq95YzsKBRCOOTW/2l5q2CN9sLsb5EdhQKMOTEEa7ibM8iCjJa9FLCFwVN3VHYUeotZC0Wtra1oaWnBtm3bZly+fft2NDQ0oLW19arbvPzyy3A6nXjXu9414/If//jH+Pu///v5JaabUtUyBAGgODNGdhQKAvnp0UiND8chtp/RTTJME1UtQ1xNFELUzHKIiDi2n5HPmKN9sMb6oaay7cyfaTnLASGgs/2MbpLRxflEwUhodmh5q6A3noClu2XHoTfNWihqaJgeNpeXlzfj8pycHABAY2PjVbeprq5GXl4ejhw5gne+851YvHgxtm3bht///vfeyEw3obpl8M35RDbZUSgICCGwqSINtW3D6B6ckB2HAkhL9xgmXToW5cTLjkI+IhQFtqINMFrPw5wckR2HQoDROT2fiCeR/k0Ji4aaWgK98aTsKBRgjM5qQHNAScyWHYW8zFa4DvBMQW85KzsKvWnWQtHo6CgAIDIycsblERERAICxsbGrbjMwMIDOzk48/vjj+NCHPoTvfOc7KCsrw//5P/8HR49ySZmveHQT9R0jKOF8IvKi9WWpEAIcak03paplej4R56WFFq1wHWCZXDlAPmF0vTnkNp5Dbv2dlrcK5lAHjMEO2VEogBidNVBTCiEUbtATbNT0RRDhsdDrXpcdhd4066vs8qC5t/d6X75cUa6uNXk8HgwMDOBb3/oW7rjjDgDA+vXr0dDQgP/8z//EunXr5hwwISFy9isFiKSkKJ8+3sWGfnh0E6vL03z+2BS4ZjtWkpKisKIkGUcvduPj714KVeEcCJpdY9cYMpMjUZiXOO/74vtZAElajNakbIjmN5C05QHZaXyKx6nvtfY1IDyrFMnJbLefK1nHqb7yNrQc+TEcPecRV8wVYHR9l49RY3IUowNtiNuyC3F8fw1K/eWbMHxyL+IjBdSwwKoBBOPv/FkLRVFR00/67SuHxsfHZ/z8rSIiIqCqKjZu3HjlMiEENmzYgF/+8pc3FbC/fwymGfi7IiQlRaG3d9Snj3n8/PS3NCnRDp8/NgWmuR6na0qTcbKqB6+daEZ5XoIPklEgM0wTF+r7sK4sdd7vRTLeS2l+RO5qTL3xHLobGqFEzb9QGAh4nPqeNTUGT18bRN5a/t3Pkdzj1A4luQDDF45AL9kuKQP5u7ceo3rzaQAWpqJz+RoPUkbGSuD4bnSd2A976RbZceYsUH/nK4q44aKcWVvPLs8mammZuTNBc3PzjJ+/VU5ODkzThK7P3BLX4/FwFwofqmkbQkZiBCLDOJ+IvGtZYSIinBoOnWP7Gc2upXsMU26DbWchylYwvYrYU39MchIKZkZPPQBATSmUnITmypa3CmZ/M8yRXtlRKADondWAokFNuvrck4KDkpgLEZMKnbuf+YVZC0U5OTnIzMzE3r17Z1z+0ksvITc3F+np6VfdZvPmzbAsC3v27Llyma7rOHjwIFauXOmF2DQb07RQ1zaMoqxY2VEoCNk0BWsXp+BUTR8mpjyy45CfuzyfqITvRyFJiU6CklIIvZ4f/GjhGN11gFCgJufLjkJzpOWtAgDoTZxhRrMzOmugJudDaHbZUWiBCCFgK1gDo7OKm2D4gVkLRQDw0EMPYffu3fjiF7+I1157DV/4whewZ88efO5znwMwPbz6zJkzV9rT1q9fjy1btuCf/umf8KMf/QgHDx7EZz/7WbS3t+Ohhx5auGdDV7T2TH+DX5zJPn1aGJsq0qAbJo5V9siOQn6uumUIaQnhiIl0yI5CktgK1sHsb4Ux0C47CgUpo7sOSmIOhMb3mUChRCdBSciBh7uf0SwsjwtmXxPU1GLZUWiBafmrAcviJhh+YE6Fop07d+KJJ57AoUOH8NBDD+H48eP46le/invvvRcAsH//fjz44IO4ePHildt84xvfwK5du/Df//3feOihhzA4OIjvfve7KC8vX5hnQjPUtA4BAIr5DT4tkJyUKGQmRbD9jG7IME3UtA6hJDtOdhSSSMtfDQiFq4poQVimDqOnnm1nAUjLWwmzuw7m+KDsKOTHjN4GwDKhpvI1HuyUuEwoManQG96QHSXkzXlvwV27dmHXrl3X/NnOnTuxc+fOGZc5nU488sgjeOSRR+aXkG5JTdsQEmOciI92yo5CQUoIgY1L0vDzfXVo7xtHRmKE7EjkhzifiABACY+BmrEYnrqjsK/ayXmF5FVmfxugu1koCkBa3iq4T/wKetNJ2Mvukh2H/JTRVQsAUJP5Gg92Qgho+avhPrMb5uQIlLBo2ZFC1pxWFFFgsSwLNa1DXE1EC259WSpUReAwVxXRdVS3DAHgfCICbIXrYI32wuxtkB2FgozR/eZJZEqR5CR0s9S4dCix6dDZfkY3YHTXQolLh3AG1pbpdGu0gjVsP/MDLBQFoa6BCYxOeFgoogUXHWFHRUECjlzohG6YsuOQH6pqGeR8IgIAaLkrAEWFh8vJycuMrlqIiHgokfGyo9At0PJWcngtXZdlmTC661gIDiFsP/MPLBQFodq2YQBAEQdZkw9srkjHyIQH5xv6ZUchP2OYJmrbhriaiAAAwh4ONbMcesMbsCxLdhwKItMnkWxJCVRa3qrp1QPNp2VHIT9kDrYD7kmoqSwUhYrL7WcsIMvFQlEQqm4ZQnS4Danx4bKjUAhYUhCP6Ag7h1rTVVq6xzDpMjjImq6w5a+GNdYPs7dRdhQKEuZYP6zxAZ5EBjAlIRsiKontZ3RNV+YTcUVRSNHy2X4mGwtFQai2bQhFWbEcFko+oSoKNpSn4lx9P4bH3bLjkB+5Mp+Ig6zpTVrOcrafkVcZ3fUAeBIZyIQQ0+1n7RdhucZlxyE/Y3TVQoRFQ0Qny45CPqTEZ0Kw/UwqFoqCzMDIFPqGpzifiHxq05I0GKaF1y90yY5CfqS6ZRAp8eGI5XwiepNwREDNKIPeyPYz8g6juxbQ7FASMmVHoXmw5a0CTAN6y1nZUcjPXJ5PxC/AQ4sQAja2n0nFQlGQqWkdAgAUZ8ZKzUGhJT0xAgXp0Th0vpMnfwQAME0LNW3DnE9EV7Hlr4Y12gezr0l2FAoCRncd1KR8CEWTHYXmQUnOhwiPZZsJzaCPDsIa7YWayhlkoYjzy+RioSjI1LYNw2lXkZXM7SPJtzZVpKGjbxyNnaOyo5AfaOsdw6RLZ6GIrqLlLAeEyuXkNG+WxwWzr5nziYKAEAq03JXQWy/A0l2y45CfmGqrAsDW0lClJGRDRCZAbzolO0pIYqEoyNS2DaEwIwaKwuWZ5FtrFqXArik4eK5DdhTyA9Vvrm7kfCJ6O+GMhJq5GB7ufkbzZPQ2AJbJHc+ChJa3EjDc0FsvyI5CfmKqrQpQbVASc2VHIQmEENByV8JouwjLPSk7TshhoSiITEx50N47jsLMGNlRKASFOTSsLEnG8cpuuDyG7DgkWU3rEBJjnIiPdsqOQn5Iy1sFa7QXZn+z7CgUwIzuOgCAmlwgOQl5g5pWDDgiuHqArnC1VkFNyoNQ2VoaqrTcFYCpQ287LztKyGGhKIjUtY/AAlDE+UQkyeaKNEy6DJyq7pUdhSSyLAs1rUMcqk/XZctdCQiF7Wc0L0Z3HZTYdAgn2+2DgVA0aNlLobecgWXqsuOQZJbugqu7ka2lIU5NLYZwRkFvZAHZ11goCiK1bUNQFYH8tGjZUShElWTHIjk2jO1nIa6zfwKjEx4Wiui6hDMSavoieBpPsP2MbollmdODrDnkNqhouSsB1ziMzhrZUUgyo6cBMA3OJwpxQlGg5SyD3nIWlsECsi+xUBREatuGkZ0SBYddlR2FQpQQAhsr0lDVMoSeIfYSh6rLuy9ykDXdiJa3EtZwN8xBFpbp5plDXYBrnCeRQUbLKgdUO/Smk7KjkGRGdz0AcAYZTbefeSZhdFTKjhJSWCgKEh7dRGPnCIo4n4gk21ieCgHg8LlO2VFIkurWIcRE2pEcFyY7CvkxLXcFAMETQrolRnctAJ5EBhuhOaBllkFvOs3VhiHO6K6DLZ6tpQSoGWWA5uD8Mh9joShINHePwqObLBSRdPHRTpTlx+PwhU6YJj/khZrL84lKsmIhBHdfpOtTwmOhpBRAb2ShiG6e2V0H4YiEiEmVHYW8TMtbCWt8AGZfk+woJIllWTB76uHILJYdhfyA0OzQspZAbz4NyzJlxwkZLBQFibq2YQBAIQdZkx/YXJGOgREXLjUPyI5CPtY7PIXBURfnE9Gc2PJWwuxvhjnCAfh0c4yuWigphSxIByEte9n0sHsWkUOWNdIDa2oUzowS2VHIT2i5K2BNDMHsaZAdJWSwUBQkatuGkBIXhpgIu+woRFhWmIjIMBsOnmX7WaipaRkCABaKaE603JUAwPYzuinm1CjM4S4Osg5SwhkJNa0EejPbTEKV0V0HAHBkcEURTdOylwJCZfuZD7FQFAQsy0Jt2zCKuJqI/IRNU7CuLAWna3sxNumRHYd8qLp1EJFhNqQnRsiOQgFAiU6GkpDFD350U8wrQ245yDpYabkrYA52TA8tp5Bj9NQDNifsSVmyo5CfEI4IqOml8DSd5PwyH2GhKAh0DUxgbNKDQs4nIj+yuSIdumHh9Yv8kBdKalqHUJQZA4XtIDRHWu4qGF21MCeGZEehAGF01wFChZqUKzsKLZDpYfeAh0XkkGR010FNzodQuJMz/ZGWu2J6t9Qh7pbqCywUBYHaN+cTcZA1+ZOs5Ejkpkbh4NlOVv5DxOCoC71DU2w7o5ui5a0EYEFvOi07CgUIo7sWSmIOhOaQHYUWiBKZACUxl22pIcjyTMEcaIWaXCA7CvmZywVkrkL2DRaKgkBt2xAiw2xIjQ+XHYVohs0VaWjrHUNz96jsKOQDNa1DADifiG6OEpcBEZ3CE0KaE8vUYfQ0QE3hfKJgp+WugNlTz9WGIcbobQQsC2oKC0U0kxIRByUpn4UiH2GhKAhMzyeK4c4f5HfWLk6BTVM41DpE1LQNwWFXkZ0SKTsKBRAhBGx5K2G0V8JyjcuOQ37O7GsBDA8HWYeAPw6750lhKDEuzyBL5mucrqblrYDZ2whzrF92lKDHQlGAGx53o2dwkvOJyC+FO21YVZKEo5e64fYYsuPQAqtpHUJhRgxUhb9a6OZoeSsBy4DeclZ2FPJzl3dD4iDr4KfEpb+52pCFolBidNdBxKRCOPmlE13tj+1nbFdfaPw0H+DqLs8nyoiVG4ToOjZVpGPSpeNkTa/sKLSAxiY9aO8dRzGL1nQLlKQ8iIg46I1sP6MbM3rqISLioUTEyY5CC0wIAS13BYyOSljuCdlxyAcsy4LZU8+2M7ouNTYdSmwa9GYWkBcaC0UBrq59CJqqICc1SnYUomsqyY5FUqwTB89yh4JgdrlozflEdCuEUKDlrIDeeh6W7pIdh/yY0dMANTlfdgzyEVveSsDkasNQYY30wJoaZdsZ3dB0AbkK1tSY7ChBjYWiAFfXNozctCjYNP5Tkn9ShMCminRUtQyhZ5DfCAarmtYhaKpAfnq07CgUoLS8lYDhht56QXYU8lPmxDCs0V6uNgghSnI+RFgM289CxB9bS1koouvTclcClskC8gJjdSGAuT0GmrpGUZTBVg/ybxvLUyEEcOg8h1oHq5q2IeSmRcOmqbKjUIBS00oARwT0xhOyo5CfMnsaAAAKVxuEDCEUaLnL31xt6JYdhxaY0VMP2JxQ4jJkRyE/piTlQoTHsoC8wFgoCmBNXaMwTIuDrMnvxUc7UZ6XgMPnu2Caluw45GUut4HmrlGUsO2M5kEo6nT7WcsZWIYuOw75IaOnHhAq1MQc2VHIh7TcFYBnCkbHJdlRaIEZ3fVQk/IguCkG3cB0AXkFC8gLjK/CAFbbNgQAKOSKIgoAmyvSMDjqwoVGbmcZbOo7hmGYFooyY2VHoQBny1sBuCdhdFbJjkJ+yOiph5KQBaHZZUchH1LTFwO2MOiNXD0QzCzdBXOgFWoyW0tpdlruCsBww2i/KDtK0GKhKIDVt48gNT4cUeH8wET+b1lRIiLDbDh4ju1nwaamdQhCsGhN86dmlAGag+1ndBXLNGH0NvIkMgQJVYOWXQG9+TQs05QdhxaI0dcMWCYUDqunOVDTSqcLyE2nZUcJWiwUBSjLslDXPsy2MwoYmqpgQ3kqztT2YWSCy0SDSW3bMLKSIxHu1GRHoQAnNDu07KXQm07xhJBmMIfaAc8UB1mHKC13JaypURjdtbKj0AIxe+oBgLsa0pwIVYOWtWS6XZ2fFxbEnAtFu3fvxn333YeKigrs2LEDL7zwwg2v/+tf/xolJSVX/ffFL35xvpkJQNfABMYmPRxkTQFlc0UaDNPC6xe6ZEchL9ENE/Xtwyhm2xl5iZa7AtbkCIyeOtlRyI8Y3ZdPIlkoCkVa1hJA0Ti8NogZPQ0QkQlQwmNlR6EAcfnzwuUiI3nXnL7+3bNnDx5++GF85CMfwebNm/Hyyy/jkUcegdPpxD333HPN21RVVSEnJwdf+9rXZlyemJg4/9SE2rZhAOCKIgooGUmRyE+PxsFzndi2OgtCCNmRaJ6au0bh1k0Uc5A1eYmWvXT6hLDxJLTUYtlxyE+YPfUQjkiI6GTZUUgCYQ+DmrF4erXhul38/BCEjJ4GFoLppmhZSwChQm8+DTW1SHacoDOnQtGTTz6JHTt24PHHHwcAbN68GcPDw3jqqaeuWyiqrq5GWVkZli1b5rWw9Ed1bcOIDLMhNT5cdhSim7K5Ig0/2FuNho4RFHBFXMCreXOofhELReQlwh4GNbMMetNJnhDSFUZPPZTkfB4PIUzLWwnXa9+bHnickC07DnmROTEEa6wfavndsqNQABGOCKjppdCbTsGx9n2y4wSdWVvPWltb0dLSgm3bts24fPv27WhoaEBra+s1b1dVVYWSkhLvpKSr1LYPozAjhh+YKOCsWZQCu03BwXMdsqOQF9S0DCElPhwxERyqT95jy10Ja7QPZn+L7CjkByz3BMzBTs4nCnFa9jIAgu1nQcjoaQAAKFxRRDdJy1kOc7gL5hA3y/G2WQtFDQ3TL9y8vLwZl+fk5AAAGhsbr7pNT08P+vv7cenSJdxzzz0oKyvD9u3bZ51rRHMzMuFG98AE284oIIU5NKwuScaxyh5MuXXZcWgezDeH6pdk8b2IvEvNWQYIwd3PCABg9DQCsNiWEuKU8BioqUXQm07KjkJeZvY0AEKFmpgjOwoFGC13OQDAw93PvG7WQtHo6CgAIDIycsblERERAICxsbGrblNVVQUAaGtrw+c//3l8+9vfxpIlS/DII4/gueeem3foUFff/uZ8IrbtUIDavDQdLreBN6p6ZEeheejoHcf4lI4iDrImL1PCoqGmlnDlAAGYbjsDBHdDImi5K2D2t8Ic6ZUdhbzI6KmHkpAJoXF1Mt0cJTIBSkIO9GZ+XvC2WWcUWZYFAFe1OF2+XFGurjWVl5fjW9/6FlavXn2lwLRp0yb09/fjqaeewnve8545B0xIiJz9SgEiKSnKK/fTcawFmiqwekk67DbVK/dJdJm3jtMbSUyMRMZL1Th6qQc7t7JFNVAdr+kDAKxbmoGkhAifPa4vjlGSb3jJRvS/9L+IUUZgT8iQHeem8Tj1nq6hZliJGUjOSJEdJegE2nHqWbEZrUd/BkffRcQW3C87DnmBZRpo6mtEVPkWJF7jeAy0Y5R8T1u8DoMHf4G4MANaZKyUDMF4nM5aKIqKmn7Sb185ND4+PuPnbxUfH4877rjjqsu3bNmCI0eOYGBgAPHx8XMK2N8/BtO05nRdf5aUFIXe3lGv3NfZ2l7kpERheGjCK/dHdJk3j9PZrC9LxS/31+NcVRfSfFhkIO85VdmFuCgHFMPw2XHjy2OU5DITFwMAek69Bseyd0hOc3N4nHqPZVmYbK2GmrOcf6deFpjHaQSU+CwMX3wdnvzbZYchLzAG2mC5p+COzrrqeAzMY5R8zUhaDMBC9+lDsJdu8fnjB+pxqijihotyZm09uzybqKVl5kDJ5ubmGT9/q9OnT+PZZ5+96nKXywVN065ZXKK58egmmjpHOZ+IAt7G8lQoQuDQOQ6fC0SWZaGmdQjFWbEcqk8LQolMgJKUB72R80hCmTXaC8s1xkHWdIWWuwJGVw3MiWHZUcgLpltLAYWtpXSLlIRsiMgE6JxT5FWzFopycnKQmZmJvXv3zrj8pZdeQm5uLtLT06+6zZkzZ/D3f//3V2YVAYBpmnjxxRexYsUK2Gw2L0QPTc3do9ANk/OJKODFRDpQUZCAwxe6oBum7Dh0k3qHpzA05kYxi9a0gLS8lTB7G2GO9cuOQpIY3XUAwEHWdIWWvwqwLM4wCxJmTwNgD4MSkyo7CgUoIQS0nOUw2i/C8rhkxwkasxaKAOChhx7C7t278cUvfhGvvfYavvCFL2DPnj343Oc+BwAYGBjAmTNnrrSn7dy5ExkZGfjMZz6D3bt349VXX8UnP/lJ1NTU4OGHH164ZxMC6to4yJqCx+alaRgZd+N8PU8CA01t6xAAoCgrVmoOCm623FUAwBPCEGb01AOaA0pc4M2pooWhxGVCxKRCb3xDdhTyAqOnAWpyAYSY02kp0TVpuSsAwwO9/YLsKEFjTq/InTt34oknnsChQ4fw0EMP4fjx4/jqV7+Ke++9FwCwf/9+PPjgg7h48SIAICYmBj/60Y9QUVGBf/mXf8Ff//VfY2JiAt///vexdOnShXs2IaCufRhJsU7ERDpkRyGat4qCBMRE2HGQ7WcBp6Z1CBFODemJnC9FC0eJTYUSl872sxA2fRKZD3GNzVMoNAkhYMtbBaOjCubkiOw4NA+WZwrmYBt3NKR5U9OKAXs428+8aNZh1pft2rULu3btuubPdu7ciZ07d864LCMjA08++eT80tEMlmWhrn0YZblzGwRO5O9URcGGJal48VgrhsZciGUBNGDUtA6hKDMWCucT0QLTclfCfWY3zMkRKGHRsuOQD1m6G2ZfC+xL75EdhfyMlr8a7jO7oTefljK8lrzD6G0CLIuFIpo3oWjQsitgtJyFZZr8csEL+DcYQHqHJjEy7uYgawoqmyvSYVoWDp/nqqJAMTzmQvfgJIrZdkY+oOVNzyMxms/IjkI+ZvQ1A5YBhfOJ6G2UhGyI6GToDWw/C2RGTwMAQElioYjmT8tZAWtqFEZ3rewoQYGFogBS++Z8oiLOJ6IgkhofjuLMGBw61wnLsmTHoTm48l6UxfciWnhKQjZEVCI8TWw/CzXmm7shcbUBvZ0QArb81TDaL8GaGpMdh26R2VMPEZXE1aLkFVrWEkBRoTez/cwbWCgKIPXtwwhzqJwJQkFn89J0dA9OXilAkH+raR2C3aYgJyVKdhQKAUIIaLkrYbRdhOWelB2HfMjoqYeISoQSHis7CvkhLW81YJkcdh/AjJ567mhIXiPsYVDTF0FvOs0vn72AhaIAUts+jIL0GCgKZ4JQcFlVkgynXcXBsx2yo9Ac1LQOoSA9BprKXyHkG1reSsDUobeclR2FfMjo5kkkXZ+SmAMRlQRP4wnZUegWmGMDsCaGuGKQvErLXQFrpBvmEM8p5ouf8gPExJQHHb3jKGTbGQUhh13F2sUpeKO6B5MuXXYcuoGJKR2tPWOcT0Q+pSYXQoRFc+VACDHHB2GND/Akkq5LCAEtbxWM9ouwXOOy49BNMthaSgtAy1kOANz9zAtYKAoQ9R0jsAAOsqagtbkiHW6PiWOV3bKj0A3UtQ/BAlgoIp8SigItZwX0lrOwdLfsOOQDfzyJ5Ioiuj5b/mrANDiTJAAZPQ2AokJJyJYdhYKIEhEHJSkPejO/WJovFooCRG3bMIQA8tI47I2CU15aFDKSInDwLHc/82fVrUNQFYH8dL4XkW9peSsB3QWj/aLsKOQDZk8DoGhQEnNkRyE/piTlQUQmwMPdzwKO2dswvVmBZpcdhYKMlrMcZk8DzIkh2VECGgtFAaK+fRhZyZEIc2iyoxAtCCEENleko7FzBG293MHEX9W0DiEvLRoOmyo7CoUYNX0RYA+Dp5HfEoYCo6ceSmI2hGqTHYX82JX2s7YLsNwTsuPQHFmmAaO3kW1ntCC03Dfbz5rPyA0S4FgoCgCGaaKhYwRFGbGyoxAtqPVlKVAVwVVFfsrlMdDUOYqiLLbAku8JVYOWvQx68ylYpiE7Di0gyzRg9DSy7Yzm5Er7GWeSBAxzsB3Q3XyN04JQ4jIhopI413CeWCgKAK09Y3B5DBRkstWDgltUuB3Li5Pw+sUueHRTdhx6m4aOERimhRLOJyJJtLxVgGscRkeV7Ci0gMyBNsDgSSTNjZKcDxERD527nwUMo6cBAGeQ0cIQQkDLWQ6j4xIsz5TsOAGLhaIAUNs2DABcUUQhYXNFGsYmPThT1yc7Cr1NTesQBIBCvheRJFrWEsDmhN5wXHYUWkAcZE03QwgFWt5K6G3nYbknZcehOTB76iEckRDRybKjUJDScpcDhg699bzsKAGLhaIAUN8+jLgoBxJinLKjEC24stx4xEc7cPBsh+wo9DY1rUPISolEuJOz0kgOodmh5SyD3ngSlqnLjkMLxOiphwiLhohKlB2FAoSWv2b6pLDljOwoNAdGT8P0SjAhZEehIKWmFgOOCO6IOA8sFAWA2rZhFGVyJgiFBkUR2FiehouNA+gf5nJRf6EbJurbh1GcGSs7CoU4LX8NLNcYjPZK2VFogZjd9VCTC3gSSXOmphRAhMdCb2D7mb+z3JMwBzs4yJoWlFBUaNlLobec5VzDW8RCkZ/rH57C4KgLhRksFFHo2FSRBgvA4fMcau0vmrtG4dZNFHM+EUmmZZYDtjC2nwUpa2oM5nAXFLad0U2Ybj9bBb31HGeS+DmjtxGAxUIRLTgtZ/n0XMOuGtlRAhILRX6utn0IAFDEb/EphCTFhmFRThwOne+EaVmy4xCm284AsFBE0gnNDi13OTyNJ2EZbD8LNkbv5SG3PImkm6PlrwYMD/SWs7Kj0A1cmUGWxNc4LSwtawmgatwR8RaxUOTn6tqG4bCpyEyOkB2FyKc2L01D3/AUKpsHZUchANWtQ0hLCEd0hF12FCLY8tcA7gkY7RdlRyEvM7rrAQioSXmyo1CAUVOKIMKioTe8ITsK3YDZ0wARkwLhjJQdhYKcsDmhpi+G3nwaFr94vmksFPm5uvZh5KdHQ1X4T0WhZWVxEiKcGoda+wHTtFDbNszVROQ31MwywB4GD9vPgo7R2wAlPgPCHiY7CgUYoSjQ8lZPzyTh7md+ybIsGD31XE1EPqPlroA12gtzsE12lIDD6oMfm3TpaO0Z43wiCkk2TcW6xak4VdOLsUmP7Dghra13DJMunYOsyW8I1QYtdyX0plOwDL4/BAvLMmH0NEDlfCK6RVrh2un2M+505JessX5YkyN8jZPPaDnLAIDtZ7eAhSI/1tA5AssCdzyjkLV5aRp0w8LrF7tkRwlp1S1DAICS7FipOYjearr9bBJG2wXZUchLrOFuwDXOk0i6ZWpKIUREPDx1R2VHoWswejiDjHxLCY+FkpzP4vEtYKHIj9W1DUMAyE9noYhCU3ZKFHJSo3DwbCd7iyWqbh1CYowT8dFO2VGIrlAzFgOOCHjqj8mOQl5yecgtdzyjWyWEAq1gLYy2i7CmxmTHobcxeuoBVYOSkC07CoUQLWcFzN5GmOOce3ozWCjyY3VtQ8hIikS4U5MdhUia2yrS0NY7hqauUdlRQpJpWahuGeRqIvI7QtVgy1s93X7G7bCDgtFdD9jCoMSlyY5CAcxWuA6wDHgaT8iOQm9j9jRAScyFUHluQ76j5S4HAK4qukksFPkp07RQ3zGCQradUYhbuzgFNk3hUGtJOnrHMT6loyQrTnYUoqtoResB3c0Pf0Fiej5RPoTgx1O6dUpCNkRMKnS2n/kVy9Rh9DVxkDX5nBKbDhGdAr3plOwoAYW/if1UW+8YptwG5xNRyAt32rCqJBnHKrvh8hiy44Sc6tYhAEApVxSRH1JTi6bnkdS+LjsKzZPlccEcaOXsEpo3IQRsBWthdFaz1cSPmANtgOHha5x8TggBLXc5jI5K7oh4E1go8lO1bcMAOMiaCABuW5qGSZeBE1U9sqOEnOqWQSREO5AYy62qyf8IocBWuA5G2wWYkyOy49A8GH1NgGVCTeF8Ipo/W+E6ABb0+uOyo9CbjO7pGWQcVk8yaDnLAdOA3npedpSAwUKRn6ptG0JclAMJHB5LhOKsWCTHhbH9zMcsy0J16xCK2XZGfkwrWg9YJvQGnhAGsssnkRxkTd6gxKZBSciBp57tZ/7C6GmAcEZBRCXKjkIhSE0pgnBGQW9m+9lcsVDkhyzLQm3bMIoyYyCEkB2HSDohBDZXpKGmbRhdAxOy44SMjv4JjE54OMia/JoanwUlLpPbYQc4s6cOIjoZijNKdhQKErbCtdM7HQ13y45CAMyeeijJ+Ty3ISmEokDNXgq95RwsU5cdJyCwUOSH+kemMDjqQlFmrOwoRH5j45I0KEJwVZEP1bRMz3bgfCLyd1rROpjddTBHemVHoVtgWRaM7jq2pJBXaQVrAQgWkf2A5RqHOdzF1zhJpeWuANwTMDprZEcJCCwU+aE6ziciukpspANLCxNw+HwndMOUHSckVLVMt8AmcT4R+TlbwToAgKeOQ60DkTXaB2tyBGpKoewoFESUyASoaSXw1B2BZVmy44Q0o7cRADjImqTSMssA1Q698aTsKAGBhSI/VNs2DKddRWZSpOwoRH5l89J0jEx4cLauX3aUoHd5PlFJViyXiZPfU6ISoaYWQ687yhPCAGT01AEAC0XkdbaiDbCGu2H2NsiOEtKMnjcHWSflSU5CoUxoDmhZS6A3nYRl8Uvn2bBQ5Idq24ZQkBEDReHJGdFbLcmPR2ykHQfPsf1soXUNTGBk3I1itp1RgNCKNsAc6oD55jfXFDiM7jpAc0CJz5QdhYKMlr8KUG3w1B6RHSWkGT0NUGLTIRwRsqNQiNPyVsKaGIL55gYKdH1zLhTt3r0b9913HyoqKrBjxw688MILc36Qzs5OrFy5Et/85jdvJWNImZjyoL13nG1nRNegKgo2VaTjfEM/BkamZMcJatWtQwCAkqxYqTmI5spWsGb6hLDmkOwodJOM7nqoyfkQiio7CgUZYQ+HlrMcev1xDrCVxLIsmD0NUNh2Rn5Ay1kGKCo8jSdkR/F7cyoU7dmzBw8//DA2btyIp59+GmvWrMEjjzyCvXv3znpby7Lw+OOPY2xsbN5hQ0Fd+wgsAEUZLBQRXcvmijRYFnDofKfsKEGtumUIMRF2pMaHy45CNCfCHg4tbyU8dUdh6W7ZcWiOLN0Fs7+VQ25pwdiK1sOaGoXRekF2lJBkjfTAmhrla5z8grCHQ80sh954gq3qs5hToejJJ5/Ejh078Pjjj2Pz5s144oknsGPHDjz11FOz3vanP/0pGhrYFzxXtW1DUIRAfjoLRUTXkhQbhsW5cTh4thMm3+AXhGVZqGweRGlOHOcTUUCxFW8G3BPQm0/LjkJzZPQ2AZbB+US0YNSsJRCOSLafSWJ0cwYZ+Rdb3ipYY/0w+5plR/FrsxaKWltb0dLSgm3bts24fPv27WhoaEBra+sNb/v//t//w5e+9KX5Jw0RtW3DyEmNhMPO5ddE13Pb0nT0j0yhsmlQdpSg1Nk/PZ9oUU6c7ChEN0XNWAQREc/2swBy+SRSSeFqA1oYQtGgFayF3nwalntCdpyQY/TUAzYnlLgM2VGIAABaznJAKNDZfnZDsxaKLq8GysubOaU+JycHANDYeO2hkaZp4tFHH8WOHTtw2223zTdnSNANE42dIyjKjJUdhcivLS9KQmSYDQfOcqj1Qqhsni7AlbJQRAFGCAW24o0w2i7AHGchORCY3XUQMSlQnFGyo1AQsxVvAAwPt8WW4I8zyLiHEvkH4YyEmr4InsY32H52A7O+YkdHRwEAkZEzt2qPiJieWn+92UM/+MEP0Nraiscee2y+GUNGc9coPLqJQs4nIrohm6ZgQ3kqTtf0YmScs0i8raplEAnRDiTFOGVHIbpptpLNgGXBU3tYdhSahWVZMHrqoSazJYUWlpKUDxGTwtWGPmZ5XDAHOIOM/I+WtxLWcDfMwTbZUfyWNtsVLlfZ3j6n4vLlyjWqww0NDfj3f/93fOMb30BU1Py+IUpIiJz9SgEiKenGfxcHL3QBANYtzUBcNE/QSI7ZjlN/8a47ivDSG6042ziAnXcUyY4TNEzTQk3rENaUpSI5OVp2nGsKlGOUJEmKQkfWIhh1R5B41y5pc7Z4nM7OM9iFsckRxBaUIZp/X1KE0nE6uOxODB54BrG2CdhiU2THCQmTzc0Ys0zEFy1B+C0ea6F0jJLv6CtvQ8uhH8HefR7xJYvnfX/BeJzOWii6XOh5+8qh8fHxGT+/zDAMPProo7jnnnuwceNG6Poft6I0TRO6rkPTZn3YK/r7x2Cagb8kLCkpCr29oze8zumqHqTEhUF3edDb6/FRMqI/mstx6i+cClCUGYPfH27EprIUDl32kpbuUYxOeJCbHOmXx0IgHaMkUf4GeA78L7ovnIaa6vtCMo/TufHUngUATERkwsW/L58LtePUzFgF4GfoPvoSHCvfJTtOSHDVnAcAjDnTMX4Lx1qoHaPkSxrU1CKMXDgCY9G987qnQD1OFUXccFHOrK1nl2cTtbS0zLi8ubl5xs8v6+zsxNmzZ/HCCy+grKzsyn8A8B//8R9X/p9mMi0LtW1DKMqKlR2FKGBsWZaO7sFJVLcMyY4SNKrenE/EQdYUyLT81YDNCXfVftlR6AaM7ro3h9xmyo5CIUCJTICasRiemsOwLFN2nJBgdtdDxKRAOIOnQ4SCh5a3CuZgG8yhLtlR/NKshaKcnBxkZmZi7969My5/6aWXkJubi/T09BmXJycn45e//OVV/wHA+9///iv/TzN19I1jfEpHMQdZE83ZqpJkhDs0DrX2oqqWIaTEhSGe7a8UwITNCVvhOuj1x2G5xmXHoeswuuuhJuVxyC35jK1kE6zRXhidNbKjBD3LsmB013EGGfktLW8lAMDD3c+uaU49YA899BAee+wxxMTE4Pbbb8e+ffuwZ88efP3rXwcADAwMoKWlBYWFhYiMjMSSJUuueT/JycnX/Vmoq20dAgAUZ3GQNdFc2W0q1pen4sCZdoxNFiMyzCY7UkAzTBPVrYNYs4izGyjw2RbdAU/lfnhqj8BefrfsOPQ2l4fc2pfdJzsKhRAtdwVgC4On5iC09FLZcYKaNdoLa2oUagoHWZN/UiIToCTnQ288Acfyd8iO43fm9BXOzp078cQTT+DQoUN46KGHcPz4cXz1q1/FvfdO9/Pt378fDz74IC5evLigYYNZTdswYiPtSIoNkx2FKKBsWZoO3bBw5Hyn7CgBr7lrDJMug21nFBTUxBwoSfnwXHqV29/6IaO3AbBMqClcbUC+IzQHbAVroDecgOWZkh0nqBnddQDA1zj5NVveKph9TTBHe2VH8Ttzniq9a9cu7Nq165o/27lzJ3bu3HnD21dXV99cshBiWdO7DBVnxXIgL9FNykyOREF6NA6c7cDdq7P4GpqHqpbp+UQl2SwUUXCwL74DUwf+F0ZXDbS0Etlx6C2M7noA4LbZ5HO24k3wVB2A3vAGbCWbZccJWkZ3PaA5oMRlyI5CdF1a3iq4jv0CeuMJ2Ct2yI7jV9gU7gf6hqcwOOpCEecTEd2S25amo7N/ArVtw7KjBLTK5kFkJEYgJsIuOwqRV2gFawB7GDyVr8qOQm9jdNdBiUnlkFvyOSWlECImBZ6aQ7KjBDWjpx5qcj6EosqOQnRdSnQylIRseBpPyo7id1go8gM1V+YTxUrNQRSo1ixKQZhDw/7T7bKjBCyPbqK2bQilXE1EQURoDtiKNkJvOAFzKvC2rg1WlmXB7KmHwpYUkkAIAVvxJhid1TCHu2XHCUqW7oLZ38oVgxQQtLxVMLvrYI4Pyo7iV1go8gO1bUMId2jISIqQHYUoIDnsKjaUpeJEdQ9GJ9yy4wSk+vZhuD0mFueyUETBxbboDsDUoVdz9YC/sEZ63hxyy0IRyWEr3gQIBZ6qA7KjBCWjtwmwDA6ypoCg5a0CAOhcVTQDC0V+oKZ1GEWZMVA4W4Xolm1ZPj3U+vD5LtlRAtLFpgEoQnA+EQUdNT4Damox3Jf2wTJN2XEIbx1yy5NIkkOJiIOWvRSemkOwTF12nKBzeQaZwhVFFADUuHQocenQG0/IjuJXWCiSbGTcja6BCbadEc1TZlIkCjNjcOBMO0zucHTTLjUNID89GuHOOe9xQBQwbOV3wRrthdFyVnYUwvTsEticUGI55JbksS3aAmtyBHrzGdlRgo7ZUwcRnQIlLFp2FKI50fJWweiqhjk5IjuK32ChSLLL84mKWCgimrc7lmWge3ASVc3sMb4ZY5MeNHWOsu2MgpaWuxIiIh7uCy/JjkKYXlGkJhdAKPwYSvKomRUQEfHwVO6XHSWoWJY1/RrnikEKIFreasCy2H72FvwNLVlN2xDsmoLc1CjZUYgC3qrSJEQ4OdT6ZlU1D8ICUJYXLzsK0YIQigpb2V0wOiph9LfKjhPSLM8UzIFWnkSSdEJRYCvZDKPtIszRXtlxgoY11gdrcoSDrCmgKPGZUGJSoTcclx3Fb7BQJFlt6zDy06OhqfynIJovm6Zi45I0nK7tw/CYS3acgHGpaQBOu4q8NC4Rp+BlL70N0OzwcFWRVEZvI2BZUJM5yJrks5VsBgB4qg9KThI8Ls8n4rB6CiRCCGgFa2F0VMGcGJIdxy+wOiHRxJQHLd2jHB5L5EVblqXDMC28dq5TdpSAcbFpAKXZcSxYU1ATzkjYijfBU/c6ZxBIxEHW5E+UqESoWUvgqT4IyzRkxwkKRncdoNmhxGfKjkJ0U7SCtQAs6A1vyI7iF3hWIFFN2zAsACWcT0TkNWkJEViUEzc91NrkUOvZ9AxNondoim1nFBJs5XcBhg5P5auyo4Qso7sOSmwahCNCdhQiAICtdAus8UEYredkRwkKRk891KQ8CEWVHYXopqhx6VDis+CpPyY7il9goUii6pZBaKpAfjrbPYi86c4VGRgYceFsXZ/sKH7vUuMAAHCQNYUENTZ9evXAxX2wDI/sOCHHsiyYPQ1sSSG/ouUshQiLhrvygOwoAc/S3TD7Wvgap4ClFayB2V0Hc6xfdhTpWCiSqLplCPnpMbDbWHEn8qZlRYmIi3JgH4daz+pi0wDiox1IjQ+XHYXIJ+xLtsOaHIZe+7rsKCHHGumGNTUKhSeR5EeEok0PtW49C3Ocu6bOh9HXBFgGZ5BRwLIVrAUA6PUcas1CkSSTLh3N3aNsOyNaAKqiYMuydFxsHED3wITsOH7LNC1UNQ9icW48hBCy4xD5hJpRBiUhB66zv4dlmrLjhJQr84l4Ekl+xla6BbAsDrWeJ/PN17jCGWQUoJToZChJefBw9zMWimSpbRuCZQGl2bGyoxAFpduWpkNVBF7lqqLrauoaxfiUjrJcziei0CGEgH3ZfbCGu6A3nZQdJ6QYXTWAIwJKXJrsKEQzKNHJUDMWw1N1AJbFAvKtMrrrIaKSoIRxrAYFLlv+Gpi9jTCHu2VHkYqFIkmqW4agKgL5GTGyoxAFpdhIB1YUJ+HQuU64PNzJ5FouNPZDAFjE+UQUYrS8VRDRKXCf+R0si0PvfcXoqoOaUggh+PGT/I+tdAussX4YbRdlRwlIlmVND7JmaykFOK1gDQCE/Koi/qaWpKplCPnp0XBwPhHRgrlzRQYmXDqOXwrtbwSu53x9P3LTohEdbpcdhcinhKLAvnQHzL4mGO08KfQFc2oU5lAH1NQi2VGIrknLXQHhiISnikOtb4U11g9rYggq284owCmRCVBSCqGH+O5nLBRJMOnS0dw1ihK2nREtqOKsWGQkRmDfqXauGnib0Qk3GjpGUFGQIDsKkRS24o0Q4bFwn/md7Cghwex6cz5RarHkJETXJlQbtJJN0JtOw5wYlh0n4Bg99QA4g4yCg61gLcyBNhiDHbKjSMNCkQR17cMwLQsl2Wz3IFpIQgjcuTITzd2jqG8fkR3Hr1xsHIAFsFBEIUuoNtgrtsPoqITR0yA7TtDTu2oARYOamCs7CtF12UpvAywDnppDsqMEHKO7DlDtUBIyZUchmjctfzUAEdKrilgokuDyfKLCdM4nIlpoG8pSEebQ8PLJVtlR/Mq5hn5EhduQkxolOwqRNLbS2wFHBFynfi07StAzumuhJuVBaGx1Jf+lxqZDTS2Gp3I/h1rfJKO7DmpyHoSiyY5CNG9KeCzUtBLo9cdCtiuBhSIJqlsGkZcWDYed84mIFprDruK2pWk4UdWLgZEp2XH8gmlauNAwgPK8BChCyI5DJI2wh8FecQ+MlrNcVbSALN0Ns7eR84koINgW3wlrtBdG6wXZUQKG5ZmC2dfM1lIKKlrBWpjDXTAHQvPLZhaKfGzKraOJ84mIfOrOFZmwLAv7z7TLjuIXGjtHMDbpYdsZEQB72V0Qjki4Tr4gO0rQMnobAdPgSSQFBC1vFURYNNyXXpEdJWAY3XWAZfI1TkFFy18FCCVk289YKPKxurZhGKaFkqxY2VGIQkZSbBiWFiZi/+kOeHRDdhzpzjf0QwigLC9edhQi6YQ9DLal98JoPTd9skNeZ3TVAAC3zaaAIFQNtkW3w2g5B3OkV3acgGB01QJC8DVOQUVxRkHNWAxP/fGQbD9jocjHLjUPQlUEijJjZUchCil3rcrE2KQHxy71yI4i3bn6fhRkxCAyzCY7CpFfsJdthXBGwXXiedlRgpLRVQslLgPCGSk7CtGc2BbdAQgB96V9sqMEBKOrBkpCNoQ9THYUIq+yFayFNdoLs7dRdhSfY6HIxyqbB1GQEcP5REQ+tignDumJEXj5ZGtIfitw2fC4G01do1iSz7YzosuEzQH7svtgtF+E3lktO05QsUxzepA15xNRAFEi4qDlroCn+jVYult2HL9mGTqM7nq2nVFQ0nJXAIoKTwi2n7FQ5ENjkx60dI1icU6c7ChEIUcIgbtWZqKlewy1bcOy40hzoaEfAFDBQhHRDLbFd0CExcB94vmQLiZ7mznYDrgneRJJAcdWthVwjYfsfJK5MvubAcPN1zgFJeGIgJq5BHrD8ZDbCZGFIh+qbhmCBaCUhSIiKdaXpyLCqeGlN0Jz9wJguu0sJtKO7BS2gBC9ldAcsC9/B4zOKhht52XHCRpGdy0AcEURBRw1rRRKXDrbz2ZhdL45g4yvcQpStsK1sMYHp2dxhRAWinyosnkADpuK/PRo2VGIQpLDpuL25Rk4XdOL7sEJ2XF8zqObON/Qj4r8BAghZMch8ju2RXdARCXBdexZWGZofXO4UIyuGojwWIjIRNlRiG6KEAK2xXfC7G2E0dMgO47fMrpqIKJToITHyo5CtCC0nOWAZode97rsKD7FQpEPVTYPoigrBprKv3YiWbauzISqCrz8RpvsKD5X1TKIKbeBFcVJsqMQ+SWhanCsfg/MgdaQ+0C4ECzLgtFZDTW1mMVpCki2oo2AzQn3xVdkR/FLlmXC6Kpl2xkFNWFzTs8sa3gDlqHLjuMzrFj4SP/wJDr7J7A4h9tRE8kUG+nA2sUpOHi+A2OTHtlxfOp0TS8cdhWLc9n+SnQ9WsEaKIm5cJ34FYfYzpM12gtrfBBqeqnsKES3RNjDYCvaAL3hGMzJEdlx/I451AnLNQaNbWcU5GyF6wDXOIzW0GlNZ6HIR87V9QGY3nmJiOTatjobbo+JA2faZUfxGdOycLq2D0vyE2DTuOsi0fUIocCx9n2wxvrhucRVBPNhdFQBANS0EslJiG6dbfFWwNDhqX5NdhS/c3lmi5rGFUUU3NTMcghnFDwhtNqYhSIfOVfbhwinhiwOkCWSLis5EmW5cXjlZBt0IzTmkDR0jGB43I0VxZwTQjQbLWMx1KwlcJ3eDcs1LjtOwNI7qyGcUVBi02VHIbplanwG1LRSeC69ytllb2N0VkOERUNEp8iOQrSghKJBy18NvfkMLPek7Dg+MedC0e7du3HfffehoqICO3bswAsvvHDD6/f09ODhhx/G+vXrsWLFCnz6059Gc3PzfPMGJMuycLauF6U5cVDYo0/kF7atycbQmBvHLnXLjuITp2p6oSoCFfksFBHNhWPN+wD3BFwnfy07SsAyOqugppVwPhEFPFvZVlhj/TBazsqO4leMrhrOIKOQoRWuBww39KZTsqP4xJwKRXv27MHDDz+MjRs34umnn8aaNWvwyCOPYO/evde8vsvlwsc//nGcP38e//iP/4h/+7d/Q09PDz70oQ9hZCT0+nt7hybROziJxWw7I/Ib5XnxyEiMwN7jLbAsS3acBWVZFk7V9GJRThzCnZrsOEQBQU3Igq10CzwXX4ExGDptqt5ijvbCGuuHmsb5RBT4tNzlEBFxcLMd9QpzrH/6Nc5B1hQi1JRCiKjEkGk/m9MZw5NPPokdO3bg8ccfBwBs3rwZw8PDeOqpp3DPPfdcdf1XX30V1dXVeO6551BeXg4AKCoqwtatW/Hiiy/iT/7kT7z4FIDJyXGMjQ3B8NMp5FNuA3/z7gzERuro6grNVVUUGHp6FJh+vKxaVTVERsYiLCxi3vclhMCOddn4zu5KnKvvx9LC4F1p09E3jp7BSWxfky07ClFAsa9+Dzz1x+E68lOE3fswvzW/CUZnNQBATed8Igp8QtFgW3Q73CeehznUBSU2VXYk6YyOSgCAmr5IchIi3xBCwFawDu6zv4c5MQwlPEZ2pAU1a6GotbUVLS0t+Ju/+ZsZl2/fvh179uxBa2srsrKyZvxs06ZN+OlPf3qlSAQANpsNAOB2e3cHkcnJcYyODiI2Ngk2m90vP8SNTrjhnNKREhfml/mILtM0Bbrun4Uiy7Lg8bgxNNQLAF4pFq1ZlILnX2vA7442B3Wh6FTt9DD95UXB+xyJFoLijIJj1bvhOvIT6M2nYctdITtSwNA7qgFHBJS4DNlRiLzCVroF7lO/gfvSK3Bu+KDsONLp7ZUQjkgo8XyNU+jQCtfDfWY3jLYLUIo3yo6zoGZtPWtoaAAA5OXlzbg8JycHANDY2HjVbSIjI7Fy5UoAgMfjQVVVFR599FHExsbi7rvvnnfotxobG0JsbBLsdoffFmGiwu3ITI7023xEgUAIAbvdgdjYJIyNDXnlPjVVwfY12ahrG0ZNq3fu0x+dqulFQXo0YiMdsqMQBRzb4juhxGXA9fozsHTvftkVzIzOKmhpJRCC+6ZQcFDCY6Hlr4an+mDIDLO9HsuyYHRUQk0v5WucQooanwHn3Z+BmlkmO8qCm/WVPTo6CmC6+PNWERHT3+aPjY3d8Paf/exn8cADD+Do0aN45JFHkJycfKtZr8kwdNhsdq/eJxH5L5vN7tU2081L0xEZZsPvjwZnW2jf8CSau0axojhJdhSigCQUFY4NH4Q12gv3+RdlxwkI5tgArNFeqGlsO6PgYi/fBnim4Kk5JDuKVNZoL6zxAbadUUiy5a2CEh4rO8aCm7X17PKQ17evhrl8uaLcuNb0iU98Ah/96Efxm9/8Bo899hgAYOfOnXMOmJBw4+3ke3oU2GzqnO9PJk1jxZ38XyAcp4qiICkpymv3964tBfjx3iqMeUzkpQdXv/Fr57sAANs25CEpYf7tev7Am//2RHOStBZddWsxeeZ3SF23HVp0wuw3CeHjdLT7NMYBJC5eAUcI/z0EglA+Tm9J0lK0nyiBUfkKErc8AKEExjmIt420HcM4gKTyVbAnLuwxxGOUAkEwHqezFoqioqaf9NtXDo2Pj8/4+fVcbkFbv3492tvb8e1vf/umCkX9/WMwzevvSGSapt/OVHkrf579QnRZoBynpmmit3fUa/e3tjQJz+6rxU/3VOIv3hlcS0lfeaMFBenRUL38dyZLUlJUUDwPCjxi+Xth1Z1Ex57vIezOT97wuqF+nE5VnwbsYRgWCRAh/Pfg70L9OL1VonQr9Je/ia6Th6HlLpcdR4rJmtMQYTEYMqMX9DXOY5QCQaAep4oibrgoZ9alA5dnE7W0tMy4vLm5ecbP3+rSpUv43e9+d9XlZWVl6Onpme0hSbJg3yqc6O0inDbcsSwDxyq70T04ITuO17T3jqG1ZwxrF6fIjkIU8JToJNgrdkCvex16V63sOH5N76yGmloCMcuqc6JApOWuhIiIh/vCS7KjSGFZFoz2Sqjpizh/lSiIzfobPCcnB5mZmdi7d++My1966SXk5uYiPT39qtscPXoUf/u3fzujuGQYBo4ePYri4mIvxA5+733v/fjKV77k88dtamrEpz/9MZ8/LpFs29dkQVMV7D7SJDuK1xyr7IYQwOpFLBQReYN92TsgIuLhOvJjWKb/r76UwRwfhDXcDS2d84koOAlFha3sLhgdlTD6W2a/QZAxhzthTQ5DTS+VHYWIFtCcvup56KGHsHv3bnzxi1/Ea6+9hi984QvYs2cPPve5zwEABgYGcObMmSvtaTt37kRaWhr+8i//Env37sX+/fvxqU99CjU1Nfibv/mbhXs2QeTLX/5XfPjDf+bzx92//xWcP3/O549LJFtMpAN3LM/A6xeCY1WRZVk4erEbi3PjERPBgf9E3iBsDjjWvg9mXzM8NQdlx/FLRkclAEBNXyw5CdHCsZfeBmh2uM//QXYUnzM6qgAAGgdZEwW1ORWKdu7ciSeeeAKHDh3CQw89hOPHj+OrX/0q7r33XgDA/v378eCDD+LixYsAgNjYWPz4xz9GcXExvvjFL+Jzn/scpqam8IMf/ABr165duGcTRIqLS5GRkSk7BlFI2bE2G6oqgmJVUUPHCPqGp7CObWdEXqUVrIWaWgz38V/Cco3LjuN39PaLEI5IKAlZsqMQLRjhjISteBP0+tdhTo7IjuNTRkclREQ8RLR3d7ImIv8y6zDry3bt2oVdu3Zd82c7d+68akB1RkYGvv71r88vXQh773vvx6pVa/DRj34Mf/In78Q///PX8OKLe/DGG0ehaTbcfvtWfO5zfwun03nl+jt2vAMjI8PYu/d3sNlsuP32u/CZz/z1jOusWrUGjz76D1ce5/e//y2+/OUn8Ktf/Q6//e0L+N73/gcAsGnTKvzZn30CH/vYjQd2EgWTy6uKXj7Rhvs35CI5Llx2pFt29FI3NFXBiuIk2VGIgooQAo4NH8TE81+A68Sv4Nz4YdmR/Mb07JJLUDMWQQjOJ6LgZiu/C55L++CpfBWOFQ/IjuMTlmXC6KiCmrWE84mIghx/iweIr3zln5CenoF/+Zd/wwc+8GHs3v0CfvSj7824zrPP/gw1NdX4h3/4Ej7ykY9h797d+OIX/+E693i1++9/Fx54YLrg961vfQ/33/8ubz4FooDwx1VFzbKj3DLDNPFGZTeWFSYgzDHn7wOIaI7UxBzYFt8Jz6V9MPqaZMfxG+ZwJ6zxQagZwbV7JNG1qLHpULOWwHNxHyzDIzuOT5iD7bCmRtl2RhQCgvIM4vD5Thw61yk7BjZVpGHjkjSv3NfGjZvxmc/8NQBg1ao1eOONYzhy5CA+8Ym/vHIdTVPxb//2HwgPn14FoaoKvv71f0VDQz3y8wtmfYzk5BQkJU0vIy0vX+KV3ESBJibSgduXZeCVk214x4acgFxVVNk8iJEJD9YuTpUdhShoOVbthN7wBqYO/QjhD/wdV9AAMNovAQC0DM4notBgL9+GyT3/Br3+OGzFG2XHWXBG++UZZCwUEQU7fqoJEEuWLJ3x56SkZExOTs24bOPG264UiQBgy5atAIBz504vfECiILJj3fSqol8fapQd5ZYcOd+FMIeGioJ42VGIgpZwRMCx9kGYPfXQqw/JjuMXjPZLEFFJUDi7hEKEmlkOJTYd7vMvwbIs2XEWnN52ASImFUpUouwoRLTAgnJF0cYl3lvJ4y8uzxm6TFEUWNbMrXkTE2fOIomNjQUAjI6OLmg2omATG+nAXasysfdoC7avyUZ2SpTsSHM2OuHGieoebFmaAZumyo5DFNS0og1Qqw7AdfxZaLkrIJyRsiNJY5kG9I5K2PLXyI5C5DNCCNjK74br0A9gdNVASyuRHWnBWLobRkcVbIu2yI5CRD7AFUVBZHh4eMafh4YGAQBxcXEApn+ZmebM4tLkZOBvA060EO5dl4Mwh4bnDjTIjnJTDp/vgm5Y2LI8XXYUoqAnhIBj44dhucbhOvEr2XGkMvuaAPck5xNRyLEVbwAcEfBc+IPsKAvK6KoBDDe0TI6nIAoFLBQFkWPHXoeu61f+/Oqrr0AIgeXLVwEAwsMj0N3dPeM2586dmfFnVeUKBCIAiHDacN+GHJxv6Ed1y6DsOHNiWRYOnGlHYWYMMpNCd2UDkS+pCVmwlW2F59KrMHqbZMeRRm+7CABQ00slJyHyLaE5YC/dAr3pJMzRXtlxFozeeh5QNajpwbtqioj+iIWiINLV1YG/+7vP4+jRI/j5z3+Cb3/7P/GOdzyAjIxMAMCGDZtw6tQb+NGPvo9Tp07gG9/4N5w8eWLGfURGTrfY/OEPe9HZ2eHz50DkT7auyERclAPP7q8PiNkDVc2D6B6cxB3LMmRHIQopjlXvhgiLwtShH17VFh4qjPZLUBKyoYRFy45C5HO2sq0AFLjPB++qIqPtPNTUEgjNITsKEfkAC0VBZNu2exAfn4h/+IdH8MwzP8b73/9hPPzwY1d+/pGP/Dne8Y534ac//SEeeeRv0N/fh0cf/YcZ93HHHXehvLwC//zPX8Azz/zI10+ByK/YbSoe2JSHho4RnKrx/28JXz3TgQinhlWlSbNfmYi8RtjD4Vi3C2ZvAzzVB2XH8TlLd8HoroPK3c4oRCmRCdAK18JTdQDW1JjsOF5njvXDHOyAlsW2M6JQEZTDrIPBL3/52yv/f+jQif+/vbsOr7J+Hzj+Pr2CFV0jNxoGI8bGVxiMEhFRQlpAGkEEkbCRHyiNAUpJKKWodBikEiMGUgqDje6xPvX8/pgcPYwYsO1Z3K/r2nVxnrzP2b0P59znE2n2jx37XpptBoORUaPGMmrU2Pte09XV9b77/3t9Ly8vZs+e/4RRC5H7hFQrwqa90azadoYa5Qug12XP+npsgpmDp67RpHYJmcRaCBXoywejO/4b5j0rsdXOW5O92i7/BXYrepmfSORhxhotsf61G/OxXzDVaqN2OBnKGnMEAJ0UioTIM7LnJx4hhMgmdFotHcPKc+VmIlv3n1c7nAfaGXkRm13hmZoyibUQatBoNJhCu6GYE7n52zdqh5OlrOePglaHroi/2qEIoRqdT0l0JatjOboFxWpWO5wMZYs5gsbdB62XvMcQIq+QQpEQQjxC9XIFqFHOlx93RXE7PkXtcNKw2e1sO3SRiqW8KOrrrnY4QuRZOp+SGKqGE3dwC7arOWvFxKdhi45EV7QiGoPMXSLyNmONVijJcVhO7VQ7lAyj2K1YLxxDX7IaGo1G7XCEEFlECkW5xKpVa9LMNySEyDidmlbAZrOz8tfTaoeSxt7jV7kem0x4UEm1QxEizzPVbovO3ZPkXYtR7Ll/Ymv7nWvYb19EX6q62qEIoTpd0QC0BctijtyYa/7+bVfPgCUJXYmqaocihMhCUigSQoh0KOztRvO6pfj9z8v8fT5W7XAc7IrCut/PUbygOzUqFFA7HCHyPI3RFd+mPbFfi8JyYpva4WQ6a8xhAPQla6gciRDq02g0GGu0RLlzFevZtHOM5kS2mCOg0aKXyeqFyFOkUCSEEOn0bLAf3vlMLN1yCrtdUTscAA6eusbF6wm0Di6NVrqEC5EtuFcJRVc0gJR9q7Anx6kdTqayRkeiyV8YrVcRtUMRIlvQl66NxrMw5kPrUZTs8V7haVhjItEVKofGJEPbhchLpFAkhBDp5GLU075xOc5diePnA+pPbK0oCmt2n6Wwtyt1KhZSOxwhxD80Gg2mkO5gTsK8d6Xa4WQaxZqC7eJxGXYmxH9otFqM1Vtiv34W26UTaofzVOxx17FfP4fOL1DtUIQQWUwKRUII8RjqVSpMtbK+fLftNFdvJaoay5EzN4i+Ek+rYD+0WulNJER2ovMpjqFaMywntmO78rfa4WQK28UTYLOgLyXDzoT4L0OFBmhc82M+vF7tUJ6K9ewBAAxlaqkciRAiq0mhSAghHoNGo6FHiwB0Wg0LN5zArlK38ru9iXzzmwiuIkM+hMiOTLWeR+PmlWsntrZGHwa9EV0Rf7VDESJb0eiNGKqGY4s5gu1GjNrhPDHr2Qi03iXQesr7DCHyGikUCSHEY/LJ70LHsAqciL7NtoMXVInh2NlbnL5wh5b1/dDrpCkXIjvSGF0xBb+M/fo5LCd+UzucDKUoCtaYSPTFq6DRG9UOR4hsx1g5DAwuObZXkT3pDrbLp9CXqa12KEIIFcinC/FA2X0Cvuwe35PIKc8pp8SZmRpWL0qV0t6s+O0012OTsvTeVpudb7aeooCnCw2rF83SewshHo++bF10xSqRsncV9qQ7aoeTYey3L6LEXUdXUuYnEuJ+NCZ3DBWfwXp6D/a462qH89is5w6CoqAvLcPOhMiLpFCUTdntdn74YRU9enQiPLwhHTo8z6xZU0lMTHA6bu/eP+jTpztNmoTQvn0bvv12SZprnThxjMGD+xIe3pDnn2/BnDmfYbVaH3r/Xbt2MH78u47HBw7sJzQ0iMOHD2XI83saFouFWbOmsWXLRrVDAWD9+jWEhgZx9eqVp7rO0aORvPnmMMfjS5cuEhoaxKZNmfNN1Jkzp+nVqyuNGwfTo0endJ2T3V57NWk0Gnq0rAjA3DXHsNqybljJ5n0xXLqRSJdwfwx6XZbdVwjx+FIntu4GlpRcNbG1LfowgExkLcRDGKs1AzSYj2xSO5THZo2KQJOvIFrfUmqHIoRQgRSKsqmlSxcxbdonBAeHMmHCZDp16sqGDesYN+4txzFHjhzmzTeH4efnx0cffUJ4eAs+/3wG33yz2HHM+fMxDB06AJPJhQ8++D86derK8uVLmTlzykPvv2LFt1y5ctnxOCCgIrNnL6BChQoZ/2Qf061bN1m+fOkji11ZJTg4lNmzF+Dt7fNU11m79keios5kUFSP9vXXc7l06SITJnzC6NHvPvoEst9rr7YCnq70aB7AqfOxfL8ta353N2KT+WlXFIEVClCjfIEsuacQ4unovIthrN4cy8kduWZia2t0JFqfEmg9fNUORYhsS+vhi758fSwntqEkx6sdTrop5iRsF46hL10LjUYWyxAiL9KrHYBIS1EUvvlmEW3atKN//8EA1KlTD09PT959dwx//XWSChUCmDt3Dv7+FXn77Q8BqF+/AVarlUWL5vPSSx0xGo0sWbIQd3cPJk6cgsFgIDg4FBcXF6ZP/4Ru3V6hYMH0Lant7u5B1arVMu0552Te3t54e3urHcZji42NpVy58gQHh6odSo5Wv0oR/r4Qy8a90ZQrnp/aAZm7TP23P/8FCrzcVP2irRAi/Yy12mD5+3eSdy7C7YX30Ghz7nd19uQ4bJdPYazRSu1QhMj2jDVaYf1rN+YjmzDVeVHtcNLFGn0Y7Fb0ZYLUDkUIoZKc+y4lF0tMTKB585aEhzd32l6qVGkALlw4T0pKCocPH6BRozCnYxo3bkJ8fBxHj0YCqUPTQkL+h8FgcBzTqFETbDYbe/f+cd/7Dx7cl4iIvRw6dIDQ0CAOHNifZujZvHlz6NatA7/8spXOnV8kLKwB/fq9QnT0WXbt2kG3bh1o0iSEvn178tdfJ52uv23br/Tq1ZWwsAY8/3wLPvtsBmaz2bE/JSWZyZMn8sILrWjcOJjOnV909JK6dOki7do9C8CECe/z0kvPOc47dOgAgwa9SpMmITz7bBMmTRpPXFycY//69Wt45pl6REYeokePToSFhdC7dzf27XN+HVas+NbxnNq2bcnkyRNJSHjwt0D3Dj376KP3GD58CGvX/kCnTi/QuHEwPXt2Zs+e3x94jY8+eo+1a3/k8uVLhIYGsX79Gse+a9euMnbsSMLDG9KqVRM++WQCSUnOc+L89NNqunZtT+PGwbRv34bFixc+dB6h0NAg9u//93e8fv0a5s2bQ0hInfseu3Dh3Ae+9oMH92Xo0IFO59wvXzp3fpF58+bQqlUTOnVqR2Ji4hPFnh11DKtA2WL5mbfuOJdvJmbafSJP3+DAqWu0blCaAp6umXYfIUTG0xhcMNXvhP1GdI6f2Np69gAodvRl5UOkEI+i8ymOvmwQ5qNbckyvIuvZCDSu+dEVLqd2KEIIlUihKBtyd/dg2LCRVK9e02n7jh2/AVCmTDkuXryA1WqlVCk/p2OKFy8JQHT0WZKTk7l69UqaY7y9vXF3dyc6+tx97//GG29RqVJl/P0DmD17AQEBFe973OXLl/jyy8/p02cAb7/9AefPRzNy5DBmzZpK9+69GDPmPa5cucSHH77jOGfz5o2MHTuSsmXLMWHCZLp3f4Uff/ye998f6zhmxowp7Nmzm8GDhzFlyixCQ5/h889nsGHDWnx9CzBx4lQAevTozYQJnwCpRaJhwwbi5ubGhx9OYuDAoezevZPhwwc7DZOy2+2MGTOCpk1b8NFHk8ifPz8jRw7jxInjAGzZspEvvphJu3btmTJlFj179mHTpvXMmPHwoXr3OnbsCMuWLaVPn/5MmDAZnU7HuHFvEh9//zcIPXv2ITT0f/j6+jJ79gKnXj5fffUFRYoUY+LEqXTo8DI//vg9CxfOdexfvHgBn3wygXr1gpk0aRqtWz/PvHmz+fTT6Q+Mb/bsBU6/4/T0KnrQa59eFy6cZ9eu7bz//kf07Zv6u3qS2LMjg17LwLZV0eu0fPb9ERKSLRl+j1txKSzccJwiPm40ryvzBQiRE+nL1kVXtCIp+77LMR8Y78catf+fuUv8Hn2wEAJjrTZgScZ8dLPaoTySYjVjjY78Z9iZfFQUIq/KlUPPLKd2YTm5Xe0wMAT8D4N/SIZc688/j7Jkydc0bNgIP7/Sjh5Dbm7uTse5ubkBkJCQ4ChKuLs7H3P3vISEhDTbAcqUKYubmwc2m/Whw82SkpJ4880x1KqV+o3ioUMH+O67FcyY8QW1a6f2TLl69QqffTadxMREXF1dmT17Fg0aNGTcuPcd1ylUqDCjR48gMvIQ1avX5NChAwQF1aNJk2YA1KoVhJubG56eXhiNRvz9AwAoXrwE/v6pRaw5cz6ldOmyTJo0De0/3fn9/QPo1asrv/yyhWbNWgKpw/o6depK1649Aahduy4dO7ZlyZKFjB8/iUOHDlC0aDFefLEjGo2GwMDauLm5cedO7ANfh/uJj49n/vylFCtWHABXV1cGD+7LwYP7adiwUZrjixcvgZeXNwaD0fGaX7qU2muoSZNmDBny+j/x1mHv3j84cGCf4z5ffz2Pdu3aM2TIcADq1q2Pq6sbn302nfbtX6ZIkSJp7le1arV0/Y7/60GvfXrZbDYGD37dkS9PGnt25ZPfhQHPV2HaysNMXX6YEZ1q4mrKmCbWYrXx6fdHSEqx8XqHmhj08sZNiJwodWLrriR+9w4p+1bh0rCn2iE9NiU5Htv5YxirN5e5S4RIJ51PSfSla2M+sgVjteZoTGnfm2cX1phIsKbIsDMh8jj5tJEDREYe4o03hlCsWDHeemsc8O/y5A96k5b6DcCDj1EUBa326d/gValS1fHvu5M5V6nyb+HB09MTgPj4OKKjz3H16hVCQ/+H1Wp1/NStG4zBYGDfvj1AamFozZrVjBjxGt99t5yLFy/Qs2cfGjS4f6+X5ORk/vzzKA0ahGK32x3XLVOmHEWKFHVc967w8BaOfxuNRoKDQ4iMPOS4d3T0OXr16sKCBV9x4sQxwsNb8NJL6VsV7C5f3wKOIhHgmAsqKSn5sa4DpOlZVrRoMeLiUouAR49GkpycnOY1DQlpiM1mcxSUsoty5co7/p3TYk+PSqV9GNi2GtFX4pi+8jApZttTX1NRFBZuOEnUpTv0aV2ZkoU8MiBSIYRadD4lMFRpguX4NmzXz6odzmNLXTLbJh8ihXhMxtrPgyUJ89EtaofyUJaTO9G4eaErVlntUIQQKsqVPYoM/iEZ1pNHbT//vJmPPnqfkiVLMWXKLDw9vYDU4WlAml5BiYmpjz08PBy9je43v05SUqLjGk9Kp9NhMrmk2e7iknYbQGzsbQA+/vgjPv74ozT7r1+/DsBrr71BwYKF2Lx5A9OmfcK0aZ9QtWp13njjLSpU8E9zXlzcHex2O4sWzWfRovlp9pcoUdLpsa+v80pR3t4+xMXdAVJ779jtdlavXsXChXOZN28ORYsWo3//ITRpEn7f53U/974Gd3s5KcrjL6Hu6uo8F41Wq3Vc525Pp9dfH3zfc69fv/bY98ssOp3Okb+Qs2J/HDUrFKBvmyrM/vEoM7+L5LWXqmMyPPkS9pv2xvD7n5d5oWEZagcUzMBIhRBqMdVui/X0HpJ3LcGtzZgcNbzDcmYfGg9ftAXLqB2KEDmKzrcU+tK1MB/ZjLFaMzRGN7VDSsOeGIstJhJj9RY5esJ9IcTTy5WFotxi2bIlfPbZDAIDazNhwmQ8PP4t7BQvXgKdTseFCzFO55w/n/q4VCk/3NzcKFiwEOfPn3c65tatmyQkJKSZuyiz3Y3/tdeGp+klAziKCEajkR49etOjR28uX77Mrl3b+frreXz44dssWrQ8zXnu7u5oNBpefrkrYWFpizl3h+PdFRd3x2kp+5s3bzo9Dg9vQXh4C+Lj49m79w+WLv2aDz4YR2BgLXx8stcywHeLfe+/P4HixUuk2V+gQPoLCxqNBrvduZB1d8LpR5/n3HPm3sm27ycjY89u6lQshNVamblrj/HRoggGtK1CUd/H62ZuVxTW7j7LjzuiCKpYiNYNSmdOsEKILKcxuWOq257kbfOw/rUbg3/OWH1SSUnAduFPDFXDZdiZEE/AWOt5rGcPYD6yGVPttmqHk4b1792pE9XnkDZJCJF5pFScTa1d+wOffjqdsLCmTJkyy6lIBGAymahRI5Bt2351WiHqt99+wcPDg4oVU7uL1qlTj127dmCxWJyO0el0BAY+uNu4TpfxqeHnVwYvL28uXbpExYqVHT+enl588cUszp6Nwmw207nzi3z77RIAihQpwosvdqBp02aOVcW0WufeGW5u7lSoEEBMTLTTdUuWLMWXX37On38edTp+164djn+npKSwe/cOx7w57703ljFjRgKpha2wsKb07Nkbm83GjRvXM/w1+S/tE3xzU6VKNQwGAzduXHd67larlTlzPnP00koPd3d3FEVxvM6AY0jevzGm7Rnj7u7udM79zsvs2LOj4KpFGNq+BrfjU3h/4T52HbmU7nMTky3MWhXJDzuiqF+lCH2erSQfyoTIZfT+IWgLlSNlzwoUc+atlpiRrOcOgd2GQYadCfFEdAX8UucqityIPemO2uE4URQFy8ldaAuVReddTO1whBAqkx5F2dCtWzeZPn0yRYoUpV27jpw8ecJpf4kSJfHy8qJHj94MGzaQd98dQ8uWrTl6NJJvv11M//6DHUOfunTpwdatmxkxYigdOrxMTMw5vvzyc5577oWHThTs4ZGPw4cPEhGxjwoVAjLkeel0Ol59dQBTp05Cq9VQv34D7ty5w7x5c4iPj8ffPwCj0UilSpVZsOArDAY95cpVIDr6HOvXr6VRoybAvz2IIiL24udXhipVqvLqqwMYNep1PvroPZo0aYbFYmbJkq85c+ZvBg8e5hTHp59Ox2w2U7RoUZYt+4bExAS6d+8FQFBQHSZOHM+nn04nODiEuLg7zJ//JaVK+VGmTOYuEerhkY+bN2/y+++70v2ae3l50alTV+bM+Yz4+Hhq1Ah0rEbn4eFBmTJl033/4OBQZs2axsSJH9K5c3euXLnMggVfOU2Yfr/XvkGDhuzcuZ1Zs6YREtKQyMhDbNy4Lktjz66ql/Pl/V51+fKnP5m37jgHTl0jPKgkAaW87lv4sdrsHI26ybKtf3HjTjJdwv0Jq1VcikRC5EIajRaXkK4krv6AlIgfcQl+We2QHslyZh8adx+0hWTJbCGelLHOi1jPHcB8cC0uDTqrHY6D/fo57LfOYwrtrnYoQohsIN2ForVr1/LFF18QExND8eLF6devH23btn3g8deuXWPGjBns2rWL27dvU6ZMGV599VVatmyZEXHnan/8sZvk5GQuX77EoEF90ux/++0PaN68FbVr12H8+I+ZP38OY8aMoECBQgwcOJSXX+7qONbPrzTTpn3KZ5/N4O23R+Hp6UXHjl3o3bvfQ2Po2LELf/55hBEjXmPcuPedhmY9jeefb4e7uzvffLOI1atX4ebmTs2atejXb5Bj7qARI8bg6enFt98u4ebNG3h7+/Dcc23p06c/kDpnT48evVm+fCm//76bn37aRHBwCFOmzGT+/K8YO3YkRqOJSpWqMGvWHMqWLe8Uw/Dhb7Jw4VyuXLlM5cpV+fTTrxzD8Fq3bktKipnVq1exevVKTCYXgoLqMnDgUPT6zK2rPv98O3bv3sno0W/Qt+9AGjdumq7z+vYdiK+vL6tXr2Lx4gXkz+9JvXrB9Os3CJPJlO77lyrlx7vvfsD8+XMZOXIofn5lGDVqLNOmfeI45n6v/bPPtuHChfNs2LCW1atXERhYm/HjJzFgQO8siz07885nYuTLgaz74xyb9kRz8K/rFPV1I6RaUbw8jBj1OnRaDcfP3WLP8SvEJVrwzmdiVOdalC/hqXb4QohMpCtYBkPFZ7Ac3YIh4H/ofIo/+iSVKOYkbOePYqgcJsVrIZ6CzrsYBv+GWI79grFaM7T5Cjz6pCxgObUDdHoM5eqpHYoQIhvQKP8dt/QAGzZs4PXXX6d79+40bNiQrVu3smzZMmbMmEGLFi3SHG82m2nfvj1xcXEMGTKEQoUKsWnTJpYvX86UKVNo3bp1ugO8cSMeu/3BIV6+fI4iRbJ2rp0noddrsVoffyJjkXHWr1/DhAnv8/336yhUqLDa4WRLOSVPc8rf/b3MFhv7Tlzlt4MXOH3Rucu5XqehZvkCBFctQrWyvugzYfhnblCwYD6uXYtTOwwhHupx8tSeHEfC8rfQ+ZbC9dk3s20RxnJqJ8m/zcW1zVj0RSqoHY7IANKeqscef5OE5aPQl62La+NX1Q4HxWYhfskw9MWr4Np0oNrhOEiOipwgp+apVqvB1/fBi1ulq4vE1KlTadmyJWPGjAGgYcOGxMbGPrBQtH37dk6cOMHKlSupXr06ACEhIVy8eJGvvvrqsQpFQgiRWxgNOkKqFSWkWlFiE8wkm61YLHYsNjuFvV1xczGoHaIQIotpXfJhCmpHyq7FWKP2YShbV+2Q7styYjua/IXRFS7/6IOFEA+l9fDBUKUplsiN2Gq0QOdT8tEnZSLruUOQkoAhQCaxFkKkeuRX1jExMURHR9OsWTOn7c2bN+fMmTPExMSkOcfd3Z2OHTtSrVo1p+1ly5YlOjr6KUMWQoicz9PdSGFvN0oU8qBM0fxSJBIiDzNUaozWtxQpvy9DsaSoHU4a9tuXsV0+haFiw2zb40mInMZU81kwupKyd5XaoWA5/isaNy90xauqHYoQIpt4ZKHozJkzAJQpU8Zpu59f6rCPqKioNOcEBwfzwQcfOL2ZsFgsbNu2jQoVpLuyUEerVs+xc+d+GXYmhBAiW9FotZhCuqIk3MR8aK3a4aRhObkdNFoMsmS2EBlG4+KBsWZrbNGHscYcUS0O27Wz2C4cw1C1GZonWIFXCJE7PbI1iItLHW937/Ls7u6pKyHFx8en60aTJ0/m7Nmz9O3b93FjFEIIIYTI1fRF/NGXD8Z8eAP22Ctqh+Og2K1YTu1EX6oGWjcvtcMRIlcxVgtH41mY5N1LUGwWVWIwH14PBleMlRupcn8hRPb0yDmK7s51fW9X47vbtY+oPCuKwieffMLChQvp3bs3TZumbyWnux42wRLA1ata9PqcUf3OKXGKvC0n5KlWq6VgwXxqhyFUIr97kRM8SZ5aW/UmZvZBlIgVFOw4JhOienwJJ/cSn3QH37rNcZe/vVxH2lP1JbZ8lcvLxmM4/RveIe2y9N6WW5eJi9qPZ/02+BbPnj3uJUdFTpAb8/SRhaJ8+VKf9L09hxISEpz234/ZbOatt95i3bp19O7dmzfffPOxA3zUqmd2uz1HrNKUU1aTEnlbTslTu92eI1cXEE8vp64sIfKWJ89TPcZaz5P4x3Iu7d+B3q9mRof22BL3bkLj5kWCZwUS5W8vV5H2NJvIXx596drc2rkSc7FaaD18s+zWyTu/A40Wa7lG2TIXJEdFTpBT8/RRq549suvA3bmJ7p2E+ty5c0777xUfH88rr7zChg0bGDNmzBMViYQQQggh8hJDlXC0XkVJ3r0UxaruxNb2hFvYYg5j8A9Fo9WpGosQuZkp+GVQIOX3b7PsnvakO1hO7sDg30CGlQoh0nhkocjPz48SJUqwceNGp+2bN2+mdOnSFCtWLM05NpuNAQMGcPjwYaZOnUqPHj0yLmIhhBBCiFxKo9NjCu2OEncNc8SPqsZiObULFAVDQENV4xAit9PmK4AxsDXWqP1YoyOz5J6Wo1vAZsVYvWWW3E8IkbM8cugZwKBBgxg9ejSenp40atSIX375hQ0bNjBt2jQAbt68SXR0NOXLl8fDw4Nly5axd+9eOnbsSNGiRTl06JDjWhqNhho1amTKkxFCCCGEyOn0xSphCPgf5siN6MvVQ1fAL8tjUGwWLMd+RlesElrP7Dl3iRC5ibF6C6yn95C8bR5u7cejdcm8OU+U5HjMf/6MvnQttF5FM+0+QoicK12Fonbt2mE2m5k/fz4rV66kZMmSTJo0iVatWgHw22+/MXr0aBYtWkS9evXYtGkTAMuXL2f58uVO19LpdBw7diyDn4YQQgghRO5hqt8Ra/QhkrfPx63tO1k+9MtyahdKwi2Mz/TO0vsKkVdp9EZcwvqRuPoDUrYvxCV8cJrFhDJKyp4VYEnGGNQ2U64vhMj50lUoAujUqROdOnW677527drRrt2/s/QvWrTo6SMTKIqSaf9BPI3sGpcQQgiRW2hM7phCupK89XMsRzZjrJF1w0MUuw3zoXVoC5ZBV7xKlt1XiLxO51sKU512pOxZgfXUzkwZ9mm7/BeWk9sxVG+Bzqdkhl9fCJE7ZP91sPOoXbt2MH78u2qHkca6dT/x6afTM/0+69evITQ0iKtXr2T6vYQQQojsSF+mDrpSNUnZvxp7bNb9f2g9vQcl7hqmwDbyxZAQWcxQrQW6ogEk716K/c7VDL22YreRvHMRGncfTLXbZui1hRC5ixSKsqkVK77lypXLaoeRxqJF87lzJ1btMIQQQohcT6PR4BLaHXR6kn6dg2K3Zvo9FcWO+eBatD4l0PnJnJJCZDWNVotL476g0ZC09XMUS8atfmg5ugX7zRhMDbqgMbhk2HWFELmPFIqEEEIIIbIprYcPLg17YL96BvOBNZl+P2tUBPbbFzEGPodGI28ThVCD1sMX18Z9sd84R9LWz1Dstqe+pj3+BikRP6ArVQN96VoZEKUQIjeTdwDZ0ODBfYmI2MuhQwcIDQ3iwIH9APz110lGjx5B69ZNeeaZerzwQitmzJhCSsq/3zSEhgaxcOFcevXqSosWjVi2bAkAhw8fpH//XjRpEkLHjm3ZvHkjHTu2Zd68OY5zY2NvM2nSeFq3DicsLIQBA3oTGXnIsf+ll57jwoXzbNiwltDQIC5dupgm9gULviIsrAHx8fFO2xcunEvTpqEkJiYC8NtvPzNgQG/Cw/9H48bBdOnyEqtXr3roazJ06ECnbQcO7Cc0NIjDh/+N8fTpvxkx4jXCwxvSvPkzvP32WzJ8TQghRI5mKFcPfYUQzAd/wnr5r0y7j6IomA+uReNZBH2ZOpl2HyHEo+n9AjGFdMcWE0nKjq9RFOWJr6WYE0naNB0UBZcGXWVIqRDikaRQdB837ySzZPNJPvx6H0s2n+TmneQsvf8bb7xFpUqV8fcPYPbsBQQEVOTatasMGtQXs9nM2LHvMXnyTMLCwlm58ltWrVrmdP6CBV/RtGlzxo37gODgUKKizvD664MxmVz48MNJdOzYhalTJzkVUFJSUhg6dCC7d++kf/9BjB8/iXz58jNs2ECOH/8TgAkTPqFQocIEB4cwe/YCfH0LpIm9WbOWmM1mduz4zWn7zz9vpmHDRri5ubFz5zbGjRtF5cpVmDhxCuPHf0zRosWYMmUix44dfeLXLTr6HAMG9ObOnTu8/faHvPnmWM6c+ZtBg/qmKVwJIYQQOYlLSFc0HgVI/nUOijkpU+5hObkd+41zmAKfQ6OVt4hCqM1YuTHGWm2wnNyOOeKHJ7qGYjWTtGkG9lsXcW02BG3+ghkbpBAiV0r3qmd5xc07ybwzfy8pZhs2u0L0lXj+OHaFD3rVxSd/1ozlLVOmLG5uHthsVqpWrQbAkSOR+PsH8OGHE3FzcwOgTp167N+/h0OHDtClSw/H+dWq1aBz526Oxx9++A6enp588sl0jEYjAJ6eXrz77mjHMZs2ref06b/46quvqVixMgD16zfg1Vd7MGfOZ0yf/jn+/hUxGAx4eXk74rpX8eIlqFatBlu3bqZly9ZAai+fqKgzDBo0DICzZ6No1eo5hgwZ/p+Yq9OqVRMOHTpA5cpVn+h1W7DgK1xdXZk+/XPHa1SzZi06dHie775bTo8essSvEEKInEljdMW1cV8S10wgeft8XJoMyNChYfaEW6T8sQxd0QD0FYIz7LpCiKdjrP0CSsItzAd+REmKxRTcGY3emK5zFbud5F/mYLt0Epew/uhLPNl7bCFE3iOFonus/+Oco0gEYLMrpJhtrP/jHF2bBagWV/36DahfvwFWq5WoqDNcuBDD6dN/c+vWLby9fZyOLVeuvNPjAwf206BBqKNIBNCoURg6nc7xOCJiLwULFqJ8eX+s1n8ny2zQIJTFixdgsVgwGAzpirV581ZMm/YxsbG38fT0YuvWTfj4+FKnTj0AunbtCUBiYiLR0ee4cCGGEyeOA2CxWNL/otwjImIfQUF1MRqNjufg6elF5cpV2bdvjxSKhBBC5Gi6IhUw1W1Pyp4VmD2LYKrzYoZcV1EUUnYuApsVl/+9InMTCZGNaDQaTA17onHNj/nQOmzXonBtOviRPYPsyXGk7FqC9WwEpgZdMJSvn0URCyFyAykU3SPq0h1Hkegum10h6tIdlSJKZbfbmTPnM77/fiVJSYkUKlSYypWrYDKZuHfI8r2Fo9u3b+Hl5e20TafT4eXl5XgcGxvL1atXaNTo/v+JxMbepkCB9HVVDQsLZ+bMKfz668+0bfsiv/yyhaZNmzkKU7dv3+aTTz5ix45taDQaSpQoSfXqNQGeavx1bOxtNm/ewObNG9LsK1Gi1BNfVwghhMguDNVbYr99CfPBNWg9i2DwD3nqa1qj9mE9dxBTvQ5oPYtkQJRCiIyk0eow1W2PrnB5kn79ioTv38VU6zn05YPRunk5HavYbViO/UrK/u/BkowxqB3GquHqBC6EyLGkUHSPMkXzE30l3qlYpNNqKFM0v4pRwZIlC1mx4htGjhzD//7XGA8PDwBefbX7I88tUKAQt27ddNpmt9uJjf13mXsPDw9Kly7DuHHv3/canp5e6Y41f/78BAeH8OuvPxMQUJELF87TrFkrx/733x9LdPQ5pk//nKpVq2M0GklOTmbNmh8eeE2NRoP9nhUfkpKc52jw8PCgfv0GtG//cprzDYb0ddEVQgghsjONRoMptAf2uOskb5+PJl8B9EWfvMezkhxPyq4laAuUxlCteQZGKoTIaHq/QNzbvUfyb3NJ+WM5KXtWoCteBV0Rf5TkOJTEWGw3zqHEXkFXvDKm4C7ofIqrHbYQIgeSvsX3aFXfD5NRh06buhqATqvBZNTRqr5flsah0zn/aiIjD1GuXAVatXrOUSS6du0qp0+fRlHsD71WzZqB/PHHbqchZbt373R6XLNmLS5fvkSBAgWpWLGy42fHjm2sXLkMvT61pqhN5+SWzZs/y+HDB1iz5gdKly5DxYqVnJ5LWFg4tWoFOYbD/fHHLuDBPYrc3d3TrF723xXZ7j6HqKgo/P0rOuKvUCGAxYsXOK4vhBBC5HQanR7XpoPQ5CtI0uaZ2J5wJTTFkkLSllkoyQm4PNMLjVb36JOEEKrS5i+EW5sxuHWYgLFma+yxlzHv/x7LyZ3YbkSjdffBJXwIrq1GSpFICPHEpEfRPXzyu/BBr7qs/+McUZfuUKZoflrV98uyiazv8vDIx+HDB4mI2EeFCgFUqlSFr7+ex9KlX1O5clUuXIhh0aIFWCzmND1r7tWt2yv8/PMW3nxzGC+91ImbN6/z5ZdfAP8Wflq1asOqVSsYNmwg3bq9QsGChdi1awfLly/llVdedSyj6eGRj1OnTnLwYMQ/Q9/u/7oEB4fg5ubOunU/0bt3P6d9lSpVYdOm9VSo4E+BAgU5cuQwS5YsRKPRPPC5NGjQkJ07tzNr1jRCQhoSGXmIjRvXOR3Ts+er9Ov3Cm+9NZw2bV5Ap9Pz3XfL2bdvD23bvvToF10IIYTIITQuHri1HE7i+ikkrpuES+N+GMqmf0l7xZJM0sZp2C6fwqVxP3S+MkRbiJxE51UMXZ0XMQa1A5sl3RNcCyFEekiPovvwye9C12YBvN2jDl2bBWR5kQigY8cu6PV6Rox4jX37/qBbt1do2/ZFVqz4hhEjXuObbxbTvHkrevXqy+nTf5OQ8ODl30uV8mPy5BnExsYyduxIliz5mqFD3wDA1dUVADc3Nz7//CsqV67KrFnTGDFiKHv2/M7rr490KvT06NGLmzdv8MYbQzh16uQD72kwGAgLC8dut9OsWUunfePGvU/FipWZOnUSY8aMYOfObYwcOYa6dYPT9BK669ln29ClSw+2bNnIiBFDOXIkkvHjJzkdU6GCP59//hVWq43333+bd98dQ2JiIpMnz3BMpC2EEELkFtr8hXBrOw5tgdIkb/0cc+SGdM3151QkCusvk9wKkYNpNBopEgkhMpxGeZrZg7PAjRvx2O0PDvHy5XMUKZK1w8KehF6vxWp9+BCxzLJ//15MJhPVqtVwbIuKOkO3bh2YOHEKoaHPqBKXyH7UzNPHkVP+7kXGK1gwH9euxakdhhAPldV5qljNJP/2FdYz+9AVq4SxVht0RSs6egM7jlPs2KIjSdm/GvvN6NQiUTn5IiWvkvZUZHeSoyInyKl5qtVq8PX1eOB+GXqWBxw/foyvv57LwIFDKVu2HDduXGfRovmUKuVHnTryLaIQQgiRk2n0RlyaDMBSuDzmQ+tIWjsJXRF/9OWD0RhdQG9ESYrDcnQL9lsX0Hj44hI+GEPp2mqHLoQQQohsSApFeUDnzt0wm1NYseIbrl69grt76upgAwYMwWQyqR2eEEIIIZ6SRqPFWK05hkqNsZzYjvnwelJ2fu10jNanBC6N+6IvVxeNVt4CCiGEEOL+5F1CHqDT6ejdu1+aSaWFEEIIkbto9EaMVZtiqNwYJfE2WM0oVjNoNGh9SqYZjiaEEEIIcS8pFAkhhBBC5DIarQ6Nh6/aYQghhBAiB5JVz4QQQgghhBBCCCEEkEsKRdl84TYhRAaSv3chhBBCCCGEyDw5vlCk0+mxWMxqhyGEyCIWixmdTkbNCiGEEEIIIURmyPGFIg8PL27fvobZnCI9DYTIxRRFwWxO4fbta3h4eKkdjhBCCCGEEELkSjn+a3lXV3cAYmOvY7NZVY7mwbRaLXa7Xe0whHio7J6nOp2efPm8HX/3QgghhBBCCCEyVo4vFEFqsSi7f3AsWDAf167FqR2GEA8leSqEEEIIIYQQeVuOH3omhBBCCCGEEEIIITKGFIqEEEIIIYQQQgghBCCFIiGEEEIIIYQQQgjxDykUCSGEEEIIIYQQQghACkVCCCGEEEIIIYQQ4h/ZftUzrVajdggZJjc9F5F7SZ6K7E5yVOQEkqciJ5A8Fdmd5KjICXJinj4qZo2iKEoWxSKEEEIIIYQQQgghsjEZeiaEEEIIIYQQQgghACkUCSGEEEIIIYQQQoh/SKFICCGEEEIIIYQQQgBSKBJCCCGEEEIIIYQQ/5BCkRBCCCGEEEIIIYQApFAkhBBCCCGEEEIIIf4hhSIhhBBCCCGEEEIIAUihSAghhBBCCCGEEEL8QwpFQgghhBBCCCGEEAKQQlGmW7t2Lc8++yzVq1enZcuW/PDDD2qHJPIwq9VK9erVCQgIcPoJDAx0HLNz505efPFFatSoQVhYGPPnz1cxYpHXHD9+nCpVqnD58mWn7enJyyNHjtCtWzcCAwMJDQ1l6tSpWCyWrApd5BEPytHw8PA0bWtAQAA3b950HCM5KjKT3W7n22+/5bnnniMwMJCmTZvyf//3f8THxzuOkbZUqCk9OSptqVCboigsXLiQ5s2bU716ddq0acOaNWucjskLbale7QBysw0bNjBixAi6d+9Ow4YN2bp1K6NGjcLFxYUWLVqoHZ7Ig6KiokhJSWHSpEmULl3asV2rTa0ZHzhwgP79+9OyZUuGDh1KREQEH3/8MYqi0Lt3b5WiFnnFmTNn6NevH1ar1Wl7evLy3Llz9OzZk8DAQKZPn87p06eZNm0a8fHxvPPOO2o8HZELPShHExISiImJ4Y033qBu3bpO+/Lnzw9IjorMN3fuXKZPn07v3r0JDg4mKiqKmTNn8vfffzNv3jxpS4XqHpWj0paK7GDOnDnMnDmTIUOGULNmTbZv386IESPQ6XS0atUq77Slisg0TZs2VYYNG+a0bejQoUqLFi1UikjkdT/99JNSsWJFJTEx8b77e/ToobRv395p28cff6wEBQUpKSkpWRGiyIMsFouyZMkSJTAwUKlbt67i7++vXLp0ybE/PXk5ZswY5ZlnnnHK06VLlyqVKlVSLl++nDVPRORaj8rRiIgIxd/fX/n7778feA3JUZGZ7Ha7UqdOHeW9995z2r5u3TrF399fOXbsmLSlQlXpyVFpS4XazGazUqdOHeWDDz5w2t61a1fl5ZdfVhQl77wvlaFnmSQmJobo6GiaNWvmtL158+acOXOGmJgYlSITednx48cpVaoUrq6uafalpKSwf//+++bsnTt3OHDgQFaFKfKYiIgIJk+eTK9evRgxYoTTvvTm5a5du2jcuDFGo9FxTIsWLbDZbOzcuTPzn4TI1R6Wo5DatppMJqeemveSHBWZKSEhgTZt2tC6dWun7WXLlgXgr7/+krZUqOpRORodHS1tqVCdTqdj8eLF9O3b12m7wWAgJSUlT70vlUJRJjlz5gwAZcqUcdru5+cHpA4BEiKrnTx5EqPRSO/evQkMDKROnTq88847xMfHExMTg8VikZwVWa5cuXJs3bqVwYMHo9PpnPalJy+TkpK4dOlSmmN8fHzw8PCQ3BVP7WE5Cqltq5eXF8OHDycoKIjAwEBef/11rl27BiA5KjKdh4cH48aNo3bt2k7bt27dCkDlypWlLRWqelSOli9fXtpSoTqtVktAQACFCxdGURSuX7/Ol19+ye7du+nYsWOeel8qcxRlkri4OCC1Ufwvd3d3AKdJ24TIKidOnCA+Pp727dvTv39/jh49yqxZs4iKimL48OGA5KzIegUKFHjgvvS0pQ865u5xkrviaT0sRyG1bb1+/ToVKlSgW7dunDlzhpkzZ9K9e3dWr14tOSpUcfjwYb788kuaNm0qbanIlv6bo+XKlZO2VGQrmzdv5rXXXgOgUaNGtGnThuPHjwN5oy2VQlEmURQFAI1Gc9/tdycPFiIrTZs2DU9PTwICAgCoU6cOvr6+jBw5kl27dgFpc/YuyVmhhge1pXdptdqHHqMoiuSuyHTjxo1DURRq1KgBQFBQEOXKlaNz58789NNPPPPMM4DkqMg6ERER9O/fnxIlSjB+/HjHN9jSlors4t4cBWlLRfZSuXJllixZwsmTJ5kxYwZ9+/Zl2LBhQN5oS6VQlEny5csHpO2FkZCQ4LRfiKx07woSkFoh/697c/buY8lZoYYHtaX/zcu739jc7xuaxMREyV2R6apXr55mW+3atcmXLx8nTpzg2WefBSRHRdZYv349b731FqVLl2bu3Ll4e3tz/fp1QNpSkT3cL0dB2lKRvZQsWZKSJUtSp04dPDw8GDVqlKMIlBfa0pxRzsqB7o5JjI6Odtp+7tw5p/1CZJUbN26wcuXKNBOpJycnA+Dr64tOp0uTs3cfS84KNZQqVeqReenu7k7hwoUd7etdN27cID4+XnJXZKrExES+++47Tpw44bRdURQsFgve3t6SoyLLLFiwgOHDh1OzZk2WLl1KoUKFAGlLRfbxoByVtlRkB7dv3+aHH37gypUrTtsrV64MwPnz5/NMWyqFokzi5+dHiRIl2Lhxo9P2zZs3U7p0aYoVK6ZSZCKv0mg0vPPOOyxZssRp+/r169HpdDRo0ICgoCA2b97sqJYDbNq0iXz58lG1atWsDlkITCZTuvIyJCSEX3/9FbPZ7HSMTqe7b086ITKKyWRi0qRJfPrpp07bf/75Z5KTkx35JzkqMtvKlSuZOHEiLVu2ZO7cuU7fWktbKrKDR+WotKVCbXa7nbfeeovly5c7bb87RUe1atXyTFsqQ88y0aBBgxg9ejSenp40atSIX375hQ0bNjBt2jS1QxN5kI+PD126dGHx4sV4eHgQFBREREQEs2fPpkuXLvj5+TFgwABeeeUVXn/9dV544QUOHjzIvHnzeOONN3B1dVX7KYg8Kj152adPH9atW0ffvn3p0aMHZ8+eZerUqXTo0EEK8yJT6XQ6BgwYwMSJExk/fjxhYWGcOnWKWbNm0aRJE+rVqwdIjorMdePGDT766COKFy9Oly5dOHbsmNP+UqVKSVsqVJXeHJW2VKjJx8eHzp078+WXX+Li4kK1atWIiIhgzpw5tG/fnrJly+aZtlSj/LcUJjLcsmXLmD9/PpcuXaJkyZL07duXtm3bqh2WyKMsFgsLFy7ku+++48KFCxQuXJgOHTrQp08fx8RqW7ZsYebMmURFRVG4cGG6dOlCr169VI5c5BXff/89o0ePZtu2bRQpUsSxPT15uX//fj7++GOOHz+Ot7c3bdu2ZciQIRgMhqx+GiIXe1COrly5kkWLFhEdHY2npyfPPfccQ4YMwcXFxXGM5KjILD/88AOjRo164P6PP/6Y559/XtpSoZr05qi0pUJtdz8vrVq1iosXL1KkSBHat2//2J+XcnqeSqFICCGEEEIIIYQQQgAyR5EQQgghhBBCCCGE+IcUioQQQgghhBBCCCEEIIUiIYQQQgghhBBCCPEPKRQJIYQQQgghhBBCCEAKRUIIIYQQQgghhBDiH1IoEkIIIYQQQgghhBCAFIqEEEIIIYQQQgghxD+kUCSEEEIIIYQQQgghACkUCSGEEEIIIYQQQoh//D+pBIQLnM4ImQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#checking the windows\n",
    "x1 = np.arange(0,100)\n",
    "x2 = np.arange(100,300)\n",
    "x3 = 100\n",
    "plt.plot(x1, val_inputs[10,:,0].cpu(), label='input')\n",
    "plt.plot(x2, val_targets[10,:,0].cpu(), label='200 timesteps in the future')\n",
    "plt.scatter(x3, Y_val_lt[10,0], s=30, label='target value')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(val_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "future_preds[101:].shape: torch.Size([3179, 99, 1])\n",
      "0.0517646776528995\n"
     ]
    }
   ],
   "source": [
    "lt_loss_criterion = nn.MSELoss() #long term loss criterion (l1 loss measures MAE for 99 steps in the future)\n",
    "future_window=200\n",
    "runningLoss_val_lt = 0.\n",
    "\n",
    "with torch.no_grad(): # makes sure gradient is not stored \n",
    "    \n",
    "    val_inputs_, val_targets_ = val_inputs.to(DEVICE), val_targets.to(DEVICE)\n",
    "    \n",
    "    future_preds = torch.zeros((val_inputs_.shape[0], 200, len(features))).to(DEVICE) #would store the predicted outputs for 200 timesteps\n",
    "    \n",
    "    # loop to calculate all 200 preds in the future\n",
    "    for t in range(0,future_window):\n",
    "        # optimiser.zero_grad()\n",
    "        # print(f'val.inputs.shape: {val_inputs_.shape} at time {t}')\n",
    "        preds = model_LSTM(val_inputs_)\n",
    "        # print(f'preds.shape: {preds.shape} at time {t}')\n",
    "        # print(f'future_preds[:,t,:].shape: {future_preds[:,t,:].shape}')\n",
    "        future_preds[:,t,:] = preds.reshape(-1,1)\n",
    "        # print(f'future_preds[:,t,:].shape: {future_preds[:,t,:].shape} at time {t}')\n",
    "        new_val_inputs = torch.cat((val_inputs_, preds),1)\n",
    "        # print(f'new_val_inputs.shape: {new_val_inputs.shape} at time {t}')\n",
    "        val_inputs_ = new_val_inputs[:,1:,:]\n",
    "        \n",
    "    \n",
    "    # Evalute long term predictions\n",
    "    print(f'future_preds[101:].shape: {future_preds[:,101:,:].shape}')\n",
    "    # future_preds[:,:,:] = 0.5\n",
    "    loss = lt_loss_criterion(future_preds[:,101:,:], val_targets_[:,101:,:])\n",
    "    # loss = lt_loss_criterion(val_targets_[:,101:,:] ,val_targets_[:,101:,:])\n",
    "\n",
    "    runningLoss_val_lt += loss.item()\n",
    "    print(runningLoss_val_lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x24c4d647a30>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "T = 20\n",
    "L = 1000\n",
    "N = 100\n",
    "\n",
    "\n",
    "np.random.seed(2)\n",
    "x = np.empty((N, L), 'int64')\n",
    "x[:] = np.array(range(L)) + np.random.randint(-4 * T, 4 * T, N).reshape(N, 1)\n",
    "data = np.sin(x / 1.0 / T).astype('float64')\n",
    "\n",
    "# plt.plot(data)\n",
    "curve1 =data[5]\n",
    "curve2 = curve1 + 0.3\n",
    "curve3 = curve1 * 3\n",
    "curve4 = (curve1 * 0.6) + 0.6\n",
    "curve5 = data[14] + 0.4\n",
    "line1 = np.zeros((1000))\n",
    "plt.plot(curve1, lw=3, label='Target sine wave', linestyle='dashed')\n",
    "# plt.plot(curve2, label='curve1 + 0.2', lw=3)\n",
    "plt.plot(line1, label='line', lw=3, )\n",
    "plt.plot(curve3, label='curve1 * 3',lw=3)\n",
    "plt.plot(curve4, label='(curve1 * 0.6) + 0.6', lw=3)\n",
    "plt.plot(curve5, label='shifted + lagging', lw=3)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "err12: 0.09, err13: 1.9948403843471156, err14: 0.4419882456353008, err15: 0.33492002308058944, err1l: 0.4987100960867789, \n"
     ]
    }
   ],
   "source": [
    "\n",
    "err12 = np.mean((curve1-curve2) ** 2)\n",
    "err14 = np.mean((curve1-curve4) ** 2)\n",
    "err13 = np.mean((curve1-curve3) ** 2)\n",
    "err15 = np.mean((curve1-curve5) ** 2)\n",
    "err1l = np.mean((curve1-line1) ** 2)\n",
    "print(f'err12: {err12}, err13: {err13}, err14: {err14}, err15: {err15}, err1l: {err1l}, ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation for curve 1 and 2 is 1.0\n",
      "correlation for curve 1 and 3 is 1.0\n",
      "correlation for curve 1 and 4 is 1.0\n",
      "correlation for curve 1 and 5 is 0.82\n"
     ]
    }
   ],
   "source": [
    "corr, _ = pearsonr(curve1, curve2)\n",
    "print(f'correlation for curve 1 and 2 is {round(corr,2)}')\n",
    "corr, _ = pearsonr(curve1, curve3)\n",
    "print(f'correlation for curve 1 and 3 is {round(corr,2)}')\n",
    "corr, _ = pearsonr(curve1, curve4)\n",
    "print(f'correlation for curve 1 and 4 is {round(corr,2)}')\n",
    "corr, _ = pearsonr(curve1, curve5)\n",
    "print(f'correlation for curve 1 and 5 is {round(corr,2)}')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0518, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = torch.mean((val_targets_[:,101:,:] - future_preds[:,101:,:] ) ** 2 )\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0518, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (val_targets_[:,101:,:] - future_preds[:,101:,:]) ** 2\n",
    "a.reshape(-1,1)\n",
    "a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# future_preds[:,101:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_inputs_np = val_inputs.cpu().detach().numpy()\n",
    "val_targets_np = val_targets.cpu().detach().numpy()\n",
    "future_preds_np = future_preds.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation for feature Hips Flexion-Extension Left is -0.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.2500332])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spearman's correlation coefficient = covariance(rank(X), rank(Y)) / (stdv(rank(X)) * stdv(rank(Y)))\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "all_corr = np.zeros(len(features))\n",
    "for f in range(len(features)):\n",
    "    corr, _ = pearsonr(val_targets_np[:,:,f].reshape(-1,1).squeeze(), future_preds_np[:,:,f].reshape(-1,1).squeeze())\n",
    "    print(f'correlation for feature {features[f]} is {round(corr,2)}')\n",
    "    all_corr[f] = corr\n",
    "\n",
    "all_corr.mean()\n",
    "    \n",
    "\n",
    "all_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.25953376578718484"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_corr  = 0. \n",
    "for i in range(val_targets_np.shape[0]):\n",
    "    # print(i)\n",
    "    corr, _ = pearsonr(val_targets_np[i,:,0].reshape(-1,1).squeeze(), future_preds_np[i,:,0].reshape(-1,1).squeeze())\n",
    "    all_corr += corr\n",
    "    # plt.plot(val_targets_np[1,:,f].reshape(-1,1).squeeze())\n",
    "    # plt.plot(future_preds_np[1,:,f].reshape(-1,1).squeeze())\n",
    "    # val_targets_np[1,:,f].reshape(-1,1).squeeze().shape\n",
    "\n",
    "all_corr/val_targets_np.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.23787800728355274"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n, _ = pearsonr(val_targets_np[:5,:,0].reshape(-1,1).squeeze(),future_preds_np[:5,:,0].reshape(-1,1).squeeze())\n",
    "plt.plot(future_preds_np[1,:,0].reshape(-1,1).squeeze())\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3179"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_targets_np.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model): \n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1701"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(200,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_inputs_np = val_inputs.cpu().detach().numpy()\n",
    "val_targets_np = val_targets.cpu().detach().numpy()\n",
    "future_preds_np = future_preds.cpu().detach().numpy()\n",
    "\n",
    "s=900\n",
    "for f in range(len(features)):\n",
    "    path = r'D:\\Study 2 Results and Models\\Model Performance' + '\\\\'  + 'Exp' + str(exp_ID) + '-' + str(date.today()) + '-' + model + '-In' + str(input_window) + '-Out' + str(output_window) + '-Plot(1)-' + str(features[f]) + '.png'\n",
    "   \n",
    "    input = val_inputs_np[s,:,f].squeeze()\n",
    "    print(input.shape)\n",
    "\n",
    "    predicted_trajectory = future_preds_np[s,:,f].squeeze()\n",
    "    print(predicted_trajectory.shape)\n",
    "\n",
    "    past_timepoints = np.arange(0,len(input))\n",
    "    # print(f'past_timepoints: {past_timepoints}')\n",
    "\n",
    "    future_timepoints = np.arange(len(input), len(input)+len(predicted_trajectory))\n",
    "    # print(f'future_timepoints: {future_timepoints}')\n",
    "\n",
    "    actual_seq = val_targets_np[s,:,f].squeeze()\n",
    "    actual_timepoints = np.arange(len(input), len(input)+len(actual_seq))\n",
    "\n",
    "    rcParams['figure.figsize'] = 20,6\n",
    "\n",
    "    # ax = sns.lineplot(x=past_timepoints[-20:], y=input[-20:]) #only plots last 20 values of input\n",
    "    ax = sns.lineplot(x=past_timepoints[:], y=input[:], label='input_timesteps')\n",
    "\n",
    "    # ax.fill_between(x=past_timepoints, y1=input_stats, y2=input_stats, alpha=0.1)\n",
    "    ax = sns.lineplot(x=future_timepoints, y=predicted_trajectory, label='hModel_preds')\n",
    "\n",
    "    ax = sns.lineplot(x=actual_timepoints, y = actual_seq, label='actual')\n",
    "    # ax = sns.lineplot(x=future_timepoints, y=H_model_preds, label='H_model_preds')\n",
    "    # ax = sns.lineplot(x=future_timepoints, y=actual, label='actual')\n",
    "    # ax = sns.lineplot(x=future_timepoints, y=input_bf_processing, label='input_bf_processing')\n",
    "\n",
    "    # ax.set(font_scale = 1)\n",
    "    # sns.set(rc={'axes.facecolor':'whitegrid', 'figure.facecolor':'whitegrid'})\n",
    "\n",
    "    fontsize = 15\n",
    "    ax.set_xlabel('Time-step', fontsize =15)\n",
    "    ax.set_ylabel('Angle degrees (normalised)', fontsize =15)\n",
    "\n",
    "    plt.legend(fontsize = 15)\n",
    "    plt.title(features[f] + ' S ' + str(s))\n",
    " \n",
    "    plt.savefig(path)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting and Plotting loss for 1 sample in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hModel_X_test_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8904/1422355462.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#do not calculate gradients for forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0min_to_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhModel_X_test_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mall_preds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hModel_X_test_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "s = 100#sample to plot\n",
    "nsteps_future = 500 #number of steps to predict in the future \n",
    "\n",
    "with torch.no_grad(): #do not calculate gradients for forward pass\n",
    "    in_to_model = hModel_X_test_tensor[s].expand((1,-1,-1)).to(DEVICE) \n",
    "    all_preds=[] \n",
    "\n",
    "    for _ in range(0,nsteps_future): #loop over the steps to predict in the future\n",
    "        # # print(f'in_to_model: {in_to_model[:,-10:,1]}')\n",
    "        # future_pred = hModel(in_to_model)\n",
    "\n",
    "        # all_preds.append(future_pred.tolist())\n",
    "\n",
    "        # new_in_to_model = torch.cat((in_to_model, future_pred),1)\n",
    "        # in_to_model = new_in_to_model[:,1:,:]\n",
    "\n",
    "        #Uncomment only for CNN\n",
    "        future_pred = hModel(in_to_model.permute(0,2,1))\n",
    "\n",
    "        all_preds.append(future_pred.permute(0,2,1).tolist())\n",
    "\n",
    "        new_in_to_model = torch.cat((in_to_model, future_pred.permute(0,2,1)),1)\n",
    "        in_to_model = new_in_to_model[:,1:,:]\n",
    "\n",
    "all_future_preds = np.array(all_preds).reshape((nsteps_future,len(features)))\n",
    "\n",
    "all_future_preds.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = hModel_X_test_tensor[s].expand((1,-1,-1))\n",
    "\n",
    "\n",
    "for f in range(len(features)):\n",
    "    path = r'D:\\Study 2 Results and Models\\Model Performance' + '\\\\'  + 'Exp' + str(exp_ID) + '-' + str(date.today()) + '-' + model + '-In' + str(input_window) + '-Out' + str(output_window) + '-Plot(1)-' + str(features[f]) + '.png'\n",
    "   \n",
    "    input = input_seq[:,:,f].squeeze()\n",
    "    print(input.shape)\n",
    "\n",
    "    predicted_trajectory = all_future_preds[:,f]\n",
    "    print(predicted_trajectory.shape)\n",
    "\n",
    "    past_timepoints = np.arange(0,len(input))\n",
    "    # print(f'past_timepoints: {past_timepoints}')\n",
    "\n",
    "    future_timepoints = np.arange(len(input), len(input)+len(predicted_trajectory))\n",
    "    # print(f'future_timepoints: {future_timepoints}')\n",
    "\n",
    "    actual_seq = hModel_Y_test_tensor[s:s+nsteps_future,:,f].reshape(-1,1).squeeze()\n",
    "    actual_timepoints = np.arange(len(input), len(input)+len(actual_seq))\n",
    "\n",
    "    rcParams['figure.figsize'] = 20,6\n",
    "\n",
    "    # ax = sns.lineplot(x=past_timepoints[-20:], y=input[-20:]) #only plots last 20 values of input\n",
    "    ax = sns.lineplot(x=past_timepoints[:], y=input[:], label='input_timesteps')\n",
    "\n",
    "    # ax.fill_between(x=past_timepoints, y1=input_stats, y2=input_stats, alpha=0.1)\n",
    "    ax = sns.lineplot(x=future_timepoints, y=predicted_trajectory, label='hModel_preds')\n",
    "\n",
    "    ax = sns.lineplot(x=actual_timepoints, y = actual_seq, label='actual')\n",
    "    # ax = sns.lineplot(x=future_timepoints, y=H_model_preds, label='H_model_preds')\n",
    "    # ax = sns.lineplot(x=future_timepoints, y=actual, label='actual')\n",
    "    # ax = sns.lineplot(x=future_timepoints, y=input_bf_processing, label='input_bf_processing')\n",
    "\n",
    "    # ax.set(font_scale = 1)\n",
    "    # sns.set(rc={'axes.facecolor':'whitegrid', 'figure.facecolor':'whitegrid'})\n",
    "\n",
    "    fontsize = 15\n",
    "    ax.set_xlabel('Time-step', fontsize =15)\n",
    "    ax.set_ylabel('Angle (degrees)', fontsize =15)\n",
    "\n",
    "    plt.legend(fontsize = 15)\n",
    "    plt.title(features[f] + ' S ' + str(s))\n",
    " \n",
    "    # plt.savefig(path)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original sequence size [n_samples, timesteps, featuers]\n",
    "\n",
    "#take only one timesteps from each sample + choose one feature only [:, 1, f]\n",
    "\n",
    "#flatten sequence and plot \n",
    "\n",
    "in_seq_to_plt = X_sample.cpu().numpy()\n",
    "\n",
    "gen_seq_to_plt = generated_sequence.cpu().numpy()\n",
    "\n",
    "in_seq_to_plt_denorm = denormalise(in_seq_to_plt, scalars)\n",
    "gen_seq_to_plt_denorm = denormalise(gen_seq_to_plt, scalars)\n",
    "\n",
    "in_seq_to_plt_denorm = in_seq_to_plt_denorm[:, :, 1]\n",
    "gen_seq_to_plt_denorm = gen_seq_to_plt_denorm[:, 0, 1]\n",
    "\n",
    "print('in_seq_to_plt_denorm')\n",
    "print(in_seq_to_plt_denorm)\n",
    "print('gen_seq_to_plt_denorm')\n",
    "print(gen_seq_to_plt_denorm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_timepoints=np.array()\n",
    "\n",
    "ax = sns.lineplot(x=past_timepoints[-20:], y=input[-20:])\n",
    "# ax.fill_between(x=past_timepoints, y1=input_stats, y2=input_stats, alpha=0.1)\n",
    "ax = sns.lineplot(x=future_timepoints, y=preds)\n",
    "ax = sns.lineplot(x=future_timepoints, y=actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark against naive method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## benchmark against naive output (making prediction only last value)\n",
    "naive_1_train_output = np.zeros(Y_train_data.shape)\n",
    "\n",
    "    \n",
    "for f in range(len(features)):\n",
    "    for s in range(Y_train_data.shape[0]):\n",
    "        naive_1_train_output[s,:,f] = np.ones((output_window)) * X_train_data[s,-1,f]\n",
    "\n",
    "\n",
    "\n",
    "# print(naive_1_test_output.shape)\n",
    "\n",
    "\n",
    "# naive_1_test_output[1,:,1]\n",
    "\n",
    "# X_test_data[1,:,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_1_mse_loss, naive_1_mse_std = mse_loss(naive_1_train_output, Y_train_data, reduction='mean', format='np')\n",
    "naive_1_mae_loss, naive_1_mae_std = mae_loss(naive_1_train_output, Y_train_data, reduction='mean', format='np')\n",
    "\n",
    "print(f'naive_1 MSE Loss: {naive_1_mse_loss}')\n",
    "print(f'naive_1 MSE std: {naive_1_mse_std} ')\n",
    "print(f'naive_1 MAE Loss: {naive_1_mae_loss}')\n",
    "print(f'naive_1 MAE std: {naive_1_mae_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## benchmark against mean output\n",
    "\n",
    "naive_average_train_output = np.zeros(Y_train_data.shape)\n",
    "\n",
    "for f in range(len(features)):\n",
    "    for s in range(Y_train_data.shape[0]):\n",
    "        naive_average_train_output[s,:,f] = np.ones((output_window)) * np.mean(X_train_data[s,:,f])\n",
    "\n",
    "\n",
    "\n",
    "print(naive_average_train_output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_average_mse_loss, naive_average_mse_std = mse_loss(naive_average_train_output, Y_train_data, reduction='mean', format='np')\n",
    "naive_average_mae_loss, naive_average_mae_std = mae_loss(naive_average_train_output, Y_train_data, reduction='mean', format='np')\n",
    "\n",
    "print(f'average MSE Loss: {naive_average_mse_loss}')\n",
    "print(f'average MSE std: {naive_average_mse_std} ')\n",
    "print(f'average MAE Loss: {naive_average_mae_loss}')\n",
    "print(f'average MAE std: {naive_average_mae_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Progress and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = 'LSTM'\n",
    "# exp_ID = '022'\n",
    "\n",
    "out_fname = r'D:\\Study 2 Results and Models\\Investigation Results' + '\\\\' + 'Study 2 Experimentation Results' + '.txt'\n",
    "comments = 'LSTM'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(out_fname):\n",
    "#     print('ERROR: File with this name already exists, try alternative name.')\n",
    "    \n",
    "# else:\n",
    "with open(out_fname, 'a+') as text_file:\n",
    "    \n",
    "    print('Experiment Report', file=text_file)\n",
    "    print(f'{exp_ID}: {date.today()}',file=text_file)\n",
    "    # print('\\n', file=text_file)\n",
    "    print(f'model: {model}', file=text_file)\n",
    "    print(f'features: {features} \\n', file=text_file)\n",
    "    print(f'Comments: {comments} \\n', file=text_file)\n",
    "\n",
    "    print('Information on Data', file=text_file)\n",
    "    print(f'train size: {train_sample_sum}', file=text_file)\n",
    "    print(f'validation size: {val_sample_sum}', file=text_file)\n",
    "    print(f'Train subjects: {train_subjects}', file=text_file)\n",
    "    print(f'Val subjects: {val_subjects}', file=text_file)\n",
    "    print(f'Scalars: {scalars} \\n', file=text_file)\n",
    "\n",
    "    print('Model Hyperparameters', file=text_file)\n",
    "    print(f'input window size: {input_window}', file=text_file)\n",
    "    print(f'output window size: {output_window}', file=text_file)\n",
    "    print(f'stride: {stride}', file=text_file)\n",
    "    # print(f'epochs: {num_epochs}', file=text_file)\n",
    "    print(f'best epoch: {best_epoch}', file=text_file)\n",
    "    print(f'hidden size: {hidden_size}', file=text_file)\n",
    "    print(f'number of layers: {num_layers}', file=text_file)\n",
    "    print(f'learning rate: {learning_rate}', file=text_file)\n",
    "    print(f'optimiser: MSE', file=text_file)\n",
    "\n",
    "    print('\\n', file = text_file)\n",
    "    print(f'Model Architecture', file=text_file)\n",
    "    print(checkpoint_model, file=text_file)\n",
    "\n",
    "    print(f'Losses during training:', file=text_file)\n",
    "    print(f'training loss: {train_history}', file=text_file)\n",
    "    print(f'validation history: {val_history}', file=text_file)\n",
    "    print(f'lt loss: {lt_history}', file=text_file)\n",
    "    print(f'r value: {r_history}', file=text_file)\n",
    "\n",
    "    print('\\n', file=text_file)\n",
    "    print(f'Denomralised MSE and MAE: ', file=text_file)\n",
    "    print(f'Validation MSE and std(degrees): {val_mse_loss:.3f}, {val_mse_std:.3f}', file=text_file)\n",
    "    print(f'Validation MAE and std (degrees): {val_mae_loss:.3f}, {val_mae_std:.3f}', file=text_file)\n",
    "    # print('\\n', file=text_file)\n",
    "    print('--------------------------------------------------------------------------------------------------------', file=text_file)\n",
    "\n",
    "\n",
    "print(\"Generation of text file complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_path = r'D:\\Study 2 Results and Models\\Investigations Plots' + '\\\\'  + 'Exp' + str(exp_ID) + '-' + str(date.today()) + '-' + model + '-In' + str(input_window) + '-Out' + str(output_window) + '(loss)' + '.png'\n",
    "fst_e = 1 #first epoch to start plotting for \n",
    "x = [i for i in range(fst_e,num_epochs)]\n",
    "# print(x)\n",
    "# print(train_history[20:145].shape)\n",
    "\n",
    "plt.plot(np.log(train_history[fst_e:]), label = 'Training loss')\n",
    "plt.plot(np.log(val_history[fst_e:]), label = 'Val loss')\n",
    "plt.plot(np.log(lt_history[fst_e:]), label='Long term preds loss')\n",
    "plt.plot(np.log(np.abs(r_history[fst_e:])), label='Val R values (absolute value)')\n",
    "plt.axhline(np.min(np.log(lt_history)), label='mininum long term loss')\n",
    "print(np.min(lt_history))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training and Validation MSE Loss')\n",
    "plt.legend()\n",
    "\n",
    "# plt.plot(x,train_history[fst_e:], label = 'Training loss')\n",
    "# plt.plot(x,val_history[fst_e:], label = 'Val loss')\n",
    "# plt.plot(train_history, label = 'Training loss')\n",
    "# plt.plot(val_history, label = 'Val loss')\n",
    "\n",
    "\n",
    "plt.savefig(plt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt_path = r'D:\\Study 2 Results and Models\\Investigations Plots' + '\\\\'  + 'Exp' + str(exp_ID) + '-' + str(date.today()) + '-' + model + '-In' + str(input_window) + '-Out' + str(output_window) + '(loss)' + '.png'\n",
    "# fst_e = 10 #first epoch to start plotting for \n",
    "# x = [i for i in range(fst_e,num_epochs)]\n",
    "# # print(x)\n",
    "# # print(train_history[20:145].shape)\n",
    "# plt.plot(x,train_history[fst_e:], label = 'Training loss')\n",
    "# plt.plot(x,val_history[fst_e:], label = 'Val loss')\n",
    "# # plt.plot(train_history, label = 'Training loss')\n",
    "# # plt.plot(val_history, label = 'Val loss')\n",
    "\n",
    "\n",
    "# plt.xlabel('Iterations')\n",
    "# plt.ylabel('MSE Loss')\n",
    "# plt.title('Train and Val MSE Loss')\n",
    "# plt.legend()\n",
    "# plt.savefig(plt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_path = r'D:\\Study 1 Results and Models\\Study 1 Models\\LSTM_4' + '\\\\' + 'Exp014-2022-02-10-LSTM-In120-Out3.pickle'\n",
    "# # # import_path = r'C:\\Users\\Rania\\rbk9\\Study 1 Results and Models\\Study 1 Models\\CNN_1' + '\\\\' + 'Exp001-2022-01-24-CNN-In96-Out1.pickle'\n",
    "# # # model_CNN = CNN(input_size, output_size=output_size, kernel_size=kernel_size, stride=1, in_seq_len=in_seq_len, out_seq_len=out_seq_len,  device = DEVICE).to(DEVICE)\n",
    "\n",
    "input_size=len(features)\n",
    "hidden_size=128\n",
    "num_layers=4\n",
    "in_seq_len=input_window\n",
    "out_seq_len=output_window\n",
    "output_size=len(features)\n",
    "# num_epochs=60\n",
    "\n",
    "# # learning_rate= 0.001\n",
    "\n",
    "model_LSTM = LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, in_seq_len=in_seq_len, out_seq_len=out_seq_len, output_size=output_size, device=DEVICE).to(DEVICE)\n",
    "model_LSTM.load_state_dict(torch.load(import_path))\n",
    "model_LSTM.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_output, actual_val_output, val_loss = test_LSTM(model_LSTM, val_dataloader, DEVICE)\n",
    "\n",
    "predicted_values_val = pred_val_output.cpu().numpy()\n",
    "actual_values_val = actual_val_output.cpu().numpy()\n",
    "\n",
    "print(f'val loss: {val_loss}')\n",
    "print(f'Shape of predicted values test: {predicted_values_val.shape}')\n",
    "print(f'shape of actual values test: {actual_values_val.shape}')\n",
    "\n",
    "predicted_val_denorm = denormalise(predicted_values_val, scalars)\n",
    "actual_val_denorm = denormalise(actual_values_val, scalars)\n",
    "\n",
    "print(f'Shape of predicted values test post denormalisation: {predicted_val_denorm.shape}')\n",
    "print(f'shape of actual values test post denormalisation: {actual_val_denorm.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mse_loss, val_mse_std = mse_loss(predicted_val_denorm, actual_val_denorm, reduction='mean', format='np')\n",
    "val_mae_loss, val_mae_std = mae_loss(predicted_val_denorm, actual_val_denorm, reduction='mean', format='np')\n",
    "\n",
    "print(f'Val MSE Loss: {val_mse_loss}')\n",
    "print(f'Val MSE std: {val_mse_std}')\n",
    "print(f'Val MAE Loss: {val_mae_loss}')\n",
    "print(f'Val MAE std: {val_mae_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in model_LSTM.parameters():\n",
    "#     print(p.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing function \n",
    "# def test_LSTM(model, dataloader):\n",
    "#     loss_function = nn.MSELoss(reduction='mean')\n",
    "#     model.eval()\n",
    "#     actual_output, pred_output = [], []\n",
    "#     running_loss = 0. \n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for idx, (batch_inputs, batch_targets) in tqdm(enumerate(dataloader)):\n",
    "\n",
    "#             batch_inputs, batch_targets = batch_inputs.to(DEVICE), batch_targets.to(DEVICE)\n",
    "\n",
    "#             # if idx==0:\n",
    "#             #     batch_preds = model(batch_inputs)\n",
    "#             #     # print(f'batch shape: {batch_preds.shape}')\n",
    "#             #     loss = loss_function(batch_preds, batch_targets)\n",
    "#             #     running_loss += loss.item()\n",
    "#             #     current_preds = batch_preds\n",
    "#             #     all_preds = batch_preds\n",
    "\n",
    "#             # else:\n",
    "#             #     batch_preds = model(batch_inputs)\n",
    "#             #     print(f'batch shape: {batch_preds.shape}')\n",
    "#             #     loss = loss_function(batch_preds, batch_targets)\n",
    "#             #     running_loss += loss.item()\n",
    "#             #     all_preds = torch.cat((current_preds, batch_preds), dim=0)\n",
    "#             #     current_preds = batch_preds\n",
    "\n",
    "#             batch_preds = model(batch_inputs)\n",
    "#             # print(f'batch preds: {batch_preds.type}')\n",
    "#             loss = loss_function(batch_preds, batch_targets)\n",
    "#             running_loss += loss.item()\n",
    "#             actual_output.append(batch_targets)\n",
    "#             pred_output.append(batch_preds)\n",
    "\n",
    "\n",
    "#             #             lst = []\n",
    "#             # print(f'{x.size()}')\n",
    "#             # for i in range(10):\n",
    "#             #     x += i  # say we do something with x at iteration i\n",
    "#             #     lst.append(x)\n",
    "#             # # lstt = torch.stack([x for _ in range(10)])\n",
    "#             # lstt = torch.stack(lst)\n",
    "#             # print(lstt.size())\n",
    "\n",
    "#         total_loss = running_loss / len(dataloader)\n",
    "\n",
    "#         actual_output_tensor = torch.vstack(actual_output)\n",
    "#         pred_output_tensor = torch.vstack(pred_output)\n",
    "    \n",
    "#     return pred_output_tensor, actual_output_tensor, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creates dataset object that gets individual samples for training/testing so that the Dataloader can generate batches\n",
    "# class gaitDataset(Dataset):\n",
    "#     def __init__(self, x, y):\n",
    "#         self.x = x \n",
    "#         self.y = y \n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.x.shape[0]\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         X_sample = self.x[index, :, :]\n",
    "#         Y_sample = self.y[index, :, :]\n",
    "#         return X_sample, Y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X_test_norm = normalise_transform(X_test_data, scalars)\n",
    "# # Y_test_norm = normalise_transform(Y_test_data, scalars)\n",
    "\n",
    "# # Convert to Tensor \n",
    "# # do not store on GPU (yet)\n",
    "# X_test = torch.from_numpy(X_train_norm).float()\n",
    "# Y_test = torch.from_numpy(Y_train_norm).float()\n",
    "\n",
    "# print(f'X_test shape: {X_test.shape}')\n",
    "# print(f'Y_test shape: {Y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = gaitDataset(X_test, Y_test)\n",
    "# # # test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False) #ADJUSTED\n",
    "\n",
    "# print(f\"Test Dataset length: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_test_output, actual_test_output, test_loss = test_LSTM(model_LSTM, test_dataloader)\n",
    "\n",
    "# predicted_values_test = pred_test_output.cpu().numpy() # premute again to make the order of the array (samples, windows, features)\n",
    "# actual_values_test =  actual_test_output.cpu().numpy()\n",
    "\n",
    "# print(f'test loss: {test_loss}')\n",
    "\n",
    "# print(f'Shape of predicted values test: {predicted_values_test.shape}')\n",
    "# print(f'shape of actual values test: {actual_values_test.shape}')\n",
    "\n",
    "# predicted_test_denorm = denormalise(predicted_values_test, scalars)\n",
    "# actual_test_denorm =  denormalise(actual_values_test, scalars)\n",
    "\n",
    "# print(f'Shape of predicted values test post denormalisation: {predicted_test_denorm.shape}')\n",
    "# print(f'shape of actual values test post denormalisation: {actual_test_denorm.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse_loss, test_mse_std = mse_loss(predicted_test_denorm, actual_test_denorm, reduction='mean', format='np')\n",
    "test_mae_loss, test_mae_std = mae_loss(predicted_test_denorm, actual_test_denorm, reduction='mean', format='np')\n",
    "\n",
    "print(f'Test MSE Loss: {test_mse_loss}')\n",
    "print(f'Test MSE std: {test_mse_std} ')\n",
    "\n",
    "# MDE= mae_loss(actual_test_denorm[:-1,:,:],actual_test_denorm[1:,:,:],reduction='mean', format='np')\n",
    "MDE= mae_loss(actual_test_denorm[:,:,:],actual_test_denorm[:,:,:],reduction='mean', format='np')\n",
    "\n",
    "print(f'Test MAE Loss: {test_mae_loss}')\n",
    "print(f'Test MAE std: {test_mae_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.abs(predicted_test_denorm[:,:,1].reshape(-1,1) - actual_test_denorm[:,:,1].reshape(-1,1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = np.array([23,36,28,33,44,22,12,10,1])\n",
    "predicted = np.array([23, 35.5, 28.1, 33, 45, 15, 13, 8, 0])\n",
    "\n",
    "actual-predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_values = np.abs(actual-predicted)\n",
    "\n",
    "plt.hist(abs_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.abs(actual-predicted).std()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27ff62b9f8f46ac7739d102fa0cb2cdb691dbf234c24f48db1edec61200604ab"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('study2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
